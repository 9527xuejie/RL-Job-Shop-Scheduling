2020-10-14 05:16:40,793	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_71f5b_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=47748)[0m 2020-10-14 05:16:43,468	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=47653)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47653)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47708)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47708)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47753)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47753)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47693)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47693)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47676)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47676)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47707)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47707)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47727)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47727)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47683)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47683)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47636)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47636)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47637)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47637)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47725)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47725)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47702)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47702)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47726)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47726)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47630)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47630)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47721)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47721)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47619)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47619)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47632)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47632)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47701)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47701)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47650)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47650)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47750)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47750)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47639)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47639)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47697)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47697)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47729)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47729)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47672)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47672)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47745)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47745)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47696)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47696)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47624)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47624)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47621)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47621)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47655)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47655)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47631)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47631)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47633)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47633)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47627)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47627)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47704)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47704)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47695)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47695)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47691)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47691)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47673)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47673)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47644)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47644)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47705)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47705)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47652)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47652)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47699)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47699)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47717)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47717)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47685)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47685)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47730)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47730)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47690)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47690)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47681)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47681)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47722)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47722)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47746)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47746)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47623)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47623)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47719)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47719)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47638)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47638)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47620)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47620)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47689)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47689)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47742)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47742)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47700)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47700)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47649)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47649)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47674)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47674)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47754)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47754)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47733)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47733)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47713)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47713)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47710)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47710)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47634)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47634)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47628)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47628)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47635)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47635)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47680)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47680)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47723)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47723)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47698)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47698)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47734)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47734)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47694)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47694)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47645)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47645)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47625)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47625)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47736)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47736)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47622)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47622)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47618)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47618)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47744)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47744)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47688)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47688)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47646)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47646)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47617)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47617)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47657)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47657)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=47715)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47715)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3691
    time_step_mean: 3418.869230769231
    time_step_min: 3176
  date: 2020-10-14_05-17-17
  done: false
  episode_len_mean: 886.753164556962
  episode_reward_max: 282.343434343434
  episode_reward_mean: 241.94642628819824
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1849032640457153
        entropy_coeff: 0.0005000000000000001
        kl: 0.004784370268074174
        model: {}
        policy_loss: -0.008921080657576871
        total_loss: 488.36802927652997
        vf_explained_var: 0.49538537859916687
        vf_loss: 488.3765920003255
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.79090909090909
    gpu_util_percent0: 0.2515151515151515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.563636363636364
    vram_util_percent0: 0.08736346740610434
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16669898461544724
    mean_env_wait_ms: 1.1432896502483918
    mean_inference_ms: 5.611584917488346
    mean_raw_obs_processing_ms: 0.4427917712483372
  time_since_restore: 28.662127017974854
  time_this_iter_s: 28.662127017974854
  time_total_s: 28.662127017974854
  timers:
    learn_throughput: 8251.875
    learn_time_ms: 19606.697
    sample_throughput: 17998.906
    sample_time_ms: 8988.991
    update_time_ms: 29.929
  timestamp: 1602652637
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      1 |          28.6621 | 161792 |  241.946 |              282.343 |              165.677 |            886.753 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3759
    time_step_mean: 3440.1076388888887
    time_step_min: 3176
  date: 2020-10-14_05-17-44
  done: false
  episode_len_mean: 883.6139240506329
  episode_reward_max: 282.343434343434
  episode_reward_mean: 240.93616545198802
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1520658334096272
        entropy_coeff: 0.0005000000000000001
        kl: 0.00744400251035889
        model: {}
        policy_loss: -0.009384491364471614
        total_loss: 127.12748146057129
        vf_explained_var: 0.7977762818336487
        vf_loss: 127.13669649759929
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.070967741935487
    gpu_util_percent0: 0.2945161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.748387096774193
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16268584286003562
    mean_env_wait_ms: 1.1424816936363293
    mean_inference_ms: 5.395590800747396
    mean_raw_obs_processing_ms: 0.4318313486659933
  time_since_restore: 55.60049891471863
  time_this_iter_s: 26.938371896743774
  time_total_s: 55.60049891471863
  timers:
    learn_throughput: 8312.531
    learn_time_ms: 19463.627
    sample_throughput: 19579.992
    sample_time_ms: 8263.129
    update_time_ms: 31.665
  timestamp: 1602652664
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      2 |          55.6005 | 323584 |  240.936 |              282.343 |              165.677 |            883.614 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3441.580717488789
    time_step_min: 3176
  date: 2020-10-14_05-18-11
  done: false
  episode_len_mean: 879.1814345991561
  episode_reward_max: 282.343434343434
  episode_reward_mean: 241.48644674594024
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1369050244490306
        entropy_coeff: 0.0005000000000000001
        kl: 0.011053523048758507
        model: {}
        policy_loss: -0.012346668188304951
        total_loss: 52.218987782796226
        vf_explained_var: 0.8901453614234924
        vf_loss: 52.23079872131348
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529032258064515
    gpu_util_percent0: 0.3329032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15997278138798643
    mean_env_wait_ms: 1.1433066295268364
    mean_inference_ms: 5.226869860796811
    mean_raw_obs_processing_ms: 0.42390818882170755
  time_since_restore: 82.07610416412354
  time_this_iter_s: 26.475605249404907
  time_total_s: 82.07610416412354
  timers:
    learn_throughput: 8330.693
    learn_time_ms: 19421.194
    sample_throughput: 20578.729
    sample_time_ms: 7862.099
    update_time_ms: 28.989
  timestamp: 1602652691
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      3 |          82.0761 | 485376 |  241.486 |              282.343 |              165.677 |            879.181 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3438.958609271523
    time_step_min: 3176
  date: 2020-10-14_05-18-37
  done: false
  episode_len_mean: 875.8180379746835
  episode_reward_max: 286.13131313131294
  episode_reward_mean: 242.47984592763052
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1127794881661732
        entropy_coeff: 0.0005000000000000001
        kl: 0.009236348637690147
        model: {}
        policy_loss: -0.01357063123335441
        total_loss: 39.06841214497884
        vf_explained_var: 0.9182509779930115
        vf_loss: 39.08161576588949
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05666666666667
    gpu_util_percent0: 0.32333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15809340242462314
    mean_env_wait_ms: 1.1446610965338264
    mean_inference_ms: 5.1034845662995005
    mean_raw_obs_processing_ms: 0.41790652629724634
  time_since_restore: 108.36743259429932
  time_this_iter_s: 26.29132843017578
  time_total_s: 108.36743259429932
  timers:
    learn_throughput: 8331.24
    learn_time_ms: 19419.917
    sample_throughput: 21313.23
    sample_time_ms: 7591.153
    update_time_ms: 31.236
  timestamp: 1602652717
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      4 |          108.367 | 647168 |   242.48 |              286.131 |              165.677 |            875.818 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3433.030183727034
    time_step_min: 3154
  date: 2020-10-14_05-19-03
  done: false
  episode_len_mean: 871.8582278481013
  episode_reward_max: 286.13131313131294
  episode_reward_mean: 243.81274773046908
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.075794368982315
        entropy_coeff: 0.0005000000000000001
        kl: 0.007777562830597162
        model: {}
        policy_loss: -0.011302593076834455
        total_loss: 29.236744085947674
        vf_explained_var: 0.947382390499115
        vf_loss: 29.247807184855144
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796666666666663
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15669124011397745
    mean_env_wait_ms: 1.1466539254814716
    mean_inference_ms: 5.010070354192989
    mean_raw_obs_processing_ms: 0.4132107995659689
  time_since_restore: 134.6725811958313
  time_this_iter_s: 26.305148601531982
  time_total_s: 134.6725811958313
  timers:
    learn_throughput: 8328.316
    learn_time_ms: 19426.737
    sample_throughput: 21785.08
    sample_time_ms: 7426.734
    update_time_ms: 31.917
  timestamp: 1602652743
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      5 |          134.673 | 808960 |  243.813 |              286.131 |              165.677 |            871.858 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3423.0584415584417
    time_step_min: 3154
  date: 2020-10-14_05-19-29
  done: false
  episode_len_mean: 861.6636528028934
  episode_reward_max: 287.7979797979799
  episode_reward_mean: 245.7200577200576
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 316
  episodes_total: 1106
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0736831625302632
        entropy_coeff: 0.0005000000000000001
        kl: 0.00783942472965767
        model: {}
        policy_loss: -0.010518070853625735
        total_loss: 27.994863510131836
        vf_explained_var: 0.9606881737709045
        vf_loss: 28.00513442357381
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666664
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15478514005490254
    mean_env_wait_ms: 1.1510963869509738
    mean_inference_ms: 4.884242593977561
    mean_raw_obs_processing_ms: 0.40715452332751345
  time_since_restore: 160.65088963508606
  time_this_iter_s: 25.97830843925476
  time_total_s: 160.65088963508606
  timers:
    learn_throughput: 8346.538
    learn_time_ms: 19384.325
    sample_throughput: 22129.131
    sample_time_ms: 7311.268
    update_time_ms: 29.973
  timestamp: 1602652769
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      6 |          160.651 | 970752 |   245.72 |              287.798 |              165.677 |            861.664 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3417.413430420712
    time_step_min: 3154
  date: 2020-10-14_05-19-55
  done: false
  episode_len_mean: 856.7879746835443
  episode_reward_max: 290.6767676767679
  episode_reward_mean: 246.58477975962143
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.053666094938914
        entropy_coeff: 0.0005000000000000001
        kl: 0.00802672584541142
        model: {}
        policy_loss: -0.011159917781090675
        total_loss: 19.830265998840332
        vf_explained_var: 0.9614474773406982
        vf_loss: 19.841150124867756
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03333333333334
    gpu_util_percent0: 0.3376666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1541021300553667
    mean_env_wait_ms: 1.1528603892767855
    mean_inference_ms: 4.839088650351036
    mean_raw_obs_processing_ms: 0.40493848422604867
  time_since_restore: 186.51650524139404
  time_this_iter_s: 25.865615606307983
  time_total_s: 186.51650524139404
  timers:
    learn_throughput: 8363.605
    learn_time_ms: 19344.767
    sample_throughput: 22402.02
    sample_time_ms: 7222.206
    update_time_ms: 28.453
  timestamp: 1602652795
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      7 |          186.517 | 1132544 |  246.585 |              290.677 |              165.677 |            856.788 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3414.711621233859
    time_step_min: 3154
  date: 2020-10-14_05-20-22
  done: false
  episode_len_mean: 853.1174402250351
  episode_reward_max: 290.6767676767679
  episode_reward_mean: 246.98339939479166
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0378634532292683
        entropy_coeff: 0.0005000000000000001
        kl: 0.006901592792322238
        model: {}
        policy_loss: -0.010785317203650871
        total_loss: 20.057416280110676
        vf_explained_var: 0.9610694050788879
        vf_loss: 20.06803019841512
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000002
    gpu_util_percent0: 0.32366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15351850877051576
    mean_env_wait_ms: 1.1545252487300288
    mean_inference_ms: 4.800308019612848
    mean_raw_obs_processing_ms: 0.40298130180269126
  time_since_restore: 212.67091727256775
  time_this_iter_s: 26.154412031173706
  time_total_s: 212.67091727256775
  timers:
    learn_throughput: 8365.458
    learn_time_ms: 19340.482
    sample_throughput: 22578.616
    sample_time_ms: 7165.718
    update_time_ms: 27.451
  timestamp: 1602652822
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      8 |          212.671 | 1294336 |  246.983 |              290.677 |              165.677 |            853.117 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3409.408622908623
    time_step_min: 3154
  date: 2020-10-14_05-20-48
  done: false
  episode_len_mean: 849.3470290771176
  episode_reward_max: 292.79797979797934
  episode_reward_mean: 247.87210920839226
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 160
  episodes_total: 1582
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9882466793060303
        entropy_coeff: 0.0005000000000000001
        kl: 0.0072092009941115975
        model: {}
        policy_loss: -0.0129079805725875
        total_loss: 17.17254622777303
        vf_explained_var: 0.9699942469596863
        vf_loss: 17.185227394104004
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.243333333333332
    gpu_util_percent0: 0.31233333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15300857096746823
    mean_env_wait_ms: 1.1563881420289905
    mean_inference_ms: 4.766105686606314
    mean_raw_obs_processing_ms: 0.40121981536207896
  time_since_restore: 239.0869402885437
  time_this_iter_s: 26.416023015975952
  time_total_s: 239.0869402885437
  timers:
    learn_throughput: 8355.445
    learn_time_ms: 19363.66
    sample_throughput: 22714.263
    sample_time_ms: 7122.925
    update_time_ms: 28.592
  timestamp: 1602652848
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |      9 |          239.087 | 1456128 |  247.872 |              292.798 |              165.677 |            849.347 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3399.7323340471094
    time_step_min: 3102
  date: 2020-10-14_05-21-14
  done: false
  episode_len_mean: 842.037447257384
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 249.20564825469876
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 314
  episodes_total: 1896
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9735767543315887
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065773469395935535
        model: {}
        policy_loss: -0.011116733350415112
        total_loss: 21.422983805338543
        vf_explained_var: 0.9707332253456116
        vf_loss: 21.43393023808797
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.673333333333336
    gpu_util_percent0: 0.2776666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15219011865343635
    mean_env_wait_ms: 1.1598676070048897
    mean_inference_ms: 4.712055423881699
    mean_raw_obs_processing_ms: 0.39851807610250684
  time_since_restore: 265.16386437416077
  time_this_iter_s: 26.076924085617065
  time_total_s: 265.16386437416077
  timers:
    learn_throughput: 8359.792
    learn_time_ms: 19353.592
    sample_throughput: 22844.971
    sample_time_ms: 7082.171
    update_time_ms: 29.91
  timestamp: 1602652874
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     10 |          265.164 | 1617920 |  249.206 |              295.071 |              165.677 |            842.037 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3394.6860809476802
    time_step_min: 3102
  date: 2020-10-14_05-21-40
  done: false
  episode_len_mean: 838.7770204479066
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 250.0402417554315
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9505527665217718
        entropy_coeff: 0.0005000000000000001
        kl: 0.006363538365500669
        model: {}
        policy_loss: -0.010260378786673149
        total_loss: 13.269665638605753
        vf_explained_var: 0.974217414855957
        vf_loss: 13.279764652252197
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19333333333333
    gpu_util_percent0: 0.36433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.151854772725269
    mean_env_wait_ms: 1.1613842391219023
    mean_inference_ms: 4.689739883693119
    mean_raw_obs_processing_ms: 0.3973975652962832
  time_since_restore: 291.1627812385559
  time_this_iter_s: 25.99891686439514
  time_total_s: 291.1627812385559
  timers:
    learn_throughput: 8378.144
    learn_time_ms: 19311.198
    sample_throughput: 23601.08
    sample_time_ms: 6855.279
    update_time_ms: 31.107
  timestamp: 1602652900
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     11 |          291.163 | 1779712 |   250.04 |              295.071 |              165.677 |            838.777 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3389.5119047619046
    time_step_min: 3102
  date: 2020-10-14_05-22-07
  done: false
  episode_len_mean: 835.617088607595
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 250.86917547993488
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9334837744633356
        entropy_coeff: 0.0005000000000000001
        kl: 0.006569515137622754
        model: {}
        policy_loss: -0.012398959409135083
        total_loss: 11.898517767588297
        vf_explained_var: 0.9744560122489929
        vf_loss: 11.910726388295492
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.612903225806456
    gpu_util_percent0: 0.3009677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15155009530940236
    mean_env_wait_ms: 1.1628961141993448
    mean_inference_ms: 4.669541617723579
    mean_raw_obs_processing_ms: 0.39634833472848807
  time_since_restore: 317.6072404384613
  time_this_iter_s: 26.444459199905396
  time_total_s: 317.6072404384613
  timers:
    learn_throughput: 8370.072
    learn_time_ms: 19329.821
    sample_throughput: 23860.561
    sample_time_ms: 6780.729
    update_time_ms: 29.62
  timestamp: 1602652927
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     12 |          317.607 | 1941504 |  250.869 |              295.071 |              165.677 |            835.617 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3380.3735772357722
    time_step_min: 3102
  date: 2020-10-14_05-22-33
  done: false
  episode_len_mean: 830.5144694533763
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 252.2050123420701
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 276
  episodes_total: 2488
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8941035866737366
        entropy_coeff: 0.0005000000000000001
        kl: 0.006269749913675089
        model: {}
        policy_loss: -0.00925206916872412
        total_loss: 16.93541383743286
        vf_explained_var: 0.9757897853851318
        vf_loss: 16.944485187530518
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.42
    gpu_util_percent0: 0.3656666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15108787609360028
    mean_env_wait_ms: 1.165648772856948
    mean_inference_ms: 4.638594505606225
    mean_raw_obs_processing_ms: 0.3947844445586566
  time_since_restore: 343.76100993156433
  time_this_iter_s: 26.153769493103027
  time_total_s: 343.76100993156433
  timers:
    learn_throughput: 8371.695
    learn_time_ms: 19326.074
    sample_throughput: 23966.023
    sample_time_ms: 6750.891
    update_time_ms: 31.014
  timestamp: 1602652953
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     13 |          343.761 | 2103296 |  252.205 |              295.071 |              165.677 |            830.514 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3372.4428141459744
    time_step_min: 3102
  date: 2020-10-14_05-22-59
  done: false
  episode_len_mean: 827.0316455696203
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 253.20390050918706
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 2686
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8781916697820028
        entropy_coeff: 0.0005000000000000001
        kl: 0.006277749276099105
        model: {}
        policy_loss: -0.011392164584928347
        total_loss: 12.567232608795166
        vf_explained_var: 0.9758232235908508
        vf_loss: 12.578436215718588
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.48275862068966
    gpu_util_percent0: 0.37758620689655176
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15079864436386975
    mean_env_wait_ms: 1.1673780787337247
    mean_inference_ms: 4.619908873990612
    mean_raw_obs_processing_ms: 0.39382058454344565
  time_since_restore: 369.5289497375488
  time_this_iter_s: 25.767939805984497
  time_total_s: 369.5289497375488
  timers:
    learn_throughput: 8394.201
    learn_time_ms: 19274.258
    sample_throughput: 23970.745
    sample_time_ms: 6749.561
    update_time_ms: 31.565
  timestamp: 1602652979
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     14 |          369.529 | 2265088 |  253.204 |              295.071 |              165.677 |            827.032 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3367.6711647727275
    time_step_min: 3102
  date: 2020-10-14_05-23-25
  done: false
  episode_len_mean: 824.3741209563995
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 253.88053531091498
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8726825912793478
        entropy_coeff: 0.0005000000000000001
        kl: 0.005345542721139888
        model: {}
        policy_loss: -0.01224327425006777
        total_loss: 12.99477251370748
        vf_explained_var: 0.971616268157959
        vf_loss: 13.006917715072632
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90967741935484
    gpu_util_percent0: 0.3554838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1505897970578734
    mean_env_wait_ms: 1.168703615299469
    mean_inference_ms: 4.606146304298784
    mean_raw_obs_processing_ms: 0.3931050816285997
  time_since_restore: 395.8331298828125
  time_this_iter_s: 26.304180145263672
  time_total_s: 395.8331298828125
  timers:
    learn_throughput: 8397.423
    learn_time_ms: 19266.864
    sample_throughput: 23948.597
    sample_time_ms: 6755.803
    update_time_ms: 31.89
  timestamp: 1602653005
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     15 |          395.833 | 2426880 |  253.881 |              295.071 |              165.677 |            824.374 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3361.8309258043337
    time_step_min: 3102
  date: 2020-10-14_05-23-52
  done: false
  episode_len_mean: 820.5810019518542
  episode_reward_max: 295.07070707070704
  episode_reward_mean: 254.76314215676598
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 3074
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.832097460826238
        entropy_coeff: 0.0005000000000000001
        kl: 0.005490843905135989
        model: {}
        policy_loss: -0.010333318127474437
        total_loss: 13.920937776565552
        vf_explained_var: 0.9784757494926453
        vf_loss: 13.931138038635254
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.663333333333334
    gpu_util_percent0: 0.289
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15032155699013144
    mean_env_wait_ms: 1.170735174262857
    mean_inference_ms: 4.587717906855984
    mean_raw_obs_processing_ms: 0.3921593817521008
  time_since_restore: 422.0534029006958
  time_this_iter_s: 26.2202730178833
  time_total_s: 422.0534029006958
  timers:
    learn_throughput: 8390.716
    learn_time_ms: 19282.265
    sample_throughput: 23920.676
    sample_time_ms: 6763.688
    update_time_ms: 31.842
  timestamp: 1602653032
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     16 |          422.053 | 2588672 |  254.763 |              295.071 |              165.677 |            820.581 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3354.8726443768996
    time_step_min: 3076
  date: 2020-10-14_05-24-18
  done: false
  episode_len_mean: 817.1323086196504
  episode_reward_max: 297.49494949494914
  episode_reward_mean: 255.8385116992711
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 244
  episodes_total: 3318
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8182818094889323
        entropy_coeff: 0.0005000000000000001
        kl: 0.005390155672406157
        model: {}
        policy_loss: -0.008973776893981267
        total_loss: 10.671323935190836
        vf_explained_var: 0.9803421497344971
        vf_loss: 10.68016799290975
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.029999999999998
    gpu_util_percent0: 0.2813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15005359526917617
    mean_env_wait_ms: 1.1726223635042803
    mean_inference_ms: 4.570883071200654
    mean_raw_obs_processing_ms: 0.3912907606901621
  time_since_restore: 448.18203711509705
  time_this_iter_s: 26.128634214401245
  time_total_s: 448.18203711509705
  timers:
    learn_throughput: 8383.696
    learn_time_ms: 19298.409
    sample_throughput: 23886.024
    sample_time_ms: 6773.501
    update_time_ms: 31.575
  timestamp: 1602653058
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     17 |          448.182 | 2750464 |  255.839 |              297.495 |              165.677 |            817.132 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3350.8645591647332
    time_step_min: 3076
  date: 2020-10-14_05-24-45
  done: false
  episode_len_mean: 815.0719217491369
  episode_reward_max: 297.49494949494914
  episode_reward_mean: 256.3523962292661
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8158556421597799
        entropy_coeff: 0.0005000000000000001
        kl: 0.005813289627743264
        model: {}
        policy_loss: -0.01080344397147807
        total_loss: 9.102331479390463
        vf_explained_var: 0.979426383972168
        vf_loss: 9.112961610158285
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.477419354838712
    gpu_util_percent0: 0.35709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14989920989298997
    mean_env_wait_ms: 1.173795937280717
    mean_inference_ms: 4.560809804155763
    mean_raw_obs_processing_ms: 0.3907649906750448
  time_since_restore: 474.5349898338318
  time_this_iter_s: 26.35295271873474
  time_total_s: 474.5349898338318
  timers:
    learn_throughput: 8377.512
    learn_time_ms: 19312.654
    sample_throughput: 23875.752
    sample_time_ms: 6776.415
    update_time_ms: 33.873
  timestamp: 1602653085
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     18 |          474.535 | 2912256 |  256.352 |              297.495 |              165.677 |            815.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3346.2407103825135
    time_step_min: 3076
  date: 2020-10-14_05-25-11
  done: false
  episode_len_mean: 812.5924620390456
  episode_reward_max: 298.2525252525252
  episode_reward_mean: 256.9936101798899
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 212
  episodes_total: 3688
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.783480112751325
        entropy_coeff: 0.0005000000000000001
        kl: 0.005505596792014937
        model: {}
        policy_loss: -0.010511156249170503
        total_loss: 13.502816677093506
        vf_explained_var: 0.978053629398346
        vf_loss: 13.513169129689535
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.540000000000003
    gpu_util_percent0: 0.31700000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14971313529095467
    mean_env_wait_ms: 1.1754131782016102
    mean_inference_ms: 4.548243499944181
    mean_raw_obs_processing_ms: 0.390110221308786
  time_since_restore: 500.7623465061188
  time_this_iter_s: 26.227356672286987
  time_total_s: 500.7623465061188
  timers:
    learn_throughput: 8388.629
    learn_time_ms: 19287.061
    sample_throughput: 23850.081
    sample_time_ms: 6783.709
    update_time_ms: 31.9
  timestamp: 1602653111
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     19 |          500.762 | 3074048 |  256.994 |              298.253 |              165.677 |            812.592 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3341.0226925038246
    time_step_min: 3076
  date: 2020-10-14_05-25-37
  done: false
  episode_len_mean: 809.9488607594936
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 257.75697481140514
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 262
  episodes_total: 3950
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7689148386319479
        entropy_coeff: 0.0005000000000000001
        kl: 0.005313604371622205
        model: {}
        policy_loss: -0.010639291993963221
        total_loss: 11.459484736124674
        vf_explained_var: 0.9803845286369324
        vf_loss: 11.469976902008057
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.173333333333336
    gpu_util_percent0: 0.2876666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14949162246224196
    mean_env_wait_ms: 1.1772141563780643
    mean_inference_ms: 4.534268152265248
    mean_raw_obs_processing_ms: 0.3893833404864095
  time_since_restore: 526.9255065917969
  time_this_iter_s: 26.1631600856781
  time_total_s: 526.9255065917969
  timers:
    learn_throughput: 8388.52
    learn_time_ms: 19287.311
    sample_throughput: 23816.126
    sample_time_ms: 6793.38
    update_time_ms: 29.822
  timestamp: 1602653137
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     20 |          526.926 | 3235840 |  257.757 |              301.434 |              165.677 |            809.949 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3338.275
    time_step_min: 3059
  date: 2020-10-14_05-26-03
  done: false
  episode_len_mean: 808.4980525803311
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 258.1885702202157
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 4108
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7616968055566152
        entropy_coeff: 0.0005000000000000001
        kl: 0.0063063996300722165
        model: {}
        policy_loss: -0.012424984869236747
        total_loss: 9.233952124913534
        vf_explained_var: 0.9793080687522888
        vf_loss: 9.246127367019653
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.773333333333333
    gpu_util_percent0: 0.336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14937316296653008
    mean_env_wait_ms: 1.1782336526846608
    mean_inference_ms: 4.526505692775724
    mean_raw_obs_processing_ms: 0.38897843585333164
  time_since_restore: 553.0237889289856
  time_this_iter_s: 26.09828233718872
  time_total_s: 553.0237889289856
  timers:
    learn_throughput: 8390.338
    learn_time_ms: 19283.133
    sample_throughput: 23762.182
    sample_time_ms: 6808.802
    update_time_ms: 27.473
  timestamp: 1602653163
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     21 |          553.024 | 3397632 |  258.189 |              301.434 |              165.677 |            808.498 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3334.966557530402
    time_step_min: 3059
  date: 2020-10-14_05-26-30
  done: false
  episode_len_mean: 806.9435408921933
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 258.63860022154626
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 4304
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7464395960172018
        entropy_coeff: 0.0005000000000000001
        kl: 0.005762962042354047
        model: {}
        policy_loss: -0.011503236756349603
        total_loss: 11.199698448181152
        vf_explained_var: 0.9805458188056946
        vf_loss: 11.210998773574829
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.389999999999997
    gpu_util_percent0: 0.3356666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492367553091594
    mean_env_wait_ms: 1.1795263057940153
    mean_inference_ms: 4.517372354132996
    mean_raw_obs_processing_ms: 0.38849792181871845
  time_since_restore: 579.0197508335114
  time_this_iter_s: 25.995961904525757
  time_total_s: 579.0197508335114
  timers:
    learn_throughput: 8410.261
    learn_time_ms: 19237.453
    sample_throughput: 23737.044
    sample_time_ms: 6816.013
    update_time_ms: 27.747
  timestamp: 1602653190
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     22 |           579.02 | 3559424 |  258.639 |              301.434 |              165.677 |            806.944 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3331.2821695213
    time_step_min: 3059
  date: 2020-10-14_05-26-56
  done: false
  episode_len_mean: 804.9172850283719
  episode_reward_max: 301.4343434343433
  episode_reward_mean: 259.23495981200034
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 278
  episodes_total: 4582
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7183542102575302
        entropy_coeff: 0.0005000000000000001
        kl: 0.005707078341705103
        model: {}
        policy_loss: -0.0085253130334119
        total_loss: 10.668026685714722
        vf_explained_var: 0.9826880097389221
        vf_loss: 10.676340103149414
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28064516129032
    gpu_util_percent0: 0.3041935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14905524892453614
    mean_env_wait_ms: 1.1811759549262795
    mean_inference_ms: 4.505667033692587
    mean_raw_obs_processing_ms: 0.3878968801500961
  time_since_restore: 605.6530785560608
  time_this_iter_s: 26.63332772254944
  time_total_s: 605.6530785560608
  timers:
    learn_throughput: 8403.661
    learn_time_ms: 19252.561
    sample_throughput: 23628.868
    sample_time_ms: 6847.218
    update_time_ms: 27.981
  timestamp: 1602653216
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     23 |          605.653 | 3721216 |  259.235 |              301.434 |              165.677 |            804.917 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3329.1632003395584
    time_step_min: 3059
  date: 2020-10-14_05-27-23
  done: false
  episode_len_mean: 803.915611814346
  episode_reward_max: 302.64646464646495
  episode_reward_mean: 259.54113924050625
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 158
  episodes_total: 4740
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7287645439306895
        entropy_coeff: 0.0005000000000000001
        kl: 0.005742862975845735
        model: {}
        policy_loss: -0.010577253678396422
        total_loss: 9.842362721761068
        vf_explained_var: 0.9788429141044617
        vf_loss: 9.852729797363281
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.290000000000003
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14896137003924376
    mean_env_wait_ms: 1.1820596733984865
    mean_inference_ms: 4.4994891976708296
    mean_raw_obs_processing_ms: 0.3875767238358607
  time_since_restore: 631.9126310348511
  time_this_iter_s: 26.259552478790283
  time_total_s: 631.9126310348511
  timers:
    learn_throughput: 8380.844
    learn_time_ms: 19304.977
    sample_throughput: 23639.847
    sample_time_ms: 6844.038
    update_time_ms: 27.718
  timestamp: 1602653243
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     24 |          631.913 | 3883008 |  259.541 |              302.646 |              165.677 |            803.916 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3326.6037620118586
    time_step_min: 3016
  date: 2020-10-14_05-27-49
  done: false
  episode_len_mean: 802.8786338686725
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 259.88879032241493
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 4919
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7098510215679804
        entropy_coeff: 0.0005000000000000001
        kl: 0.005541195278055966
        model: {}
        policy_loss: -0.010613679187372327
        total_loss: 9.693046967188517
        vf_explained_var: 0.9825088977813721
        vf_loss: 9.703461488087973
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.83870967741936
    gpu_util_percent0: 0.32774193548387104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14886218165630333
    mean_env_wait_ms: 1.1830763263130484
    mean_inference_ms: 4.492823262735617
    mean_raw_obs_processing_ms: 0.38722421753253344
  time_since_restore: 658.272055387497
  time_this_iter_s: 26.359424352645874
  time_total_s: 658.272055387497
  timers:
    learn_throughput: 8385.247
    learn_time_ms: 19294.839
    sample_throughput: 23590.162
    sample_time_ms: 6858.452
    update_time_ms: 27.593
  timestamp: 1602653269
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     25 |          658.272 | 4044800 |  259.889 |              306.586 |              165.677 |            802.879 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3322.5940212150435
    time_step_min: 3016
  date: 2020-10-14_05-28-16
  done: false
  episode_len_mean: 801.3819297909073
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 260.41701496065576
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 294
  episodes_total: 5213
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6829637338717779
        entropy_coeff: 0.0005000000000000001
        kl: 0.005084280312682192
        model: {}
        policy_loss: -0.010475764116563369
        total_loss: 10.875974178314209
        vf_explained_var: 0.9832141399383545
        vf_loss: 10.886282841364542
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.65666666666667
    gpu_util_percent0: 0.30799999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14870781247239143
    mean_env_wait_ms: 1.1845997698642667
    mean_inference_ms: 4.482779397547706
    mean_raw_obs_processing_ms: 0.3867169882212869
  time_since_restore: 684.5655243396759
  time_this_iter_s: 26.293468952178955
  time_total_s: 684.5655243396759
  timers:
    learn_throughput: 8389.496
    learn_time_ms: 19285.069
    sample_throughput: 23535.051
    sample_time_ms: 6874.512
    update_time_ms: 27.6
  timestamp: 1602653296
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     26 |          684.566 | 4206592 |  260.417 |              306.586 |              165.677 |            801.382 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3320.630613772455
    time_step_min: 3016
  date: 2020-10-14_05-28-42
  done: false
  episode_len_mean: 800.5368577810871
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 260.703392826252
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 5372
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.684943675994873
        entropy_coeff: 0.0005000000000000001
        kl: 0.005801629857160151
        model: {}
        policy_loss: -0.012382653173214445
        total_loss: 8.087796211242676
        vf_explained_var: 0.9820259213447571
        vf_loss: 8.099941174189249
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94838709677419
    gpu_util_percent0: 0.35032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14863130526340237
    mean_env_wait_ms: 1.1853806173898176
    mean_inference_ms: 4.477749404679098
    mean_raw_obs_processing_ms: 0.38645915626397453
  time_since_restore: 710.8536818027496
  time_this_iter_s: 26.28815746307373
  time_total_s: 710.8536818027496
  timers:
    learn_throughput: 8388.205
    learn_time_ms: 19288.036
    sample_throughput: 23495.328
    sample_time_ms: 6886.135
    update_time_ms: 28.031
  timestamp: 1602653322
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     27 |          710.854 | 4368384 |  260.703 |              306.586 |              165.677 |            800.537 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3318.2403480152257
    time_step_min: 3016
  date: 2020-10-14_05-29-09
  done: false
  episode_len_mean: 799.6009017132552
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 261.03152353107265
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 173
  episodes_total: 5545
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.666491910815239
        entropy_coeff: 0.0005000000000000001
        kl: 0.005610223355082174
        model: {}
        policy_loss: -0.009728390451831123
        total_loss: 9.874438285827637
        vf_explained_var: 0.9808456897735596
        vf_loss: 9.883938709894815
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000006
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14855176731378222
    mean_env_wait_ms: 1.1862405807772654
    mean_inference_ms: 4.472451304291244
    mean_raw_obs_processing_ms: 0.3861835504752526
  time_since_restore: 737.0910837650299
  time_this_iter_s: 26.237401962280273
  time_total_s: 737.0910837650299
  timers:
    learn_throughput: 8392.833
    learn_time_ms: 19277.401
    sample_throughput: 23503.017
    sample_time_ms: 6883.882
    update_time_ms: 28.082
  timestamp: 1602653349
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     28 |          737.091 | 4530176 |  261.032 |              306.586 |              165.677 |            799.601 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3315.009802235598
    time_step_min: 3016
  date: 2020-10-14_05-29-35
  done: false
  episode_len_mean: 798.0891665240458
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 261.47272139502155
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 298
  episodes_total: 5843
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6293653895457586
        entropy_coeff: 0.0005000000000000001
        kl: 0.005152029761423667
        model: {}
        policy_loss: -0.00865899131288946
        total_loss: 11.526518026987711
        vf_explained_var: 0.9827463626861572
        vf_loss: 11.534976243972778
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666668
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14842418334561713
    mean_env_wait_ms: 1.1876289094816284
    mean_inference_ms: 4.464052093689302
    mean_raw_obs_processing_ms: 0.38575403481858317
  time_since_restore: 763.4202361106873
  time_this_iter_s: 26.32915234565735
  time_total_s: 763.4202361106873
  timers:
    learn_throughput: 8387.54
    learn_time_ms: 19289.566
    sample_throughput: 23519.563
    sample_time_ms: 6879.039
    update_time_ms: 30.348
  timestamp: 1602653375
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     29 |           763.42 | 4691968 |  261.473 |              306.586 |              165.677 |            798.089 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3313.2441432396254
    time_step_min: 3016
  date: 2020-10-14_05-30-02
  done: false
  episode_len_mean: 797.2253497668221
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 261.7548873141811
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 161
  episodes_total: 6004
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6357591251532236
        entropy_coeff: 0.0005000000000000001
        kl: 0.005754904045412938
        model: {}
        policy_loss: -0.010691317261565322
        total_loss: 7.284097353617351
        vf_explained_var: 0.9835760593414307
        vf_loss: 7.294531146685283
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.66129032258064
    gpu_util_percent0: 0.3070967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14835768157598322
    mean_env_wait_ms: 1.1883336193343592
    mean_inference_ms: 4.459760263324877
    mean_raw_obs_processing_ms: 0.3855338537431134
  time_since_restore: 789.735335111618
  time_this_iter_s: 26.315099000930786
  time_total_s: 789.735335111618
  timers:
    learn_throughput: 8384.261
    learn_time_ms: 19297.11
    sample_throughput: 23496.891
    sample_time_ms: 6885.677
    update_time_ms: 30.808
  timestamp: 1602653402
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     30 |          789.735 | 4853760 |  261.755 |              306.586 |              165.677 |            797.225 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3311.225345247766
    time_step_min: 3016
  date: 2020-10-14_05-30-28
  done: false
  episode_len_mean: 796.2982371017306
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.065031685119
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 6183
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6245539039373398
        entropy_coeff: 0.0005000000000000001
        kl: 0.005535768112167716
        model: {}
        policy_loss: -0.008870539992737273
        total_loss: 8.690814336140951
        vf_explained_var: 0.9829214215278625
        vf_loss: 8.699443578720093
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.773333333333333
    gpu_util_percent0: 0.296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14828870203012978
    mean_env_wait_ms: 1.1891266356883716
    mean_inference_ms: 4.455133265774231
    mean_raw_obs_processing_ms: 0.3852902879375601
  time_since_restore: 816.1268291473389
  time_this_iter_s: 26.391494035720825
  time_total_s: 816.1268291473389
  timers:
    learn_throughput: 8370.403
    learn_time_ms: 19329.057
    sample_throughput: 23509.042
    sample_time_ms: 6882.118
    update_time_ms: 31.009
  timestamp: 1602653428
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     31 |          816.127 | 5015552 |  262.065 |              306.586 |              165.677 |            796.298 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3308.3779087806392
    time_step_min: 3016
  date: 2020-10-14_05-30-55
  done: false
  episode_len_mean: 795.0565338276182
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.4935452766777
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 291
  episodes_total: 6474
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6012036552031835
        entropy_coeff: 0.0005000000000000001
        kl: 0.005087119837601979
        model: {}
        policy_loss: -0.008742266528618833
        total_loss: 9.66909925142924
        vf_explained_var: 0.9845057129859924
        vf_loss: 9.677633126576742
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56774193548387
    gpu_util_percent0: 0.2841935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481828881293648
    mean_env_wait_ms: 1.1903498318457528
    mean_inference_ms: 4.4481823646083996
    mean_raw_obs_processing_ms: 0.3849353898603169
  time_since_restore: 842.3986527919769
  time_this_iter_s: 26.27182364463806
  time_total_s: 842.3986527919769
  timers:
    learn_throughput: 8359.762
    learn_time_ms: 19353.661
    sample_throughput: 23504.239
    sample_time_ms: 6883.524
    update_time_ms: 31.02
  timestamp: 1602653455
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     32 |          842.399 | 5177344 |  262.494 |              306.586 |              165.677 |            795.057 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3306.840647699758
    time_step_min: 3016
  date: 2020-10-14_05-31-21
  done: false
  episode_len_mean: 794.4975889089814
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.688544577785
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 6636
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6159155319134394
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054813608682403965
        model: {}
        policy_loss: -0.011568933009736307
        total_loss: 6.278260111808777
        vf_explained_var: 0.9860866665840149
        vf_loss: 6.289588888486226
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.86
    gpu_util_percent0: 0.36133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14812603291869922
    mean_env_wait_ms: 1.1909885821921484
    mean_inference_ms: 4.444503618596388
    mean_raw_obs_processing_ms: 0.3847467731638417
  time_since_restore: 868.6608514785767
  time_this_iter_s: 26.26219868659973
  time_total_s: 868.6608514785767
  timers:
    learn_throughput: 8362.617
    learn_time_ms: 19347.053
    sample_throughput: 23603.866
    sample_time_ms: 6854.47
    update_time_ms: 28.699
  timestamp: 1602653481
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     33 |          868.661 | 5339136 |  262.689 |              306.586 |              165.677 |            794.498 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3305.0464464759657
    time_step_min: 3016
  date: 2020-10-14_05-31-47
  done: false
  episode_len_mean: 793.947577092511
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 262.9731010545988
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 6810
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6004733194907507
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055042091601838665
        model: {}
        policy_loss: -0.010720125612958023
        total_loss: 7.237987200419108
        vf_explained_var: 0.9852418899536133
        vf_loss: 7.248456994692485
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.753333333333334
    gpu_util_percent0: 0.3046666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806872310485408
    mean_env_wait_ms: 1.191669754658394
    mean_inference_ms: 4.440669149899705
    mean_raw_obs_processing_ms: 0.3845460444467353
  time_since_restore: 894.7789969444275
  time_this_iter_s: 26.11814546585083
  time_total_s: 894.7789969444275
  timers:
    learn_throughput: 8371.981
    learn_time_ms: 19325.415
    sample_throughput: 23577.345
    sample_time_ms: 6862.181
    update_time_ms: 27.516
  timestamp: 1602653507
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     34 |          894.779 | 5500928 |  262.973 |              306.586 |              165.677 |            793.948 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3301.853327681221
    time_step_min: 3016
  date: 2020-10-14_05-32-14
  done: false
  episode_len_mean: 793.0447572132301
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 263.4259839777081
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 295
  episodes_total: 7105
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5662781298160553
        entropy_coeff: 0.0005000000000000001
        kl: 0.004756226437166333
        model: {}
        policy_loss: -0.010773680560911695
        total_loss: 9.240336974461874
        vf_explained_var: 0.9854938387870789
        vf_loss: 9.25091822942098
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.236666666666665
    gpu_util_percent0: 0.23766666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14797670960359907
    mean_env_wait_ms: 1.1927828215704095
    mean_inference_ms: 4.43458156202792
    mean_raw_obs_processing_ms: 0.3842407105439576
  time_since_restore: 920.8315756320953
  time_this_iter_s: 26.052578687667847
  time_total_s: 920.8315756320953
  timers:
    learn_throughput: 8380.822
    learn_time_ms: 19305.028
    sample_throughput: 23609.764
    sample_time_ms: 6852.758
    update_time_ms: 25.421
  timestamp: 1602653534
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     35 |          920.832 | 5662720 |  263.426 |              306.586 |              165.677 |            793.045 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3300.475276243094
    time_step_min: 3016
  date: 2020-10-14_05-32-40
  done: false
  episode_len_mean: 792.5401761144744
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 263.65293134982176
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 163
  episodes_total: 7268
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5714666495720545
        entropy_coeff: 0.0005000000000000001
        kl: 0.005248239535527925
        model: {}
        policy_loss: -0.009869688520363221
        total_loss: 6.231145858764648
        vf_explained_var: 0.985950767993927
        vf_loss: 6.241038918495178
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53870967741936
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479278011802464
    mean_env_wait_ms: 1.1933580355734097
    mean_inference_ms: 4.431373733052616
    mean_raw_obs_processing_ms: 0.3840779660066616
  time_since_restore: 947.1074914932251
  time_this_iter_s: 26.27591586112976
  time_total_s: 947.1074914932251
  timers:
    learn_throughput: 8379.808
    learn_time_ms: 19307.363
    sample_throughput: 23658.127
    sample_time_ms: 6838.749
    update_time_ms: 26.694
  timestamp: 1602653560
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     36 |          947.107 | 5824512 |  263.653 |              306.586 |              165.677 |             792.54 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3298.755060728745
    time_step_min: 3016
  date: 2020-10-14_05-33-07
  done: false
  episode_len_mean: 792.0992202204894
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 263.90828967274246
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 170
  episodes_total: 7438
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5747150182723999
        entropy_coeff: 0.0005000000000000001
        kl: 0.005940825406772395
        model: {}
        policy_loss: -0.009506770729785785
        total_loss: 6.700782895088196
        vf_explained_var: 0.985985517501831
        vf_loss: 6.710279941558838
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.170000000000005
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788020115926256
    mean_env_wait_ms: 1.1939564063030295
    mean_inference_ms: 4.428114428170018
    mean_raw_obs_processing_ms: 0.3839118348944442
  time_since_restore: 973.532075881958
  time_this_iter_s: 26.42458438873291
  time_total_s: 973.532075881958
  timers:
    learn_throughput: 8375.909
    learn_time_ms: 19316.352
    sample_throughput: 23650.933
    sample_time_ms: 6840.829
    update_time_ms: 28.01
  timestamp: 1602653587
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     37 |          973.532 | 5986304 |  263.908 |              306.586 |              165.677 |            792.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3295.8188894654904
    time_step_min: 3016
  date: 2020-10-14_05-33-33
  done: false
  episode_len_mean: 791.482032057911
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 264.3536541735869
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 298
  episodes_total: 7736
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5408161381880442
        entropy_coeff: 0.0005000000000000001
        kl: 0.005141363246366382
        model: {}
        policy_loss: -0.009341358963865787
        total_loss: 7.610303680102031
        vf_explained_var: 0.9880974888801575
        vf_loss: 7.61965827147166
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06666666666667
    gpu_util_percent0: 0.29233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780001637811532
    mean_env_wait_ms: 1.1949522189398931
    mean_inference_ms: 4.4227652265219435
    mean_raw_obs_processing_ms: 0.3836441503920528
  time_since_restore: 999.6864273548126
  time_this_iter_s: 26.154351472854614
  time_total_s: 999.6864273548126
  timers:
    learn_throughput: 8379.684
    learn_time_ms: 19307.65
    sample_throughput: 23643.646
    sample_time_ms: 6842.938
    update_time_ms: 25.461
  timestamp: 1602653613
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     38 |          999.686 | 6148096 |  264.354 |              306.586 |              165.677 |            791.482 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3294.1681910569105
    time_step_min: 3016
  date: 2020-10-14_05-34-00
  done: false
  episode_len_mean: 791.1965822784811
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 264.5999744278225
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 164
  episodes_total: 7900
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5411226153373718
        entropy_coeff: 0.0005000000000000001
        kl: 0.005913850696136554
        model: {}
        policy_loss: -0.011923030091566034
        total_loss: 5.805917024612427
        vf_explained_var: 0.9870100021362305
        vf_loss: 5.817814946174622
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.2258064516129
    gpu_util_percent0: 0.34161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477574695647492
    mean_env_wait_ms: 1.1954620089010073
    mean_inference_ms: 4.419961347136594
    mean_raw_obs_processing_ms: 0.38350445410544276
  time_since_restore: 1026.0900309085846
  time_this_iter_s: 26.403603553771973
  time_total_s: 1026.0900309085846
  timers:
    learn_throughput: 8383.812
    learn_time_ms: 19298.143
    sample_throughput: 23583.351
    sample_time_ms: 6860.433
    update_time_ms: 23.508
  timestamp: 1602653640
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     39 |          1026.09 | 6309888 |    264.6 |              306.586 |              165.677 |            791.197 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3292.529499626587
    time_step_min: 3016
  date: 2020-10-14_05-34-26
  done: false
  episode_len_mean: 790.8907219052344
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 264.81483026744746
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 8062
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5440810869137446
        entropy_coeff: 0.0005000000000000001
        kl: 0.005609358583266537
        model: {}
        policy_loss: -0.010298911026135707
        total_loss: 7.1397877136866255
        vf_explained_var: 0.9844804406166077
        vf_loss: 7.150078336397807
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.016666666666666
    gpu_util_percent0: 0.31133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771812835007142
    mean_env_wait_ms: 1.1959591340054336
    mean_inference_ms: 4.4172915993658215
    mean_raw_obs_processing_ms: 0.3833723719302144
  time_since_restore: 1052.5157871246338
  time_this_iter_s: 26.425756216049194
  time_total_s: 1052.5157871246338
  timers:
    learn_throughput: 8388.343
    learn_time_ms: 19287.719
    sample_throughput: 23512.217
    sample_time_ms: 6881.189
    update_time_ms: 23.059
  timestamp: 1602653666
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     40 |          1052.52 | 6471680 |  264.815 |              306.586 |              165.677 |            790.891 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3290.0100840336136
    time_step_min: 3016
  date: 2020-10-14_05-34-53
  done: false
  episode_len_mean: 790.2048336922709
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 265.21100089190537
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 296
  episodes_total: 8358
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.509361982345581
        entropy_coeff: 0.0005000000000000001
        kl: 0.005469926167279482
        model: {}
        policy_loss: -0.010394827579148114
        total_loss: 8.237964987754822
        vf_explained_var: 0.9871106147766113
        vf_loss: 8.248341043790182
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.3270967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147647791407638
    mean_env_wait_ms: 1.196842137619695
    mean_inference_ms: 4.41258842262394
    mean_raw_obs_processing_ms: 0.38313902128457855
  time_since_restore: 1079.0867893695831
  time_this_iter_s: 26.57100224494934
  time_total_s: 1079.0867893695831
  timers:
    learn_throughput: 8388.508
    learn_time_ms: 19287.34
    sample_throughput: 23458.672
    sample_time_ms: 6896.895
    update_time_ms: 24.783
  timestamp: 1602653693
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     41 |          1079.09 | 6633472 |  265.211 |              306.586 |              165.677 |            790.205 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3288.4494355597367
    time_step_min: 3016
  date: 2020-10-14_05-35-20
  done: false
  episode_len_mean: 789.8383731833098
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 265.45972382048325
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 8532
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.507927214105924
        entropy_coeff: 0.0005000000000000001
        kl: 0.005216488498263061
        model: {}
        policy_loss: -0.010664286892279051
        total_loss: 5.711553494135539
        vf_explained_var: 0.9873116612434387
        vf_loss: 5.722210804621379
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.174193548387098
    gpu_util_percent0: 0.3125806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760827253139003
    mean_env_wait_ms: 1.1973261300867803
    mean_inference_ms: 4.409960444759652
    mean_raw_obs_processing_ms: 0.38300931512858866
  time_since_restore: 1105.6858234405518
  time_this_iter_s: 26.599034070968628
  time_total_s: 1105.6858234405518
  timers:
    learn_throughput: 8385.682
    learn_time_ms: 19293.84
    sample_throughput: 23403.677
    sample_time_ms: 6913.102
    update_time_ms: 33.287
  timestamp: 1602653720
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     42 |          1105.69 | 6795264 |   265.46 |              306.586 |              165.677 |            789.838 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3286.827025155781
    time_step_min: 3016
  date: 2020-10-14_05-35-46
  done: false
  episode_len_mean: 789.556590752243
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 265.7101588695791
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 8694
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5209666738907496
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053579983456681175
        model: {}
        policy_loss: -0.011443275609053671
        total_loss: 5.225985884666443
        vf_explained_var: 0.9881942868232727
        vf_loss: 5.237421830495198
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.536666666666665
    gpu_util_percent0: 0.30833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757335529933654
    mean_env_wait_ms: 1.1977694713825295
    mean_inference_ms: 4.407615080952954
    mean_raw_obs_processing_ms: 0.38289364861530184
  time_since_restore: 1132.1379568576813
  time_this_iter_s: 26.452133417129517
  time_total_s: 1132.1379568576813
  timers:
    learn_throughput: 8384.906
    learn_time_ms: 19295.625
    sample_throughput: 23355.823
    sample_time_ms: 6927.266
    update_time_ms: 35.45
  timestamp: 1602653746
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     43 |          1132.14 | 6957056 |   265.71 |              306.586 |              165.677 |            789.557 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3283.8893731143144
    time_step_min: 3016
  date: 2020-10-14_05-36-13
  done: false
  episode_len_mean: 789.039099922023
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 266.1412037271455
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 283
  episodes_total: 8977
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4873087281982104
        entropy_coeff: 0.0005000000000000001
        kl: 0.005598535644821823
        model: {}
        policy_loss: -0.010866215345837796
        total_loss: 7.635515332221985
        vf_explained_var: 0.9877046942710876
        vf_loss: 7.646345337231954
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.826666666666668
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751398400940244
    mean_env_wait_ms: 1.198514888418807
    mean_inference_ms: 4.403640196661616
    mean_raw_obs_processing_ms: 0.38269854199873643
  time_since_restore: 1158.321647644043
  time_this_iter_s: 26.183690786361694
  time_total_s: 1158.321647644043
  timers:
    learn_throughput: 8381.554
    learn_time_ms: 19303.342
    sample_throughput: 23365.271
    sample_time_ms: 6924.465
    update_time_ms: 36.009
  timestamp: 1602653773
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     44 |          1158.32 | 7118848 |  266.141 |              306.586 |              165.677 |            789.039 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3282.002955341506
    time_step_min: 3016
  date: 2020-10-14_05-36-39
  done: false
  episode_len_mean: 788.8099083369708
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 266.4225383472437
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 187
  episodes_total: 9164
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47847431153059006
        entropy_coeff: 0.0005000000000000001
        kl: 0.005613565483751397
        model: {}
        policy_loss: -0.008747315519334128
        total_loss: 5.270971179008484
        vf_explained_var: 0.9885722994804382
        vf_loss: 5.279677311579387
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70967741935484
    gpu_util_percent0: 0.3574193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747625193244218
    mean_env_wait_ms: 1.1989821794806181
    mean_inference_ms: 4.401140023180142
    mean_raw_obs_processing_ms: 0.3825786617441055
  time_since_restore: 1184.500508069992
  time_this_iter_s: 26.178860425949097
  time_total_s: 1184.500508069992
  timers:
    learn_throughput: 8375.034
    learn_time_ms: 19318.369
    sample_throughput: 23380.224
    sample_time_ms: 6920.036
    update_time_ms: 37.226
  timestamp: 1602653799
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     45 |           1184.5 | 7280640 |  266.423 |              306.586 |              165.677 |             788.81 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3280.2321678321678
    time_step_min: 3016
  date: 2020-10-14_05-37-06
  done: false
  episode_len_mean: 788.601201330044
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 266.6601128738852
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 9323
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49601898342370987
        entropy_coeff: 0.0005000000000000001
        kl: 0.00511051594124486
        model: {}
        policy_loss: -0.012326105507478738
        total_loss: 5.88115390141805
        vf_explained_var: 0.9861772060394287
        vf_loss: 5.893472512563069
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.363333333333333
    gpu_util_percent0: 0.30600000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744529154038372
    mean_env_wait_ms: 1.1993648508492047
    mean_inference_ms: 4.399089842511455
    mean_raw_obs_processing_ms: 0.3824793253482413
  time_since_restore: 1210.5817911624908
  time_this_iter_s: 26.08128309249878
  time_total_s: 1210.5817911624908
  timers:
    learn_throughput: 8380.193
    learn_time_ms: 19306.476
    sample_throughput: 23376.024
    sample_time_ms: 6921.28
    update_time_ms: 36.053
  timestamp: 1602653826
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     46 |          1210.58 | 7442432 |   266.66 |              306.586 |              165.677 |            788.601 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3277.4158125915083
    time_step_min: 3016
  date: 2020-10-14_05-37-32
  done: false
  episode_len_mean: 788.1692387904067
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.0626968327698
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 267
  episodes_total: 9590
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4737584764758746
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053332661821817355
        model: {}
        policy_loss: -0.00873103212021912
        total_loss: 7.418438156445821
        vf_explained_var: 0.9876915812492371
        vf_loss: 7.427139401435852
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.590000000000007
    gpu_util_percent0: 0.265
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739611237324854
    mean_env_wait_ms: 1.1999839075576653
    mean_inference_ms: 4.395730344489956
    mean_raw_obs_processing_ms: 0.382314231840932
  time_since_restore: 1236.945025920868
  time_this_iter_s: 26.363234758377075
  time_total_s: 1236.945025920868
  timers:
    learn_throughput: 8377.766
    learn_time_ms: 19312.071
    sample_throughput: 23420.676
    sample_time_ms: 6908.084
    update_time_ms: 36.725
  timestamp: 1602653852
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     47 |          1236.95 | 7604224 |  267.063 |              306.586 |              165.677 |            788.169 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3275.51033988534
    time_step_min: 3016
  date: 2020-10-14_05-37-59
  done: false
  episode_len_mean: 787.9017966516946
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.34459437164617
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 206
  episodes_total: 9796
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.45716172208388645
        entropy_coeff: 0.0005000000000000001
        kl: 0.005090787773951888
        model: {}
        policy_loss: -0.010548666915080199
        total_loss: 5.173902789751689
        vf_explained_var: 0.9894575476646423
        vf_loss: 5.184425473213196
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.706451612903226
    gpu_util_percent0: 0.30612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735788003035763
    mean_env_wait_ms: 1.2004378127309858
    mean_inference_ms: 4.393279281298089
    mean_raw_obs_processing_ms: 0.38219918908802275
  time_since_restore: 1263.118402481079
  time_this_iter_s: 26.17337656021118
  time_total_s: 1263.118402481079
  timers:
    learn_throughput: 8376.918
    learn_time_ms: 19314.026
    sample_throughput: 23426.388
    sample_time_ms: 6906.4
    update_time_ms: 37.103
  timestamp: 1602653879
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     48 |          1263.12 | 7766016 |  267.345 |              306.586 |              165.677 |            787.902 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3273.9775360128942
    time_step_min: 3016
  date: 2020-10-14_05-38-25
  done: false
  episode_len_mean: 787.6919136112506
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.5595584169165
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 9955
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47419848789771396
        entropy_coeff: 0.0005000000000000001
        kl: 0.005596162169240415
        model: {}
        policy_loss: -0.010498188300213466
        total_loss: 5.168001453081767
        vf_explained_var: 0.9879141449928284
        vf_loss: 5.178456664085388
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.622580645161293
    gpu_util_percent0: 0.2919354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733017823554972
    mean_env_wait_ms: 1.2007760123198168
    mean_inference_ms: 4.391440140784344
    mean_raw_obs_processing_ms: 0.38211112495922983
  time_since_restore: 1289.5804135799408
  time_this_iter_s: 26.462011098861694
  time_total_s: 1289.5804135799408
  timers:
    learn_throughput: 8371.445
    learn_time_ms: 19326.651
    sample_throughput: 23452.446
    sample_time_ms: 6898.726
    update_time_ms: 36.963
  timestamp: 1602653905
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     49 |          1289.58 | 7927808 |   267.56 |              306.586 |              165.677 |            787.692 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3271.724764336214
    time_step_min: 3016
  date: 2020-10-14_05-38-52
  done: false
  episode_len_mean: 787.2959263611438
  episode_reward_max: 306.5858585858589
  episode_reward_mean: 267.90575555792947
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 257
  episodes_total: 10212
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4528145541747411
        entropy_coeff: 0.0005000000000000001
        kl: 0.004967670189216733
        model: {}
        policy_loss: -0.008880688847663501
        total_loss: 6.6970163981119795
        vf_explained_var: 0.9885924458503723
        vf_loss: 6.705875118573506
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82
    gpu_util_percent0: 0.32166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472883068756666
    mean_env_wait_ms: 1.2013130071778
    mean_inference_ms: 4.388539246922412
    mean_raw_obs_processing_ms: 0.3819709756975906
  time_since_restore: 1315.7247340679169
  time_this_iter_s: 26.144320487976074
  time_total_s: 1315.7247340679169
  timers:
    learn_throughput: 8370.874
    learn_time_ms: 19327.969
    sample_throughput: 23558.935
    sample_time_ms: 6867.543
    update_time_ms: 37.594
  timestamp: 1602653932
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     50 |          1315.72 | 8089600 |  267.906 |              306.586 |              165.677 |            787.296 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3269.6870192307692
    time_step_min: 3011
  date: 2020-10-14_05-39-18
  done: false
  episode_len_mean: 786.9913693901036
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 268.2156490102405
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 10428
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.43528473128875095
        entropy_coeff: 0.0005000000000000001
        kl: 0.005461744071605305
        model: {}
        policy_loss: -0.010802024335134774
        total_loss: 4.955641071001689
        vf_explained_var: 0.9899734854698181
        vf_loss: 4.966524203618367
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000002
    gpu_util_percent0: 0.3346666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472519185787302
    mean_env_wait_ms: 1.2017341332080647
    mean_inference_ms: 4.386208846047322
    mean_raw_obs_processing_ms: 0.38186067174619087
  time_since_restore: 1342.1837906837463
  time_this_iter_s: 26.459056615829468
  time_total_s: 1342.1837906837463
  timers:
    learn_throughput: 8380.186
    learn_time_ms: 19306.493
    sample_throughput: 23519.602
    sample_time_ms: 6879.028
    update_time_ms: 35.971
  timestamp: 1602653958
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     51 |          1342.18 | 8251392 |  268.216 |              307.343 |              165.677 |            786.991 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3268.3336490197935
    time_step_min: 3011
  date: 2020-10-14_05-39-45
  done: false
  episode_len_mean: 786.7751015396241
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 268.4333931551273
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 10587
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.454522043466568
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056253249834602075
        model: {}
        policy_loss: -0.010930568018617729
        total_loss: 5.377806742986043
        vf_explained_var: 0.9872221946716309
        vf_loss: 5.388823827107747
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.374193548387098
    gpu_util_percent0: 0.33064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722716301354427
    mean_env_wait_ms: 1.2020360258134917
    mean_inference_ms: 4.384558716761071
    mean_raw_obs_processing_ms: 0.3817826789762271
  time_since_restore: 1368.5846908092499
  time_this_iter_s: 26.40090012550354
  time_total_s: 1368.5846908092499
  timers:
    learn_throughput: 8385.959
    learn_time_ms: 19293.202
    sample_throughput: 23511.684
    sample_time_ms: 6881.345
    update_time_ms: 27.445
  timestamp: 1602653985
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     52 |          1368.58 | 8413184 |  268.433 |              307.343 |              165.677 |            786.775 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3266.329290872061
    time_step_min: 3011
  date: 2020-10-14_05-40-11
  done: false
  episode_len_mean: 786.4809787626962
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 268.7601919471726
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 243
  episodes_total: 10830
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4375423813859622
        entropy_coeff: 0.0005000000000000001
        kl: 0.005306682161365946
        model: {}
        policy_loss: -0.009265490055743916
        total_loss: 6.695862730344136
        vf_explained_var: 0.9884181618690491
        vf_loss: 6.7052143812179565
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.170000000000005
    gpu_util_percent0: 0.2823333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719176306373977
    mean_env_wait_ms: 1.2024810713162826
    mean_inference_ms: 4.382058375740324
    mean_raw_obs_processing_ms: 0.38166120412657245
  time_since_restore: 1394.7161531448364
  time_this_iter_s: 26.131462335586548
  time_total_s: 1394.7161531448364
  timers:
    learn_throughput: 8393.432
    learn_time_ms: 19276.023
    sample_throughput: 23560.642
    sample_time_ms: 6867.045
    update_time_ms: 25.565
  timestamp: 1602654011
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     53 |          1394.72 | 8574976 |   268.76 |              307.343 |              165.677 |            786.481 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3264.1519216823785
    time_step_min: 3011
  date: 2020-10-14_05-40-38
  done: false
  episode_len_mean: 786.3327305605786
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 269.0695837214824
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 230
  episodes_total: 11060
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4162285253405571
        entropy_coeff: 0.0005000000000000001
        kl: 0.005450062919408083
        model: {}
        policy_loss: -0.010840619617738412
        total_loss: 5.2562704881032305
        vf_explained_var: 0.9900563359260559
        vf_loss: 5.267183065414429
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.146666666666665
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715680138660897
    mean_env_wait_ms: 1.202884406789963
    mean_inference_ms: 4.379849732054512
    mean_raw_obs_processing_ms: 0.3815605327369565
  time_since_restore: 1420.8157141208649
  time_this_iter_s: 26.099560976028442
  time_total_s: 1420.8157141208649
  timers:
    learn_throughput: 8402.561
    learn_time_ms: 19255.083
    sample_throughput: 23515.658
    sample_time_ms: 6880.182
    update_time_ms: 24.158
  timestamp: 1602654038
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     54 |          1420.82 | 8736768 |   269.07 |              307.343 |              165.677 |            786.333 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3262.643731570012
    time_step_min: 3011
  date: 2020-10-14_05-41-04
  done: false
  episode_len_mean: 786.1614225866833
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 269.29395388955055
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 11219
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.432820700109005
        entropy_coeff: 0.0005000000000000001
        kl: 0.00562728460257252
        model: {}
        policy_loss: -0.010601595315771798
        total_loss: 5.018101851145427
        vf_explained_var: 0.9877817034721375
        vf_loss: 5.028779149055481
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.551612903225802
    gpu_util_percent0: 0.38064516129032255
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713419340728046
    mean_env_wait_ms: 1.2031495334669633
    mean_inference_ms: 4.378350653853634
    mean_raw_obs_processing_ms: 0.38149032891183743
  time_since_restore: 1447.0490758419037
  time_this_iter_s: 26.23336172103882
  time_total_s: 1447.0490758419037
  timers:
    learn_throughput: 8400.538
    learn_time_ms: 19259.718
    sample_throughput: 23511.993
    sample_time_ms: 6881.254
    update_time_ms: 23.723
  timestamp: 1602654064
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     55 |          1447.05 | 8898560 |  269.294 |              307.343 |              165.677 |            786.161 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3260.655247941125
    time_step_min: 3011
  date: 2020-10-14_05-41-31
  done: false
  episode_len_mean: 786.0182660374061
  episode_reward_max: 307.34343434343407
  episode_reward_mean: 269.5930278135312
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 11442
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4235244368513425
        entropy_coeff: 0.0005000000000000001
        kl: 0.005360846097270648
        model: {}
        policy_loss: -0.00987688044673026
        total_loss: 5.838384707768758
        vf_explained_var: 0.9893782138824463
        vf_loss: 5.848339041074117
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.622580645161285
    gpu_util_percent0: 0.32483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710495596665796
    mean_env_wait_ms: 1.2035096274673147
    mean_inference_ms: 4.376285410601281
    mean_raw_obs_processing_ms: 0.3813896063021229
  time_since_restore: 1473.573431968689
  time_this_iter_s: 26.52435612678528
  time_total_s: 1473.573431968689
  timers:
    learn_throughput: 8384.463
    learn_time_ms: 19296.645
    sample_throughput: 23497.72
    sample_time_ms: 6885.434
    update_time_ms: 24.95
  timestamp: 1602654091
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     56 |          1473.57 | 9060352 |  269.593 |              307.343 |              165.677 |            786.018 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3258.3082925992626
    time_step_min: 2985
  date: 2020-10-14_05-41-58
  done: false
  episode_len_mean: 785.8540508170074
  episode_reward_max: 311.2828282828276
  episode_reward_mean: 269.9349539539461
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 247
  episodes_total: 11689
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.39607641845941544
        entropy_coeff: 0.0005000000000000001
        kl: 0.005123959970660508
        model: {}
        policy_loss: -0.010596955481976996
        total_loss: 5.108719905217488
        vf_explained_var: 0.9907188415527344
        vf_loss: 5.119386752446492
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07
    gpu_util_percent0: 0.3233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707102146482853
    mean_env_wait_ms: 1.2038877017337877
    mean_inference_ms: 4.374097740295545
    mean_raw_obs_processing_ms: 0.38128996122840664
  time_since_restore: 1499.9742832183838
  time_this_iter_s: 26.400851249694824
  time_total_s: 1499.9742832183838
  timers:
    learn_throughput: 8387.903
    learn_time_ms: 19288.729
    sample_throughput: 23465.056
    sample_time_ms: 6895.019
    update_time_ms: 25.262
  timestamp: 1602654118
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     57 |          1499.97 | 9222144 |  269.935 |              311.283 |              165.677 |            785.854 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3256.762327666413
    time_step_min: 2979
  date: 2020-10-14_05-42-24
  done: false
  episode_len_mean: 785.7850814277276
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 270.16314269179003
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 11851
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4082030827800433
        entropy_coeff: 0.0005000000000000001
        kl: 0.005373570253141224
        model: {}
        policy_loss: -0.012292492187649865
        total_loss: 4.091778020064036
        vf_explained_var: 0.9902692437171936
        vf_loss: 4.104140281677246
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583870967741937
    gpu_util_percent0: 0.31999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470498212946755
    mean_env_wait_ms: 1.2041232980827907
    mean_inference_ms: 4.372704751122336
    mean_raw_obs_processing_ms: 0.381225453308251
  time_since_restore: 1526.0928332805634
  time_this_iter_s: 26.118550062179565
  time_total_s: 1526.0928332805634
  timers:
    learn_throughput: 8392.244
    learn_time_ms: 19278.753
    sample_throughput: 23451.664
    sample_time_ms: 6898.956
    update_time_ms: 25.261
  timestamp: 1602654144
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     58 |          1526.09 | 9383936 |  270.163 |              312.192 |              165.677 |            785.785 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3255.045117789062
    time_step_min: 2979
  date: 2020-10-14_05-42-51
  done: false
  episode_len_mean: 785.7113196578357
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 270.4221955456902
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 190
  episodes_total: 12041
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.40389055013656616
        entropy_coeff: 0.0005000000000000001
        kl: 0.005006047586599986
        model: {}
        policy_loss: -0.011286400248839831
        total_loss: 5.464943885803223
        vf_explained_var: 0.989168643951416
        vf_loss: 5.476307074228923
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690000000000005
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702499363960062
    mean_env_wait_ms: 1.2043913884290558
    mean_inference_ms: 4.371093760230965
    mean_raw_obs_processing_ms: 0.381147137109652
  time_since_restore: 1552.3649487495422
  time_this_iter_s: 26.272115468978882
  time_total_s: 1552.3649487495422
  timers:
    learn_throughput: 8393.969
    learn_time_ms: 19274.791
    sample_throughput: 23513.973
    sample_time_ms: 6880.675
    update_time_ms: 27.414
  timestamp: 1602654171
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     59 |          1552.36 | 9545728 |  270.422 |              312.192 |              165.677 |            785.711 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3252.57003257329
    time_step_min: 2979
  date: 2020-10-14_05-43-17
  done: false
  episode_len_mean: 785.6135034124147
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 270.78911145908216
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 267
  episodes_total: 12308
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.37611258774995804
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053737818185860915
        model: {}
        policy_loss: -0.009690653552146008
        total_loss: 5.3002186218897505
        vf_explained_var: 0.9910141825675964
        vf_loss: 5.30996310710907
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.083333333333336
    gpu_util_percent0: 0.29733333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699301415674818
    mean_env_wait_ms: 1.2047466768835953
    mean_inference_ms: 4.368900815679799
    mean_raw_obs_processing_ms: 0.3810501455584507
  time_since_restore: 1578.4703397750854
  time_this_iter_s: 26.105391025543213
  time_total_s: 1578.4703397750854
  timers:
    learn_throughput: 8395.279
    learn_time_ms: 19271.784
    sample_throughput: 23524.09
    sample_time_ms: 6877.716
    update_time_ms: 28.707
  timestamp: 1602654197
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     60 |          1578.47 | 9707520 |  270.789 |              312.192 |              165.677 |            785.614 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3250.9123976232536
    time_step_min: 2979
  date: 2020-10-14_05-43-44
  done: false
  episode_len_mean: 785.6085563211024
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 271.0354053271053
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 12482
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38262255241473514
        entropy_coeff: 0.0005000000000000001
        kl: 0.005104592729670306
        model: {}
        policy_loss: -0.010922989531536587
        total_loss: 4.422545830408732
        vf_explained_var: 0.9898379445075989
        vf_loss: 4.43353255589803
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.822580645161292
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7870967741935475
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697284701010066
    mean_env_wait_ms: 1.2049653875104303
    mean_inference_ms: 4.367545665427405
    mean_raw_obs_processing_ms: 0.3809880779908063
  time_since_restore: 1604.751181602478
  time_this_iter_s: 26.280841827392578
  time_total_s: 1604.751181602478
  timers:
    learn_throughput: 8390.671
    learn_time_ms: 19282.368
    sample_throughput: 23627.783
    sample_time_ms: 6847.532
    update_time_ms: 28.747
  timestamp: 1602654224
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     61 |          1604.75 | 9869312 |  271.035 |              312.192 |              165.677 |            785.609 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3249.362592093797
    time_step_min: 2979
  date: 2020-10-14_05-44-10
  done: false
  episode_len_mean: 785.619397676073
  episode_reward_max: 312.19191919191906
  episode_reward_mean: 271.2794149701904
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 169
  episodes_total: 12651
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38693956782420474
        entropy_coeff: 0.0005000000000000001
        kl: 0.005684521088066201
        model: {}
        policy_loss: -0.010501185975347957
        total_loss: 4.495320757230123
        vf_explained_var: 0.9902675747871399
        vf_loss: 4.505873401959737
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.770000000000003
    gpu_util_percent0: 0.3066666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469532060295318
    mean_env_wait_ms: 1.2051684410669554
    mean_inference_ms: 4.366224827813754
    mean_raw_obs_processing_ms: 0.38092721387884393
  time_since_restore: 1631.1027607917786
  time_this_iter_s: 26.351579189300537
  time_total_s: 1631.1027607917786
  timers:
    learn_throughput: 8383.057
    learn_time_ms: 19299.881
    sample_throughput: 23710.503
    sample_time_ms: 6823.643
    update_time_ms: 28.878
  timestamp: 1602654250
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     62 |           1631.1 | 10031104 |  271.279 |              312.192 |              165.677 |            785.619 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3247.032268073224
    time_step_min: 2942
  date: 2020-10-14_05-44-37
  done: false
  episode_len_mean: 785.6776315789474
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 271.6277832504612
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 269
  episodes_total: 12920
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.36051372687021893
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046567306853830814
        model: {}
        policy_loss: -0.01011651087901555
        total_loss: 5.73840069770813
        vf_explained_var: 0.9904703497886658
        vf_loss: 5.748581091562907
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.825806451612905
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146923376392046
    mean_env_wait_ms: 1.205471404256596
    mean_inference_ms: 4.36421156007573
    mean_raw_obs_processing_ms: 0.3808344205369545
  time_since_restore: 1657.3763408660889
  time_this_iter_s: 26.273580074310303
  time_total_s: 1657.3763408660889
  timers:
    learn_throughput: 8381.09
    learn_time_ms: 19304.409
    sample_throughput: 23680.564
    sample_time_ms: 6832.27
    update_time_ms: 29.197
  timestamp: 1602654277
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     63 |          1657.38 | 10192896 |  271.628 |              317.798 |              165.677 |            785.678 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3245.41433593153
    time_step_min: 2942
  date: 2020-10-14_05-45-03
  done: false
  episode_len_mean: 785.8016623455849
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 271.8679767015896
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 194
  episodes_total: 13114
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.35336187730232876
        entropy_coeff: 0.0005000000000000001
        kl: 0.005059316637925804
        model: {}
        policy_loss: -0.008540955333349606
        total_loss: 4.587351322174072
        vf_explained_var: 0.9907009601593018
        vf_loss: 4.596005876859029
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.613333333333333
    gpu_util_percent0: 0.2976666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690254204625156
    mean_env_wait_ms: 1.205676849597817
    mean_inference_ms: 4.3628106987464115
    mean_raw_obs_processing_ms: 0.3807750161163913
  time_since_restore: 1683.603833436966
  time_this_iter_s: 26.227492570877075
  time_total_s: 1683.603833436966
  timers:
    learn_throughput: 8372.092
    learn_time_ms: 19325.157
    sample_throughput: 23717.306
    sample_time_ms: 6821.685
    update_time_ms: 30.561
  timestamp: 1602654303
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     64 |           1683.6 | 10354688 |  271.868 |              317.798 |              165.677 |            785.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3243.9032087580217
    time_step_min: 2942
  date: 2020-10-14_05-45-30
  done: false
  episode_len_mean: 785.8371129360355
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.09423855065376
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 159
  episodes_total: 13273
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3649902318914731
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056065128883346915
        model: {}
        policy_loss: -0.012942626276829591
        total_loss: 3.630454699198405
        vf_explained_var: 0.9914636611938477
        vf_loss: 3.6435096859931946
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36774193548387
    gpu_util_percent0: 0.314516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468859717498133
    mean_env_wait_ms: 1.2058359140391997
    mean_inference_ms: 4.361695029332891
    mean_raw_obs_processing_ms: 0.3807254263151171
  time_since_restore: 1710.1077604293823
  time_this_iter_s: 26.503926992416382
  time_total_s: 1710.1077604293823
  timers:
    learn_throughput: 8369.131
    learn_time_ms: 19331.995
    sample_throughput: 23656.84
    sample_time_ms: 6839.122
    update_time_ms: 31.935
  timestamp: 1602654330
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     65 |          1710.11 | 10516480 |  272.094 |              317.798 |              165.677 |            785.837 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3241.8498035436282
    time_step_min: 2942
  date: 2020-10-14_05-45-57
  done: false
  episode_len_mean: 785.8663904712585
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.41918631457725
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 244
  episodes_total: 13517
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.35202441612879437
        entropy_coeff: 0.0005000000000000001
        kl: 0.005318918381817639
        model: {}
        policy_loss: -0.008715767316364994
        total_loss: 4.88840115070343
        vf_explained_var: 0.9913294911384583
        vf_loss: 4.897226373354594
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.513333333333332
    gpu_util_percent0: 0.27899999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468615903975649
    mean_env_wait_ms: 1.206065080572647
    mean_inference_ms: 4.360013789486395
    mean_raw_obs_processing_ms: 0.3806449071434777
  time_since_restore: 1736.3365361690521
  time_this_iter_s: 26.2287757396698
  time_total_s: 1736.3365361690521
  timers:
    learn_throughput: 8376.131
    learn_time_ms: 19315.838
    sample_throughput: 23706.31
    sample_time_ms: 6824.849
    update_time_ms: 30.677
  timestamp: 1602654357
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     66 |          1736.34 | 10678272 |  272.419 |              317.798 |              165.677 |            785.866 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3240.02428529755
    time_step_min: 2942
  date: 2020-10-14_05-46-23
  done: false
  episode_len_mean: 785.9453420669578
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.7044351815094
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 13740
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.33126793801784515
        entropy_coeff: 0.0005000000000000001
        kl: 0.00451814573413382
        model: {}
        policy_loss: -0.010082538991506832
        total_loss: 4.8893570105234785
        vf_explained_var: 0.991076648235321
        vf_loss: 4.899548768997192
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.190322580645166
    gpu_util_percent0: 0.36193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683932761058996
    mean_env_wait_ms: 1.206264111625232
    mean_inference_ms: 4.358523033204477
    mean_raw_obs_processing_ms: 0.38058322504096204
  time_since_restore: 1762.5870475769043
  time_this_iter_s: 26.250511407852173
  time_total_s: 1762.5870475769043
  timers:
    learn_throughput: 8380.421
    learn_time_ms: 19305.952
    sample_throughput: 23722.022
    sample_time_ms: 6820.329
    update_time_ms: 29.467
  timestamp: 1602654383
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     67 |          1762.59 | 10840064 |  272.704 |              317.798 |              165.677 |            785.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3238.678869991352
    time_step_min: 2942
  date: 2020-10-14_05-46-50
  done: false
  episode_len_mean: 785.9606588032221
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 272.9143186758261
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 164
  episodes_total: 13904
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.34320970873037976
        entropy_coeff: 0.0005000000000000001
        kl: 0.005112749873660505
        model: {}
        policy_loss: -0.01100804218246291
        total_loss: 3.653915266195933
        vf_explained_var: 0.9914707541465759
        vf_loss: 3.665062963962555
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.19333333333333
    gpu_util_percent0: 0.34333333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468234333563059
    mean_env_wait_ms: 1.2063977758842375
    mean_inference_ms: 4.357472578542846
    mean_raw_obs_processing_ms: 0.3805364266754847
  time_since_restore: 1788.8891973495483
  time_this_iter_s: 26.302149772644043
  time_total_s: 1788.8891973495483
  timers:
    learn_throughput: 8369.731
    learn_time_ms: 19330.61
    sample_throughput: 23746.704
    sample_time_ms: 6813.24
    update_time_ms: 29.35
  timestamp: 1602654410
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     68 |          1788.89 | 11001856 |  272.914 |              317.798 |              165.677 |            785.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3236.9997868561277
    time_step_min: 2942
  date: 2020-10-14_05-47-16
  done: false
  episode_len_mean: 785.9723463092959
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.1579612332643
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 14103
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.34640548129876453
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053304360480979085
        model: {}
        policy_loss: -0.009727492325813122
        total_loss: 5.206940253575643
        vf_explained_var: 0.9899138808250427
        vf_loss: 5.21680764357249
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.325806451612905
    gpu_util_percent0: 0.29419354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680375589049743
    mean_env_wait_ms: 1.2065545668181696
    mean_inference_ms: 4.356198686716671
    mean_raw_obs_processing_ms: 0.3804783575405877
  time_since_restore: 1815.1527166366577
  time_this_iter_s: 26.263519287109375
  time_total_s: 1815.1527166366577
  timers:
    learn_throughput: 8378.193
    learn_time_ms: 19311.085
    sample_throughput: 23678.37
    sample_time_ms: 6832.903
    update_time_ms: 27.59
  timestamp: 1602654436
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     69 |          1815.15 | 11163648 |  273.158 |              317.798 |              165.677 |            785.972 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3234.9337891578875
    time_step_min: 2942
  date: 2020-10-14_05-47-43
  done: false
  episode_len_mean: 786.0126035791379
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.47167940107147
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 258
  episodes_total: 14361
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.31610632687807083
        entropy_coeff: 0.0005000000000000001
        kl: 0.005180867893310885
        model: {}
        policy_loss: -0.009844114615892371
        total_loss: 4.691938559214274
        vf_explained_var: 0.9920149445533752
        vf_loss: 4.701908349990845
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.348387096774193
    gpu_util_percent0: 0.32935483870967747
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7999999999999994
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678089930536167
    mean_env_wait_ms: 1.206744313108156
    mean_inference_ms: 4.354630809580834
    mean_raw_obs_processing_ms: 0.3804070207492199
  time_since_restore: 1841.5785646438599
  time_this_iter_s: 26.42584800720215
  time_total_s: 1841.5785646438599
  timers:
    learn_throughput: 8370.322
    learn_time_ms: 19329.244
    sample_throughput: 23627.175
    sample_time_ms: 6847.708
    update_time_ms: 25.438
  timestamp: 1602654463
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     70 |          1841.58 | 11325440 |  273.472 |              317.798 |              165.677 |            786.013 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3233.61435070306
    time_step_min: 2942
  date: 2020-10-14_05-48-10
  done: false
  episode_len_mean: 786.0021326362136
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.68195021208226
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 175
  episodes_total: 14536
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3203122367461522
        entropy_coeff: 0.0005000000000000001
        kl: 0.004666645700732867
        model: {}
        policy_loss: -0.01171706517440422
        total_loss: 3.8123207092285156
        vf_explained_var: 0.9915096759796143
        vf_loss: 3.824168781439463
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703225806451613
    gpu_util_percent0: 0.27129032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467655613512355
    mean_env_wait_ms: 1.2068633979374035
    mean_inference_ms: 4.353615612052208
    mean_raw_obs_processing_ms: 0.3803599267933382
  time_since_restore: 1867.8969566822052
  time_this_iter_s: 26.318392038345337
  time_total_s: 1867.8969566822052
  timers:
    learn_throughput: 8379.146
    learn_time_ms: 19308.889
    sample_throughput: 23547.191
    sample_time_ms: 6870.968
    update_time_ms: 25.491
  timestamp: 1602654490
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     71 |           1867.9 | 11487232 |  273.682 |              317.798 |              165.677 |            786.002 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3232.17431880109
    time_step_min: 2942
  date: 2020-10-14_05-48-37
  done: false
  episode_len_mean: 785.9649850421539
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 273.89390986283826
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 172
  episodes_total: 14708
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3309631322820981
        entropy_coeff: 0.0005000000000000001
        kl: 0.004985143624556561
        model: {}
        policy_loss: -0.010613858525175601
        total_loss: 3.6648205518722534
        vf_explained_var: 0.9920353889465332
        vf_loss: 3.675584355990092
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.366666666666667
    gpu_util_percent0: 0.329
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675062999554825
    mean_env_wait_ms: 1.206974922729365
    mean_inference_ms: 4.352615553496095
    mean_raw_obs_processing_ms: 0.3803138781220055
  time_since_restore: 1894.1803233623505
  time_this_iter_s: 26.283366680145264
  time_total_s: 1894.1803233623505
  timers:
    learn_throughput: 8378.876
    learn_time_ms: 19309.512
    sample_throughput: 23575.174
    sample_time_ms: 6862.813
    update_time_ms: 25.294
  timestamp: 1602654517
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     72 |          1894.18 | 11649024 |  273.894 |              317.798 |              165.677 |            785.965 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3230.119863472092
    time_step_min: 2942
  date: 2020-10-14_05-49-03
  done: false
  episode_len_mean: 785.9823647294589
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.20767798222704
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 262
  episodes_total: 14970
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3055466338992119
        entropy_coeff: 0.0005000000000000001
        kl: 0.00490261847153306
        model: {}
        policy_loss: -0.008200637511132905
        total_loss: 4.579335172971089
        vf_explained_var: 0.992362916469574
        vf_loss: 4.587680856386821
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.393548387096775
    gpu_util_percent0: 0.35064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672782122884798
    mean_env_wait_ms: 1.2071358619458468
    mean_inference_ms: 4.3511163597621065
    mean_raw_obs_processing_ms: 0.38024098997761624
  time_since_restore: 1920.4730758666992
  time_this_iter_s: 26.292752504348755
  time_total_s: 1920.4730758666992
  timers:
    learn_throughput: 8376.13
    learn_time_ms: 19315.841
    sample_throughput: 23591.576
    sample_time_ms: 6858.041
    update_time_ms: 24.977
  timestamp: 1602654543
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     73 |          1920.47 | 11810816 |  274.208 |              317.798 |              165.677 |            785.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3228.4809114927343
    time_step_min: 2942
  date: 2020-10-14_05-49-30
  done: false
  episode_len_mean: 785.9644646624473
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.45477720240376
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 198
  episodes_total: 15168
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.2969528113802274
        entropy_coeff: 0.0005000000000000001
        kl: 0.005436556258549293
        model: {}
        policy_loss: -0.009357313266567266
        total_loss: 3.4393052458763123
        vf_explained_var: 0.9927964806556702
        vf_loss: 3.4488067428270974
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.356666666666673
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671200797570194
    mean_env_wait_ms: 1.207253573582583
    mean_inference_ms: 4.350062969840952
    mean_raw_obs_processing_ms: 0.38019264643451484
  time_since_restore: 1946.6698021888733
  time_this_iter_s: 26.196726322174072
  time_total_s: 1946.6698021888733
  timers:
    learn_throughput: 8374.151
    learn_time_ms: 19320.406
    sample_throughput: 23621.404
    sample_time_ms: 6849.381
    update_time_ms: 25.378
  timestamp: 1602654570
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     74 |          1946.67 | 11972608 |  274.455 |              317.798 |              165.677 |            785.964 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3227.1255309416456
    time_step_min: 2942
  date: 2020-10-14_05-49-56
  done: false
  episode_len_mean: 785.9414910964712
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.6555747284336
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 163
  episodes_total: 15331
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.30914776027202606
        entropy_coeff: 0.0005000000000000001
        kl: 0.00479302101302892
        model: {}
        policy_loss: -0.010842657589819282
        total_loss: 3.573561429977417
        vf_explained_var: 0.991995096206665
        vf_loss: 3.5845548113187156
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60645161290323
    gpu_util_percent0: 0.31258064516129036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669892801088777
    mean_env_wait_ms: 1.207342534604058
    mean_inference_ms: 4.349181380191991
    mean_raw_obs_processing_ms: 0.38015012347953747
  time_since_restore: 1973.1083779335022
  time_this_iter_s: 26.438575744628906
  time_total_s: 1973.1083779335022
  timers:
    learn_throughput: 8372.0
    learn_time_ms: 19325.371
    sample_throughput: 23667.465
    sample_time_ms: 6836.051
    update_time_ms: 26.204
  timestamp: 1602654596
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     75 |          1973.11 | 12134400 |  274.656 |              317.798 |              165.677 |            785.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3225.0863776691535
    time_step_min: 2942
  date: 2020-10-14_05-50-23
  done: false
  episode_len_mean: 785.8928479712378
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 274.96832539571363
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 245
  episodes_total: 15576
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.2980467254916827
        entropy_coeff: 0.0005000000000000001
        kl: 0.005235202998543779
        model: {}
        policy_loss: -0.008933737524785101
        total_loss: 5.186015446980794
        vf_explained_var: 0.9907950758934021
        vf_loss: 5.195096095403035
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.81666666666667
    gpu_util_percent0: 0.3196666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667941789319203
    mean_env_wait_ms: 1.207469351927622
    mean_inference_ms: 4.347910922958269
    mean_raw_obs_processing_ms: 0.3800875324920706
  time_since_restore: 1999.2511112689972
  time_this_iter_s: 26.142733335494995
  time_total_s: 1999.2511112689972
  timers:
    learn_throughput: 8377.833
    learn_time_ms: 19311.915
    sample_throughput: 23649.612
    sample_time_ms: 6841.212
    update_time_ms: 26.719
  timestamp: 1602654623
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     76 |          1999.25 | 12296192 |  274.968 |              317.798 |              165.677 |            785.893 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3223.327393785669
    time_step_min: 2942
  date: 2020-10-14_05-50-50
  done: false
  episode_len_mean: 785.8466894543614
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.25340184986976
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 222
  episodes_total: 15798
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.27664656440416974
        entropy_coeff: 0.0005000000000000001
        kl: 0.004761220188811421
        model: {}
        policy_loss: -0.007907829436589964
        total_loss: 2.923918386300405
        vf_explained_var: 0.9942214488983154
        vf_loss: 2.931962788105011
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27741935483871
    gpu_util_percent0: 0.38032258064516133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666215503763455
    mean_env_wait_ms: 1.2075740570976705
    mean_inference_ms: 4.34677412469158
    mean_raw_obs_processing_ms: 0.3800326885536981
  time_since_restore: 2025.5834021568298
  time_this_iter_s: 26.33229088783264
  time_total_s: 2025.5834021568298
  timers:
    learn_throughput: 8374.185
    learn_time_ms: 19320.328
    sample_throughput: 23659.892
    sample_time_ms: 6838.239
    update_time_ms: 29.12
  timestamp: 1602654650
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     77 |          2025.58 | 12457984 |  275.253 |              317.798 |              165.677 |            785.847 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3221.937609841828
    time_step_min: 2942
  date: 2020-10-14_05-51-16
  done: false
  episode_len_mean: 785.8052631578947
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.4586750968329
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 162
  episodes_total: 15960
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.290430227915446
        entropy_coeff: 0.0005000000000000001
        kl: 0.004963470622897148
        model: {}
        policy_loss: -0.011461609130492434
        total_loss: 3.2642138799031577
        vf_explained_var: 0.9922669529914856
        vf_loss: 3.2758195996284485
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.343333333333337
    gpu_util_percent0: 0.33333333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146649991691755
    mean_env_wait_ms: 1.2076454657019677
    mean_inference_ms: 4.345969574600448
    mean_raw_obs_processing_ms: 0.3799928515376835
  time_since_restore: 2051.79900097847
  time_this_iter_s: 26.215598821640015
  time_total_s: 2051.79900097847
  timers:
    learn_throughput: 8376.461
    learn_time_ms: 19315.078
    sample_throughput: 23677.498
    sample_time_ms: 6833.154
    update_time_ms: 29.66
  timestamp: 1602654676
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     78 |           2051.8 | 12619776 |  275.459 |              317.798 |              165.677 |            785.805 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3220.2338080495356
    time_step_min: 2942
  date: 2020-10-14_05-51-43
  done: false
  episode_len_mean: 785.7675856100877
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.70733668743304
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 218
  episodes_total: 16178
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2907758379975955
        entropy_coeff: 0.0005000000000000001
        kl: 0.004659340716898441
        model: {}
        policy_loss: -0.01010728104544493
        total_loss: 4.916222969690959
        vf_explained_var: 0.9909214377403259
        vf_loss: 4.926475167274475
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980645161290326
    gpu_util_percent0: 0.3683870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663394529686158
    mean_env_wait_ms: 1.2077396368844608
    mean_inference_ms: 4.344892610634557
    mean_raw_obs_processing_ms: 0.379939228252742
  time_since_restore: 2078.1762402057648
  time_this_iter_s: 26.377239227294922
  time_total_s: 2078.1762402057648
  timers:
    learn_throughput: 8364.481
    learn_time_ms: 19342.743
    sample_throughput: 23740.001
    sample_time_ms: 6815.164
    update_time_ms: 30.278
  timestamp: 1602654703
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     79 |          2078.18 | 12781568 |  275.707 |              317.798 |              165.677 |            785.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3218.2997620935766
    time_step_min: 2942
  date: 2020-10-14_05-52-10
  done: false
  episode_len_mean: 785.7324767066561
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 275.9981687651744
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 243
  episodes_total: 16421
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.26534537474314374
        entropy_coeff: 0.0005000000000000001
        kl: 0.005787815627021094
        model: {}
        policy_loss: -0.008603085453311602
        total_loss: 3.8923116525014243
        vf_explained_var: 0.9929180145263672
        vf_loss: 3.9010471304257712
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.712903225806457
    gpu_util_percent0: 0.29354838709677417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466163825697337
    mean_env_wait_ms: 1.2078354536504632
    mean_inference_ms: 4.343757174416379
    mean_raw_obs_processing_ms: 0.37988264176460357
  time_since_restore: 2104.627058982849
  time_this_iter_s: 26.45081877708435
  time_total_s: 2104.627058982849
  timers:
    learn_throughput: 8364.988
    learn_time_ms: 19341.569
    sample_throughput: 23740.117
    sample_time_ms: 6815.131
    update_time_ms: 31.92
  timestamp: 1602654730
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     80 |          2104.63 | 12943360 |  275.998 |              317.798 |              165.677 |            785.732 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3216.9125762241138
    time_step_min: 2942
  date: 2020-10-14_05-52-36
  done: false
  episode_len_mean: 785.7410041588813
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.2069930819252
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 170
  episodes_total: 16591
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2716033384203911
        entropy_coeff: 0.0005000000000000001
        kl: 0.004915759743501742
        model: {}
        policy_loss: -0.009365745064618144
        total_loss: 3.0018479426701865
        vf_explained_var: 0.9930481910705566
        vf_loss: 3.0113492012023926
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.649999999999995
    gpu_util_percent0: 0.2856666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466045984038076
    mean_env_wait_ms: 1.2078951488092793
    mean_inference_ms: 4.342959930498004
    mean_raw_obs_processing_ms: 0.37984374872943544
  time_since_restore: 2130.533602952957
  time_this_iter_s: 25.906543970108032
  time_total_s: 2130.533602952957
  timers:
    learn_throughput: 8372.709
    learn_time_ms: 19323.734
    sample_throughput: 23830.718
    sample_time_ms: 6789.221
    update_time_ms: 33.431
  timestamp: 1602654756
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     81 |          2130.53 | 13105152 |  276.207 |              317.798 |              165.677 |            785.741 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3215.4304046794796
    time_step_min: 2942
  date: 2020-10-14_05-53-02
  done: false
  episode_len_mean: 785.725181742343
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.43181547328845
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 16782
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2780342971285184
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047616838322331505
        model: {}
        policy_loss: -0.009352372610010207
        total_loss: 4.1234805782636
        vf_explained_var: 0.9917870163917542
        vf_loss: 4.132971882820129
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.303333333333338
    gpu_util_percent0: 0.26599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659141071533316
    mean_env_wait_ms: 1.207957623264003
    mean_inference_ms: 4.342077070457794
    mean_raw_obs_processing_ms: 0.3797990036613627
  time_since_restore: 2156.6429176330566
  time_this_iter_s: 26.109314680099487
  time_total_s: 2156.6429176330566
  timers:
    learn_throughput: 8380.337
    learn_time_ms: 19306.144
    sample_throughput: 23834.668
    sample_time_ms: 6788.095
    update_time_ms: 33.982
  timestamp: 1602654782
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     82 |          2156.64 | 13266944 |  276.432 |              317.798 |              165.677 |            785.725 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3213.4538294245576
    time_step_min: 2942
  date: 2020-10-14_05-53-29
  done: false
  episode_len_mean: 785.7449680183088
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.734377398775
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 259
  episodes_total: 17041
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2593287130196889
        entropy_coeff: 0.0005000000000000001
        kl: 0.004659708434094985
        model: {}
        policy_loss: -0.008873817008861806
        total_loss: 4.259411493937175
        vf_explained_var: 0.9925722479820251
        vf_loss: 4.268415053685506
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.577419354838714
    gpu_util_percent0: 0.3293548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146573644888884
    mean_env_wait_ms: 1.2080415206798174
    mean_inference_ms: 4.340929237306863
    mean_raw_obs_processing_ms: 0.3797414528827236
  time_since_restore: 2183.0458402633667
  time_this_iter_s: 26.40292263031006
  time_total_s: 2183.0458402633667
  timers:
    learn_throughput: 8375.802
    learn_time_ms: 19316.598
    sample_throughput: 23838.929
    sample_time_ms: 6786.882
    update_time_ms: 34.308
  timestamp: 1602654809
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     83 |          2183.05 | 13428736 |  276.734 |              317.798 |              165.677 |            785.745 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3212.0624636501107
    time_step_min: 2942
  date: 2020-10-14_05-53-55
  done: false
  episode_len_mean: 785.7514806642666
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 276.94040861524303
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 17222
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.2544606688121955
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049046853091567755
        model: {}
        policy_loss: -0.00876463590914985
        total_loss: 3.397805949052175
        vf_explained_var: 0.9926281571388245
        vf_loss: 3.4066977898279824
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.24
    gpu_util_percent0: 0.2813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656194955789117
    mean_env_wait_ms: 1.20809375076207
    mean_inference_ms: 4.340145234778413
    mean_raw_obs_processing_ms: 0.37970431481200567
  time_since_restore: 2209.1496114730835
  time_this_iter_s: 26.103771209716797
  time_total_s: 2209.1496114730835
  timers:
    learn_throughput: 8382.91
    learn_time_ms: 19300.22
    sample_throughput: 23813.046
    sample_time_ms: 6794.259
    update_time_ms: 32.336
  timestamp: 1602654835
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     84 |          2209.15 | 13590528 |   276.94 |              317.798 |              165.677 |            785.751 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3210.8620630075447
    time_step_min: 2942
  date: 2020-10-14_05-54-22
  done: false
  episode_len_mean: 785.7808636651141
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.12649872887914
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 169
  episodes_total: 17391
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2648752157886823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005617462059793373
        model: {}
        policy_loss: -0.009434527669024343
        total_loss: 3.6594002644220986
        vf_explained_var: 0.9920105934143066
        vf_loss: 3.6689672072728476
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796774193548387
    gpu_util_percent0: 0.3635483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465509288908674
    mean_env_wait_ms: 1.2081381748952567
    mean_inference_ms: 4.33940812099314
    mean_raw_obs_processing_ms: 0.3796678144405575
  time_since_restore: 2235.3111848831177
  time_this_iter_s: 26.16157341003418
  time_total_s: 2235.3111848831177
  timers:
    learn_throughput: 8387.562
    learn_time_ms: 19289.514
    sample_throughput: 23866.244
    sample_time_ms: 6779.115
    update_time_ms: 29.414
  timestamp: 1602654862
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     85 |          2235.31 | 13752320 |  277.126 |              317.798 |              165.677 |            785.781 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3209.1355710796843
    time_step_min: 2942
  date: 2020-10-14_05-54-49
  done: false
  episode_len_mean: 785.8791607598525
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.39138478633794
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 244
  episodes_total: 17635
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2523770493765672
        entropy_coeff: 0.0005000000000000001
        kl: 0.004732664519300063
        model: {}
        policy_loss: -0.009494934978041178
        total_loss: 3.6212570667266846
        vf_explained_var: 0.993767499923706
        vf_loss: 3.630878190199534
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.203333333333337
    gpu_util_percent0: 0.30233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653551756417232
    mean_env_wait_ms: 1.2081983462550667
    mean_inference_ms: 4.338382823780063
    mean_raw_obs_processing_ms: 0.37961572254694137
  time_since_restore: 2261.5944368839264
  time_this_iter_s: 26.283252000808716
  time_total_s: 2261.5944368839264
  timers:
    learn_throughput: 8385.447
    learn_time_ms: 19294.38
    sample_throughput: 23842.336
    sample_time_ms: 6785.912
    update_time_ms: 30.871
  timestamp: 1602654889
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     86 |          2261.59 | 13914112 |  277.391 |              317.798 |              165.677 |            785.879 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3207.5718453683444
    time_step_min: 2942
  date: 2020-10-14_05-55-15
  done: false
  episode_len_mean: 785.9679009579295
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.62622980689184
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 216
  episodes_total: 17851
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.23134482031067213
        entropy_coeff: 0.0005000000000000001
        kl: 0.00436233247940739
        model: {}
        policy_loss: -0.009052471694303676
        total_loss: 2.8776000340779624
        vf_explained_var: 0.9944209456443787
        vf_loss: 2.886768182118734
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.793548387096774
    gpu_util_percent0: 0.3332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465221648747247
    mean_env_wait_ms: 1.2082401855995044
    mean_inference_ms: 4.337512785426937
    mean_raw_obs_processing_ms: 0.3795743211509042
  time_since_restore: 2287.9621074199677
  time_this_iter_s: 26.36767053604126
  time_total_s: 2287.9621074199677
  timers:
    learn_throughput: 8390.247
    learn_time_ms: 19283.341
    sample_throughput: 23789.084
    sample_time_ms: 6801.102
    update_time_ms: 29.246
  timestamp: 1602654915
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     87 |          2287.96 | 14075904 |  277.626 |              317.798 |              165.677 |            785.968 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3206.3540693795862
    time_step_min: 2942
  date: 2020-10-14_05-55-42
  done: false
  episode_len_mean: 786.028696714032
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 277.8067010020273
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 165
  episodes_total: 18016
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.24300405383110046
        entropy_coeff: 0.0005000000000000001
        kl: 0.004606607835739851
        model: {}
        policy_loss: -0.0074194288851382835
        total_loss: 2.8293345173199973
        vf_explained_var: 0.9936473369598389
        vf_loss: 2.8368754188219705
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.093333333333337
    gpu_util_percent0: 0.2993333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651227896290492
    mean_env_wait_ms: 1.2082685719191104
    mean_inference_ms: 4.336852883930542
    mean_raw_obs_processing_ms: 0.3795419624675235
  time_since_restore: 2314.274481534958
  time_this_iter_s: 26.312374114990234
  time_total_s: 2314.274481534958
  timers:
    learn_throughput: 8389.887
    learn_time_ms: 19284.169
    sample_throughput: 23765.548
    sample_time_ms: 6807.838
    update_time_ms: 30.71
  timestamp: 1602654942
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     88 |          2314.27 | 14237696 |  277.807 |              317.798 |              165.677 |            786.029 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3204.880536322673
    time_step_min: 2942
  date: 2020-10-14_05-56-09
  done: false
  episode_len_mean: 786.1340941512126
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.02559336368176
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 210
  episodes_total: 18226
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.24540683875481287
        entropy_coeff: 0.0005000000000000001
        kl: 0.004586299997754395
        model: {}
        policy_loss: -0.01081360654400972
        total_loss: 3.405385891596476
        vf_explained_var: 0.9936063289642334
        vf_loss: 3.4163222511609397
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216129032258067
    gpu_util_percent0: 0.38193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650034622692418
    mean_env_wait_ms: 1.2083048870612831
    mean_inference_ms: 4.336047269432526
    mean_raw_obs_processing_ms: 0.3794998035257537
  time_since_restore: 2340.8545854091644
  time_this_iter_s: 26.580103874206543
  time_total_s: 2340.8545854091644
  timers:
    learn_throughput: 8386.816
    learn_time_ms: 19291.23
    sample_throughput: 23724.2
    sample_time_ms: 6819.703
    update_time_ms: 31.687
  timestamp: 1602654969
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     89 |          2340.85 | 14399488 |  278.026 |              317.798 |              165.677 |            786.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3203.2486712224754
    time_step_min: 2942
  date: 2020-10-14_05-56-35
  done: false
  episode_len_mean: 786.212444492581
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.2610011082338
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 240
  episodes_total: 18466
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.22144933665792146
        entropy_coeff: 0.0005000000000000001
        kl: 0.004486944526433945
        model: {}
        policy_loss: -0.007682374775564919
        total_loss: 3.3583309253056846
        vf_explained_var: 0.9941323399543762
        vf_loss: 3.366123914718628
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.941935483870974
    gpu_util_percent0: 0.28387096774193543
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648592409390737
    mean_env_wait_ms: 1.2083337767237718
    mean_inference_ms: 4.335113369057025
    mean_raw_obs_processing_ms: 0.37945283902956684
  time_since_restore: 2366.9686341285706
  time_this_iter_s: 26.114048719406128
  time_total_s: 2366.9686341285706
  timers:
    learn_throughput: 8396.207
    learn_time_ms: 19269.653
    sample_throughput: 23765.889
    sample_time_ms: 6807.74
    update_time_ms: 31.915
  timestamp: 1602654995
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     90 |          2366.97 | 14561280 |  278.261 |              317.798 |              165.677 |            786.212 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3202.0811667382895
    time_step_min: 2942
  date: 2020-10-14_05-57-02
  done: false
  episode_len_mean: 786.2794464707144
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.43792895702353
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 18644
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.22619690001010895
        entropy_coeff: 0.0005000000000000001
        kl: 0.004538902974066635
        model: {}
        policy_loss: -0.009121924629046893
        total_loss: 3.2765854597091675
        vf_explained_var: 0.9929046630859375
        vf_loss: 3.285820504029592
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.72666666666667
    gpu_util_percent0: 0.37600000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647575525732714
    mean_env_wait_ms: 1.2083527449195028
    mean_inference_ms: 4.334439851243566
    mean_raw_obs_processing_ms: 0.3794201216996983
  time_since_restore: 2393.3836238384247
  time_this_iter_s: 26.414989709854126
  time_total_s: 2393.3836238384247
  timers:
    learn_throughput: 8373.084
    learn_time_ms: 19322.869
    sample_throughput: 23778.117
    sample_time_ms: 6804.239
    update_time_ms: 32.383
  timestamp: 1602655022
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     91 |          2393.38 | 14723072 |  278.438 |              317.798 |              165.677 |            786.279 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3200.830274009045
    time_step_min: 2942
  date: 2020-10-14_05-57-29
  done: false
  episode_len_mean: 786.3159963873984
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.6219153764709
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 179
  episodes_total: 18823
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2380774455765883
        entropy_coeff: 0.0005000000000000001
        kl: 0.004809639339024822
        model: {}
        policy_loss: -0.00822032520469899
        total_loss: 3.004819949467977
        vf_explained_var: 0.9936773777008057
        vf_loss: 3.013159394264221
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.31612903225807
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646628604898604
    mean_env_wait_ms: 1.2083687225375017
    mean_inference_ms: 4.3337579199298215
    mean_raw_obs_processing_ms: 0.3793858135806802
  time_since_restore: 2419.590413093567
  time_this_iter_s: 26.206789255142212
  time_total_s: 2419.590413093567
  timers:
    learn_throughput: 8371.772
    learn_time_ms: 19325.897
    sample_throughput: 23759.002
    sample_time_ms: 6809.714
    update_time_ms: 32.077
  timestamp: 1602655049
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     92 |          2419.59 | 14884864 |  278.622 |              317.798 |              165.677 |            786.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3199.0758649656113
    time_step_min: 2942
  date: 2020-10-14_05-57-55
  done: false
  episode_len_mean: 786.3570642201835
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 278.8779909183578
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 252
  episodes_total: 19075
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.22229414929946265
        entropy_coeff: 0.0005000000000000001
        kl: 0.004338357403563957
        model: {}
        policy_loss: -0.007636490821217497
        total_loss: 3.7447853287061057
        vf_explained_var: 0.9936156868934631
        vf_loss: 3.752533038457235
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64838709677419
    gpu_util_percent0: 0.2664516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464515315379992
    mean_env_wait_ms: 1.208386809956172
    mean_inference_ms: 4.33285341580754
    mean_raw_obs_processing_ms: 0.37933863506368687
  time_since_restore: 2445.8738775253296
  time_this_iter_s: 26.283464431762695
  time_total_s: 2445.8738775253296
  timers:
    learn_throughput: 8375.203
    learn_time_ms: 19317.978
    sample_throughput: 23779.476
    sample_time_ms: 6803.851
    update_time_ms: 33.896
  timestamp: 1602655075
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     93 |          2445.87 | 15046656 |  278.878 |              317.798 |              165.677 |            786.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3197.790709757872
    time_step_min: 2942
  date: 2020-10-14_05-58-22
  done: false
  episode_len_mean: 786.3781259728131
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 279.0753875792269
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 19274
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.21226180344820023
        entropy_coeff: 0.0005000000000000001
        kl: 0.004013098815145592
        model: {}
        policy_loss: -0.00900954079982815
        total_loss: 3.312894284725189
        vf_explained_var: 0.9932640194892883
        vf_loss: 3.3220099409421286
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.951612903225808
    gpu_util_percent0: 0.2687096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146441118786232
    mean_env_wait_ms: 1.208397006336467
    mean_inference_ms: 4.332152624498839
    mean_raw_obs_processing_ms: 0.3793046885166991
  time_since_restore: 2472.4252514839172
  time_this_iter_s: 26.551373958587646
  time_total_s: 2472.4252514839172
  timers:
    learn_throughput: 8363.905
    learn_time_ms: 19344.073
    sample_throughput: 23723.531
    sample_time_ms: 6819.896
    update_time_ms: 36.235
  timestamp: 1602655102
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     94 |          2472.43 | 15208448 |  279.075 |              317.798 |              165.677 |            786.378 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3196.661001236094
    time_step_min: 2942
  date: 2020-10-14_05-58-49
  done: false
  episode_len_mean: 786.3810944250155
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 279.24538431008284
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 170
  episodes_total: 19444
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2249310463666916
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045103810261934996
        model: {}
        policy_loss: -0.007979410632591074
        total_loss: 3.3010860482851663
        vf_explained_var: 0.9927241802215576
        vf_loss: 3.309178034464518
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.096666666666668
    gpu_util_percent0: 0.30633333333333324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643221576998106
    mean_env_wait_ms: 1.2084037185499368
    mean_inference_ms: 4.3315600881029415
    mean_raw_obs_processing_ms: 0.37927514627140047
  time_since_restore: 2498.6662912368774
  time_this_iter_s: 26.241039752960205
  time_total_s: 2498.6662912368774
  timers:
    learn_throughput: 8361.849
    learn_time_ms: 19348.83
    sample_throughput: 23720.355
    sample_time_ms: 6820.809
    update_time_ms: 38.099
  timestamp: 1602655129
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     95 |          2498.67 | 15370240 |  279.245 |              317.798 |              165.677 |            786.381 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3195.1431988193985
    time_step_min: 2942
  date: 2020-10-14_05-59-16
  done: false
  episode_len_mean: 786.3685146602978
  episode_reward_max: 317.7979797979791
  episode_reward_mean: 279.4854192619831
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 19679
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.22384763633211455
        entropy_coeff: 0.0005000000000000001
        kl: 0.004928315756842494
        model: {}
        policy_loss: -0.008493121097368809
        total_loss: 3.06818288564682
        vf_explained_var: 0.9944605231285095
        vf_loss: 3.0767878691355386
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.596774193548388
    gpu_util_percent0: 0.332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641953798000584
    mean_env_wait_ms: 1.20841358093139
    mean_inference_ms: 4.3307622033434185
    mean_raw_obs_processing_ms: 0.37923348261185685
  time_since_restore: 2524.949718475342
  time_this_iter_s: 26.283427238464355
  time_total_s: 2524.949718475342
  timers:
    learn_throughput: 8362.252
    learn_time_ms: 19347.897
    sample_throughput: 23713.999
    sample_time_ms: 6822.637
    update_time_ms: 36.119
  timestamp: 1602655156
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     96 |          2524.95 | 15532032 |  279.485 |              317.798 |              165.677 |            786.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3193.613414511422
    time_step_min: 2936
  date: 2020-10-14_05-59-43
  done: false
  episode_len_mean: 786.345995377349
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 279.7114390818038
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 223
  episodes_total: 19902
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.2050087774793307
        entropy_coeff: 0.0005000000000000001
        kl: 0.004004717067194481
        model: {}
        policy_loss: -0.008170259432517923
        total_loss: 2.8481951554616294
        vf_explained_var: 0.9946429133415222
        vf_loss: 2.856467843055725
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.858064516129033
    gpu_util_percent0: 0.342258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640838368479617
    mean_env_wait_ms: 1.2084122154678134
    mean_inference_ms: 4.330016348448912
    mean_raw_obs_processing_ms: 0.37919634972826444
  time_since_restore: 2551.553069829941
  time_this_iter_s: 26.603351354599
  time_total_s: 2551.553069829941
  timers:
    learn_throughput: 8353.239
    learn_time_ms: 19368.774
    sample_throughput: 23738.962
    sample_time_ms: 6815.462
    update_time_ms: 36.597
  timestamp: 1602655183
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     97 |          2551.55 | 15693824 |  279.711 |              318.707 |              165.677 |            786.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3192.5208821915076
    time_step_min: 2936
  date: 2020-10-14_06-00-10
  done: false
  episode_len_mean: 786.3337485674423
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 279.88309675055393
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 167
  episodes_total: 20069
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.21761814504861832
        entropy_coeff: 0.0005000000000000001
        kl: 0.004347234149463475
        model: {}
        policy_loss: -0.009369057399453595
        total_loss: 2.5891530513763428
        vf_explained_var: 0.994077205657959
        vf_loss: 2.598630905151367
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.293333333333337
    gpu_util_percent0: 0.3143333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640015855879734
    mean_env_wait_ms: 1.208411171580152
    mean_inference_ms: 4.32947427984912
    mean_raw_obs_processing_ms: 0.37917015428158724
  time_since_restore: 2577.891927719116
  time_this_iter_s: 26.338857889175415
  time_total_s: 2577.891927719116
  timers:
    learn_throughput: 8349.973
    learn_time_ms: 19376.349
    sample_throughput: 23755.88
    sample_time_ms: 6810.608
    update_time_ms: 35.796
  timestamp: 1602655210
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     98 |          2577.89 | 15855616 |  279.883 |              318.707 |              165.677 |            786.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3191.188209736348
    time_step_min: 2936
  date: 2020-10-14_06-00-36
  done: false
  episode_len_mean: 786.2985898826546
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.09027211270575
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 213
  episodes_total: 20282
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.22063125918308893
        entropy_coeff: 0.0005000000000000001
        kl: 0.004395618801936507
        model: {}
        policy_loss: -0.010148239402042236
        total_loss: 3.3593016862869263
        vf_explained_var: 0.9935986399650574
        vf_loss: 3.369560261567434
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.093548387096778
    gpu_util_percent0: 0.2990322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638956739038553
    mean_env_wait_ms: 1.2084065290296404
    mean_inference_ms: 4.328765722143481
    mean_raw_obs_processing_ms: 0.37913381846292715
  time_since_restore: 2604.229161262512
  time_this_iter_s: 26.337233543395996
  time_total_s: 2604.229161262512
  timers:
    learn_throughput: 8356.177
    learn_time_ms: 19361.963
    sample_throughput: 23790.233
    sample_time_ms: 6800.774
    update_time_ms: 34.207
  timestamp: 1602655236
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |     99 |          2604.23 | 16017408 |   280.09 |              318.707 |              165.677 |            786.299 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3189.571414629386
    time_step_min: 2936
  date: 2020-10-14_06-01-03
  done: false
  episode_len_mean: 786.253204034891
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.3382083591137
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 239
  episodes_total: 20521
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.19899060080448785
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054977779897550745
        model: {}
        policy_loss: -0.00914790567185264
        total_loss: 2.60590926806132
        vf_explained_var: 0.995249330997467
        vf_loss: 2.6151567300160727
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.076666666666668
    gpu_util_percent0: 0.321
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637787664836138
    mean_env_wait_ms: 1.2084013720336102
    mean_inference_ms: 4.3280291373926945
    mean_raw_obs_processing_ms: 0.3790953878379599
  time_since_restore: 2630.3904836177826
  time_this_iter_s: 26.161322355270386
  time_total_s: 2630.3904836177826
  timers:
    learn_throughput: 8351.841
    learn_time_ms: 19372.017
    sample_throughput: 23812.235
    sample_time_ms: 6794.49
    update_time_ms: 34.164
  timestamp: 1602655263
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    100 |          2630.39 | 16179200 |  280.338 |              318.707 |              165.677 |            786.253 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3188.356313497823
    time_step_min: 2936
  date: 2020-10-14_06-01-30
  done: false
  episode_len_mean: 786.2260604889361
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.5185764300654
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 177
  episodes_total: 20698
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.20405827338496843
        entropy_coeff: 0.0005000000000000001
        kl: 0.004492445072780053
        model: {}
        policy_loss: -0.008968284473667154
        total_loss: 2.520637015501658
        vf_explained_var: 0.9943375587463379
        vf_loss: 2.529707392056783
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15666666666667
    gpu_util_percent0: 0.27433333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636987591214531
    mean_env_wait_ms: 1.2083955126254418
    mean_inference_ms: 4.327481617145878
    mean_raw_obs_processing_ms: 0.37906940388804883
  time_since_restore: 2656.619069337845
  time_this_iter_s: 26.228585720062256
  time_total_s: 2656.619069337845
  timers:
    learn_throughput: 8360.849
    learn_time_ms: 19351.144
    sample_throughput: 23808.992
    sample_time_ms: 6795.416
    update_time_ms: 33.856
  timestamp: 1602655290
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    101 |          2656.62 | 16340992 |  280.519 |              318.707 |              165.677 |            786.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3187.0574139748874
    time_step_min: 2936
  date: 2020-10-14_06-01-56
  done: false
  episode_len_mean: 786.187900832775
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.71924035995056
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 196
  episodes_total: 20894
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21233457823594412
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587849175247054
        model: {}
        policy_loss: -0.00771534312904502
        total_loss: 2.953911562760671
        vf_explained_var: 0.9940600395202637
        vf_loss: 2.961733063062032
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.990322580645167
    gpu_util_percent0: 0.3009677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636091604137336
    mean_env_wait_ms: 1.208386256238851
    mean_inference_ms: 4.326873065363303
    mean_raw_obs_processing_ms: 0.3790387728175054
  time_since_restore: 2682.97140789032
  time_this_iter_s: 26.352338552474976
  time_total_s: 2682.97140789032
  timers:
    learn_throughput: 8360.715
    learn_time_ms: 19351.456
    sample_throughput: 23792.101
    sample_time_ms: 6800.24
    update_time_ms: 34.27
  timestamp: 1602655316
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    102 |          2682.97 | 16502784 |  280.719 |              318.707 |              165.677 |            786.188 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3185.383810155362
    time_step_min: 2936
  date: 2020-10-14_06-02-23
  done: false
  episode_len_mean: 786.1320245979186
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 280.9684020909186
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 246
  episodes_total: 21140
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.19720646987358728
        entropy_coeff: 0.0005000000000000001
        kl: 0.004155141243245453
        model: {}
        policy_loss: -0.00845454789911552
        total_loss: 2.8708957632382712
        vf_explained_var: 0.994846522808075
        vf_loss: 2.8794489900271096
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.293333333333337
    gpu_util_percent0: 0.32733333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146349210826311
    mean_env_wait_ms: 1.2083722112468198
    mean_inference_ms: 4.326134806928
    mean_raw_obs_processing_ms: 0.37900003817778666
  time_since_restore: 2709.264338493347
  time_this_iter_s: 26.292930603027344
  time_total_s: 2709.264338493347
  timers:
    learn_throughput: 8363.451
    learn_time_ms: 19345.124
    sample_throughput: 23792.555
    sample_time_ms: 6800.11
    update_time_ms: 39.266
  timestamp: 1602655343
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    103 |          2709.26 | 16664576 |  280.968 |              318.707 |              165.677 |            786.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3184.148584573494
    time_step_min: 2936
  date: 2020-10-14_06-02-50
  done: false
  episode_len_mean: 786.0835482207324
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 281.152766352635
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 189
  episodes_total: 21329
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.19045389195283255
        entropy_coeff: 0.0005000000000000001
        kl: 0.004448773960272471
        model: {}
        policy_loss: -0.010119000047173662
        total_loss: 2.200990895430247
        vf_explained_var: 0.9952632784843445
        vf_loss: 2.2112051248550415
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.310000000000002
    gpu_util_percent0: 0.3503333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634110563024608
    mean_env_wait_ms: 1.208358044284784
    mean_inference_ms: 4.325583161231252
    mean_raw_obs_processing_ms: 0.3789746029131672
  time_since_restore: 2735.614042043686
  time_this_iter_s: 26.349703550338745
  time_total_s: 2735.614042043686
  timers:
    learn_throughput: 8363.886
    learn_time_ms: 19344.119
    sample_throughput: 23866.594
    sample_time_ms: 6779.015
    update_time_ms: 40.226
  timestamp: 1602655370
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    104 |          2735.61 | 16826368 |  281.153 |              318.707 |              165.677 |            786.084 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3182.9835668730507
    time_step_min: 2936
  date: 2020-10-14_06-03-16
  done: false
  episode_len_mean: 786.0381700683434
  episode_reward_max: 318.7070707070704
  episode_reward_mean: 281.3386541034502
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 21509
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.2013479694724083
        entropy_coeff: 0.0005000000000000001
        kl: 0.004613580415025353
        model: {}
        policy_loss: -0.00932157018299525
        total_loss: 2.9009637435277305
        vf_explained_var: 0.9937450885772705
        vf_loss: 2.9103859464327493
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.98666666666667
    gpu_util_percent0: 0.3243333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633327017840922
    mean_env_wait_ms: 1.2083463323485966
    mean_inference_ms: 4.325060326327236
    mean_raw_obs_processing_ms: 0.3789497231370364
  time_since_restore: 2761.858407497406
  time_this_iter_s: 26.244365453720093
  time_total_s: 2761.858407497406
  timers:
    learn_throughput: 8361.24
    learn_time_ms: 19350.239
    sample_throughput: 23887.016
    sample_time_ms: 6773.219
    update_time_ms: 38.761
  timestamp: 1602655396
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    105 |          2761.86 | 16988160 |  281.339 |              318.707 |              165.677 |            786.038 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3181.3797165470273
    time_step_min: 2934
  date: 2020-10-14_06-03-42
  done: false
  episode_len_mean: 785.9590533088235
  episode_reward_max: 319.0101010101008
  episode_reward_mean: 281.5944230912061
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 251
  episodes_total: 21760
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19328687836726507
        entropy_coeff: 0.0005000000000000001
        kl: 0.004194587546711166
        model: {}
        policy_loss: -0.007614445697981864
        total_loss: 2.6770079930623374
        vf_explained_var: 0.9951998591423035
        vf_loss: 2.6847190459569297
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34666666666667
    gpu_util_percent0: 0.33499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632187426291265
    mean_env_wait_ms: 1.2083258346366192
    mean_inference_ms: 4.324340023809571
    mean_raw_obs_processing_ms: 0.3789124027910592
  time_since_restore: 2787.37642455101
  time_this_iter_s: 25.518017053604126
  time_total_s: 2787.37642455101
  timers:
    learn_throughput: 8385.212
    learn_time_ms: 19294.922
    sample_throughput: 23963.82
    sample_time_ms: 6751.511
    update_time_ms: 38.548
  timestamp: 1602655422
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    106 |          2787.38 | 17149952 |  281.594 |               319.01 |              165.677 |            785.959 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3180.01595914459
    time_step_min: 2934
  date: 2020-10-14_06-04-09
  done: false
  episode_len_mean: 785.8881096589098
  episode_reward_max: 319.0101010101008
  episode_reward_mean: 281.7961057820795
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 199
  episodes_total: 21959
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.17738542829950651
        entropy_coeff: 0.0005000000000000001
        kl: 0.004116727815320094
        model: {}
        policy_loss: -0.007019599470368121
        total_loss: 1.9695641497770946
        vf_explained_var: 0.9958812594413757
        vf_loss: 1.9766724308331807
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03666666666667
    gpu_util_percent0: 0.3743333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631382606357735
    mean_env_wait_ms: 1.208303538675972
    mean_inference_ms: 4.323792290112855
    mean_raw_obs_processing_ms: 0.3788861435265502
  time_since_restore: 2813.654299259186
  time_this_iter_s: 26.27787470817566
  time_total_s: 2813.654299259186
  timers:
    learn_throughput: 8389.77
    learn_time_ms: 19284.438
    sample_throughput: 24012.262
    sample_time_ms: 6737.891
    update_time_ms: 37.82
  timestamp: 1602655449
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    107 |          2813.65 | 17311744 |  281.796 |               319.01 |              165.677 |            785.888 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3178.8184285520424
    time_step_min: 2934
  date: 2020-10-14_06-04-36
  done: false
  episode_len_mean: 785.8128303591596
  episode_reward_max: 319.0101010101008
  episode_reward_mean: 281.97153600609664
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 176
  episodes_total: 22135
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.1931574965516726
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041800564310203
        model: {}
        policy_loss: -0.008269173985657593
        total_loss: 2.717939575513204
        vf_explained_var: 0.9939755797386169
        vf_loss: 2.7263052463531494
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.633333333333333
    gpu_util_percent0: 0.3766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463065021125743
    mean_env_wait_ms: 1.2082882138782662
    mean_inference_ms: 4.323303724584603
    mean_raw_obs_processing_ms: 0.37886326437918544
  time_since_restore: 2839.7066247463226
  time_this_iter_s: 26.05232548713684
  time_total_s: 2839.7066247463226
  timers:
    learn_throughput: 8402.181
    learn_time_ms: 19255.953
    sample_throughput: 24017.166
    sample_time_ms: 6736.515
    update_time_ms: 37.98
  timestamp: 1602655476
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    108 |          2839.71 | 17473536 |  281.972 |               319.01 |              165.677 |            785.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3177.2293027834958
    time_step_min: 2934
  date: 2020-10-14_06-05-02
  done: false
  episode_len_mean: 785.7118530437115
  episode_reward_max: 319.31313131313163
  episode_reward_mean: 282.21284445419593
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 239
  episodes_total: 22374
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.19076383486390114
        entropy_coeff: 0.0005000000000000001
        kl: 0.004903997915486495
        model: {}
        policy_loss: -0.007217070371552836
        total_loss: 2.5927430192629495
        vf_explained_var: 0.9952414035797119
        vf_loss: 2.600055476029714
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.136666666666667
    gpu_util_percent0: 0.3716666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629606240870388
    mean_env_wait_ms: 1.2082617405664433
    mean_inference_ms: 4.322667370601252
    mean_raw_obs_processing_ms: 0.3788306696538864
  time_since_restore: 2865.777428627014
  time_this_iter_s: 26.07080388069153
  time_total_s: 2865.777428627014
  timers:
    learn_throughput: 8410.708
    learn_time_ms: 19236.43
    sample_throughput: 24044.738
    sample_time_ms: 6728.79
    update_time_ms: 38.196
  timestamp: 1602655502
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    109 |          2865.78 | 17635328 |  282.213 |              319.313 |              165.677 |            785.712 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3175.8369454416525
    time_step_min: 2934
  date: 2020-10-14_06-05-29
  done: false
  episode_len_mean: 785.6018768536143
  episode_reward_max: 319.31313131313163
  episode_reward_mean: 282.42800811443186
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 217
  episodes_total: 22591
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.1729131688674291
        entropy_coeff: 0.0005000000000000001
        kl: 0.004271870207351943
        model: {}
        policy_loss: -0.009325368009740487
        total_loss: 2.267487347126007
        vf_explained_var: 0.9952816963195801
        vf_loss: 2.2768991589546204
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.013333333333332
    gpu_util_percent0: 0.3186666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628771982120864
    mean_env_wait_ms: 1.2082341715628184
    mean_inference_ms: 4.322082102984466
    mean_raw_obs_processing_ms: 0.3788020240887705
  time_since_restore: 2891.9109058380127
  time_this_iter_s: 26.133477210998535
  time_total_s: 2891.9109058380127
  timers:
    learn_throughput: 8408.851
    learn_time_ms: 19240.678
    sample_throughput: 24072.514
    sample_time_ms: 6721.026
    update_time_ms: 36.742
  timestamp: 1602655529
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    110 |          2891.91 | 17797120 |  282.428 |              319.313 |              165.677 |            785.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3174.7451402937813
    time_step_min: 2934
  date: 2020-10-14_06-05-55
  done: false
  episode_len_mean: 785.5117719406132
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 282.5992783851872
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 175
  episodes_total: 22766
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.1819970396657785
        entropy_coeff: 0.0005000000000000001
        kl: 0.004261158329124252
        model: {}
        policy_loss: -0.008556412833665187
        total_loss: 2.0311067402362823
        vf_explained_var: 0.995327889919281
        vf_loss: 2.039754162232081
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.75
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628073921998203
    mean_env_wait_ms: 1.208215803581277
    mean_inference_ms: 4.321614658939902
    mean_raw_obs_processing_ms: 0.3787800025107893
  time_since_restore: 2917.903029680252
  time_this_iter_s: 25.99212384223938
  time_total_s: 2917.903029680252
  timers:
    learn_throughput: 8415.336
    learn_time_ms: 19225.851
    sample_throughput: 24098.256
    sample_time_ms: 6713.847
    update_time_ms: 34.895
  timestamp: 1602655555
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    111 |           2917.9 | 17958912 |  282.599 |              319.919 |              165.677 |            785.512 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3173.2476385321897
    time_step_min: 2934
  date: 2020-10-14_06-06-22
  done: false
  episode_len_mean: 785.411156036694
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 282.82829468547476
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 23001
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18373206754525503
        entropy_coeff: 0.0005000000000000001
        kl: 0.004336539811144273
        model: {}
        policy_loss: -0.009234675051023563
        total_loss: 2.506518284479777
        vf_explained_var: 0.9953103065490723
        vf_loss: 2.5158448815345764
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627102482596657
    mean_env_wait_ms: 1.208185069116834
    mean_inference_ms: 4.321020901798689
    mean_raw_obs_processing_ms: 0.37875076412789777
  time_since_restore: 2944.0672931671143
  time_this_iter_s: 26.164263486862183
  time_total_s: 2944.0672931671143
  timers:
    learn_throughput: 8412.84
    learn_time_ms: 19231.557
    sample_throughput: 24159.599
    sample_time_ms: 6696.8
    update_time_ms: 35.42
  timestamp: 1602655582
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    112 |          2944.07 | 18120704 |  282.828 |              319.919 |              165.677 |            785.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3171.819636081407
    time_step_min: 2934
  date: 2020-10-14_06-06-49
  done: false
  episode_len_mean: 785.3100344530577
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 283.04338605695193
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 219
  episodes_total: 23220
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.16369764630993208
        entropy_coeff: 0.0005000000000000001
        kl: 0.004032092770406355
        model: {}
        policy_loss: -0.006494078901596367
        total_loss: 2.238641540209452
        vf_explained_var: 0.9954802989959717
        vf_loss: 2.245217442512512
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.283870967741937
    gpu_util_percent0: 0.3080645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626285038027037
    mean_env_wait_ms: 1.208154573896081
    mean_inference_ms: 4.320463874435935
    mean_raw_obs_processing_ms: 0.37872261913762423
  time_since_restore: 2970.497499704361
  time_this_iter_s: 26.430206537246704
  time_total_s: 2970.497499704361
  timers:
    learn_throughput: 8414.029
    learn_time_ms: 19228.837
    sample_throughput: 24108.406
    sample_time_ms: 6711.02
    update_time_ms: 29.077
  timestamp: 1602655609
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    113 |           2970.5 | 18282496 |  283.043 |              319.919 |              165.677 |             785.31 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3170.680347513481
    time_step_min: 2932
  date: 2020-10-14_06-07-15
  done: false
  episode_len_mean: 785.2377532700692
  episode_reward_max: 319.9191919191919
  episode_reward_mean: 283.21170281942267
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 174
  episodes_total: 23394
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1712109073996544
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048668616606543464
        model: {}
        policy_loss: -0.005204594825045206
        total_loss: 1.9320645332336426
        vf_explained_var: 0.9956938624382019
        vf_loss: 1.9373546938101451
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.830000000000002
    gpu_util_percent0: 0.3326666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462562710167581
    mean_env_wait_ms: 1.2081325768045397
    mean_inference_ms: 4.320012352639639
    mean_raw_obs_processing_ms: 0.3787012683419407
  time_since_restore: 2996.926306247711
  time_this_iter_s: 26.42880654335022
  time_total_s: 2996.926306247711
  timers:
    learn_throughput: 8417.932
    learn_time_ms: 19219.922
    sample_throughput: 24048.66
    sample_time_ms: 6727.693
    update_time_ms: 27.905
  timestamp: 1602655635
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    114 |          2996.93 | 18444288 |  283.212 |              319.919 |              165.677 |            785.238 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3169.2730317547803
    time_step_min: 2928
  date: 2020-10-14_06-07-42
  done: false
  episode_len_mean: 785.1526995553673
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 283.4290951009138
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 23615
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.17937743663787842
        entropy_coeff: 0.0005000000000000001
        kl: 0.00433262715038533
        model: {}
        policy_loss: -0.005900731290845822
        total_loss: 2.396419088045756
        vf_explained_var: 0.9953761100769043
        vf_loss: 2.402409533659617
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25
    gpu_util_percent0: 0.32933333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624767396789626
    mean_env_wait_ms: 1.2081029167436927
    mean_inference_ms: 4.3194848487269315
    mean_raw_obs_processing_ms: 0.37867531385011044
  time_since_restore: 3023.07022356987
  time_this_iter_s: 26.143917322158813
  time_total_s: 3023.07022356987
  timers:
    learn_throughput: 8424.094
    learn_time_ms: 19205.864
    sample_throughput: 24045.36
    sample_time_ms: 6728.616
    update_time_ms: 30.06
  timestamp: 1602655662
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    115 |          3023.07 | 18606080 |  283.429 |              321.283 |              165.677 |            785.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3167.808538325917
    time_step_min: 2926
  date: 2020-10-14_06-08-09
  done: false
  episode_len_mean: 785.0846540880503
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 283.65480592084356
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 23850
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15947192907333374
        entropy_coeff: 0.0005000000000000001
        kl: 0.004415036373150845
        model: {}
        policy_loss: -0.008368280687136576
        total_loss: 2.0854901870091758
        vf_explained_var: 0.9960618615150452
        vf_loss: 2.093938171863556
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.320000000000004
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623912051536173
    mean_env_wait_ms: 1.2080644159390423
    mean_inference_ms: 4.318915269971725
    mean_raw_obs_processing_ms: 0.3786467236875089
  time_since_restore: 3049.1643958091736
  time_this_iter_s: 26.09417223930359
  time_total_s: 3049.1643958091736
  timers:
    learn_throughput: 8398.499
    learn_time_ms: 19264.394
    sample_throughput: 24053.142
    sample_time_ms: 6726.439
    update_time_ms: 30.508
  timestamp: 1602655689
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    116 |          3049.16 | 18767872 |  283.655 |              321.283 |              165.677 |            785.085 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3166.7363617420297
    time_step_min: 2926
  date: 2020-10-14_06-08-36
  done: false
  episode_len_mean: 785.0346334762519
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 283.8157922731456
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 173
  episodes_total: 24023
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.16986916462580362
        entropy_coeff: 0.0005000000000000001
        kl: 0.00415391381829977
        model: {}
        policy_loss: -0.007707703747049284
        total_loss: 2.029173066218694
        vf_explained_var: 0.995383083820343
        vf_loss: 2.036965638399124
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.993548387096777
    gpu_util_percent0: 0.3070967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623289879282866
    mean_env_wait_ms: 1.208037823960146
    mean_inference_ms: 4.318492507911022
    mean_raw_obs_processing_ms: 0.3786264887266298
  time_since_restore: 3075.596272945404
  time_this_iter_s: 26.43187713623047
  time_total_s: 3075.596272945404
  timers:
    learn_throughput: 8401.22
    learn_time_ms: 19258.156
    sample_throughput: 24008.927
    sample_time_ms: 6738.827
    update_time_ms: 37.897
  timestamp: 1602655716
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    117 |           3075.6 | 18929664 |  283.816 |              321.283 |              165.677 |            785.035 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3165.460171872418
    time_step_min: 2926
  date: 2020-10-14_06-09-03
  done: false
  episode_len_mean: 784.9489105315286
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.00260403640226
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 209
  episodes_total: 24232
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17321191728115082
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039366522105410695
        model: {}
        policy_loss: -0.009872476609113315
        total_loss: 2.352258563041687
        vf_explained_var: 0.9954938888549805
        vf_loss: 2.362217585245768
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19
    gpu_util_percent0: 0.3236666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622531188751162
    mean_env_wait_ms: 1.2080074796192384
    mean_inference_ms: 4.3180213590034136
    mean_raw_obs_processing_ms: 0.3786042194528362
  time_since_restore: 3102.0575137138367
  time_this_iter_s: 26.461240768432617
  time_total_s: 3102.0575137138367
  timers:
    learn_throughput: 8388.072
    learn_time_ms: 19288.341
    sample_throughput: 23972.033
    sample_time_ms: 6749.198
    update_time_ms: 36.611
  timestamp: 1602655743
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    118 |          3102.06 | 19091456 |  284.003 |              321.283 |              165.677 |            784.949 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3163.957946410309
    time_step_min: 2926
  date: 2020-10-14_06-09-29
  done: false
  episode_len_mean: 784.8994401994034
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.23111142479416
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 241
  episodes_total: 24473
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.15587976823250452
        entropy_coeff: 0.0005000000000000001
        kl: 0.004592253516117732
        model: {}
        policy_loss: -0.007124626184425627
        total_loss: 2.442015528678894
        vf_explained_var: 0.995435893535614
        vf_loss: 2.4492180744806924
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82903225806452
    gpu_util_percent0: 0.3264516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621693434276706
    mean_env_wait_ms: 1.2079618863829467
    mean_inference_ms: 4.317464061134043
    mean_raw_obs_processing_ms: 0.37857598607898957
  time_since_restore: 3128.1601824760437
  time_this_iter_s: 26.10266876220703
  time_total_s: 3128.1601824760437
  timers:
    learn_throughput: 8389.587
    learn_time_ms: 19284.859
    sample_throughput: 23953.05
    sample_time_ms: 6754.547
    update_time_ms: 37.267
  timestamp: 1602655769
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    119 |          3128.16 | 19253248 |  284.231 |              321.283 |              165.677 |            784.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3162.8459570320433
    time_step_min: 2926
  date: 2020-10-14_06-09-56
  done: false
  episode_len_mean: 784.8869011399132
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.39533954612443
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 178
  episodes_total: 24651
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.15485632419586182
        entropy_coeff: 0.0005000000000000001
        kl: 0.003862540858487288
        model: {}
        policy_loss: -0.008335680021749189
        total_loss: 2.322477638721466
        vf_explained_var: 0.994878351688385
        vf_loss: 2.3308907548586526
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09666666666667
    gpu_util_percent0: 0.39299999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462107891224564
    mean_env_wait_ms: 1.20793240897332
    mean_inference_ms: 4.31705465454753
    mean_raw_obs_processing_ms: 0.3785569100706026
  time_since_restore: 3154.365068912506
  time_this_iter_s: 26.204886436462402
  time_total_s: 3154.365068912506
  timers:
    learn_throughput: 8387.072
    learn_time_ms: 19290.642
    sample_throughput: 23950.439
    sample_time_ms: 6755.283
    update_time_ms: 37.054
  timestamp: 1602655796
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    120 |          3154.37 | 19415040 |  284.395 |              321.283 |              165.677 |            784.887 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3161.62143864598
    time_step_min: 2925
  date: 2020-10-14_06-10-23
  done: false
  episode_len_mean: 784.8894658455098
  episode_reward_max: 321.28282828282823
  episode_reward_mean: 284.5751220696275
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 192
  episodes_total: 24843
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16783262168367705
        entropy_coeff: 0.0005000000000000001
        kl: 0.004981446312740445
        model: {}
        policy_loss: -0.00818172721968343
        total_loss: 2.158005952835083
        vf_explained_var: 0.9955466389656067
        vf_loss: 2.16627166668574
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43870967741936
    gpu_util_percent0: 0.3687096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620417428364194
    mean_env_wait_ms: 1.2078984161491606
    mean_inference_ms: 4.316627345998688
    mean_raw_obs_processing_ms: 0.3785365322005948
  time_since_restore: 3180.6590688228607
  time_this_iter_s: 26.293999910354614
  time_total_s: 3180.6590688228607
  timers:
    learn_throughput: 8373.309
    learn_time_ms: 19322.35
    sample_throughput: 23973.255
    sample_time_ms: 6748.854
    update_time_ms: 39.091
  timestamp: 1602655823
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    121 |          3180.66 | 19576832 |  284.575 |              321.283 |              165.677 |            784.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3160.123423782921
    time_step_min: 2925
  date: 2020-10-14_06-10-50
  done: false
  episode_len_mean: 784.8919802295918
  episode_reward_max: 322.949494949495
  episode_reward_mean: 284.7932775619717
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 245
  episodes_total: 25088
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1570940762758255
        entropy_coeff: 0.0005000000000000001
        kl: 0.003961495550659795
        model: {}
        policy_loss: -0.00754543371052326
        total_loss: 2.493976056575775
        vf_explained_var: 0.9955318570137024
        vf_loss: 2.501599987347921
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896666666666665
    gpu_util_percent0: 0.3383333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146195912166377
    mean_env_wait_ms: 1.2078506369126227
    mean_inference_ms: 4.316103307946183
    mean_raw_obs_processing_ms: 0.3785101519312619
  time_since_restore: 3207.163900613785
  time_this_iter_s: 26.504831790924072
  time_total_s: 3207.163900613785
  timers:
    learn_throughput: 8367.614
    learn_time_ms: 19335.5
    sample_throughput: 23903.796
    sample_time_ms: 6768.465
    update_time_ms: 39.222
  timestamp: 1602655850
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    122 |          3207.16 | 19738624 |  284.793 |              322.949 |              165.677 |            784.892 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3159.0028513722227
    time_step_min: 2920
  date: 2020-10-14_06-11-17
  done: false
  episode_len_mean: 784.9051782111634
  episode_reward_max: 322.949494949495
  episode_reward_mean: 284.96320377715995
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 191
  episodes_total: 25279
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1477253089348475
        entropy_coeff: 0.0005000000000000001
        kl: 0.003945230622775853
        model: {}
        policy_loss: -0.00920618103312639
        total_loss: 1.7173584500948589
        vf_explained_var: 0.9963318705558777
        vf_loss: 1.7266384760538738
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.967741935483872
    gpu_util_percent0: 0.3538709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618963576400174
    mean_env_wait_ms: 1.2078107207848623
    mean_inference_ms: 4.315690506549405
    mean_raw_obs_processing_ms: 0.37849028338420243
  time_since_restore: 3233.384102344513
  time_this_iter_s: 26.22020173072815
  time_total_s: 3233.384102344513
  timers:
    learn_throughput: 8364.536
    learn_time_ms: 19342.615
    sample_throughput: 23981.799
    sample_time_ms: 6746.45
    update_time_ms: 41.01
  timestamp: 1602655877
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    123 |          3233.38 | 19900416 |  284.963 |              322.949 |              165.677 |            784.905 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3158.009083402147
    time_step_min: 2920
  date: 2020-10-14_06-11-43
  done: false
  episode_len_mean: 784.9131544836796
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.11666331407866
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 25459
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.1585122545560201
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038400359141329923
        model: {}
        policy_loss: -0.008884707873221487
        total_loss: 1.9652710159619649
        vf_explained_var: 0.99581378698349
        vf_loss: 1.9742349584897358
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.266666666666666
    gpu_util_percent0: 0.39333333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618390666822678
    mean_env_wait_ms: 1.2077748707125033
    mean_inference_ms: 4.3153027624052545
    mean_raw_obs_processing_ms: 0.37847270719707493
  time_since_restore: 3259.5251739025116
  time_this_iter_s: 26.141071557998657
  time_total_s: 3259.5251739025116
  timers:
    learn_throughput: 8367.75
    learn_time_ms: 19335.187
    sample_throughput: 24055.342
    sample_time_ms: 6725.824
    update_time_ms: 39.1
  timestamp: 1602655903
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    124 |          3259.53 | 20062208 |  285.117 |              322.949 |              165.677 |            784.913 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3156.7205641704977
    time_step_min: 2920
  date: 2020-10-14_06-12-10
  done: false
  episode_len_mean: 784.9009496380478
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.3147034287766
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 235
  episodes_total: 25694
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.15613733480374017
        entropy_coeff: 0.0005000000000000001
        kl: 0.003700155473779887
        model: {}
        policy_loss: -0.0072184071856706096
        total_loss: 2.7436450521151223
        vf_explained_var: 0.9951500296592712
        vf_loss: 2.7509415547053018
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.946666666666665
    gpu_util_percent0: 0.35933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461757898485961
    mean_env_wait_ms: 1.2077226392023475
    mean_inference_ms: 4.3147989057756915
    mean_raw_obs_processing_ms: 0.378446798524241
  time_since_restore: 3285.9001932144165
  time_this_iter_s: 26.375019311904907
  time_total_s: 3285.9001932144165
  timers:
    learn_throughput: 8356.786
    learn_time_ms: 19360.553
    sample_throughput: 24065.993
    sample_time_ms: 6722.847
    update_time_ms: 38.364
  timestamp: 1602655930
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    125 |           3285.9 | 20224000 |  285.315 |              322.949 |              165.677 |            784.901 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3155.521772729029
    time_step_min: 2911
  date: 2020-10-14_06-12-37
  done: false
  episode_len_mean: 784.8975259562314
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.50459163404463
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 215
  episodes_total: 25909
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.14190075919032097
        entropy_coeff: 0.0005000000000000001
        kl: 0.005255653406493366
        model: {}
        policy_loss: -0.00850060413358733
        total_loss: 2.407831311225891
        vf_explained_var: 0.9952466487884521
        vf_loss: 2.416402816772461
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290327
    gpu_util_percent0: 0.3329032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461693997585607
    mean_env_wait_ms: 1.207672640006243
    mean_inference_ms: 4.314371451787069
    mean_raw_obs_processing_ms: 0.37842553344780594
  time_since_restore: 3312.133764266968
  time_this_iter_s: 26.23357105255127
  time_total_s: 3312.133764266968
  timers:
    learn_throughput: 8357.164
    learn_time_ms: 19359.678
    sample_throughput: 24027.679
    sample_time_ms: 6733.568
    update_time_ms: 39.955
  timestamp: 1602655957
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    126 |          3312.13 | 20385792 |  285.505 |              322.949 |              165.677 |            784.898 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3154.512933681302
    time_step_min: 2911
  date: 2020-10-14_06-13-03
  done: false
  episode_len_mean: 784.8730639472474
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.6587141155458
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 175
  episodes_total: 26084
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.146172267695268
        entropy_coeff: 0.0005000000000000001
        kl: 0.004243072898437579
        model: {}
        policy_loss: -0.008694331017371345
        total_loss: 1.5353152354558308
        vf_explained_var: 0.996583878993988
        vf_loss: 1.5440826614697774
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.680000000000003
    gpu_util_percent0: 0.376
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461639472436592
    mean_env_wait_ms: 1.2076350418356065
    mean_inference_ms: 4.314010985732989
    mean_raw_obs_processing_ms: 0.37840860810405985
  time_since_restore: 3338.3463790416718
  time_this_iter_s: 26.21261477470398
  time_total_s: 3338.3463790416718
  timers:
    learn_throughput: 8358.789
    learn_time_ms: 19355.913
    sample_throughput: 24066.632
    sample_time_ms: 6722.669
    update_time_ms: 32.278
  timestamp: 1602655983
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    127 |          3338.35 | 20547584 |  285.659 |              322.949 |              165.677 |            784.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3153.20134687821
    time_step_min: 2911
  date: 2020-10-14_06-13-30
  done: false
  episode_len_mean: 784.8453878605907
  episode_reward_max: 322.949494949495
  episode_reward_mean: 285.85933908658234
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 227
  episodes_total: 26311
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.1509584461649259
        entropy_coeff: 0.0005000000000000001
        kl: 0.004119076804878811
        model: {}
        policy_loss: -0.008656273285547892
        total_loss: 1.9305044114589691
        vf_explained_var: 0.9963381290435791
        vf_loss: 1.9392361442248027
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.053333333333335
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461565430471034
    mean_env_wait_ms: 1.2075813259571007
    mean_inference_ms: 4.313556460634677
    mean_raw_obs_processing_ms: 0.37838580640389863
  time_since_restore: 3364.6358699798584
  time_this_iter_s: 26.289490938186646
  time_total_s: 3364.6358699798584
  timers:
    learn_throughput: 8362.465
    learn_time_ms: 19347.406
    sample_throughput: 24098.668
    sample_time_ms: 6713.732
    update_time_ms: 31.923
  timestamp: 1602656010
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    128 |          3364.64 | 20709376 |  285.859 |              322.949 |              165.677 |            784.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3151.8901675218835
    time_step_min: 2911
  date: 2020-10-14_06-13-57
  done: false
  episode_len_mean: 784.8092492085029
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.0533169780116
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 221
  episodes_total: 26532
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.13286865005890527
        entropy_coeff: 0.0005000000000000001
        kl: 0.003924006809635709
        model: {}
        policy_loss: -0.008037711608267273
        total_loss: 1.8678389092286427
        vf_explained_var: 0.9963827729225159
        vf_loss: 1.8759430646896362
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.274193548387093
    gpu_util_percent0: 0.2741935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615012986277034
    mean_env_wait_ms: 1.2075239670062257
    mean_inference_ms: 4.313133575567706
    mean_raw_obs_processing_ms: 0.37836425983830935
  time_since_restore: 3391.032051086426
  time_this_iter_s: 26.396181106567383
  time_total_s: 3391.032051086426
  timers:
    learn_throughput: 8355.784
    learn_time_ms: 19362.874
    sample_throughput: 24079.738
    sample_time_ms: 6719.01
    update_time_ms: 38.328
  timestamp: 1602656037
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    129 |          3391.03 | 20871168 |  286.053 |              322.949 |              165.677 |            784.809 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3150.8030655074203
    time_step_min: 2911
  date: 2020-10-14_06-14-24
  done: false
  episode_len_mean: 784.7905810122792
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.2144105021463
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 180
  episodes_total: 26712
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.13310043513774872
        entropy_coeff: 0.0005000000000000001
        kl: 0.003846823936328292
        model: {}
        policy_loss: -0.005511985975317657
        total_loss: 1.8974691927433014
        vf_explained_var: 0.9959127902984619
        vf_loss: 1.9030477007230122
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.990000000000002
    gpu_util_percent0: 0.311
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614483454012275
    mean_env_wait_ms: 1.2074811798294123
    mean_inference_ms: 4.312777313608598
    mean_raw_obs_processing_ms: 0.37834772136302086
  time_since_restore: 3417.3294548988342
  time_this_iter_s: 26.297403812408447
  time_total_s: 3417.3294548988342
  timers:
    learn_throughput: 8354.934
    learn_time_ms: 19364.846
    sample_throughput: 24053.534
    sample_time_ms: 6726.33
    update_time_ms: 38.657
  timestamp: 1602656064
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    130 |          3417.33 | 21032960 |  286.214 |              322.949 |              165.677 |            784.791 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3149.6121750771636
    time_step_min: 2911
  date: 2020-10-14_06-14-51
  done: false
  episode_len_mean: 784.7560087670419
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.39055325347533
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 207
  episodes_total: 26919
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.14104019726316133
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035227101955873272
        model: {}
        policy_loss: -0.006806320839435405
        total_loss: 1.7736331522464752
        vf_explained_var: 0.9965450763702393
        vf_loss: 1.7805099785327911
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70645161290323
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613865797632677
    mean_env_wait_ms: 1.2074277083500042
    mean_inference_ms: 4.312384606071557
    mean_raw_obs_processing_ms: 0.37832927537313615
  time_since_restore: 3443.702335357666
  time_this_iter_s: 26.372880458831787
  time_total_s: 3443.702335357666
  timers:
    learn_throughput: 8356.581
    learn_time_ms: 19361.029
    sample_throughput: 24001.824
    sample_time_ms: 6740.821
    update_time_ms: 36.65
  timestamp: 1602656091
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    131 |           3443.7 | 21194752 |  286.391 |              322.949 |              165.677 |            784.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3148.2026464192254
    time_step_min: 2911
  date: 2020-10-14_06-15-18
  done: false
  episode_len_mean: 784.6928090135866
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.6085364860355
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 240
  episodes_total: 27159
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12843968719244003
        entropy_coeff: 0.0005000000000000001
        kl: 0.004247498620922367
        model: {}
        policy_loss: -0.005535748534991096
        total_loss: 1.692270855108897
        vf_explained_var: 0.9967558979988098
        vf_loss: 1.6978708108266194
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.87666666666667
    gpu_util_percent0: 0.3673333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461316922293094
    mean_env_wait_ms: 1.2073637264246897
    mean_inference_ms: 4.311947606492484
    mean_raw_obs_processing_ms: 0.37830634629058923
  time_since_restore: 3469.907158136368
  time_this_iter_s: 26.204822778701782
  time_total_s: 3469.907158136368
  timers:
    learn_throughput: 8366.268
    learn_time_ms: 19338.611
    sample_throughput: 24027.839
    sample_time_ms: 6733.523
    update_time_ms: 35.245
  timestamp: 1602656118
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    132 |          3469.91 | 21356544 |  286.609 |              322.949 |              165.677 |            784.693 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3147.181092560047
    time_step_min: 2911
  date: 2020-10-14_06-15-45
  done: false
  episode_len_mean: 784.6425749817117
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.76279436648844
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 181
  episodes_total: 27340
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.12154343537986279
        entropy_coeff: 0.0005000000000000001
        kl: 0.003221072295370201
        model: {}
        policy_loss: -0.007368193871419256
        total_loss: 1.6749194363753002
        vf_explained_var: 0.9962978363037109
        vf_loss: 1.6823483606179555
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.254838709677422
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612648010114632
    mean_env_wait_ms: 1.2073155551581745
    mean_inference_ms: 4.311601484975086
    mean_raw_obs_processing_ms: 0.3782895332993095
  time_since_restore: 3496.4881885051727
  time_this_iter_s: 26.58103036880493
  time_total_s: 3496.4881885051727
  timers:
    learn_throughput: 8361.289
    learn_time_ms: 19350.126
    sample_throughput: 23965.412
    sample_time_ms: 6751.063
    update_time_ms: 32.993
  timestamp: 1602656145
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    133 |          3496.49 | 21518336 |  286.763 |              322.949 |              165.677 |            784.643 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3146.0563451961175
    time_step_min: 2911
  date: 2020-10-14_06-16-12
  done: false
  episode_len_mean: 784.6101608744598
  episode_reward_max: 322.949494949495
  episode_reward_mean: 286.93258216768396
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 197
  episodes_total: 27537
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.13145457083980241
        entropy_coeff: 0.0005000000000000001
        kl: 0.003317481663543731
        model: {}
        policy_loss: -0.007001562073128298
        total_loss: 2.089047600825628
        vf_explained_var: 0.9958324432373047
        vf_loss: 2.0961148937543235
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.933333333333334
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612077437701493
    mean_env_wait_ms: 1.207263755273939
    mean_inference_ms: 4.311249066980734
    mean_raw_obs_processing_ms: 0.37827302472272717
  time_since_restore: 3522.6984674930573
  time_this_iter_s: 26.21027898788452
  time_total_s: 3522.6984674930573
  timers:
    learn_throughput: 8360.243
    learn_time_ms: 19352.548
    sample_throughput: 23951.437
    sample_time_ms: 6755.002
    update_time_ms: 32.85
  timestamp: 1602656172
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    134 |           3522.7 | 21680128 |  286.933 |              322.949 |              165.677 |             784.61 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3144.7354954954953
    time_step_min: 2911
  date: 2020-10-14_06-16-38
  done: false
  episode_len_mean: 784.5569515443876
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.138361438563
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 241
  episodes_total: 27778
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12677601476510367
        entropy_coeff: 0.0005000000000000001
        kl: 0.004002009110990912
        model: {}
        policy_loss: -0.007657081184637112
        total_loss: 1.9598196148872375
        vf_explained_var: 0.9963613152503967
        vf_loss: 1.9675400753815968
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.52258064516129
    gpu_util_percent0: 0.28225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611387992203992
    mean_env_wait_ms: 1.207194904834772
    mean_inference_ms: 4.310820246412951
    mean_raw_obs_processing_ms: 0.37825019591581804
  time_since_restore: 3548.9124257564545
  time_this_iter_s: 26.213958263397217
  time_total_s: 3548.9124257564545
  timers:
    learn_throughput: 8367.276
    learn_time_ms: 19336.281
    sample_throughput: 23951.476
    sample_time_ms: 6754.991
    update_time_ms: 33.012
  timestamp: 1602656198
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    135 |          3548.91 | 21841920 |  287.138 |              322.949 |              165.677 |            784.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3143.672083035075
    time_step_min: 2911
  date: 2020-10-14_06-17-05
  done: false
  episode_len_mean: 784.5266733409611
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.30164415898105
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 190
  episodes_total: 27968
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.11515582290788491
        entropy_coeff: 0.0005000000000000001
        kl: 0.003492694755550474
        model: {}
        policy_loss: -0.006612631312843102
        total_loss: 1.3455493052800496
        vf_explained_var: 0.9970569610595703
        vf_loss: 1.3522194921970367
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.563333333333336
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610907475773144
    mean_env_wait_ms: 1.2071415526831488
    mean_inference_ms: 4.3104856453006395
    mean_raw_obs_processing_ms: 0.37823413205564343
  time_since_restore: 3575.2570672035217
  time_this_iter_s: 26.34464144706726
  time_total_s: 3575.2570672035217
  timers:
    learn_throughput: 8362.438
    learn_time_ms: 19347.467
    sample_throughput: 23949.548
    sample_time_ms: 6755.535
    update_time_ms: 32.714
  timestamp: 1602656225
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 71f5b_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | RUNNING  | 172.17.0.4:47748 |    136 |          3575.26 | 22003712 |  287.302 |              322.949 |              165.677 |            784.527 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_71f5b_00000:
  custom_metrics:
    time_step_max: 3906
    time_step_mean: 3142.6615231458436
    time_step_min: 2911
  date: 2020-10-14_06-17-32
  done: true
  episode_len_mean: 784.5001065567948
  episode_reward_max: 322.949494949495
  episode_reward_mean: 287.45032480089657
  episode_reward_min: 165.67676767676747
  episodes_this_iter: 186
  episodes_total: 28154
  experiment_id: 113fc5486ae245788f33d4633921d417
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.12704486399888992
        entropy_coeff: 0.0005000000000000001
        kl: 0.003533348091877997
        model: {}
        policy_loss: -0.007090890683078517
        total_loss: 2.213720997174581
        vf_explained_var: 0.9953908920288086
        vf_loss: 2.220875402291616
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.456666666666667
    gpu_util_percent0: 0.23366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 47748
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610390168019027
    mean_env_wait_ms: 1.2070898323552974
    mean_inference_ms: 4.310163708771382
    mean_raw_obs_processing_ms: 0.37821901199488794
  time_since_restore: 3601.4923491477966
  time_this_iter_s: 26.235281944274902
  time_total_s: 3601.4923491477966
  timers:
    learn_throughput: 8358.048
    learn_time_ms: 19357.63
    sample_throughput: 23975.279
    sample_time_ms: 6748.284
    update_time_ms: 31.386
  timestamp: 1602656252
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 71f5b_00000
  
2020-10-14 06:17:33,185	WARNING util.py:136 -- The `process_trial` operation took 0.5144245624542236 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | TERMINATED |       |    137 |          3601.49 | 22165504 |   287.45 |              322.949 |              165.677 |              784.5 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.37 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_71f5b_00000 | TERMINATED |       |    137 |          3601.49 | 22165504 |   287.45 |              322.949 |              165.677 |              784.5 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


