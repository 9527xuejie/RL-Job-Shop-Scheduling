2020-10-14 03:13:50,436	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_48d97_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=7847)[0m 2020-10-14 03:13:53,142	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=7754)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7754)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7764)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7764)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7753)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7753)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7850)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7850)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7805)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7805)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7758)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7758)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7869)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7869)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7858)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7858)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7833)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7833)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7763)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7763)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7824)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7824)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7826)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7826)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7755)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7755)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7864)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7864)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7766)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7766)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7874)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7874)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7857)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7857)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7779)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7779)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7830)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7830)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7845)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7845)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7825)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7825)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7873)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7873)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7879)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7879)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7866)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7866)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7855)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7855)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7777)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7777)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7757)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7757)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7771)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7771)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7835)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7835)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7849)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7849)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7860)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7860)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7841)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7841)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7774)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7774)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7862)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7862)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7831)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7831)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7769)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7769)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7765)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7765)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7801)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7801)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7760)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7760)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7761)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7761)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7750)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7750)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7832)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7832)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7749)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7749)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7840)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7840)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7852)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7852)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7829)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7829)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7799)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7799)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7752)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7752)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7746)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7746)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7812)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7812)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7756)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7756)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7844)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7844)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7781)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7781)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7883)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7883)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7767)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7767)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7782)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7782)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7837)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7837)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7807)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7807)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7827)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7827)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7838)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7838)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7814)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7814)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7816)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7816)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7797)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7797)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7823)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7823)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7846)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7846)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7822)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7822)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7819)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7819)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7853)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7853)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7818)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7818)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7748)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7748)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7747)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7747)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7751)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7751)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7783)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7783)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7776)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7776)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7815)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7815)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7817)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7817)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7828)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7828)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7768)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7768)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7863)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7863)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3771.2241379310344
    time_step_min: 3428
  date: 2020-10-14_03-14-27
  done: false
  episode_len_mean: 902.7784810126582
  episode_reward_max: 270.95959595959573
  episode_reward_mean: 218.82112261859064
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1621165374914806
        entropy_coeff: 0.0005000000000000001
        kl: 0.004109993615808587
        model: {}
        policy_loss: -0.008680239833969003
        total_loss: 463.3200505574544
        vf_explained_var: 0.5503719449043274
        vf_loss: 463.32850392659503
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.290909090909093
    gpu_util_percent0: 0.2863636363636364
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.563636363636364
    vram_util_percent0: 0.08750757824224535
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17134570457126375
    mean_env_wait_ms: 1.1720016157969386
    mean_inference_ms: 5.43567587100786
    mean_raw_obs_processing_ms: 0.4517567691408436
  time_since_restore: 28.470484972000122
  time_this_iter_s: 28.470484972000122
  time_total_s: 28.470484972000122
  timers:
    learn_throughput: 8181.884
    learn_time_ms: 19774.418
    sample_throughput: 18764.311
    sample_time_ms: 8622.326
    update_time_ms: 45.887
  timestamp: 1602645267
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      1 |          28.4705 | 161792 |  218.821 |               270.96 |              107.323 |            902.778 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3747.29197080292
    time_step_min: 3428
  date: 2020-10-14_03-14-54
  done: false
  episode_len_mean: 900.9272151898734
  episode_reward_max: 274.14141414141386
  episode_reward_mean: 222.16164812683766
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1380364000797272
        entropy_coeff: 0.0005000000000000001
        kl: 0.00867712350251774
        model: {}
        policy_loss: -0.00887471608408911
        total_loss: 109.33460108439128
        vf_explained_var: 0.8141503930091858
        vf_loss: 109.34317715962727
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.325806451612905
    gpu_util_percent0: 0.30774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7548387096774185
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16754224945353002
    mean_env_wait_ms: 1.1677623585649486
    mean_inference_ms: 5.333912259627957
    mean_raw_obs_processing_ms: 0.44295324629846167
  time_since_restore: 55.67880463600159
  time_this_iter_s: 27.208319664001465
  time_total_s: 55.67880463600159
  timers:
    learn_throughput: 8271.114
    learn_time_ms: 19561.09
    sample_throughput: 19727.694
    sample_time_ms: 8201.263
    update_time_ms: 40.292
  timestamp: 1602645294
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      2 |          55.6788 | 323584 |  222.162 |              274.141 |              107.323 |            900.927 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3727.0833333333335
    time_step_min: 3406
  date: 2020-10-14_03-15-21
  done: false
  episode_len_mean: 894.8776371308016
  episode_reward_max: 274.14141414141386
  episode_reward_mean: 223.82911392405032
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1231509645779927
        entropy_coeff: 0.0005000000000000001
        kl: 0.010296327294781804
        model: {}
        policy_loss: -0.01334596873978929
        total_loss: 55.0855909983317
        vf_explained_var: 0.8887900710105896
        vf_loss: 55.09846909840902
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.445161290322584
    gpu_util_percent0: 0.33870967741935476
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16476142107271669
    mean_env_wait_ms: 1.167937971253417
    mean_inference_ms: 5.21781581307199
    mean_raw_obs_processing_ms: 0.4355017363981093
  time_since_restore: 82.92913389205933
  time_this_iter_s: 27.25032925605774
  time_total_s: 82.92913389205933
  timers:
    learn_throughput: 8243.176
    learn_time_ms: 19627.387
    sample_throughput: 20392.394
    sample_time_ms: 7933.939
    update_time_ms: 39.715
  timestamp: 1602645321
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      3 |          82.9291 | 485376 |  223.829 |              274.141 |              107.323 |            894.878 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3720.9745762711864
    time_step_min: 3388
  date: 2020-10-14_03-15-48
  done: false
  episode_len_mean: 890.2642405063291
  episode_reward_max: 276.26262626262604
  episode_reward_mean: 224.71463048203523
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1073287626107533
        entropy_coeff: 0.0005000000000000001
        kl: 0.008640646080796918
        model: {}
        policy_loss: -0.01210791828634683
        total_loss: 43.94570732116699
        vf_explained_var: 0.9174127578735352
        vf_loss: 43.95750586191813
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.609677419354846
    gpu_util_percent0: 0.2796774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16262739293763773
    mean_env_wait_ms: 1.1687319727519907
    mean_inference_ms: 5.120159309922911
    mean_raw_obs_processing_ms: 0.42947707229408477
  time_since_restore: 109.43868684768677
  time_this_iter_s: 26.50955295562744
  time_total_s: 109.43868684768677
  timers:
    learn_throughput: 8272.77
    learn_time_ms: 19557.174
    sample_throughput: 20969.894
    sample_time_ms: 7715.442
    update_time_ms: 38.881
  timestamp: 1602645348
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      4 |          109.439 | 647168 |  224.715 |              276.263 |              107.323 |            890.264 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3710.621657754011
    time_step_min: 3388
  date: 2020-10-14_03-16-15
  done: false
  episode_len_mean: 884.3227848101266
  episode_reward_max: 276.2626262626261
  episode_reward_mean: 226.2800792737499
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0708771049976349
        entropy_coeff: 0.0005000000000000001
        kl: 0.0078018741915002465
        model: {}
        policy_loss: -0.012055049980214486
        total_loss: 35.78077920277914
        vf_explained_var: 0.9343737959861755
        vf_loss: 35.792589823404946
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08387096774194
    gpu_util_percent0: 0.2787096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16097557376618757
    mean_env_wait_ms: 1.1699147613526728
    mean_inference_ms: 5.040014419037739
    mean_raw_obs_processing_ms: 0.42444938576903485
  time_since_restore: 136.25011372566223
  time_this_iter_s: 26.811426877975464
  time_total_s: 136.25011372566223
  timers:
    learn_throughput: 8262.895
    learn_time_ms: 19580.546
    sample_throughput: 21336.774
    sample_time_ms: 7582.777
    update_time_ms: 38.079
  timestamp: 1602645375
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      5 |           136.25 | 808960 |   226.28 |              276.263 |              107.323 |            884.323 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3688.6055396370584
    time_step_min: 3373
  date: 2020-10-14_03-16-41
  done: false
  episode_len_mean: 871.2506887052342
  episode_reward_max: 278.53535353535335
  episode_reward_mean: 230.34310042574478
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 299
  episodes_total: 1089
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0434323847293854
        entropy_coeff: 0.0005000000000000001
        kl: 0.007448257490371664
        model: {}
        policy_loss: -0.012186863285023719
        total_loss: 35.14606253306071
        vf_explained_var: 0.9557506442070007
        vf_loss: 35.15802574157715
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90666666666667
    gpu_util_percent0: 0.32033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15879486941305607
    mean_env_wait_ms: 1.1738387330816868
    mean_inference_ms: 4.930163068509885
    mean_raw_obs_processing_ms: 0.4179767320023722
  time_since_restore: 162.72091388702393
  time_this_iter_s: 26.470800161361694
  time_total_s: 162.72091388702393
  timers:
    learn_throughput: 8272.929
    learn_time_ms: 19556.797
    sample_throughput: 21641.096
    sample_time_ms: 7476.146
    update_time_ms: 38.56
  timestamp: 1602645401
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      6 |          162.721 | 970752 |  230.343 |              278.535 |              107.323 |            871.251 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3680.719312602291
    time_step_min: 3329
  date: 2020-10-14_03-17-08
  done: false
  episode_len_mean: 863.7784810126582
  episode_reward_max: 285.20202020202015
  episode_reward_mean: 231.55858426032455
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 1264
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0492589275042217
        entropy_coeff: 0.0005000000000000001
        kl: 0.006972051497238378
        model: {}
        policy_loss: -0.01170268045583119
        total_loss: 21.69623072942098
        vf_explained_var: 0.9608769416809082
        vf_loss: 21.70776096979777
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.166666666666675
    gpu_util_percent0: 0.3663333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15786896429496408
    mean_env_wait_ms: 1.1758861513863466
    mean_inference_ms: 4.882645140822294
    mean_raw_obs_processing_ms: 0.41514696551037883
  time_since_restore: 189.0348060131073
  time_this_iter_s: 26.313892126083374
  time_total_s: 189.0348060131073
  timers:
    learn_throughput: 8283.082
    learn_time_ms: 19532.825
    sample_throughput: 21902.931
    sample_time_ms: 7386.774
    update_time_ms: 36.13
  timestamp: 1602645428
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      7 |          189.035 | 1132544 |  231.559 |              285.202 |              107.323 |            863.778 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3671.960144927536
    time_step_min: 3329
  date: 2020-10-14_03-17-34
  done: false
  episode_len_mean: 858.0611814345991
  episode_reward_max: 285.20202020202015
  episode_reward_mean: 232.9800110812767
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0351685583591461
        entropy_coeff: 0.0005000000000000001
        kl: 0.006892501260153949
        model: {}
        policy_loss: -0.011367103511778017
        total_loss: 19.161619822184246
        vf_explained_var: 0.9610426425933838
        vf_loss: 19.172815481821697
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.129032258064516
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15713632290783117
    mean_env_wait_ms: 1.1777967541195784
    mean_inference_ms: 4.844947331960065
    mean_raw_obs_processing_ms: 0.41285656629902334
  time_since_restore: 215.4286231994629
  time_this_iter_s: 26.39381718635559
  time_total_s: 215.4286231994629
  timers:
    learn_throughput: 8289.015
    learn_time_ms: 19518.845
    sample_throughput: 22116.987
    sample_time_ms: 7315.282
    update_time_ms: 44.343
  timestamp: 1602645454
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      8 |          215.429 | 1294336 |   232.98 |              285.202 |              107.323 |            858.061 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3662.9545454545455
    time_step_min: 3329
  date: 2020-10-14_03-18-00
  done: false
  episode_len_mean: 853.1422250316056
  episode_reward_max: 287.0202020202019
  episode_reward_mean: 234.41216846084077
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 160
  episodes_total: 1582
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9972319106260935
        entropy_coeff: 0.0005000000000000001
        kl: 0.007517460539626579
        model: {}
        policy_loss: -0.012781010950978574
        total_loss: 18.780067920684814
        vf_explained_var: 0.9643420577049255
        vf_loss: 18.792595386505127
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85333333333334
    gpu_util_percent0: 0.3026666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1564818892334054
    mean_env_wait_ms: 1.1797789814034563
    mean_inference_ms: 4.811034825161571
    mean_raw_obs_processing_ms: 0.41074713347627007
  time_since_restore: 241.72326278686523
  time_this_iter_s: 26.294639587402344
  time_total_s: 241.72326278686523
  timers:
    learn_throughput: 8298.754
    learn_time_ms: 19495.938
    sample_throughput: 22254.789
    sample_time_ms: 7269.986
    update_time_ms: 41.731
  timestamp: 1602645480
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |      9 |          241.723 | 1456128 |  234.412 |               287.02 |              107.323 |            853.142 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3647.884034519957
    time_step_min: 3261
  date: 2020-10-14_03-18-27
  done: false
  episode_len_mean: 843.7589662447257
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 236.817302774581
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 314
  episodes_total: 1896
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9691165437301
        entropy_coeff: 0.0005000000000000001
        kl: 0.0063012606697157025
        model: {}
        policy_loss: -0.011411164688373296
        total_loss: 24.393256028493244
        vf_explained_var: 0.9697794914245605
        vf_loss: 24.40452178319295
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.720000000000002
    gpu_util_percent0: 0.4706666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1554478672226877
    mean_env_wait_ms: 1.1836322190953148
    mean_inference_ms: 4.756859171696043
    mean_raw_obs_processing_ms: 0.40747030307763654
  time_since_restore: 268.305606842041
  time_this_iter_s: 26.58234405517578
  time_total_s: 268.305606842041
  timers:
    learn_throughput: 8296.969
    learn_time_ms: 19500.133
    sample_throughput: 22348.252
    sample_time_ms: 7239.582
    update_time_ms: 39.705
  timestamp: 1602645507
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     10 |          268.306 | 1617920 |  236.817 |              295.505 |              107.323 |            843.759 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3640.322564612326
    time_step_min: 3261
  date: 2020-10-14_03-18-54
  done: false
  episode_len_mean: 839.8476144109055
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 237.9823306089127
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9603535681962967
        entropy_coeff: 0.0005000000000000001
        kl: 0.006699105259031057
        model: {}
        policy_loss: -0.013546019990523442
        total_loss: 14.318873723347982
        vf_explained_var: 0.9735555052757263
        vf_loss: 14.332229693730673
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96129032258065
    gpu_util_percent0: 0.35774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15501384578436053
    mean_env_wait_ms: 1.1852816865258073
    mean_inference_ms: 4.7343166260456595
    mean_raw_obs_processing_ms: 0.40611136788057267
  time_since_restore: 295.0709490776062
  time_this_iter_s: 26.765342235565186
  time_total_s: 295.0709490776062
  timers:
    learn_throughput: 8308.86
    learn_time_ms: 19472.225
    sample_throughput: 22805.1
    sample_time_ms: 7094.553
    update_time_ms: 39.516
  timestamp: 1602645534
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     11 |          295.071 | 1779712 |  237.982 |              295.505 |              107.323 |            839.848 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3634.2548387096776
    time_step_min: 3261
  date: 2020-10-14_03-19-21
  done: false
  episode_len_mean: 836.2640144665461
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 238.97284326081777
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.944629356265068
        entropy_coeff: 0.0005000000000000001
        kl: 0.007740743652296563
        model: {}
        policy_loss: -0.011666560147811348
        total_loss: 15.514042218526205
        vf_explained_var: 0.9695773720741272
        vf_loss: 15.525406757990519
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.132258064516126
    gpu_util_percent0: 0.25000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15461799522010705
    mean_env_wait_ms: 1.186909432377722
    mean_inference_ms: 4.713716768682217
    mean_raw_obs_processing_ms: 0.4048314290671175
  time_since_restore: 321.800833940506
  time_this_iter_s: 26.72988486289978
  time_total_s: 321.800833940506
  timers:
    learn_throughput: 8296.106
    learn_time_ms: 19502.161
    sample_throughput: 23067.112
    sample_time_ms: 7013.969
    update_time_ms: 40.064
  timestamp: 1602645561
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     12 |          321.801 | 1941504 |  238.973 |              295.505 |              107.323 |            836.264 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3624.788824979458
    time_step_min: 3261
  date: 2020-10-14_03-19-47
  done: false
  episode_len_mean: 831.7665589660743
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 240.26908829816728
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 264
  episodes_total: 2476
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8990111748377482
        entropy_coeff: 0.0005000000000000001
        kl: 0.006856335133003692
        model: {}
        policy_loss: -0.012046161340549588
        total_loss: 21.309036095937092
        vf_explained_var: 0.9721997380256653
        vf_loss: 21.32084560394287
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.966666666666672
    gpu_util_percent0: 0.274
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15404804916779058
    mean_env_wait_ms: 1.1895413202852019
    mean_inference_ms: 4.683451461168151
    mean_raw_obs_processing_ms: 0.402976904433761
  time_since_restore: 348.2735447883606
  time_this_iter_s: 26.472710847854614
  time_total_s: 348.2735447883606
  timers:
    learn_throughput: 8309.545
    learn_time_ms: 19470.62
    sample_throughput: 23221.81
    sample_time_ms: 6967.243
    update_time_ms: 39.635
  timestamp: 1602645587
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     13 |          348.274 | 2103296 |  240.269 |              295.505 |              107.323 |            831.767 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3618.495839636914
    time_step_min: 3261
  date: 2020-10-14_03-20-14
  done: false
  episode_len_mean: 828.9441548771407
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 241.32862504418702
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 2686
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8919987479845682
        entropy_coeff: 0.0005000000000000001
        kl: 0.006052382290363312
        model: {}
        policy_loss: -0.010693902731873095
        total_loss: 12.854787826538086
        vf_explained_var: 0.9796460270881653
        vf_loss: 12.865322589874268
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.325806451612905
    gpu_util_percent0: 0.26903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15363504502934977
    mean_env_wait_ms: 1.1913020077859229
    mean_inference_ms: 4.662645243951002
    mean_raw_obs_processing_ms: 0.4016938268914505
  time_since_restore: 374.73120164871216
  time_this_iter_s: 26.457656860351562
  time_total_s: 374.73120164871216
  timers:
    learn_throughput: 8305.548
    learn_time_ms: 19479.99
    sample_throughput: 23264.227
    sample_time_ms: 6954.54
    update_time_ms: 38.522
  timestamp: 1602645614
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     14 |          374.731 | 2265088 |  241.329 |              295.505 |              107.323 |            828.944 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3614.1402569593147
    time_step_min: 3261
  date: 2020-10-14_03-20-41
  done: false
  episode_len_mean: 827.4785513361463
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 242.05008950262095
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8864663541316986
        entropy_coeff: 0.0005000000000000001
        kl: 0.006266986446765562
        model: {}
        policy_loss: -0.011825939640402794
        total_loss: 11.424020608266195
        vf_explained_var: 0.9787198901176453
        vf_loss: 11.43566338221232
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.938709677419357
    gpu_util_percent0: 0.3861290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15336184976226608
    mean_env_wait_ms: 1.1924984822709317
    mean_inference_ms: 4.648412160015693
    mean_raw_obs_processing_ms: 0.4008194184667104
  time_since_restore: 401.3878610134125
  time_this_iter_s: 26.656659364700317
  time_total_s: 401.3878610134125
  timers:
    learn_throughput: 8308.887
    learn_time_ms: 19472.164
    sample_throughput: 23299.26
    sample_time_ms: 6944.083
    update_time_ms: 39.923
  timestamp: 1602645641
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     15 |          401.388 | 2426880 |   242.05 |              295.505 |              107.323 |            827.479 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3609.2964213369346
    time_step_min: 3261
  date: 2020-10-14_03-21-07
  done: false
  episode_len_mean: 826.3575233022636
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 242.80506462763432
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 160
  episodes_total: 3004
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8594326078891754
        entropy_coeff: 0.0005000000000000001
        kl: 0.00622925612454613
        model: {}
        policy_loss: -0.011599689605645835
        total_loss: 10.48609709739685
        vf_explained_var: 0.9815452098846436
        vf_loss: 10.497503678003946
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98
    gpu_util_percent0: 0.3393333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15310395450694253
    mean_env_wait_ms: 1.1936140014964935
    mean_inference_ms: 4.6349297878169615
    mean_raw_obs_processing_ms: 0.3999790390829319
  time_since_restore: 427.9809219837189
  time_this_iter_s: 26.593060970306396
  time_total_s: 427.9809219837189
  timers:
    learn_throughput: 8311.834
    learn_time_ms: 19465.26
    sample_throughput: 23233.594
    sample_time_ms: 6963.709
    update_time_ms: 38.532
  timestamp: 1602645667
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     16 |          427.981 | 2588672 |  242.805 |              295.505 |              107.323 |            826.358 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3599.8479926448053
    time_step_min: 3261
  date: 2020-10-14_03-21-35
  done: false
  episode_len_mean: 824.7621785173978
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 244.21925762924232
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 301
  episodes_total: 3305
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8330417821804682
        entropy_coeff: 0.0005000000000000001
        kl: 0.00592450723828127
        model: {}
        policy_loss: -0.010085929917598454
        total_loss: 15.710782845815023
        vf_explained_var: 0.9800810217857361
        vf_loss: 15.720693031946817
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8875
    gpu_util_percent0: 0.355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.753125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1526760485032269
    mean_env_wait_ms: 1.1954825410277934
    mean_inference_ms: 4.612519209317078
    mean_raw_obs_processing_ms: 0.3986181524950563
  time_since_restore: 454.9668025970459
  time_this_iter_s: 26.985880613327026
  time_total_s: 454.9668025970459
  timers:
    learn_throughput: 8295.287
    learn_time_ms: 19504.087
    sample_throughput: 23175.811
    sample_time_ms: 6981.072
    update_time_ms: 40.694
  timestamp: 1602645695
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     17 |          454.967 | 2750464 |  244.219 |              295.505 |              107.323 |            824.762 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3595.008444962143
    time_step_min: 3261
  date: 2020-10-14_03-22-01
  done: false
  episode_len_mean: 823.8201956271577
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 244.87837233090377
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 171
  episodes_total: 3476
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.823181688785553
        entropy_coeff: 0.0005000000000000001
        kl: 0.006371172416644792
        model: {}
        policy_loss: -0.011092360441883406
        total_loss: 10.869799057642618
        vf_explained_var: 0.9816603064537048
        vf_loss: 10.880666017532349
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333334
    gpu_util_percent0: 0.29133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15245808367421862
    mean_env_wait_ms: 1.1963712718833723
    mean_inference_ms: 4.601152522631464
    mean_raw_obs_processing_ms: 0.3979223847305483
  time_since_restore: 481.3257179260254
  time_this_iter_s: 26.358915328979492
  time_total_s: 481.3257179260254
  timers:
    learn_throughput: 8297.196
    learn_time_ms: 19499.601
    sample_throughput: 23152.346
    sample_time_ms: 6988.147
    update_time_ms: 33.767
  timestamp: 1602645721
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     18 |          481.326 | 2912256 |  244.878 |              295.505 |              107.323 |             823.82 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3590.344376391982
    time_step_min: 3261
  date: 2020-10-14_03-22-28
  done: false
  episode_len_mean: 822.8646119977985
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 245.54553515340507
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 3634
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.820036510626475
        entropy_coeff: 0.0005000000000000001
        kl: 0.006021461876419683
        model: {}
        policy_loss: -0.012707485565139601
        total_loss: 9.866514841715494
        vf_explained_var: 0.9809303283691406
        vf_loss: 9.879030307133993
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.835483870967742
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15226990166062343
    mean_env_wait_ms: 1.1971392832751875
    mean_inference_ms: 4.591298084914903
    mean_raw_obs_processing_ms: 0.39731312463180357
  time_since_restore: 507.77226734161377
  time_this_iter_s: 26.44654941558838
  time_total_s: 507.77226734161377
  timers:
    learn_throughput: 8289.11
    learn_time_ms: 19518.621
    sample_throughput: 23173.926
    sample_time_ms: 6981.64
    update_time_ms: 34.982
  timestamp: 1602645748
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     19 |          507.772 | 3074048 |  245.546 |              295.505 |              107.323 |            822.865 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3584.9151915455745
    time_step_min: 3249
  date: 2020-10-14_03-22-55
  done: false
  episode_len_mean: 821.9960804807944
  episode_reward_max: 297.32323232323273
  episode_reward_mean: 246.2629034003477
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 193
  episodes_total: 3827
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7892957677443823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005970731726847589
        model: {}
        policy_loss: -0.010454207542352378
        total_loss: 12.134648323059082
        vf_explained_var: 0.9820330142974854
        vf_loss: 12.144899845123291
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.3629032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15205186053511877
    mean_env_wait_ms: 1.1979821667495876
    mean_inference_ms: 4.579900792674368
    mean_raw_obs_processing_ms: 0.39659663125721345
  time_since_restore: 534.5423679351807
  time_this_iter_s: 26.770100593566895
  time_total_s: 534.5423679351807
  timers:
    learn_throughput: 8290.385
    learn_time_ms: 19515.619
    sample_throughput: 23133.634
    sample_time_ms: 6993.8
    update_time_ms: 35.993
  timestamp: 1602645775
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     20 |          534.542 | 3235840 |  246.263 |              297.323 |              107.323 |            821.996 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3576.8825412459987
    time_step_min: 3249
  date: 2020-10-14_03-23-21
  done: false
  episode_len_mean: 820.9390689739215
  episode_reward_max: 297.32323232323273
  episode_reward_mean: 247.46869130003404
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 276
  episodes_total: 4103
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7702900916337967
        entropy_coeff: 0.0005000000000000001
        kl: 0.005835725654227038
        model: {}
        policy_loss: -0.012004603505677855
        total_loss: 10.29564905166626
        vf_explained_var: 0.9854583144187927
        vf_loss: 10.30745498339335
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333335
    gpu_util_percent0: 0.3803333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15177847653504692
    mean_env_wait_ms: 1.1991133444326951
    mean_inference_ms: 4.565463394385833
    mean_raw_obs_processing_ms: 0.3957274547686938
  time_since_restore: 561.0020604133606
  time_this_iter_s: 26.45969247817993
  time_total_s: 561.0020604133606
  timers:
    learn_throughput: 8291.479
    learn_time_ms: 19513.046
    sample_throughput: 23226.84
    sample_time_ms: 6965.735
    update_time_ms: 33.988
  timestamp: 1602645801
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     21 |          561.002 | 3397632 |  247.469 |              297.323 |              107.323 |            820.939 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3572.4107481060605
    time_step_min: 3249
  date: 2020-10-14_03-23-48
  done: false
  episode_len_mean: 820.4036568213784
  episode_reward_max: 297.32323232323273
  episode_reward_mean: 248.11220503203612
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 163
  episodes_total: 4266
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.762136846780777
        entropy_coeff: 0.0005000000000000001
        kl: 0.00587383983656764
        model: {}
        policy_loss: -0.012054619951716935
        total_loss: 9.693988800048828
        vf_explained_var: 0.9822675585746765
        vf_loss: 9.70583693186442
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.429032258064513
    gpu_util_percent0: 0.3016129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15162935346021864
    mean_env_wait_ms: 1.1996809466026548
    mean_inference_ms: 4.557609902473701
    mean_raw_obs_processing_ms: 0.3952472996883145
  time_since_restore: 587.5435025691986
  time_this_iter_s: 26.541442155838013
  time_total_s: 587.5435025691986
  timers:
    learn_throughput: 8301.997
    learn_time_ms: 19488.323
    sample_throughput: 23205.603
    sample_time_ms: 6972.109
    update_time_ms: 33.874
  timestamp: 1602645828
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     22 |          587.544 | 3559424 |  248.112 |              297.323 |              107.323 |            820.404 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3568.2400730260156
    time_step_min: 3239
  date: 2020-10-14_03-24-15
  done: false
  episode_len_mean: 819.8386075949367
  episode_reward_max: 298.838383838384
  episode_reward_mean: 248.82584890496273
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 4424
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7569443633159002
        entropy_coeff: 0.0005000000000000001
        kl: 0.005998353357426822
        model: {}
        policy_loss: -0.012208390917900639
        total_loss: 10.326914469401041
        vf_explained_var: 0.9792599081993103
        vf_loss: 10.33890144030253
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.3
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15149193903924785
    mean_env_wait_ms: 1.2001920543396456
    mean_inference_ms: 4.55035069764995
    mean_raw_obs_processing_ms: 0.39479611765229466
  time_since_restore: 614.218982219696
  time_this_iter_s: 26.675479650497437
  time_total_s: 614.218982219696
  timers:
    learn_throughput: 8294.162
    learn_time_ms: 19506.732
    sample_throughput: 23204.329
    sample_time_ms: 6972.492
    update_time_ms: 34.039
  timestamp: 1602645855
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     23 |          614.219 | 3721216 |  248.826 |              298.838 |              107.323 |            819.839 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3561.0428015564203
    time_step_min: 3239
  date: 2020-10-14_03-24-42
  done: false
  episode_len_mean: 819.2707797772065
  episode_reward_max: 299.2929292929294
  episode_reward_mean: 249.94563241671196
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 244
  episodes_total: 4668
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7197075833876928
        entropy_coeff: 0.0005000000000000001
        kl: 0.005537901306524873
        model: {}
        policy_loss: -0.010951873955491465
        total_loss: 11.808536291122437
        vf_explained_var: 0.9827486872673035
        vf_loss: 11.819294055302938
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71
    gpu_util_percent0: 0.3863333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15129211678685747
    mean_env_wait_ms: 1.2008860310570153
    mean_inference_ms: 4.539826159167419
    mean_raw_obs_processing_ms: 0.39414967148698354
  time_since_restore: 640.7956004142761
  time_this_iter_s: 26.576618194580078
  time_total_s: 640.7956004142761
  timers:
    learn_throughput: 8288.723
    learn_time_ms: 19519.533
    sample_throughput: 23214.049
    sample_time_ms: 6969.573
    update_time_ms: 35.32
  timestamp: 1602645882
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     24 |          640.796 | 3883008 |  249.946 |              299.293 |              107.323 |            819.271 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3554.94686985173
    time_step_min: 3239
  date: 2020-10-14_03-25-09
  done: false
  episode_len_mean: 818.7715394038383
  episode_reward_max: 302.02020202020185
  episode_reward_mean: 250.88173486601403
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 230
  episodes_total: 4898
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.706352710723877
        entropy_coeff: 0.0005000000000000001
        kl: 0.005637997140487035
        model: {}
        policy_loss: -0.011160943327316394
        total_loss: 8.13277781009674
        vf_explained_var: 0.9867730736732483
        vf_loss: 8.143728057543436
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.009375
    gpu_util_percent0: 0.2834375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.765625
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15112164139196624
    mean_env_wait_ms: 1.2014605195832375
    mean_inference_ms: 4.530811598915055
    mean_raw_obs_processing_ms: 0.3935970666414366
  time_since_restore: 667.8496024608612
  time_this_iter_s: 27.054002046585083
  time_total_s: 667.8496024608612
  timers:
    learn_throughput: 8281.085
    learn_time_ms: 19537.536
    sample_throughput: 23146.129
    sample_time_ms: 6990.024
    update_time_ms: 34.48
  timestamp: 1602645909
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     25 |           667.85 | 4044800 |  250.882 |               302.02 |              107.323 |            818.772 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3551.3155165536496
    time_step_min: 3239
  date: 2020-10-14_03-25-35
  done: false
  episode_len_mean: 818.4770569620254
  episode_reward_max: 302.02020202020185
  episode_reward_mean: 251.44326972254174
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 5056
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7149504125118256
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056422999283919735
        model: {}
        policy_loss: -0.01032269986656805
        total_loss: 8.663956960042318
        vf_explained_var: 0.9832354187965393
        vf_loss: 8.674073060353598
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.496666666666673
    gpu_util_percent0: 0.4056666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15101030584320446
    mean_env_wait_ms: 1.201804015690556
    mean_inference_ms: 4.524954698609433
    mean_raw_obs_processing_ms: 0.39323649746575867
  time_since_restore: 694.1785354614258
  time_this_iter_s: 26.328933000564575
  time_total_s: 694.1785354614258
  timers:
    learn_throughput: 8281.687
    learn_time_ms: 19536.117
    sample_throughput: 23235.205
    sample_time_ms: 6963.227
    update_time_ms: 35.629
  timestamp: 1602645935
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     26 |          694.179 | 4206592 |  251.443 |               302.02 |              107.323 |            818.477 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3547.504347826087
    time_step_min: 3239
  date: 2020-10-14_03-26-02
  done: false
  episode_len_mean: 817.9539965497412
  episode_reward_max: 302.02020202020185
  episode_reward_mean: 252.06013557077375
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 161
  episodes_total: 5217
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7004147122303644
        entropy_coeff: 0.0005000000000000001
        kl: 0.00600345351267606
        model: {}
        policy_loss: -0.010853229614440352
        total_loss: 8.496254205703735
        vf_explained_var: 0.9839918613433838
        vf_loss: 8.50685747464498
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.003225806451617
    gpu_util_percent0: 0.29580645161290314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15090162521464984
    mean_env_wait_ms: 1.2021342924379792
    mean_inference_ms: 4.519225313680838
    mean_raw_obs_processing_ms: 0.3928807122474712
  time_since_restore: 720.7899436950684
  time_this_iter_s: 26.611408233642578
  time_total_s: 720.7899436950684
  timers:
    learn_throughput: 8294.272
    learn_time_ms: 19506.475
    sample_throughput: 23240.261
    sample_time_ms: 6961.712
    update_time_ms: 35.043
  timestamp: 1602645962
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     27 |           720.79 | 4368384 |   252.06 |               302.02 |              107.323 |            817.954 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3541.553414096916
    time_step_min: 3212
  date: 2020-10-14_03-26-29
  done: false
  episode_len_mean: 817.4360655737705
  episode_reward_max: 302.9292929292925
  episode_reward_mean: 252.9422917701605
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 273
  episodes_total: 5490
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6699864814678828
        entropy_coeff: 0.0005000000000000001
        kl: 0.005773540275792281
        model: {}
        policy_loss: -0.011313577182590961
        total_loss: 11.954463640848795
        vf_explained_var: 0.9834496974945068
        vf_loss: 11.965535004933676
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89
    gpu_util_percent0: 0.29100000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507271910726941
    mean_env_wait_ms: 1.202601923538253
    mean_inference_ms: 4.510008036324827
    mean_raw_obs_processing_ms: 0.3923082715207751
  time_since_restore: 747.171443939209
  time_this_iter_s: 26.381500244140625
  time_total_s: 747.171443939209
  timers:
    learn_throughput: 8307.397
    learn_time_ms: 19475.654
    sample_throughput: 23142.158
    sample_time_ms: 6991.224
    update_time_ms: 36.161
  timestamp: 1602645989
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     28 |          747.171 | 4530176 |  252.942 |              302.929 |              107.323 |            817.436 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3537.503365214311
    time_step_min: 3212
  date: 2020-10-14_03-26-55
  done: false
  episode_len_mean: 817.286040787623
  episode_reward_max: 302.9292929292925
  episode_reward_mean: 253.5962739206409
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 5688
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6623594363530477
        entropy_coeff: 0.0005000000000000001
        kl: 0.005243997322395444
        model: {}
        policy_loss: -0.009220112289767712
        total_loss: 7.972414493560791
        vf_explained_var: 0.9861915111541748
        vf_loss: 7.981441458066304
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25161290322581
    gpu_util_percent0: 0.2867741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15061711910642828
    mean_env_wait_ms: 1.2029621928934868
    mean_inference_ms: 4.504094103873604
    mean_raw_obs_processing_ms: 0.39194908519751653
  time_since_restore: 773.7113254070282
  time_this_iter_s: 26.539881467819214
  time_total_s: 773.7113254070282
  timers:
    learn_throughput: 8313.199
    learn_time_ms: 19462.064
    sample_throughput: 23067.647
    sample_time_ms: 7013.806
    update_time_ms: 35.833
  timestamp: 1602646015
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     29 |          773.711 | 4691968 |  253.596 |              302.929 |              107.323 |            817.286 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3534.2841144038593
    time_step_min: 3212
  date: 2020-10-14_03-27-22
  done: false
  episode_len_mean: 817.0797126240165
  episode_reward_max: 305.9595959595957
  episode_reward_mean: 254.0841359195788
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 5846
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6751842598120371
        entropy_coeff: 0.0005000000000000001
        kl: 0.005583611821445326
        model: {}
        policy_loss: -0.008562183628479639
        total_loss: 9.158657868703207
        vf_explained_var: 0.9818723797798157
        vf_loss: 9.166999419530233
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029032258064515
    gpu_util_percent0: 0.33064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1505295617014415
    mean_env_wait_ms: 1.2031863399854088
    mean_inference_ms: 4.499458694018968
    mean_raw_obs_processing_ms: 0.3916625355765893
  time_since_restore: 800.2277266979218
  time_this_iter_s: 26.516401290893555
  time_total_s: 800.2277266979218
  timers:
    learn_throughput: 8318.179
    learn_time_ms: 19450.411
    sample_throughput: 23114.634
    sample_time_ms: 6999.548
    update_time_ms: 42.634
  timestamp: 1602646042
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     30 |          800.228 | 4853760 |  254.084 |               305.96 |              107.323 |             817.08 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3530.2177688710754
    time_step_min: 3169
  date: 2020-10-14_03-27-48
  done: false
  episode_len_mean: 816.7492537313433
  episode_reward_max: 309.4444444444443
  episode_reward_mean: 254.71963415246984
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 6030
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6538680344820023
        entropy_coeff: 0.0005000000000000001
        kl: 0.005568233706677954
        model: {}
        policy_loss: -0.01098362467989015
        total_loss: 10.944342613220215
        vf_explained_var: 0.9808550477027893
        vf_loss: 10.95509640375773
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89
    gpu_util_percent0: 0.34333333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15042980537067466
    mean_env_wait_ms: 1.2034318854620838
    mean_inference_ms: 4.494268174723467
    mean_raw_obs_processing_ms: 0.39133934778768903
  time_since_restore: 826.6208298206329
  time_this_iter_s: 26.39310312271118
  time_total_s: 826.6208298206329
  timers:
    learn_throughput: 8328.774
    learn_time_ms: 19425.668
    sample_throughput: 23054.831
    sample_time_ms: 7017.705
    update_time_ms: 42.96
  timestamp: 1602646068
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     31 |          826.621 | 5015552 |   254.72 |              309.444 |              107.323 |            816.749 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3524.061341853035
    time_step_min: 3169
  date: 2020-10-14_03-28-15
  done: false
  episode_len_mean: 816.1202792764202
  episode_reward_max: 309.4444444444443
  episode_reward_mean: 255.6600758457311
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 272
  episodes_total: 6302
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6287688066562017
        entropy_coeff: 0.0005000000000000001
        kl: 0.005341685532281796
        model: {}
        policy_loss: -0.010679997292754706
        total_loss: 9.308965921401978
        vf_explained_var: 0.9860467910766602
        vf_loss: 9.319426457087198
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.29451612903225804
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15029495286206773
    mean_env_wait_ms: 1.2037329703472854
    mean_inference_ms: 4.487062269601786
    mean_raw_obs_processing_ms: 0.39089117759460906
  time_since_restore: 853.0984427928925
  time_this_iter_s: 26.47761297225952
  time_total_s: 853.0984427928925
  timers:
    learn_throughput: 8338.281
    learn_time_ms: 19403.519
    sample_throughput: 23007.548
    sample_time_ms: 7032.127
    update_time_ms: 43.229
  timestamp: 1602646095
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     32 |          853.098 | 5177344 |   255.66 |              309.444 |              107.323 |             816.12 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3520.1591050341826
    time_step_min: 3169
  date: 2020-10-14_03-28-42
  done: false
  episode_len_mean: 815.7110219203457
  episode_reward_max: 309.4444444444443
  episode_reward_mean: 256.26594752713913
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 6478
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6290171841780344
        entropy_coeff: 0.0005000000000000001
        kl: 0.005798434528211753
        model: {}
        policy_loss: -0.010629309714810612
        total_loss: 7.30441677570343
        vf_explained_var: 0.9854683876037598
        vf_loss: 7.314780632654826
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666667
    gpu_util_percent0: 0.426
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15021334480267767
    mean_env_wait_ms: 1.2039504642806043
    mean_inference_ms: 4.48277153280792
    mean_raw_obs_processing_ms: 0.3906298267491722
  time_since_restore: 879.3862257003784
  time_this_iter_s: 26.287782907485962
  time_total_s: 879.3862257003784
  timers:
    learn_throughput: 8354.715
    learn_time_ms: 19365.353
    sample_throughput: 23016.542
    sample_time_ms: 7029.379
    update_time_ms: 43.578
  timestamp: 1602646122
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     33 |          879.386 | 5339136 |  256.266 |              309.444 |              107.323 |            815.711 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3516.4803639120546
    time_step_min: 3163
  date: 2020-10-14_03-29-09
  done: false
  episode_len_mean: 815.2666867560645
  episode_reward_max: 310.3535353535349
  episode_reward_mean: 256.8258599251517
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 159
  episodes_total: 6637
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6299049854278564
        entropy_coeff: 0.0005000000000000001
        kl: 0.005551038348736863
        model: {}
        policy_loss: -0.010303838042697558
        total_loss: 7.630209843317668
        vf_explained_var: 0.983752965927124
        vf_loss: 7.640273650487264
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7741935483871
    gpu_util_percent0: 0.37290322580645163
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7870967741935475
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15014237614057363
    mean_env_wait_ms: 1.2041138305291825
    mean_inference_ms: 4.47898759493776
    mean_raw_obs_processing_ms: 0.39039502993352765
  time_since_restore: 906.128687620163
  time_this_iter_s: 26.742461919784546
  time_total_s: 906.128687620163
  timers:
    learn_throughput: 8356.867
    learn_time_ms: 19360.365
    sample_throughput: 22948.185
    sample_time_ms: 7050.318
    update_time_ms: 43.279
  timestamp: 1602646149
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     34 |          906.129 | 5500928 |  256.826 |              310.354 |              107.323 |            815.267 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3511.214536414156
    time_step_min: 3163
  date: 2020-10-14_03-29-35
  done: false
  episode_len_mean: 814.5463662790697
  episode_reward_max: 312.47474747474746
  episode_reward_mean: 257.59965938454303
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 6880
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6098584433396658
        entropy_coeff: 0.0005000000000000001
        kl: 0.00519350233177344
        model: {}
        policy_loss: -0.010517341147836609
        total_loss: 10.362518469492594
        vf_explained_var: 0.9834849834442139
        vf_loss: 10.37282125155131
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78000000000001
    gpu_util_percent0: 0.35766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15003796901627595
    mean_env_wait_ms: 1.204360416030957
    mean_inference_ms: 4.473435272064993
    mean_raw_obs_processing_ms: 0.3900501152263673
  time_since_restore: 932.22811627388
  time_this_iter_s: 26.09942865371704
  time_total_s: 932.22811627388
  timers:
    learn_throughput: 8384.329
    learn_time_ms: 19296.953
    sample_throughput: 23049.071
    sample_time_ms: 7019.459
    update_time_ms: 42.97
  timestamp: 1602646175
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     35 |          932.228 | 5662720 |    257.6 |              312.475 |              107.323 |            814.546 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3506.182937181664
    time_step_min: 3163
  date: 2020-10-14_03-30-02
  done: false
  episode_len_mean: 813.8714486638537
  episode_reward_max: 312.47474747474746
  episode_reward_mean: 258.40114222392697
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 230
  episodes_total: 7110
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5835195481777191
        entropy_coeff: 0.0005000000000000001
        kl: 0.005151468561962247
        model: {}
        policy_loss: -0.01129642988477523
        total_loss: 7.82729434967041
        vf_explained_var: 0.9858905673027039
        vf_loss: 7.838367303212483
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.369999999999994
    gpu_util_percent0: 0.30633333333333324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14994308748634708
    mean_env_wait_ms: 1.2045480418518912
    mean_inference_ms: 4.468486339260859
    mean_raw_obs_processing_ms: 0.3897449323402857
  time_since_restore: 958.7731108665466
  time_this_iter_s: 26.544994592666626
  time_total_s: 958.7731108665466
  timers:
    learn_throughput: 8379.35
    learn_time_ms: 19308.42
    sample_throughput: 23024.667
    sample_time_ms: 7026.899
    update_time_ms: 43.054
  timestamp: 1602646202
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     36 |          958.773 | 5824512 |  258.401 |              312.475 |              107.323 |            813.871 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3502.6598394685857
    time_step_min: 3138
  date: 2020-10-14_03-30-28
  done: false
  episode_len_mean: 813.5203632361034
  episode_reward_max: 314.14141414141363
  episode_reward_mean: 258.91297398864805
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 7268
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.594015454252561
        entropy_coeff: 0.0005000000000000001
        kl: 0.005375155985044937
        model: {}
        policy_loss: -0.011208146383675436
        total_loss: 7.364322900772095
        vf_explained_var: 0.9836664795875549
        vf_loss: 7.375290552775065
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29677419354839
    gpu_util_percent0: 0.3609677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1498819844784886
    mean_env_wait_ms: 1.2046726897274829
    mean_inference_ms: 4.46523517463777
    mean_raw_obs_processing_ms: 0.38954254857616516
  time_since_restore: 985.2650861740112
  time_this_iter_s: 26.4919753074646
  time_total_s: 985.2650861740112
  timers:
    learn_throughput: 8384.965
    learn_time_ms: 19295.488
    sample_throughput: 23020.499
    sample_time_ms: 7028.171
    update_time_ms: 43.221
  timestamp: 1602646228
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     37 |          985.265 | 5986304 |  258.913 |              314.141 |              107.323 |             813.52 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3498.950593952484
    time_step_min: 3131
  date: 2020-10-14_03-30-55
  done: false
  episode_len_mean: 813.0660402684564
  episode_reward_max: 315.2020202020202
  episode_reward_mean: 259.496000271168
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 7450
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.584243044257164
        entropy_coeff: 0.0005000000000000001
        kl: 0.005464488291181624
        model: {}
        policy_loss: -0.009593600155009577
        total_loss: 7.8663210074106855
        vf_explained_var: 0.9848172664642334
        vf_loss: 7.875660300254822
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333338
    gpu_util_percent0: 0.3473333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14981562646580848
    mean_env_wait_ms: 1.2048241030446716
    mean_inference_ms: 4.46164216582151
    mean_raw_obs_processing_ms: 0.38931804998092706
  time_since_restore: 1011.5310080051422
  time_this_iter_s: 26.26592183113098
  time_total_s: 1011.5310080051422
  timers:
    learn_throughput: 8376.227
    learn_time_ms: 19315.618
    sample_throughput: 23115.494
    sample_time_ms: 6999.288
    update_time_ms: 41.845
  timestamp: 1602646255
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     38 |          1011.53 | 6148096 |  259.496 |              315.202 |              107.323 |            813.066 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3493.6595938557666
    time_step_min: 3131
  date: 2020-10-14_03-31-21
  done: false
  episode_len_mean: 812.4883480062144
  episode_reward_max: 315.2020202020202
  episode_reward_mean: 260.30808473131094
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 274
  episodes_total: 7724
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5534585614999136
        entropy_coeff: 0.0005000000000000001
        kl: 0.005156314660174151
        model: {}
        policy_loss: -0.011462989670690149
        total_loss: 9.30127509435018
        vf_explained_var: 0.9854690432548523
        vf_loss: 9.312499205271402
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.525806451612905
    gpu_util_percent0: 0.32322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14971444011725155
    mean_env_wait_ms: 1.204984031952665
    mean_inference_ms: 4.456371835021787
    mean_raw_obs_processing_ms: 0.38898699452807683
  time_since_restore: 1037.8944327831268
  time_this_iter_s: 26.36342477798462
  time_total_s: 1037.8944327831268
  timers:
    learn_throughput: 8378.011
    learn_time_ms: 19311.504
    sample_throughput: 23159.648
    sample_time_ms: 6985.944
    update_time_ms: 41.273
  timestamp: 1602646281
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     39 |          1037.89 | 6309888 |  260.308 |              315.202 |              107.323 |            812.488 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3490.3612878595063
    time_step_min: 3131
  date: 2020-10-14_03-31-48
  done: false
  episode_len_mean: 812.1498734177216
  episode_reward_max: 315.2020202020202
  episode_reward_mean: 260.81715253803856
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 7900
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5477771113316218
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048639319914703565
        model: {}
        policy_loss: -0.011131139976593355
        total_loss: 7.1255602439244585
        vf_explained_var: 0.9854187965393066
        vf_loss: 7.136478662490845
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.932258064516127
    gpu_util_percent0: 0.28741935483870973
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14965665007846524
    mean_env_wait_ms: 1.2051145934750949
    mean_inference_ms: 4.453294981809407
    mean_raw_obs_processing_ms: 0.38879832875699916
  time_since_restore: 1064.4298827648163
  time_this_iter_s: 26.535449981689453
  time_total_s: 1064.4298827648163
  timers:
    learn_throughput: 8374.044
    learn_time_ms: 19320.654
    sample_throughput: 23167.329
    sample_time_ms: 6983.628
    update_time_ms: 36.322
  timestamp: 1602646308
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     40 |          1064.43 | 6471680 |  260.817 |              315.202 |              107.323 |             812.15 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3487.4048628428927
    time_step_min: 3126
  date: 2020-10-14_03-32-14
  done: false
  episode_len_mean: 811.8263458198958
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 261.24589983185854
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 162
  episodes_total: 8062
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5474757154782613
        entropy_coeff: 0.0005000000000000001
        kl: 0.005657938658259809
        model: {}
        policy_loss: -0.011913959480201205
        total_loss: 7.367338418960571
        vf_explained_var: 0.9843635559082031
        vf_loss: 7.379243055979411
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333338
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14960365715646812
    mean_env_wait_ms: 1.205211841827048
    mean_inference_ms: 4.450486593715282
    mean_raw_obs_processing_ms: 0.388625183333466
  time_since_restore: 1090.6652839183807
  time_this_iter_s: 26.235401153564453
  time_total_s: 1090.6652839183807
  timers:
    learn_throughput: 8375.854
    learn_time_ms: 19316.478
    sample_throughput: 23207.28
    sample_time_ms: 6971.605
    update_time_ms: 35.884
  timestamp: 1602646334
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     41 |          1090.67 | 6633472 |  261.246 |               315.96 |              107.323 |            811.826 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3483.423398328691
    time_step_min: 3126
  date: 2020-10-14_03-32-41
  done: false
  episode_len_mean: 811.4016146523678
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 261.8762087680029
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 237
  episodes_total: 8299
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5258320172627767
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053638086731856065
        model: {}
        policy_loss: -0.010679875093046576
        total_loss: 8.638081908226013
        vf_explained_var: 0.9858747124671936
        vf_loss: 8.648756424585978
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81612903225807
    gpu_util_percent0: 0.32741935483870976
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1495280504974576
    mean_env_wait_ms: 1.2053310888634847
    mean_inference_ms: 4.446466449733846
    mean_raw_obs_processing_ms: 0.38837110199356156
  time_since_restore: 1117.102144241333
  time_this_iter_s: 26.43686032295227
  time_total_s: 1117.102144241333
  timers:
    learn_throughput: 8366.156
    learn_time_ms: 19338.869
    sample_throughput: 23293.784
    sample_time_ms: 6945.716
    update_time_ms: 34.239
  timestamp: 1602646361
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     42 |           1117.1 | 6795264 |  261.876 |               315.96 |              107.323 |            811.402 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3479.495464719048
    time_step_min: 3126
  date: 2020-10-14_03-33-08
  done: false
  episode_len_mean: 811.0968233501349
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 262.46187108454126
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 8531
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5051681821544965
        entropy_coeff: 0.0005000000000000001
        kl: 0.005095119161220889
        model: {}
        policy_loss: -0.008532960782758892
        total_loss: 7.788020968437195
        vf_explained_var: 0.986561119556427
        vf_loss: 7.796551942825317
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.470000000000002
    gpu_util_percent0: 0.35533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14945809966102266
    mean_env_wait_ms: 1.2054432148142031
    mean_inference_ms: 4.442833006989102
    mean_raw_obs_processing_ms: 0.38815377094468545
  time_since_restore: 1143.505609512329
  time_this_iter_s: 26.403465270996094
  time_total_s: 1143.505609512329
  timers:
    learn_throughput: 8366.031
    learn_time_ms: 19339.158
    sample_throughput: 23253.864
    sample_time_ms: 6957.639
    update_time_ms: 34.436
  timestamp: 1602646388
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     43 |          1143.51 | 6957056 |  262.462 |               315.96 |              107.323 |            811.097 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3476.717622571693
    time_step_min: 3126
  date: 2020-10-14_03-33-35
  done: false
  episode_len_mean: 810.8812428078251
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 262.87111041368803
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 159
  episodes_total: 8690
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5089509064952532
        entropy_coeff: 0.0005000000000000001
        kl: 0.005177010122376184
        model: {}
        policy_loss: -0.008594492411551377
        total_loss: 6.384830077489217
        vf_explained_var: 0.9861769676208496
        vf_loss: 6.393420100212097
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8225806451613
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14941202881615082
    mean_env_wait_ms: 1.2055123319727912
    mean_inference_ms: 4.4404067374570815
    mean_raw_obs_processing_ms: 0.38800545879472054
  time_since_restore: 1170.023116350174
  time_this_iter_s: 26.51750683784485
  time_total_s: 1170.023116350174
  timers:
    learn_throughput: 8368.305
    learn_time_ms: 19333.903
    sample_throughput: 23318.888
    sample_time_ms: 6938.238
    update_time_ms: 35.971
  timestamp: 1602646415
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     44 |          1170.02 | 7118848 |  262.871 |               315.96 |              107.323 |            810.881 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3473.827500283158
    time_step_min: 3126
  date: 2020-10-14_03-34-01
  done: false
  episode_len_mean: 810.6123323187916
  episode_reward_max: 318.838383838384
  episode_reward_mean: 263.3335382912657
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 8871
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5027876198291779
        entropy_coeff: 0.0005000000000000001
        kl: 0.00566875849229594
        model: {}
        policy_loss: -0.011540132186686
        total_loss: 6.751582582791646
        vf_explained_var: 0.9869011044502258
        vf_loss: 6.76309057076772
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.110000000000003
    gpu_util_percent0: 0.41966666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1493631711158551
    mean_env_wait_ms: 1.2055971847017088
    mean_inference_ms: 4.437738460031511
    mean_raw_obs_processing_ms: 0.38784490866474036
  time_since_restore: 1196.471836090088
  time_this_iter_s: 26.44871973991394
  time_total_s: 1196.471836090088
  timers:
    learn_throughput: 8355.743
    learn_time_ms: 19362.969
    sample_throughput: 23297.053
    sample_time_ms: 6944.741
    update_time_ms: 34.421
  timestamp: 1602646441
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     45 |          1196.47 | 7280640 |  263.334 |              318.838 |              107.323 |            810.612 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3469.661536770364
    time_step_min: 3126
  date: 2020-10-14_03-34-28
  done: false
  episode_len_mean: 810.1766057555532
  episode_reward_max: 318.838383838384
  episode_reward_mean: 263.9787081892344
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 268
  episodes_total: 9139
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4825672581791878
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052553107573961215
        model: {}
        policy_loss: -0.009422173636266962
        total_loss: 9.06334114074707
        vf_explained_var: 0.9857136607170105
        vf_loss: 9.072741746902466
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354838
    gpu_util_percent0: 0.3216129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492872638585971
    mean_env_wait_ms: 1.2056626101072643
    mean_inference_ms: 4.433876837141565
    mean_raw_obs_processing_ms: 0.3876007110605038
  time_since_restore: 1222.8800563812256
  time_this_iter_s: 26.408220291137695
  time_total_s: 1222.8800563812256
  timers:
    learn_throughput: 8359.242
    learn_time_ms: 19354.866
    sample_throughput: 23309.962
    sample_time_ms: 6940.895
    update_time_ms: 32.986
  timestamp: 1602646468
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     46 |          1222.88 | 7442432 |  263.979 |              318.838 |              107.323 |            810.177 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3466.5811422413794
    time_step_min: 3113
  date: 2020-10-14_03-34-55
  done: false
  episode_len_mean: 809.9514052778374
  episode_reward_max: 318.838383838384
  episode_reward_mean: 264.4207630911127
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 9322
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4710475852092107
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053789357577140135
        model: {}
        policy_loss: -0.010178057884331793
        total_loss: 5.923376639684041
        vf_explained_var: 0.9880004525184631
        vf_loss: 5.933521270751953
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666668
    gpu_util_percent0: 0.43333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14924124420724663
    mean_env_wait_ms: 1.2057295060926485
    mean_inference_ms: 4.431415656162936
    mean_raw_obs_processing_ms: 0.3874550222733739
  time_since_restore: 1249.4312057495117
  time_this_iter_s: 26.551149368286133
  time_total_s: 1249.4312057495117
  timers:
    learn_throughput: 8355.525
    learn_time_ms: 19363.476
    sample_throughput: 23326.802
    sample_time_ms: 6935.884
    update_time_ms: 32.995
  timestamp: 1602646495
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     47 |          1249.43 | 7604224 |  264.421 |              318.838 |              107.323 |            809.951 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3464.1419040559144
    time_step_min: 3113
  date: 2020-10-14_03-35-21
  done: false
  episode_len_mean: 809.7520295202952
  episode_reward_max: 318.838383838384
  episode_reward_mean: 264.78094066654944
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 163
  episodes_total: 9485
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47280077387889224
        entropy_coeff: 0.0005000000000000001
        kl: 0.005530967842787504
        model: {}
        policy_loss: -0.010288277165576195
        total_loss: 6.600515087445577
        vf_explained_var: 0.9858220219612122
        vf_loss: 6.610763390858968
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.593548387096774
    gpu_util_percent0: 0.2900000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14920011397849728
    mean_env_wait_ms: 1.2057738198372099
    mean_inference_ms: 4.429254492021905
    mean_raw_obs_processing_ms: 0.38732412764503327
  time_since_restore: 1275.8536865711212
  time_this_iter_s: 26.422480821609497
  time_total_s: 1275.8536865711212
  timers:
    learn_throughput: 8354.818
    learn_time_ms: 19365.114
    sample_throughput: 23283.792
    sample_time_ms: 6948.696
    update_time_ms: 32.919
  timestamp: 1602646521
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     48 |          1275.85 | 7766016 |  264.781 |              318.838 |              107.323 |            809.752 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3460.9023582954073
    time_step_min: 3113
  date: 2020-10-14_03-35-48
  done: false
  episode_len_mean: 809.4886714727086
  episode_reward_max: 318.838383838384
  episode_reward_mean: 265.2531234070883
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 9710
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4582233751813571
        entropy_coeff: 0.0005000000000000001
        kl: 0.005469115450978279
        model: {}
        policy_loss: -0.011072663590312004
        total_loss: 8.390069405237833
        vf_explained_var: 0.9859001040458679
        vf_loss: 8.401097774505615
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.877419354838707
    gpu_util_percent0: 0.2848387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14914628869145277
    mean_env_wait_ms: 1.2058459049648926
    mean_inference_ms: 4.426392742119853
    mean_raw_obs_processing_ms: 0.38714654456943587
  time_since_restore: 1302.4196972846985
  time_this_iter_s: 26.56601071357727
  time_total_s: 1302.4196972846985
  timers:
    learn_throughput: 8346.539
    learn_time_ms: 19384.322
    sample_throughput: 23289.205
    sample_time_ms: 6947.081
    update_time_ms: 34.483
  timestamp: 1602646548
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     49 |          1302.42 | 7927808 |  265.253 |              318.838 |              107.323 |            809.489 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3457.4240589363203
    time_step_min: 3113
  date: 2020-10-14_03-36-15
  done: false
  episode_len_mean: 809.1631996784242
  episode_reward_max: 318.838383838384
  episode_reward_mean: 265.76986323896176
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 241
  episodes_total: 9951
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.44216354191303253
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046632324034969015
        model: {}
        policy_loss: -0.010077782101385916
        total_loss: 5.821465929349263
        vf_explained_var: 0.9897493720054626
        vf_loss: 5.831531683603923
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.52
    gpu_util_percent0: 0.316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14908902858230716
    mean_env_wait_ms: 1.2058792354151502
    mean_inference_ms: 4.423405769237251
    mean_raw_obs_processing_ms: 0.38696809575694885
  time_since_restore: 1328.8937306404114
  time_this_iter_s: 26.47403335571289
  time_total_s: 1328.8937306404114
  timers:
    learn_throughput: 8352.967
    learn_time_ms: 19369.405
    sample_throughput: 23263.252
    sample_time_ms: 6954.832
    update_time_ms: 33.347
  timestamp: 1602646575
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     50 |          1328.89 | 8089600 |   265.77 |              318.838 |              107.323 |            809.163 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3455.361271102284
    time_step_min: 3113
  date: 2020-10-14_03-36-42
  done: false
  episode_len_mean: 808.9449169303797
  episode_reward_max: 318.838383838384
  episode_reward_mean: 266.06398738172857
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 161
  episodes_total: 10112
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4463478699326515
        entropy_coeff: 0.0005000000000000001
        kl: 0.005972774194863935
        model: {}
        policy_loss: -0.011093871532163272
        total_loss: 6.32016642888387
        vf_explained_var: 0.9864138960838318
        vf_loss: 6.331334034601848
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.187096774193545
    gpu_util_percent0: 0.4009677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14905287936225942
    mean_env_wait_ms: 1.2059150106929717
    mean_inference_ms: 4.421500477748879
    mean_raw_obs_processing_ms: 0.38685332492758373
  time_since_restore: 1355.522954940796
  time_this_iter_s: 26.62922430038452
  time_total_s: 1355.522954940796
  timers:
    learn_throughput: 8341.713
    learn_time_ms: 19395.537
    sample_throughput: 23227.387
    sample_time_ms: 6965.571
    update_time_ms: 35.003
  timestamp: 1602646602
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     51 |          1355.52 | 8251392 |  266.064 |              318.838 |              107.323 |            808.945 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3453.1218655478583
    time_step_min: 3113
  date: 2020-10-14_03-37-08
  done: false
  episode_len_mean: 808.7273345641823
  episode_reward_max: 318.838383838384
  episode_reward_mean: 266.3973276639683
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 10291
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4436478490630786
        entropy_coeff: 0.0005000000000000001
        kl: 0.005474852320427696
        model: {}
        policy_loss: -0.012421490389291042
        total_loss: 7.783152262369792
        vf_explained_var: 0.9851331114768982
        vf_loss: 7.795658628145854
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4741935483871
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14901552295774304
    mean_env_wait_ms: 1.2059550201326488
    mean_inference_ms: 4.419442872751358
    mean_raw_obs_processing_ms: 0.3867299082627893
  time_since_restore: 1381.9209115505219
  time_this_iter_s: 26.397956609725952
  time_total_s: 1381.9209115505219
  timers:
    learn_throughput: 8351.151
    learn_time_ms: 19373.616
    sample_throughput: 23168.171
    sample_time_ms: 6983.374
    update_time_ms: 34.623
  timestamp: 1602646628
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     52 |          1381.92 | 8413184 |  266.397 |              318.838 |              107.323 |            808.727 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3449.9697632404677
    time_step_min: 3113
  date: 2020-10-14_03-37-35
  done: false
  episode_len_mean: 808.4174637749787
  episode_reward_max: 318.838383838384
  episode_reward_mean: 266.9089942899015
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 268
  episodes_total: 10559
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4222855319579442
        entropy_coeff: 0.0005000000000000001
        kl: 0.005193327243129413
        model: {}
        policy_loss: -0.01050349560197598
        total_loss: 8.29350999991099
        vf_explained_var: 0.9866161942481995
        vf_loss: 8.304094870885214
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.52333333333333
    gpu_util_percent0: 0.32799999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14895655799407284
    mean_env_wait_ms: 1.2059945532685301
    mean_inference_ms: 4.4164290677257565
    mean_raw_obs_processing_ms: 0.38654722626469246
  time_since_restore: 1408.3077187538147
  time_this_iter_s: 26.386807203292847
  time_total_s: 1408.3077187538147
  timers:
    learn_throughput: 8349.378
    learn_time_ms: 19377.731
    sample_throughput: 23183.625
    sample_time_ms: 6978.719
    update_time_ms: 32.379
  timestamp: 1602646655
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     53 |          1408.31 | 8574976 |  266.909 |              318.838 |              107.323 |            808.417 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3447.5629788824517
    time_step_min: 3113
  date: 2020-10-14_03-38-02
  done: false
  episode_len_mean: 808.2284065524944
  episode_reward_max: 318.838383838384
  episode_reward_mean: 267.28260358612175
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 10744
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.41082995136578876
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055609165380398435
        model: {}
        policy_loss: -0.011005585916185131
        total_loss: 6.849740187327067
        vf_explained_var: 0.9859861731529236
        vf_loss: 6.860811988512675
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50967741935484
    gpu_util_percent0: 0.2848387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903227
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14891956560415615
    mean_env_wait_ms: 1.2060246280914972
    mean_inference_ms: 4.414460225371454
    mean_raw_obs_processing_ms: 0.38643296540262506
  time_since_restore: 1434.8457701206207
  time_this_iter_s: 26.53805136680603
  time_total_s: 1434.8457701206207
  timers:
    learn_throughput: 8351.505
    learn_time_ms: 19372.795
    sample_throughput: 23162.492
    sample_time_ms: 6985.086
    update_time_ms: 31.351
  timestamp: 1602646682
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     54 |          1434.85 | 8736768 |  267.283 |              318.838 |              107.323 |            808.228 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3445.4221036164536
    time_step_min: 3113
  date: 2020-10-14_03-38-28
  done: false
  episode_len_mean: 808.0611421761848
  episode_reward_max: 318.838383838384
  episode_reward_mean: 267.5956929270707
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 165
  episodes_total: 10909
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4211726163824399
        entropy_coeff: 0.0005000000000000001
        kl: 0.005527137080207467
        model: {}
        policy_loss: -0.009184041584376246
        total_loss: 6.2019104560216265
        vf_explained_var: 0.9864926934242249
        vf_loss: 6.211167017618815
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.377419354838707
    gpu_util_percent0: 0.30645161290322576
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14888715283071977
    mean_env_wait_ms: 1.2060437371930828
    mean_inference_ms: 4.412715528999709
    mean_raw_obs_processing_ms: 0.38633047313252417
  time_since_restore: 1461.3025708198547
  time_this_iter_s: 26.45680069923401
  time_total_s: 1461.3025708198547
  timers:
    learn_throughput: 8355.287
    learn_time_ms: 19364.027
    sample_throughput: 23135.288
    sample_time_ms: 6993.3
    update_time_ms: 31.507
  timestamp: 1602646708
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     55 |           1461.3 | 8898560 |  267.596 |              318.838 |              107.323 |            808.061 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3442.654525784349
    time_step_min: 3113
  date: 2020-10-14_03-38-55
  done: false
  episode_len_mean: 807.7900125740973
  episode_reward_max: 318.838383838384
  episode_reward_mean: 267.99374198242526
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 11134
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.41311071813106537
        entropy_coeff: 0.0005000000000000001
        kl: 0.005275824495280783
        model: {}
        policy_loss: -0.009874430494771028
        total_loss: 9.78791888554891
        vf_explained_var: 0.9832718968391418
        vf_loss: 9.797868172327677
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.923333333333336
    gpu_util_percent0: 0.28866666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14884644030014696
    mean_env_wait_ms: 1.2060839153855125
    mean_inference_ms: 4.410431487239823
    mean_raw_obs_processing_ms: 0.3861944452685975
  time_since_restore: 1487.6164059638977
  time_this_iter_s: 26.31383514404297
  time_total_s: 1487.6164059638977
  timers:
    learn_throughput: 8362.199
    learn_time_ms: 19348.02
    sample_throughput: 23132.353
    sample_time_ms: 6994.187
    update_time_ms: 32.783
  timestamp: 1602646735
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     56 |          1487.62 | 9060352 |  267.994 |              318.838 |              107.323 |             807.79 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3439.825699408702
    time_step_min: 3113
  date: 2020-10-14_03-39-22
  done: false
  episode_len_mean: 807.5094522113778
  episode_reward_max: 318.838383838384
  episode_reward_mean: 268.39280432923266
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 11373
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38683905204137164
        entropy_coeff: 0.0005000000000000001
        kl: 0.005008873762562871
        model: {}
        policy_loss: -0.010195984796155244
        total_loss: 6.402861674626668
        vf_explained_var: 0.9888530373573303
        vf_loss: 6.413125991821289
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.093548387096778
    gpu_util_percent0: 0.3599999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1487997645346369
    mean_env_wait_ms: 1.2060937550056967
    mean_inference_ms: 4.4080925444575465
    mean_raw_obs_processing_ms: 0.38605893634611665
  time_since_restore: 1514.224918603897
  time_this_iter_s: 26.60851263999939
  time_total_s: 1514.224918603897
  timers:
    learn_throughput: 8360.17
    learn_time_ms: 19352.717
    sample_throughput: 23122.958
    sample_time_ms: 6997.029
    update_time_ms: 31.582
  timestamp: 1602646762
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     57 |          1514.22 | 9222144 |  268.393 |              318.838 |              107.323 |            807.509 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3437.9520577742974
    time_step_min: 3113
  date: 2020-10-14_03-39-49
  done: false
  episode_len_mean: 807.2903337667967
  episode_reward_max: 318.838383838384
  episode_reward_mean: 268.6658260104293
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 162
  episodes_total: 11535
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.39349231868982315
        entropy_coeff: 0.0005000000000000001
        kl: 0.005527043676314254
        model: {}
        policy_loss: -0.012349717571244886
        total_loss: 5.995402574539185
        vf_explained_var: 0.986660897731781
        vf_loss: 6.007810831069946
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.903225806451612
    gpu_util_percent0: 0.37709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14877020572395921
    mean_env_wait_ms: 1.2061089777934801
    mean_inference_ms: 4.406554000587183
    mean_raw_obs_processing_ms: 0.38596780992198254
  time_since_restore: 1540.7052874565125
  time_this_iter_s: 26.480368852615356
  time_total_s: 1540.7052874565125
  timers:
    learn_throughput: 8363.281
    learn_time_ms: 19345.517
    sample_throughput: 23087.873
    sample_time_ms: 7007.662
    update_time_ms: 32.935
  timestamp: 1602646789
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     58 |          1540.71 | 9383936 |  268.666 |              318.838 |              107.323 |             807.29 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3435.5406862325663
    time_step_min: 3113
  date: 2020-10-14_03-40-15
  done: false
  episode_len_mean: 806.9947992156194
  episode_reward_max: 318.838383838384
  episode_reward_mean: 269.03050024501124
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 194
  episodes_total: 11729
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3928256755073865
        entropy_coeff: 0.0005000000000000001
        kl: 0.005106201278977096
        model: {}
        policy_loss: -0.012295511667616665
        total_loss: 7.120609879493713
        vf_explained_var: 0.9857527613639832
        vf_loss: 7.132974028587341
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.720000000000006
    gpu_util_percent0: 0.24033333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14873763124376818
    mean_env_wait_ms: 1.2061348560655192
    mean_inference_ms: 4.404760647513109
    mean_raw_obs_processing_ms: 0.3858645727769002
  time_since_restore: 1567.013571023941
  time_this_iter_s: 26.30828356742859
  time_total_s: 1567.013571023941
  timers:
    learn_throughput: 8377.7
    learn_time_ms: 19312.222
    sample_throughput: 23064.004
    sample_time_ms: 7014.914
    update_time_ms: 32.235
  timestamp: 1602646815
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     59 |          1567.01 | 9545728 |  269.031 |              318.838 |              107.323 |            806.995 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3432.3968852047224
    time_step_min: 3113
  date: 2020-10-14_03-40-42
  done: false
  episode_len_mean: 806.6457238214434
  episode_reward_max: 318.838383838384
  episode_reward_mean: 269.52862795666294
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 256
  episodes_total: 11985
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.37009695669015247
        entropy_coeff: 0.0005000000000000001
        kl: 0.005217065801844001
        model: {}
        policy_loss: -0.010196032354239529
        total_loss: 6.351018945376079
        vf_explained_var: 0.9890618920326233
        vf_loss: 6.361269474029541
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.550000000000008
    gpu_util_percent0: 0.31299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486922065198195
    mean_env_wait_ms: 1.2061403689482424
    mean_inference_ms: 4.4024255708539135
    mean_raw_obs_processing_ms: 0.3857250627698858
  time_since_restore: 1593.1693036556244
  time_this_iter_s: 26.15573263168335
  time_total_s: 1593.1693036556244
  timers:
    learn_throughput: 8387.781
    learn_time_ms: 19289.01
    sample_throughput: 23100.766
    sample_time_ms: 7003.75
    update_time_ms: 32.135
  timestamp: 1602646842
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     60 |          1593.17 | 9707520 |  269.529 |              318.838 |              107.323 |            806.646 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3430.259485318377
    time_step_min: 3113
  date: 2020-10-14_03-41-08
  done: false
  episode_len_mean: 806.4523261548578
  episode_reward_max: 318.838383838384
  episode_reward_mean: 269.86510261251334
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 12166
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3646986410021782
        entropy_coeff: 0.0005000000000000001
        kl: 0.005362459147969882
        model: {}
        policy_loss: -0.009549301854955653
        total_loss: 5.4394270579020185
        vf_explained_var: 0.9883525967597961
        vf_loss: 5.449024677276611
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.119354838709683
    gpu_util_percent0: 0.3680645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486631558279337
    mean_env_wait_ms: 1.2061555184192296
    mean_inference_ms: 4.400888824709704
    mean_raw_obs_processing_ms: 0.38563573303690946
  time_since_restore: 1619.755310535431
  time_this_iter_s: 26.58600687980652
  time_total_s: 1619.755310535431
  timers:
    learn_throughput: 8392.965
    learn_time_ms: 19277.098
    sample_throughput: 23083.733
    sample_time_ms: 7008.918
    update_time_ms: 31.388
  timestamp: 1602646868
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     61 |          1619.76 | 9869312 |  269.865 |              318.838 |              107.323 |            806.452 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3428.190158601057
    time_step_min: 3111
  date: 2020-10-14_03-41-35
  done: false
  episode_len_mean: 806.2169895436492
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 270.18828964034435
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 171
  episodes_total: 12337
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.37214011202255887
        entropy_coeff: 0.0005000000000000001
        kl: 0.005121894335995118
        model: {}
        policy_loss: -0.010580289565647641
        total_loss: 5.60819411277771
        vf_explained_var: 0.9875487685203552
        vf_loss: 5.618832627932231
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.63225806451613
    gpu_util_percent0: 0.3535483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903227
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14863580598237175
    mean_env_wait_ms: 1.2061659154378463
    mean_inference_ms: 4.399418707275412
    mean_raw_obs_processing_ms: 0.3855491470280013
  time_since_restore: 1646.5678985118866
  time_this_iter_s: 26.81258797645569
  time_total_s: 1646.5678985118866
  timers:
    learn_throughput: 8384.433
    learn_time_ms: 19296.714
    sample_throughput: 23023.722
    sample_time_ms: 7027.187
    update_time_ms: 33.403
  timestamp: 1602646895
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     62 |          1646.57 | 10031104 |  270.188 |              319.747 |              107.323 |            806.217 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3425.127830940989
    time_step_min: 3111
  date: 2020-10-14_03-42-02
  done: false
  episode_len_mean: 805.8195040534097
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 270.6337456587813
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 245
  episodes_total: 12582
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.35922815650701523
        entropy_coeff: 0.0005000000000000001
        kl: 0.00536204210948199
        model: {}
        policy_loss: -0.011327358738829693
        total_loss: 6.121419548988342
        vf_explained_var: 0.9893788695335388
        vf_loss: 6.1327922741572065
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.967741935483875
    gpu_util_percent0: 0.33838709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485965405071395
    mean_env_wait_ms: 1.2061706350247103
    mean_inference_ms: 4.397407788700356
    mean_raw_obs_processing_ms: 0.3854290997990661
  time_since_restore: 1673.2215285301208
  time_this_iter_s: 26.653630018234253
  time_total_s: 1673.2215285301208
  timers:
    learn_throughput: 8383.164
    learn_time_ms: 19299.634
    sample_throughput: 22950.281
    sample_time_ms: 7049.674
    update_time_ms: 33.908
  timestamp: 1602646922
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     63 |          1673.22 | 10192896 |  270.634 |              319.747 |              107.323 |             805.82 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3422.466368767639
    time_step_min: 3107
  date: 2020-10-14_03-42-29
  done: false
  episode_len_mean: 805.4814033442725
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 271.0310559888618
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 216
  episodes_total: 12798
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.34552522252003354
        entropy_coeff: 0.0005000000000000001
        kl: 0.004593537227871518
        model: {}
        policy_loss: -0.009283156353679564
        total_loss: 5.999730269114177
        vf_explained_var: 0.9880654811859131
        vf_loss: 6.009071469306946
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67
    gpu_util_percent0: 0.415
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856375615205178
    mean_env_wait_ms: 1.2061781335263222
    mean_inference_ms: 4.395694413260087
    mean_raw_obs_processing_ms: 0.385331442412979
  time_since_restore: 1699.3328342437744
  time_this_iter_s: 26.111305713653564
  time_total_s: 1699.3328342437744
  timers:
    learn_throughput: 8393.738
    learn_time_ms: 19275.321
    sample_throughput: 23022.372
    sample_time_ms: 7027.599
    update_time_ms: 34.119
  timestamp: 1602646949
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     64 |          1699.33 | 10354688 |  271.031 |              319.747 |              107.323 |            805.481 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3420.6085778431525
    time_step_min: 3107
  date: 2020-10-14_03-42-55
  done: false
  episode_len_mean: 805.2426884790493
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 271.30751141322935
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 161
  episodes_total: 12959
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3530290946364403
        entropy_coeff: 0.0005000000000000001
        kl: 0.005374249963400264
        model: {}
        policy_loss: -0.009723703609779477
        total_loss: 5.924156467119853
        vf_explained_var: 0.9867007732391357
        vf_loss: 5.9339894851048784
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.24838709677419
    gpu_util_percent0: 0.35387096774193555
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14853908434932153
    mean_env_wait_ms: 1.20617888645133
    mean_inference_ms: 4.394431998802608
    mean_raw_obs_processing_ms: 0.38525790856549164
  time_since_restore: 1725.6836478710175
  time_this_iter_s: 26.350813627243042
  time_total_s: 1725.6836478710175
  timers:
    learn_throughput: 8404.867
    learn_time_ms: 19249.798
    sample_throughput: 23003.11
    sample_time_ms: 7033.484
    update_time_ms: 34.573
  timestamp: 1602646975
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     65 |          1725.68 | 10516480 |  271.308 |              319.747 |              107.323 |            805.243 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3418.0436701156423
    time_step_min: 3107
  date: 2020-10-14_03-43-23
  done: false
  episode_len_mean: 804.9158956468982
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 271.68127505909996
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 13186
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.34985073904196423
        entropy_coeff: 0.0005000000000000001
        kl: 0.004857385375847419
        model: {}
        policy_loss: -0.009454788290895522
        total_loss: 7.686990896860759
        vf_explained_var: 0.9862106442451477
        vf_loss: 7.69655970732371
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36129032258065
    gpu_util_percent0: 0.31645161290322577
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14850839404516253
    mean_env_wait_ms: 1.2062010720097898
    mean_inference_ms: 4.39275346020013
    mean_raw_obs_processing_ms: 0.3851583257142526
  time_since_restore: 1752.5750923156738
  time_this_iter_s: 26.891444444656372
  time_total_s: 1752.5750923156738
  timers:
    learn_throughput: 8394.07
    learn_time_ms: 19274.559
    sample_throughput: 22898.221
    sample_time_ms: 7065.702
    update_time_ms: 35.03
  timestamp: 1602647003
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     66 |          1752.58 | 10678272 |  271.681 |              319.747 |              107.323 |            804.916 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3415.701389926767
    time_step_min: 3107
  date: 2020-10-14_03-43-49
  done: false
  episode_len_mean: 804.590137067938
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 272.05470602930365
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 238
  episodes_total: 13424
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3292933677633603
        entropy_coeff: 0.0005000000000000001
        kl: 0.004817910143174231
        model: {}
        policy_loss: -0.0093964245946457
        total_loss: 7.071157455444336
        vf_explained_var: 0.9875838160514832
        vf_loss: 7.080688397089641
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.287096774193547
    gpu_util_percent0: 0.2799999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14847203044135368
    mean_env_wait_ms: 1.2061875333972882
    mean_inference_ms: 4.3909585289153314
    mean_raw_obs_processing_ms: 0.38505320501762463
  time_since_restore: 1778.9255418777466
  time_this_iter_s: 26.350449562072754
  time_total_s: 1778.9255418777466
  timers:
    learn_throughput: 8409.861
    learn_time_ms: 19238.369
    sample_throughput: 22868.611
    sample_time_ms: 7074.85
    update_time_ms: 34.927
  timestamp: 1602647029
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     67 |          1778.93 | 10840064 |  272.055 |              319.747 |              107.323 |             804.59 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3413.870755831119
    time_step_min: 3107
  date: 2020-10-14_03-44-16
  done: false
  episode_len_mean: 804.3247976453274
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 272.33647735634486
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 166
  episodes_total: 13590
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3318946411212285
        entropy_coeff: 0.0005000000000000001
        kl: 0.004911121679469943
        model: {}
        policy_loss: -0.010973521867223704
        total_loss: 5.2956951061884565
        vf_explained_var: 0.9877960085868835
        vf_loss: 5.306819200515747
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55666666666667
    gpu_util_percent0: 0.3356666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14844944788356798
    mean_env_wait_ms: 1.2061922030508176
    mean_inference_ms: 4.389783937919036
    mean_raw_obs_processing_ms: 0.38498652452093857
  time_since_restore: 1805.2828977108002
  time_this_iter_s: 26.35735583305359
  time_total_s: 1805.2828977108002
  timers:
    learn_throughput: 8413.708
    learn_time_ms: 19229.572
    sample_throughput: 22875.981
    sample_time_ms: 7072.571
    update_time_ms: 32.646
  timestamp: 1602647056
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     68 |          1805.28 | 11001856 |  272.336 |              319.747 |              107.323 |            804.325 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3411.824821766332
    time_step_min: 3090
  date: 2020-10-14_03-44-43
  done: false
  episode_len_mean: 804.0359733101247
  episode_reward_max: 321.4141414141416
  episode_reward_mean: 272.6631414229324
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 13788
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.33505240579446155
        entropy_coeff: 0.0005000000000000001
        kl: 0.005111830464253823
        model: {}
        policy_loss: -0.008610475711369267
        total_loss: 6.609688838322957
        vf_explained_var: 0.9871423244476318
        vf_loss: 6.618458827336629
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.051612903225806
    gpu_util_percent0: 0.37516129032258067
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14842369634585764
    mean_env_wait_ms: 1.2061991094341684
    mean_inference_ms: 4.388446096358232
    mean_raw_obs_processing_ms: 0.3849077851837373
  time_since_restore: 1831.9114620685577
  time_this_iter_s: 26.62856435775757
  time_total_s: 1831.9114620685577
  timers:
    learn_throughput: 8400.924
    learn_time_ms: 19258.834
    sample_throughput: 22869.178
    sample_time_ms: 7074.675
    update_time_ms: 31.83
  timestamp: 1602647083
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     69 |          1831.91 | 11163648 |  272.663 |              321.414 |              107.323 |            804.036 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3409.2248499571306
    time_step_min: 3090
  date: 2020-10-14_03-45-10
  done: false
  episode_len_mean: 803.721897706226
  episode_reward_max: 321.4141414141416
  episode_reward_mean: 273.07626413731265
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 14038
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3201984415451686
        entropy_coeff: 0.0005000000000000001
        kl: 0.00546536249263833
        model: {}
        policy_loss: -0.008231578317160407
        total_loss: 6.717953443527222
        vf_explained_var: 0.9885758757591248
        vf_loss: 6.726336717605591
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.448387096774198
    gpu_util_percent0: 0.2922580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7999999999999994
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14839072035417505
    mean_env_wait_ms: 1.2061975714042459
    mean_inference_ms: 4.386738263725903
    mean_raw_obs_processing_ms: 0.38480568855220415
  time_since_restore: 1858.482069015503
  time_this_iter_s: 26.57060694694519
  time_total_s: 1858.482069015503
  timers:
    learn_throughput: 8388.636
    learn_time_ms: 19287.045
    sample_throughput: 22820.975
    sample_time_ms: 7089.618
    update_time_ms: 32.088
  timestamp: 1602647110
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     70 |          1858.48 | 11325440 |  273.076 |              321.414 |              107.323 |            803.722 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3407.4377909437158
    time_step_min: 3090
  date: 2020-10-14_03-45-37
  done: false
  episode_len_mean: 803.5146272855134
  episode_reward_max: 327.777777777778
  episode_reward_mean: 273.35449431019043
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 14220
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.31465202818314236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004968973381134371
        model: {}
        policy_loss: -0.008591292018536478
        total_loss: 5.156265695889791
        vf_explained_var: 0.9888489246368408
        vf_loss: 5.165006558100383
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.229032258064514
    gpu_util_percent0: 0.2719354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483666323699025
    mean_env_wait_ms: 1.2061932229793364
    mean_inference_ms: 4.385524624447114
    mean_raw_obs_processing_ms: 0.38473464722993234
  time_since_restore: 1885.0801982879639
  time_this_iter_s: 26.598129272460938
  time_total_s: 1885.0801982879639
  timers:
    learn_throughput: 8378.46
    learn_time_ms: 19310.47
    sample_throughput: 22891.773
    sample_time_ms: 7067.692
    update_time_ms: 32.904
  timestamp: 1602647137
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     71 |          1885.08 | 11487232 |  273.354 |              327.778 |              107.323 |            803.515 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3405.5899561189663
    time_step_min: 3082
  date: 2020-10-14_03-46-03
  done: false
  episode_len_mean: 803.2679352732829
  episode_reward_max: 327.777777777778
  episode_reward_mean: 273.6328876654593
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 14399
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3225397790471713
        entropy_coeff: 0.0005000000000000001
        kl: 0.005117148510180414
        model: {}
        policy_loss: -0.010752269362759156
        total_loss: 5.90200936794281
        vf_explained_var: 0.987377405166626
        vf_loss: 5.91291864713033
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96333333333333
    gpu_util_percent0: 0.2893333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483443776019543
    mean_env_wait_ms: 1.206192943235031
    mean_inference_ms: 4.384365626196935
    mean_raw_obs_processing_ms: 0.3846662407692548
  time_since_restore: 1911.3507871627808
  time_this_iter_s: 26.270588874816895
  time_total_s: 1911.3507871627808
  timers:
    learn_throughput: 8389.057
    learn_time_ms: 19286.078
    sample_throughput: 22998.596
    sample_time_ms: 7034.864
    update_time_ms: 32.782
  timestamp: 1602647163
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     72 |          1911.35 | 11649024 |  273.633 |              327.778 |              107.323 |            803.268 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3403.018207954001
    time_step_min: 3082
  date: 2020-10-14_03-46-30
  done: false
  episode_len_mean: 802.948262917207
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.02298184906874
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 252
  episodes_total: 14651
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3073352202773094
        entropy_coeff: 0.0005000000000000001
        kl: 0.004935716240045925
        model: {}
        policy_loss: -0.008306973903624263
        total_loss: 5.957005699475606
        vf_explained_var: 0.9897264838218689
        vf_loss: 5.965462525685628
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84193548387097
    gpu_util_percent0: 0.34290322580645166
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14831392589526696
    mean_env_wait_ms: 1.2061854310531288
    mean_inference_ms: 4.382793235467193
    mean_raw_obs_processing_ms: 0.38457037994175297
  time_since_restore: 1937.70339345932
  time_this_iter_s: 26.352606296539307
  time_total_s: 1937.70339345932
  timers:
    learn_throughput: 8390.811
    learn_time_ms: 19282.045
    sample_throughput: 23088.752
    sample_time_ms: 7007.395
    update_time_ms: 33.0
  timestamp: 1602647190
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     73 |           1937.7 | 11810816 |  274.023 |              327.778 |              107.323 |            802.948 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3400.8968870281587
    time_step_min: 3082
  date: 2020-10-14_03-46-56
  done: false
  episode_len_mean: 802.6835903306174
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.3427473849667
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 200
  episodes_total: 14851
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29465169956286746
        entropy_coeff: 0.0005000000000000001
        kl: 0.004808453610166907
        model: {}
        policy_loss: -0.008224970602896065
        total_loss: 6.00279422601064
        vf_explained_var: 0.9875608086585999
        vf_loss: 6.011164704958598
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.740000000000002
    gpu_util_percent0: 0.30766666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482888835223074
    mean_env_wait_ms: 1.2061795549016798
    mean_inference_ms: 4.381583629436701
    mean_raw_obs_processing_ms: 0.38449767296029863
  time_since_restore: 1963.6753969192505
  time_this_iter_s: 25.97200345993042
  time_total_s: 1963.6753969192505
  timers:
    learn_throughput: 8405.197
    learn_time_ms: 19249.044
    sample_throughput: 23011.79
    sample_time_ms: 7030.831
    update_time_ms: 30.846
  timestamp: 1602647216
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     74 |          1963.68 | 11972608 |  274.343 |              327.778 |              107.323 |            802.684 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3399.1860185617948
    time_step_min: 3082
  date: 2020-10-14_03-47-23
  done: false
  episode_len_mean: 802.465876556362
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.5935182438943
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 168
  episodes_total: 15019
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.30256233861049014
        entropy_coeff: 0.0005000000000000001
        kl: 0.005048536811955273
        model: {}
        policy_loss: -0.009929008044612905
        total_loss: 5.197199583053589
        vf_explained_var: 0.9883285164833069
        vf_loss: 5.207279006640117
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.838709677419363
    gpu_util_percent0: 0.347741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826996382935612
    mean_env_wait_ms: 1.2061783155680486
    mean_inference_ms: 4.380599142067704
    mean_raw_obs_processing_ms: 0.3844387235582967
  time_since_restore: 1990.255497455597
  time_this_iter_s: 26.580100536346436
  time_total_s: 1990.255497455597
  timers:
    learn_throughput: 8398.789
    learn_time_ms: 19263.73
    sample_throughput: 22991.533
    sample_time_ms: 7037.025
    update_time_ms: 38.679
  timestamp: 1602647243
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     75 |          1990.26 | 12134400 |  274.594 |              327.778 |              107.323 |            802.466 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3396.8445306337103
    time_step_min: 3082
  date: 2020-10-14_03-47-50
  done: false
  episode_len_mean: 802.1712337747476
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.9597886561961
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 235
  episodes_total: 15254
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.29972270627816516
        entropy_coeff: 0.0005000000000000001
        kl: 0.005038650939241052
        model: {}
        policy_loss: -0.010396160088324299
        total_loss: 6.214753150939941
        vf_explained_var: 0.9886576533317566
        vf_loss: 6.225298086802165
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.833333333333336
    gpu_util_percent0: 0.33833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14824388093141533
    mean_env_wait_ms: 1.206171359593108
    mean_inference_ms: 4.37922666888936
    mean_raw_obs_processing_ms: 0.3843538063204405
  time_since_restore: 2016.5359036922455
  time_this_iter_s: 26.28040623664856
  time_total_s: 2016.5359036922455
  timers:
    learn_throughput: 8411.742
    learn_time_ms: 19234.066
    sample_throughput: 23087.928
    sample_time_ms: 7007.645
    update_time_ms: 38.559
  timestamp: 1602647270
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     76 |          2016.54 | 12296192 |   274.96 |              327.778 |              107.323 |            802.171 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3394.59173628651
    time_step_min: 3082
  date: 2020-10-14_03-48-16
  done: false
  episode_len_mean: 801.9298585545437
  episode_reward_max: 327.777777777778
  episode_reward_mean: 275.29824499597794
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 229
  episodes_total: 15483
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.28374748677015305
        entropy_coeff: 0.0005000000000000001
        kl: 0.004746896796859801
        model: {}
        policy_loss: -0.00899387260627312
        total_loss: 5.213112831115723
        vf_explained_var: 0.9900777339935303
        vf_loss: 5.22224775950114
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.961290322580645
    gpu_util_percent0: 0.31935483870967735
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14821711380310723
    mean_env_wait_ms: 1.2061616463961484
    mean_inference_ms: 4.377920851809678
    mean_raw_obs_processing_ms: 0.38427541548470956
  time_since_restore: 2042.9026262760162
  time_this_iter_s: 26.366722583770752
  time_total_s: 2042.9026262760162
  timers:
    learn_throughput: 8407.165
    learn_time_ms: 19244.538
    sample_throughput: 23146.061
    sample_time_ms: 6990.045
    update_time_ms: 38.543
  timestamp: 1602647296
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     77 |           2042.9 | 12457984 |  275.298 |              327.778 |              107.323 |             801.93 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3393.0229443055823
    time_step_min: 3082
  date: 2020-10-14_03-48-43
  done: false
  episode_len_mean: 801.7162671780121
  episode_reward_max: 327.777777777778
  episode_reward_mean: 275.5174564436309
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 162
  episodes_total: 15645
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.28466908385356265
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842583555728197
        model: {}
        policy_loss: -0.008349898465288183
        total_loss: 5.032889048258464
        vf_explained_var: 0.9882912635803223
        vf_loss: 5.041380882263184
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.740000000000006
    gpu_util_percent0: 0.3596666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482003731523636
    mean_env_wait_ms: 1.206157754898941
    mean_inference_ms: 4.377030393355734
    mean_raw_obs_processing_ms: 0.3842220817391211
  time_since_restore: 2068.992142677307
  time_this_iter_s: 26.089516401290894
  time_total_s: 2068.992142677307
  timers:
    learn_throughput: 8408.951
    learn_time_ms: 19240.449
    sample_throughput: 23223.925
    sample_time_ms: 6966.609
    update_time_ms: 38.542
  timestamp: 1602647323
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     78 |          2068.99 | 12619776 |  275.517 |              327.778 |              107.323 |            801.716 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3391.2272870961624
    time_step_min: 3082
  date: 2020-10-14_03-49-10
  done: false
  episode_len_mean: 801.4931584589192
  episode_reward_max: 327.777777777778
  episode_reward_mean: 275.8049853475163
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 214
  episodes_total: 15859
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2888108814756076
        entropy_coeff: 0.0005000000000000001
        kl: 0.00441952309726427
        model: {}
        policy_loss: -0.009793649398488924
        total_loss: 6.388848265012105
        vf_explained_var: 0.9879357814788818
        vf_loss: 6.398785909016927
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103225806451615
    gpu_util_percent0: 0.3067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481782822785086
    mean_env_wait_ms: 1.2061615880483014
    mean_inference_ms: 4.3758911096265365
    mean_raw_obs_processing_ms: 0.38415183270901126
  time_since_restore: 2095.700961112976
  time_this_iter_s: 26.708818435668945
  time_total_s: 2095.700961112976
  timers:
    learn_throughput: 8408.927
    learn_time_ms: 19240.506
    sample_throughput: 23202.318
    sample_time_ms: 6973.097
    update_time_ms: 38.417
  timestamp: 1602647350
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     79 |           2095.7 | 12781568 |  275.805 |              327.778 |              107.323 |            801.493 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3389.1448400348563
    time_step_min: 3071
  date: 2020-10-14_03-49-36
  done: false
  episode_len_mean: 801.2234914328284
  episode_reward_max: 327.777777777778
  episode_reward_mean: 276.122784211622
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 249
  episodes_total: 16108
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.26880109558502835
        entropy_coeff: 0.0005000000000000001
        kl: 0.00491958010631303
        model: {}
        policy_loss: -0.009118710830459046
        total_loss: 6.228283405303955
        vf_explained_var: 0.9892032742500305
        vf_loss: 6.237536231676738
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806453
    gpu_util_percent0: 0.30999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14815089114717805
    mean_env_wait_ms: 1.2061396626754597
    mean_inference_ms: 4.374546905819118
    mean_raw_obs_processing_ms: 0.3840686617018741
  time_since_restore: 2122.0987272262573
  time_this_iter_s: 26.39776611328125
  time_total_s: 2122.0987272262573
  timers:
    learn_throughput: 8417.062
    learn_time_ms: 19221.909
    sample_throughput: 23201.527
    sample_time_ms: 6973.334
    update_time_ms: 38.801
  timestamp: 1602647376
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     80 |           2122.1 | 12943360 |  276.123 |              327.778 |              107.323 |            801.223 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3387.7225405039117
    time_step_min: 3071
  date: 2020-10-14_03-50-03
  done: false
  episode_len_mean: 801.005222734255
  episode_reward_max: 327.777777777778
  episode_reward_mean: 276.3411907089325
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 167
  episodes_total: 16275
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2703153267502785
        entropy_coeff: 0.0005000000000000001
        kl: 0.006194658034170668
        model: {}
        policy_loss: -0.006865756008968067
        total_loss: 4.854857683181763
        vf_explained_var: 0.9887514710426331
        vf_loss: 4.861858447392781
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.236666666666665
    gpu_util_percent0: 0.33866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813467466657068
    mean_env_wait_ms: 1.2061391249488904
    mean_inference_ms: 4.373682057614782
    mean_raw_obs_processing_ms: 0.38401740182079097
  time_since_restore: 2148.331395626068
  time_this_iter_s: 26.23266839981079
  time_total_s: 2148.331395626068
  timers:
    learn_throughput: 8431.28
    learn_time_ms: 19189.494
    sample_throughput: 23222.988
    sample_time_ms: 6966.89
    update_time_ms: 38.571
  timestamp: 1602647403
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     81 |          2148.33 | 13105152 |  276.341 |              327.778 |              107.323 |            801.005 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3385.9176406135866
    time_step_min: 3071
  date: 2020-10-14_03-50-30
  done: false
  episode_len_mean: 800.7972070431086
  episode_reward_max: 327.777777777778
  episode_reward_mean: 276.6207061507608
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 195
  episodes_total: 16470
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.27451565364996594
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055320160851503415
        model: {}
        policy_loss: -0.009508050012906702
        total_loss: 4.978119134902954
        vf_explained_var: 0.9896482825279236
        vf_loss: 4.987764398256938
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.130000000000003
    gpu_util_percent0: 0.3776666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14811617022210335
    mean_env_wait_ms: 1.2061396554216808
    mean_inference_ms: 4.372707001427994
    mean_raw_obs_processing_ms: 0.38395905478233894
  time_since_restore: 2174.555953025818
  time_this_iter_s: 26.224557399749756
  time_total_s: 2174.555953025818
  timers:
    learn_throughput: 8425.31
    learn_time_ms: 19203.091
    sample_throughput: 23277.859
    sample_time_ms: 6950.467
    update_time_ms: 38.258
  timestamp: 1602647430
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     82 |          2174.56 | 13266944 |  276.621 |              327.778 |              107.323 |            800.797 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3383.7427132061894
    time_step_min: 3071
  date: 2020-10-14_03-50-56
  done: false
  episode_len_mean: 800.5460636515913
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 276.9474355906516
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 246
  episodes_total: 16716
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.26038045932849246
        entropy_coeff: 0.0005000000000000001
        kl: 0.004677352922347684
        model: {}
        policy_loss: -0.010525610453138748
        total_loss: 6.345296025276184
        vf_explained_var: 0.9889521598815918
        vf_loss: 6.35595182577769
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.720000000000002
    gpu_util_percent0: 0.2933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14809217451035803
    mean_env_wait_ms: 1.2061296380477282
    mean_inference_ms: 4.371479033559593
    mean_raw_obs_processing_ms: 0.38388192223767614
  time_since_restore: 2200.679352283478
  time_this_iter_s: 26.123399257659912
  time_total_s: 2200.679352283478
  timers:
    learn_throughput: 8433.715
    learn_time_ms: 19183.953
    sample_throughput: 23307.357
    sample_time_ms: 6941.671
    update_time_ms: 39.463
  timestamp: 1602647456
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     83 |          2200.68 | 13428736 |  276.947 |              328.687 |              107.323 |            800.546 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3382.0927957308036
    time_step_min: 3071
  date: 2020-10-14_03-51-23
  done: false
  episode_len_mean: 800.3411013189803
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.1866025249239
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 191
  episodes_total: 16907
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.253933717807134
        entropy_coeff: 0.0005000000000000001
        kl: 0.004451247902276616
        model: {}
        policy_loss: -0.00829052090436259
        total_loss: 4.614181836446126
        vf_explained_var: 0.9898722767829895
        vf_loss: 4.622599244117737
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.412903225806453
    gpu_util_percent0: 0.3416129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14807325592149384
    mean_env_wait_ms: 1.206122340269996
    mean_inference_ms: 4.370529881163301
    mean_raw_obs_processing_ms: 0.3838222191320919
  time_since_restore: 2227.2359890937805
  time_this_iter_s: 26.556636810302734
  time_total_s: 2227.2359890937805
  timers:
    learn_throughput: 8409.854
    learn_time_ms: 19238.384
    sample_throughput: 23305.219
    sample_time_ms: 6942.308
    update_time_ms: 41.638
  timestamp: 1602647483
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     84 |          2227.24 | 13590528 |  277.187 |              328.687 |              107.323 |            800.341 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3380.7096584907877
    time_step_min: 3071
  date: 2020-10-14_03-51-50
  done: false
  episode_len_mean: 800.1650081948021
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.39619621643726
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 17084
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2647946501771609
        entropy_coeff: 0.0005000000000000001
        kl: 0.004680490974957745
        model: {}
        policy_loss: -0.00973389969052126
        total_loss: 5.266422748565674
        vf_explained_var: 0.9886471629142761
        vf_loss: 5.276289105415344
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.31935483870968
    gpu_util_percent0: 0.36258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14805670964919143
    mean_env_wait_ms: 1.2061190464218465
    mean_inference_ms: 4.369681132990315
    mean_raw_obs_processing_ms: 0.3837689265822695
  time_since_restore: 2253.5805294513702
  time_this_iter_s: 26.34454035758972
  time_total_s: 2253.5805294513702
  timers:
    learn_throughput: 8415.251
    learn_time_ms: 19226.046
    sample_throughput: 23337.658
    sample_time_ms: 6932.658
    update_time_ms: 39.322
  timestamp: 1602647510
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     85 |          2253.58 | 13752320 |  277.396 |              328.687 |              107.323 |            800.165 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3378.686061307114
    time_step_min: 3071
  date: 2020-10-14_03-52-16
  done: false
  episode_len_mean: 799.8880106162012
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.696302979017
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 17332
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.25041167934735614
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044287769900014
        model: {}
        policy_loss: -0.007105277982191183
        total_loss: 5.639120936393738
        vf_explained_var: 0.9900476336479187
        vf_loss: 5.64635153611501
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.663333333333334
    gpu_util_percent0: 0.3546666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148034118954579
    mean_env_wait_ms: 1.2061096461356098
    mean_inference_ms: 4.368506401440109
    mean_raw_obs_processing_ms: 0.38369436831857917
  time_since_restore: 2279.844558954239
  time_this_iter_s: 26.264029502868652
  time_total_s: 2279.844558954239
  timers:
    learn_throughput: 8415.175
    learn_time_ms: 19226.219
    sample_throughput: 23345.21
    sample_time_ms: 6930.415
    update_time_ms: 39.037
  timestamp: 1602647536
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     86 |          2279.84 | 13914112 |  277.696 |              328.687 |              107.323 |            799.888 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3377.040349774247
    time_step_min: 3071
  date: 2020-10-14_03-52-43
  done: false
  episode_len_mean: 799.67426877245
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.9641935058435
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 207
  episodes_total: 17539
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.23861983915170035
        entropy_coeff: 0.0005000000000000001
        kl: 0.004112338880077004
        model: {}
        policy_loss: -0.008755873218130242
        total_loss: 4.376618842283885
        vf_explained_var: 0.9909425377845764
        vf_loss: 4.3854940533638
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.010000000000005
    gpu_util_percent0: 0.3513333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480156308870624
    mean_env_wait_ms: 1.2061047355203327
    mean_inference_ms: 4.367563050051251
    mean_raw_obs_processing_ms: 0.3836360693332482
  time_since_restore: 2305.903045654297
  time_this_iter_s: 26.058486700057983
  time_total_s: 2305.903045654297
  timers:
    learn_throughput: 8419.335
    learn_time_ms: 19216.72
    sample_throughput: 23397.015
    sample_time_ms: 6915.07
    update_time_ms: 40.029
  timestamp: 1602647563
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     87 |           2305.9 | 14075904 |  277.964 |              328.687 |              107.323 |            799.674 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3375.6159071610527
    time_step_min: 3071
  date: 2020-10-14_03-53-10
  done: false
  episode_len_mean: 799.488789744169
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.1795620404645
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 168
  episodes_total: 17707
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.25132378935813904
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046780900641654926
        model: {}
        policy_loss: -0.010096744537198296
        total_loss: 4.369649648666382
        vf_explained_var: 0.9898576736450195
        vf_loss: 4.37987208366394
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43870967741936
    gpu_util_percent0: 0.3848387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14800079202662914
    mean_env_wait_ms: 1.206099013249575
    mean_inference_ms: 4.366797928150097
    mean_raw_obs_processing_ms: 0.38358812661995845
  time_since_restore: 2332.5187537670135
  time_this_iter_s: 26.615708112716675
  time_total_s: 2332.5187537670135
  timers:
    learn_throughput: 8402.619
    learn_time_ms: 19254.95
    sample_throughput: 23383.903
    sample_time_ms: 6918.948
    update_time_ms: 41.267
  timestamp: 1602647590
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     88 |          2332.52 | 14237696 |   278.18 |              328.687 |              107.323 |            799.489 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3373.67232521778
    time_step_min: 3071
  date: 2020-10-14_03-53-36
  done: false
  episode_len_mean: 799.202061281337
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.483424777018
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 17950
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.24475285286704698
        entropy_coeff: 0.0005000000000000001
        kl: 0.004660970376183589
        model: {}
        policy_loss: -0.0100580982301229
        total_loss: 4.857591191927592
        vf_explained_var: 0.9910823702812195
        vf_loss: 4.867771665255229
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.213333333333335
    gpu_util_percent0: 0.358
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798024122751588
    mean_env_wait_ms: 1.2060938980696627
    mean_inference_ms: 4.365725508079366
    mean_raw_obs_processing_ms: 0.38351900334996936
  time_since_restore: 2358.723653793335
  time_this_iter_s: 26.20490002632141
  time_total_s: 2358.723653793335
  timers:
    learn_throughput: 8411.779
    learn_time_ms: 19233.982
    sample_throughput: 23482.762
    sample_time_ms: 6889.82
    update_time_ms: 40.932
  timestamp: 1602647616
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     89 |          2358.72 | 14399488 |  278.483 |              328.687 |              107.323 |            799.202 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3371.87450353045
    time_step_min: 3071
  date: 2020-10-14_03-54-03
  done: false
  episode_len_mean: 798.9411667583929
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.75678913516
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 220
  episodes_total: 18170
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.22985550637046495
        entropy_coeff: 0.0005000000000000001
        kl: 0.004359815580149491
        model: {}
        policy_loss: -0.010557659346280465
        total_loss: 4.799871762593587
        vf_explained_var: 0.9905244708061218
        vf_loss: 4.8105442126592
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14796128033757608
    mean_env_wait_ms: 1.2060880790568735
    mean_inference_ms: 4.364760064776773
    mean_raw_obs_processing_ms: 0.38345914861653585
  time_since_restore: 2385.0239593982697
  time_this_iter_s: 26.300305604934692
  time_total_s: 2385.0239593982697
  timers:
    learn_throughput: 8404.807
    learn_time_ms: 19249.936
    sample_throughput: 23578.007
    sample_time_ms: 6861.988
    update_time_ms: 40.89
  timestamp: 1602647643
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     90 |          2385.02 | 14561280 |  278.757 |              328.687 |              107.323 |            798.941 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3370.5411951232845
    time_step_min: 3071
  date: 2020-10-14_03-54-30
  done: false
  episode_len_mean: 798.7363770250369
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.9538735415024
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 163
  episodes_total: 18333
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2376074567437172
        entropy_coeff: 0.0005000000000000001
        kl: 0.00426829334658881
        model: {}
        policy_loss: -0.007931897892073417
        total_loss: 4.145262360572815
        vf_explained_var: 0.9900648593902588
        vf_loss: 4.1533130804697675
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97741935483871
    gpu_util_percent0: 0.3574193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479477061770071
    mean_env_wait_ms: 1.2060849251405206
    mean_inference_ms: 4.364056936798986
    mean_raw_obs_processing_ms: 0.3834148017125138
  time_since_restore: 2411.3781321048737
  time_this_iter_s: 26.354172706604004
  time_total_s: 2411.3781321048737
  timers:
    learn_throughput: 8401.395
    learn_time_ms: 19257.755
    sample_throughput: 23564.573
    sample_time_ms: 6865.9
    update_time_ms: 41.707
  timestamp: 1602647670
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     91 |          2411.38 | 14723072 |  278.954 |              328.687 |              107.323 |            798.736 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3368.685742050424
    time_step_min: 3071
  date: 2020-10-14_03-54-57
  done: false
  episode_len_mean: 798.4581739833019
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.2153721431933
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 18565
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.2376383182903131
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520108496459822
        model: {}
        policy_loss: -0.010269694961607456
        total_loss: 5.752100348472595
        vf_explained_var: 0.9893476963043213
        vf_loss: 5.762488961219788
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26
    gpu_util_percent0: 0.3503333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14792929864608287
    mean_env_wait_ms: 1.2060841841220937
    mean_inference_ms: 4.363107053305117
    mean_raw_obs_processing_ms: 0.38335383105573767
  time_since_restore: 2437.799352645874
  time_this_iter_s: 26.421220541000366
  time_total_s: 2437.799352645874
  timers:
    learn_throughput: 8390.676
    learn_time_ms: 19282.356
    sample_throughput: 23580.052
    sample_time_ms: 6861.393
    update_time_ms: 40.581
  timestamp: 1602647697
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     92 |           2437.8 | 14884864 |  279.215 |              328.687 |              107.323 |            798.458 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3366.9289331982727
    time_step_min: 3059
  date: 2020-10-14_03-55-23
  done: false
  episode_len_mean: 798.2009149422842
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.47802134328003
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 234
  episodes_total: 18799
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.21689422676960626
        entropy_coeff: 0.0005000000000000001
        kl: 0.004144047600372384
        model: {}
        policy_loss: -0.008971595564313853
        total_loss: 5.403254389762878
        vf_explained_var: 0.9898805618286133
        vf_loss: 5.412334322929382
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28709677419355
    gpu_util_percent0: 0.3374193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790969798382994
    mean_env_wait_ms: 1.2060737465792941
    mean_inference_ms: 4.362109641224967
    mean_raw_obs_processing_ms: 0.38329059968253465
  time_since_restore: 2463.9531483650208
  time_this_iter_s: 26.15379571914673
  time_total_s: 2463.9531483650208
  timers:
    learn_throughput: 8388.032
    learn_time_ms: 19288.435
    sample_throughput: 23587.009
    sample_time_ms: 6859.369
    update_time_ms: 41.138
  timestamp: 1602647723
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     93 |          2463.95 | 15046656 |  279.478 |              328.687 |              107.323 |            798.201 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3365.643325229891
    time_step_min: 3059
  date: 2020-10-14_03-55-50
  done: false
  episode_len_mean: 798.0363847289601
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.6674746835577
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 165
  episodes_total: 18964
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.22126816088954607
        entropy_coeff: 0.0005000000000000001
        kl: 0.004749035385126869
        model: {}
        policy_loss: -0.009598569934799647
        total_loss: 3.8700090646743774
        vf_explained_var: 0.990976095199585
        vf_loss: 3.8797181646029153
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96333333333333
    gpu_util_percent0: 0.3533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478973851244819
    mean_env_wait_ms: 1.206072980445414
    mean_inference_ms: 4.361449987142154
    mean_raw_obs_processing_ms: 0.3832494964493159
  time_since_restore: 2490.275136947632
  time_this_iter_s: 26.321988582611084
  time_total_s: 2490.275136947632
  timers:
    learn_throughput: 8392.197
    learn_time_ms: 19278.862
    sample_throughput: 23637.477
    sample_time_ms: 6844.724
    update_time_ms: 40.986
  timestamp: 1602647750
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     94 |          2490.28 | 15208448 |  279.667 |              328.687 |              107.323 |            798.036 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3363.907727676472
    time_step_min: 3059
  date: 2020-10-14_03-56-17
  done: false
  episode_len_mean: 797.7913560294041
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.9184931005482
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 19181
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.23035281896591187
        entropy_coeff: 0.0005000000000000001
        kl: 0.004143829729097585
        model: {}
        policy_loss: -0.010221730219200253
        total_loss: 4.654616395632426
        vf_explained_var: 0.9907565712928772
        vf_loss: 4.664953231811523
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27
    gpu_util_percent0: 0.3376666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788134484898569
    mean_env_wait_ms: 1.206073768230957
    mean_inference_ms: 4.360601383401348
    mean_raw_obs_processing_ms: 0.38319542176324
  time_since_restore: 2516.4443142414093
  time_this_iter_s: 26.169177293777466
  time_total_s: 2516.4443142414093
  timers:
    learn_throughput: 8390.248
    learn_time_ms: 19283.339
    sample_throughput: 23695.887
    sample_time_ms: 6827.852
    update_time_ms: 35.534
  timestamp: 1602647777
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     95 |          2516.44 | 15370240 |  279.918 |              328.687 |              107.323 |            797.791 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3362.0694974718813
    time_step_min: 3059
  date: 2020-10-14_03-56-43
  done: false
  episode_len_mean: 797.5166803953872
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.1897813597257
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 19424
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21208161860704422
        entropy_coeff: 0.0005000000000000001
        kl: 0.004079505316137026
        model: {}
        policy_loss: -0.008522810928601151
        total_loss: 5.027996857961019
        vf_explained_var: 0.9906296730041504
        vf_loss: 5.0366257429122925
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4741935483871
    gpu_util_percent0: 0.3206451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478621089200887
    mean_env_wait_ms: 1.2060633842132369
    mean_inference_ms: 4.359626558651934
    mean_raw_obs_processing_ms: 0.38313407511157627
  time_since_restore: 2542.7139298915863
  time_this_iter_s: 26.269615650177002
  time_total_s: 2542.7139298915863
  timers:
    learn_throughput: 8389.173
    learn_time_ms: 19285.811
    sample_throughput: 23729.435
    sample_time_ms: 6818.199
    update_time_ms: 34.015
  timestamp: 1602647803
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     96 |          2542.71 | 15532032 |   280.19 |              328.687 |              107.323 |            797.517 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3360.7272262288375
    time_step_min: 3059
  date: 2020-10-14_03-57-10
  done: false
  episode_len_mean: 797.329964783341
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.3930129653602
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 169
  episodes_total: 19593
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.21102898692091307
        entropy_coeff: 0.0005000000000000001
        kl: 0.004033662747436513
        model: {}
        policy_loss: -0.009322172450993094
        total_loss: 4.123458504676819
        vf_explained_var: 0.9900892376899719
        vf_loss: 4.132886111736298
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11666666666667
    gpu_util_percent0: 0.3473333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478498981053024
    mean_env_wait_ms: 1.2060625173708135
    mean_inference_ms: 4.358982688094911
    mean_raw_obs_processing_ms: 0.383093463245057
  time_since_restore: 2568.868100643158
  time_this_iter_s: 26.154170751571655
  time_total_s: 2568.868100643158
  timers:
    learn_throughput: 8387.373
    learn_time_ms: 19289.949
    sample_throughput: 23716.676
    sample_time_ms: 6821.867
    update_time_ms: 34.842
  timestamp: 1602647830
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     97 |          2568.87 | 15693824 |  280.393 |              328.687 |              107.323 |             797.33 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3359.103664709455
    time_step_min: 3059
  date: 2020-10-14_03-57-36
  done: false
  episode_len_mean: 797.149661581978
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.6225248749746
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 205
  episodes_total: 19798
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.22252398480971655
        entropy_coeff: 0.0005000000000000001
        kl: 0.00425146275665611
        model: {}
        policy_loss: -0.009861140764163187
        total_loss: 5.083629727363586
        vf_explained_var: 0.9898825287818909
        vf_loss: 5.093602140744527
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62
    gpu_util_percent0: 0.3043333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478345088345788
    mean_env_wait_ms: 1.2060589498701377
    mean_inference_ms: 4.35818522667165
    mean_raw_obs_processing_ms: 0.383042471195468
  time_since_restore: 2595.151549577713
  time_this_iter_s: 26.283448934555054
  time_total_s: 2595.151549577713
  timers:
    learn_throughput: 8394.593
    learn_time_ms: 19273.358
    sample_throughput: 23748.257
    sample_time_ms: 6812.795
    update_time_ms: 34.434
  timestamp: 1602647856
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     98 |          2595.15 | 15855616 |  280.623 |              328.687 |              107.323 |             797.15 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3357.242201559688
    time_step_min: 3059
  date: 2020-10-14_03-58-03
  done: false
  episode_len_mean: 796.9059163922977
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.8911019805961
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 20046
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.20550085604190826
        entropy_coeff: 0.0005000000000000001
        kl: 0.005275413820830484
        model: {}
        policy_loss: -0.00833877339027822
        total_loss: 4.737703998883565
        vf_explained_var: 0.9914646744728088
        vf_loss: 4.746145447095235
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.990322580645167
    gpu_util_percent0: 0.35516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14781743717756352
    mean_env_wait_ms: 1.2060555618154472
    mean_inference_ms: 4.357280308539461
    mean_raw_obs_processing_ms: 0.38298488040615636
  time_since_restore: 2621.58505153656
  time_this_iter_s: 26.433501958847046
  time_total_s: 2621.58505153656
  timers:
    learn_throughput: 8388.762
    learn_time_ms: 19286.755
    sample_throughput: 23728.509
    sample_time_ms: 6818.465
    update_time_ms: 36.531
  timestamp: 1602647883
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |     99 |          2621.59 | 16017408 |  280.891 |              328.687 |              107.323 |            796.906 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3355.8999653173464
    time_step_min: 3059
  date: 2020-10-14_03-58-30
  done: false
  episode_len_mean: 796.7325092707046
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.09657764293115
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 20225
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2001587301492691
        entropy_coeff: 0.0005000000000000001
        kl: 0.004580363204392294
        model: {}
        policy_loss: -0.009310966458239514
        total_loss: 3.8330214420954385
        vf_explained_var: 0.9911468625068665
        vf_loss: 3.8424323201179504
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.226666666666674
    gpu_util_percent0: 0.30066666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780472821919807
    mean_env_wait_ms: 1.2060516873564524
    mean_inference_ms: 4.356622507363703
    mean_raw_obs_processing_ms: 0.38294370398452826
  time_since_restore: 2647.774418592453
  time_this_iter_s: 26.189367055892944
  time_total_s: 2647.774418592453
  timers:
    learn_throughput: 8390.43
    learn_time_ms: 19282.922
    sample_throughput: 23747.636
    sample_time_ms: 6812.973
    update_time_ms: 35.442
  timestamp: 1602647910
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    100 |          2647.77 | 16179200 |  281.097 |              328.687 |              107.323 |            796.733 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3354.3903803680982
    time_step_min: 3044
  date: 2020-10-14_03-58-56
  done: false
  episode_len_mean: 796.561835725131
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.3255936946978
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 192
  episodes_total: 20417
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21187574292222658
        entropy_coeff: 0.0005000000000000001
        kl: 0.004318523104302585
        model: {}
        policy_loss: -0.009248630599662041
        total_loss: 4.492278416951497
        vf_explained_var: 0.9903406500816345
        vf_loss: 4.501633008321126
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.151612903225807
    gpu_util_percent0: 0.3748387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477917247905785
    mean_env_wait_ms: 1.206051422216247
    mean_inference_ms: 4.355926919371771
    mean_raw_obs_processing_ms: 0.3829003561815066
  time_since_restore: 2674.0642397403717
  time_this_iter_s: 26.2898211479187
  time_total_s: 2674.0642397403717
  timers:
    learn_throughput: 8394.308
    learn_time_ms: 19274.012
    sample_throughput: 23732.245
    sample_time_ms: 6817.391
    update_time_ms: 32.902
  timestamp: 1602647936
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    101 |          2674.06 | 16340992 |  281.326 |              328.687 |              107.323 |            796.562 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3352.425870260836
    time_step_min: 3044
  date: 2020-10-14_03-59-23
  done: false
  episode_len_mean: 796.3256725372556
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.6141871589906
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 20668
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.1968988093237082
        entropy_coeff: 0.0005000000000000001
        kl: 0.004188684746623039
        model: {}
        policy_loss: -0.010462574491005702
        total_loss: 4.660730878512065
        vf_explained_var: 0.9915547370910645
        vf_loss: 4.671291907628377
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.486666666666668
    gpu_util_percent0: 0.3986666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777502140324295
    mean_env_wait_ms: 1.206041985514376
    mean_inference_ms: 4.355035643091659
    mean_raw_obs_processing_ms: 0.38284227691769307
  time_since_restore: 2700.1046516895294
  time_this_iter_s: 26.040411949157715
  time_total_s: 2700.1046516895294
  timers:
    learn_throughput: 8410.878
    learn_time_ms: 19236.043
    sample_throughput: 23733.603
    sample_time_ms: 6817.001
    update_time_ms: 32.419
  timestamp: 1602647963
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    102 |           2700.1 | 16502784 |  281.614 |              328.687 |              107.323 |            796.326 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3350.9843862599087
    time_step_min: 3044
  date: 2020-10-14_03-59-50
  done: false
  episode_len_mean: 796.1232679675888
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.8326478090585
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 20857
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.18859607353806496
        entropy_coeff: 0.0005000000000000001
        kl: 0.004518361024868985
        model: {}
        policy_loss: -0.008264472465574121
        total_loss: 3.2326915661493936
        vf_explained_var: 0.9925307631492615
        vf_loss: 3.2410502632459006
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193333333333335
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147762006224249
    mean_env_wait_ms: 1.2060369297358324
    mean_inference_ms: 4.3543740640476
    mean_raw_obs_processing_ms: 0.3828001895347714
  time_since_restore: 2726.31157207489
  time_this_iter_s: 26.206920385360718
  time_total_s: 2726.31157207489
  timers:
    learn_throughput: 8403.076
    learn_time_ms: 19253.901
    sample_throughput: 23772.975
    sample_time_ms: 6805.711
    update_time_ms: 31.086
  timestamp: 1602647990
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    103 |          2726.31 | 16664576 |  281.833 |              328.687 |              107.323 |            796.123 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3349.5890084769976
    time_step_min: 3044
  date: 2020-10-14_04-00-16
  done: false
  episode_len_mean: 795.959077946768
  episode_reward_max: 329.2929292929293
  episode_reward_mean: 282.0518012827898
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 21040
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.19958882282177606
        entropy_coeff: 0.0005000000000000001
        kl: 0.004384696406001846
        model: {}
        policy_loss: -0.008615456722812572
        total_loss: 3.3687493602434793
        vf_explained_var: 0.9923567771911621
        vf_loss: 3.377464552720388
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53870967741935
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477497917693933
    mean_env_wait_ms: 1.2060335927929586
    mean_inference_ms: 4.353748739991599
    mean_raw_obs_processing_ms: 0.38276055458114894
  time_since_restore: 2752.6539311408997
  time_this_iter_s: 26.34235906600952
  time_total_s: 2752.6539311408997
  timers:
    learn_throughput: 8400.598
    learn_time_ms: 19259.581
    sample_throughput: 23783.07
    sample_time_ms: 6802.822
    update_time_ms: 29.465
  timestamp: 1602648016
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    104 |          2752.65 | 16826368 |  282.052 |              329.293 |              107.323 |            795.959 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3347.6731466227347
    time_step_min: 3044
  date: 2020-10-14_04-00-43
  done: false
  episode_len_mean: 795.7551087518203
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 282.34511223001846
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 21287
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19368390614787737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004042028042022139
        model: {}
        policy_loss: -0.0101562580451476
        total_loss: 5.010983943939209
        vf_explained_var: 0.9909693598747253
        vf_loss: 5.021237293879191
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02
    gpu_util_percent0: 0.3330000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477341079761414
    mean_env_wait_ms: 1.2060207498914604
    mean_inference_ms: 4.352891223917629
    mean_raw_obs_processing_ms: 0.38270413446928236
  time_since_restore: 2778.7910442352295
  time_this_iter_s: 26.137113094329834
  time_total_s: 2778.7910442352295
  timers:
    learn_throughput: 8399.317
    learn_time_ms: 19262.518
    sample_throughput: 23802.984
    sample_time_ms: 6797.131
    update_time_ms: 28.672
  timestamp: 1602648043
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    105 |          2778.79 | 16988160 |  282.345 |              330.354 |              107.323 |            795.755 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3346.148046255712
    time_step_min: 3044
  date: 2020-10-14_04-01-10
  done: false
  episode_len_mean: 795.6133190618019
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 282.572539900118
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 201
  episodes_total: 21488
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.17996865883469582
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036390256136655807
        model: {}
        policy_loss: -0.008919239606863508
        total_loss: 3.695371667544047
        vf_explained_var: 0.9921171069145203
        vf_loss: 3.7043808499972024
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35483870967742
    gpu_util_percent0: 0.3403225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772111974755095
    mean_env_wait_ms: 1.20601735331456
    mean_inference_ms: 4.352232255054872
    mean_raw_obs_processing_ms: 0.38266243005211165
  time_since_restore: 2805.258857488632
  time_this_iter_s: 26.46781325340271
  time_total_s: 2805.258857488632
  timers:
    learn_throughput: 8389.687
    learn_time_ms: 19284.629
    sample_throughput: 23810.172
    sample_time_ms: 6795.079
    update_time_ms: 29.268
  timestamp: 1602648070
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    106 |          2805.26 | 17149952 |  282.573 |              330.354 |              107.323 |            795.613 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3344.9089100666174
    time_step_min: 3044
  date: 2020-10-14_04-01-36
  done: false
  episode_len_mean: 795.4940899436698
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 282.77048115283407
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 170
  episodes_total: 21658
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.191970224181811
        entropy_coeff: 0.0005000000000000001
        kl: 0.004277789848856628
        model: {}
        policy_loss: -0.008316074284569671
        total_loss: 3.8877825935681662
        vf_explained_var: 0.99090975522995
        vf_loss: 3.8961947361628213
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.159999999999993
    gpu_util_percent0: 0.41
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771077626540183
    mean_env_wait_ms: 1.206013716875274
    mean_inference_ms: 4.3516913382447955
    mean_raw_obs_processing_ms: 0.3826282895811006
  time_since_restore: 2831.319810152054
  time_this_iter_s: 26.06095266342163
  time_total_s: 2831.319810152054
  timers:
    learn_throughput: 8390.489
    learn_time_ms: 19282.785
    sample_throughput: 23834.39
    sample_time_ms: 6788.175
    update_time_ms: 27.55
  timestamp: 1602648096
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    107 |          2831.32 | 17311744 |   282.77 |              330.354 |              107.323 |            795.494 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3343.0362255866075
    time_step_min: 3037
  date: 2020-10-14_04-02-03
  done: false
  episode_len_mean: 795.3190139237616
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.04181739790045
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 21905
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.1884261133770148
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038155772684452436
        model: {}
        policy_loss: -0.009960336741642095
        total_loss: 5.1879085302352905
        vf_explained_var: 0.9906181693077087
        vf_loss: 5.197963118553162
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.973333333333336
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476956655977698
    mean_env_wait_ms: 1.205999047096541
    mean_inference_ms: 4.3508504217059585
    mean_raw_obs_processing_ms: 0.3825723340141509
  time_since_restore: 2857.415226459503
  time_this_iter_s: 26.09541630744934
  time_total_s: 2857.415226459503
  timers:
    learn_throughput: 8397.68
    learn_time_ms: 19266.273
    sample_throughput: 23842.205
    sample_time_ms: 6785.95
    update_time_ms: 26.692
  timestamp: 1602648123
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    108 |          2857.42 | 17473536 |  283.042 |              330.354 |              107.323 |            795.319 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3341.4933864830587
    time_step_min: 3037
  date: 2020-10-14_04-02-29
  done: false
  episode_len_mean: 795.1809838140881
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.2698903311074
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 213
  episodes_total: 22118
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.17300691331426302
        entropy_coeff: 0.0005000000000000001
        kl: 0.004342181685691078
        model: {}
        policy_loss: -0.008004099535658801
        total_loss: 3.749170660972595
        vf_explained_var: 0.9924371838569641
        vf_loss: 3.757261315981547
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.603333333333342
    gpu_util_percent0: 0.26066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768260561150529
    mean_env_wait_ms: 1.2059901586665864
    mean_inference_ms: 4.350192292194111
    mean_raw_obs_processing_ms: 0.38253035958362486
  time_since_restore: 2883.472855567932
  time_this_iter_s: 26.057629108428955
  time_total_s: 2883.472855567932
  timers:
    learn_throughput: 8409.919
    learn_time_ms: 19238.235
    sample_throughput: 23880.798
    sample_time_ms: 6774.983
    update_time_ms: 25.797
  timestamp: 1602648149
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    109 |          2883.47 | 17635328 |   283.27 |              330.354 |              107.323 |            795.181 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3340.2776354236908
    time_step_min: 3037
  date: 2020-10-14_04-02-56
  done: false
  episode_len_mean: 795.0727778525597
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.4541742638391
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 169
  episodes_total: 22287
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.17893226817250252
        entropy_coeff: 0.0005000000000000001
        kl: 0.00458470363325129
        model: {}
        policy_loss: -0.007602003766805865
        total_loss: 3.63955157995224
        vf_explained_var: 0.9913334846496582
        vf_loss: 3.6472429831822715
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.970967741935485
    gpu_util_percent0: 0.37000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767301526947174
    mean_env_wait_ms: 1.205984069901019
    mean_inference_ms: 4.34966892480149
    mean_raw_obs_processing_ms: 0.382497734720863
  time_since_restore: 2909.6453382968903
  time_this_iter_s: 26.17248272895813
  time_total_s: 2909.6453382968903
  timers:
    learn_throughput: 8418.754
    learn_time_ms: 19218.045
    sample_throughput: 23827.208
    sample_time_ms: 6790.221
    update_time_ms: 26.126
  timestamp: 1602648176
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    110 |          2909.65 | 17797120 |  283.454 |              330.354 |              107.323 |            795.073 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3338.6874387964035
    time_step_min: 3037
  date: 2020-10-14_04-03-23
  done: false
  episode_len_mean: 794.925626443931
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.70362367230143
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 221
  episodes_total: 22508
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18554598838090897
        entropy_coeff: 0.0005000000000000001
        kl: 0.004358186890992026
        model: {}
        policy_loss: -0.009096708813255342
        total_loss: 4.001087586085002
        vf_explained_var: 0.9921403527259827
        vf_loss: 4.010276973247528
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.26033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14766005516517416
    mean_env_wait_ms: 1.2059721412959878
    mean_inference_ms: 4.3489627987537185
    mean_raw_obs_processing_ms: 0.38245128017978824
  time_since_restore: 2935.8678936958313
  time_this_iter_s: 26.22255539894104
  time_total_s: 2935.8678936958313
  timers:
    learn_throughput: 8416.538
    learn_time_ms: 19223.106
    sample_throughput: 23877.159
    sample_time_ms: 6776.015
    update_time_ms: 27.702
  timestamp: 1602648203
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    111 |          2935.87 | 17958912 |  283.704 |              330.354 |              107.323 |            794.926 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3337.065976657124
    time_step_min: 3037
  date: 2020-10-14_04-03-49
  done: false
  episode_len_mean: 794.7628698289884
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.95583522391445
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 22747
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.1673268067340056
        entropy_coeff: 0.0005000000000000001
        kl: 0.003918773960322142
        model: {}
        policy_loss: -0.00786621808720156
        total_loss: 3.714820146560669
        vf_explained_var: 0.9928756356239319
        vf_loss: 3.722770015398661
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.770967741935486
    gpu_util_percent0: 0.3412903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476462227669698
    mean_env_wait_ms: 1.2059579934585132
    mean_inference_ms: 4.3482481301795275
    mean_raw_obs_processing_ms: 0.3824050234537547
  time_since_restore: 2962.123511314392
  time_this_iter_s: 26.25561761856079
  time_total_s: 2962.123511314392
  timers:
    learn_throughput: 8414.546
    learn_time_ms: 19227.657
    sample_throughput: 23822.279
    sample_time_ms: 6791.626
    update_time_ms: 28.301
  timestamp: 1602648229
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    112 |          2962.12 | 18120704 |  283.956 |              330.354 |              107.323 |            794.763 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3335.922131864288
    time_step_min: 3037
  date: 2020-10-14_04-04-16
  done: false
  episode_len_mean: 794.6623898053592
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.1282952594814
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 167
  episodes_total: 22914
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1715995396176974
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037443676264956594
        model: {}
        policy_loss: -0.010066550826498618
        total_loss: 3.5865272084871926
        vf_explained_var: 0.9916996359825134
        vf_loss: 3.5966795285542807
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77
    gpu_util_percent0: 0.4443333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476370265409598
    mean_env_wait_ms: 1.2059499148510064
    mean_inference_ms: 4.347750214118457
    mean_raw_obs_processing_ms: 0.382373096374052
  time_since_restore: 2988.1426124572754
  time_this_iter_s: 26.0191011428833
  time_total_s: 2988.1426124572754
  timers:
    learn_throughput: 8424.937
    learn_time_ms: 19203.942
    sample_throughput: 23808.735
    sample_time_ms: 6795.489
    update_time_ms: 27.697
  timestamp: 1602648256
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    113 |          2988.14 | 18282496 |  284.128 |              330.354 |              107.323 |            794.662 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3334.568723459572
    time_step_min: 3037
  date: 2020-10-14_04-04-43
  done: false
  episode_len_mean: 794.5371107266436
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.33656198664846
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 206
  episodes_total: 23120
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.17907493809858957
        entropy_coeff: 0.0005000000000000001
        kl: 0.003939100890420377
        model: {}
        policy_loss: -0.008535448170732707
        total_loss: 4.618942618370056
        vf_explained_var: 0.9906483292579651
        vf_loss: 4.627567529678345
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.548387096774196
    gpu_util_percent0: 0.34774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762620281852398
    mean_env_wait_ms: 1.2059394323290642
    mean_inference_ms: 4.347146668749778
    mean_raw_obs_processing_ms: 0.38233438945893067
  time_since_restore: 3014.7692217826843
  time_this_iter_s: 26.626609325408936
  time_total_s: 3014.7692217826843
  timers:
    learn_throughput: 8421.826
    learn_time_ms: 19211.035
    sample_throughput: 23776.21
    sample_time_ms: 6804.785
    update_time_ms: 35.684
  timestamp: 1602648283
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    114 |          3014.77 | 18444288 |  284.337 |              330.354 |              107.323 |            794.537 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3332.8622567092516
    time_step_min: 3037
  date: 2020-10-14_04-05-09
  done: false
  episode_len_mean: 794.3871961656967
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.59424136953226
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 23368
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.17048650359114012
        entropy_coeff: 0.0005000000000000001
        kl: 0.005069626378826797
        model: {}
        policy_loss: -0.007811213474875937
        total_loss: 4.577751437822978
        vf_explained_var: 0.9916302561759949
        vf_loss: 4.585647861162822
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.610000000000003
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14761242637030708
    mean_env_wait_ms: 1.2059196530179448
    mean_inference_ms: 4.346422837748653
    mean_raw_obs_processing_ms: 0.3822860287991808
  time_since_restore: 3040.9420375823975
  time_this_iter_s: 26.172815799713135
  time_total_s: 3040.9420375823975
  timers:
    learn_throughput: 8419.233
    learn_time_ms: 19216.952
    sample_throughput: 23793.71
    sample_time_ms: 6799.78
    update_time_ms: 36.542
  timestamp: 1602648309
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    115 |          3040.94 | 18606080 |  284.594 |              330.354 |              107.323 |            794.387 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3331.6826092506703
    time_step_min: 3037
  date: 2020-10-14_04-05-36
  done: false
  episode_len_mean: 794.2877288365969
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.77253527502
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 23543
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.16459816321730614
        entropy_coeff: 0.0005000000000000001
        kl: 0.004231553291901946
        model: {}
        policy_loss: -0.00915714621078223
        total_loss: 3.058228353659312
        vf_explained_var: 0.992760181427002
        vf_loss: 3.06746776898702
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.40645161290323
    gpu_util_percent0: 0.3403225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760273536959428
    mean_env_wait_ms: 1.2059090294350547
    mean_inference_ms: 4.345922897323704
    mean_raw_obs_processing_ms: 0.3822537924330166
  time_since_restore: 3067.2079179286957
  time_this_iter_s: 26.265880346298218
  time_total_s: 3067.2079179286957
  timers:
    learn_throughput: 8433.0
    learn_time_ms: 19185.579
    sample_throughput: 23770.608
    sample_time_ms: 6806.389
    update_time_ms: 37.358
  timestamp: 1602648336
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    116 |          3067.21 | 18767872 |  284.773 |              330.354 |              107.323 |            794.288 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3330.438072329831
    time_step_min: 3037
  date: 2020-10-14_04-06-03
  done: false
  episode_len_mean: 794.161127258941
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.95809861537145
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 196
  episodes_total: 23739
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.1774490475654602
        entropy_coeff: 0.0005000000000000001
        kl: 0.00433512757687519
        model: {}
        policy_loss: -0.007354454952292144
        total_loss: 3.6468114654223123
        vf_explained_var: 0.9922370910644531
        vf_loss: 3.6542545755704245
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.486666666666668
    gpu_util_percent0: 0.32366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14759254749861186
    mean_env_wait_ms: 1.2058968590977854
    mean_inference_ms: 4.3453805201699
    mean_raw_obs_processing_ms: 0.3822181644800635
  time_since_restore: 3093.3404943943024
  time_this_iter_s: 26.13257646560669
  time_total_s: 3093.3404943943024
  timers:
    learn_throughput: 8432.624
    learn_time_ms: 19186.435
    sample_throughput: 23747.937
    sample_time_ms: 6812.887
    update_time_ms: 36.863
  timestamp: 1602648363
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    117 |          3093.34 | 18929664 |  284.958 |              330.354 |              107.323 |            794.161 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3328.9412477033575
    time_step_min: 3022
  date: 2020-10-14_04-06-29
  done: false
  episode_len_mean: 794.0208003334723
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.19025393577283
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 23990
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17093541473150253
        entropy_coeff: 0.0005000000000000001
        kl: 0.004781136366849144
        model: {}
        policy_loss: -0.010186252620769665
        total_loss: 4.849270820617676
        vf_explained_var: 0.9914939999580383
        vf_loss: 4.859542489051819
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.50967741935484
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757951754527046
    mean_env_wait_ms: 1.2058783289270616
    mean_inference_ms: 4.34467806594358
    mean_raw_obs_processing_ms: 0.3821719350802817
  time_since_restore: 3119.629865884781
  time_this_iter_s: 26.289371490478516
  time_total_s: 3119.629865884781
  timers:
    learn_throughput: 8424.709
    learn_time_ms: 19204.462
    sample_throughput: 23753.97
    sample_time_ms: 6811.156
    update_time_ms: 38.843
  timestamp: 1602648389
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    118 |          3119.63 | 19091456 |   285.19 |              331.717 |              107.323 |            794.021 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3327.818954085861
    time_step_min: 3022
  date: 2020-10-14_04-06-56
  done: false
  episode_len_mean: 793.9302970133201
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.3641340182665
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 24174
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.15821321805318198
        entropy_coeff: 0.0005000000000000001
        kl: 0.004321871092543006
        model: {}
        policy_loss: -0.009441652397072176
        total_loss: 2.875088612238566
        vf_explained_var: 0.993636429309845
        vf_loss: 2.8846094210942588
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103333333333342
    gpu_util_percent0: 0.2846666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756964787565846
    mean_env_wait_ms: 1.2058629095569062
    mean_inference_ms: 4.344160931092416
    mean_raw_obs_processing_ms: 0.38213873866110637
  time_since_restore: 3145.9784219264984
  time_this_iter_s: 26.34855604171753
  time_total_s: 3145.9784219264984
  timers:
    learn_throughput: 8413.467
    learn_time_ms: 19230.121
    sample_throughput: 23738.124
    sample_time_ms: 6815.703
    update_time_ms: 39.664
  timestamp: 1602648416
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    119 |          3145.98 | 19253248 |  285.364 |              331.717 |              107.323 |             793.93 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3326.625138768965
    time_step_min: 3022
  date: 2020-10-14_04-07-23
  done: false
  episode_len_mean: 793.8232565775971
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.54715981387574
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 24363
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.1737185480693976
        entropy_coeff: 0.0005000000000000001
        kl: 0.003957886598072946
        model: {}
        policy_loss: -0.008577238186262548
        total_loss: 3.7827311952908835
        vf_explained_var: 0.991386890411377
        vf_loss: 3.7913953065872192
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.716129032258067
    gpu_util_percent0: 0.3490322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475605571060887
    mean_env_wait_ms: 1.2058519473407092
    mean_inference_ms: 4.343669832070243
    mean_raw_obs_processing_ms: 0.38210721710705575
  time_since_restore: 3172.4154431819916
  time_this_iter_s: 26.437021255493164
  time_total_s: 3172.4154431819916
  timers:
    learn_throughput: 8400.086
    learn_time_ms: 19260.755
    sample_throughput: 23746.829
    sample_time_ms: 6813.204
    update_time_ms: 38.5
  timestamp: 1602648443
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    120 |          3172.42 | 19415040 |  285.547 |              331.717 |              107.323 |            793.823 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3325.165187296417
    time_step_min: 3022
  date: 2020-10-14_04-07-50
  done: false
  episode_len_mean: 793.6956751483619
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.76811731656863
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 24602
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.1705658994615078
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009722937422338873
        total_loss: .inf
        vf_explained_var: 0.9931064248085022
        vf_loss: 3.7914165258407593
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76666666666667
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754828756780122
    mean_env_wait_ms: 1.2058295293720813
    mean_inference_ms: 4.343022303350182
    mean_raw_obs_processing_ms: 0.3820638514780949
  time_since_restore: 3198.5315370559692
  time_this_iter_s: 26.11609387397766
  time_total_s: 3198.5315370559692
  timers:
    learn_throughput: 8404.452
    learn_time_ms: 19250.749
    sample_throughput: 23748.74
    sample_time_ms: 6812.656
    update_time_ms: 37.221
  timestamp: 1602648470
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 48d97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    121 |          3198.53 | 19576832 |  285.768 |              331.717 |              107.323 |            793.696 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3323.9429817476985
    time_step_min: 3022
  date: 2020-10-14_04-08-16
  done: false
  episode_len_mean: 793.5747802950899
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.9527244549013
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 204
  episodes_total: 24806
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6645352591003756e-16
        cur_lr: 5.0e-05
        entropy: 0.1545626918474833
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009436539253025936
        total_loss: .inf
        vf_explained_var: 0.9927486777305603
        vf_loss: 3.441520075003306
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.764516129032256
    gpu_util_percent0: 0.3458064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753793213971822
    mean_env_wait_ms: 1.2058124696571686
    mean_inference_ms: 4.342473827223069
    mean_raw_obs_processing_ms: 0.38202905622451416
  time_since_restore: 3224.5981345176697
  time_this_iter_s: 26.06659746170044
  time_total_s: 3224.5981345176697
  timers:
    learn_throughput: 8409.585
    learn_time_ms: 19239.0
    sample_throughput: 23780.184
    sample_time_ms: 6803.648
    update_time_ms: 38.32
  timestamp: 1602648496
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 48d97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    122 |           3224.6 | 19738624 |  285.953 |              331.717 |              107.323 |            793.575 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3322.9309916195516
    time_step_min: 3022
  date: 2020-10-14_04-08-43
  done: false
  episode_len_mean: 793.4654337296345
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.1000906143214
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 24981
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9968028886505646e-16
        cur_lr: 5.0e-05
        entropy: 0.16905799259742102
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009564494304261947
        total_loss: .inf
        vf_explained_var: 0.9925346970558167
        vf_loss: 3.3702242573102317
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.273333333333333
    gpu_util_percent0: 0.3683333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752948435458565
    mean_env_wait_ms: 1.2057996600272867
    mean_inference_ms: 4.342043258310753
    mean_raw_obs_processing_ms: 0.3820014838375365
  time_since_restore: 3250.676547765732
  time_this_iter_s: 26.078413248062134
  time_total_s: 3250.676547765732
  timers:
    learn_throughput: 8407.487
    learn_time_ms: 19243.801
    sample_throughput: 23778.586
    sample_time_ms: 6804.105
    update_time_ms: 38.926
  timestamp: 1602648523
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 48d97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    123 |          3250.68 | 19900416 |    286.1 |              331.717 |              107.323 |            793.465 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3321.6161271102283
    time_step_min: 3022
  date: 2020-10-14_04-09-10
  done: false
  episode_len_mean: 793.3317603204188
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.2998846777646
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 236
  episodes_total: 25217
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.995204332975845e-16
        cur_lr: 5.0e-05
        entropy: 0.16849863901734352
        entropy_coeff: 0.0005000000000000001
        kl: 0.004204125434625894
        model: {}
        policy_loss: -0.008447214650611082
        total_loss: 3.7170286973317466
        vf_explained_var: 0.9931346774101257
        vf_loss: 3.725560188293457
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29
    gpu_util_percent0: 0.2903333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751825967820154
    mean_env_wait_ms: 1.2057776075050493
    mean_inference_ms: 4.341417736124264
    mean_raw_obs_processing_ms: 0.3819595901815995
  time_since_restore: 3276.9830560684204
  time_this_iter_s: 26.3065083026886
  time_total_s: 3276.9830560684204
  timers:
    learn_throughput: 8411.603
    learn_time_ms: 19234.384
    sample_throughput: 23820.479
    sample_time_ms: 6792.139
    update_time_ms: 30.743
  timestamp: 1602648550
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    124 |          3276.98 | 20062208 |    286.3 |              331.717 |              107.323 |            793.332 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3320.4379602283916
    time_step_min: 3022
  date: 2020-10-14_04-09-36
  done: false
  episode_len_mean: 793.1923575893384
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.479889114044
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 220
  episodes_total: 25437
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9976021664879224e-16
        cur_lr: 5.0e-05
        entropy: 0.14990362152457237
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050418119644746184
        model: {}
        policy_loss: -0.009882040584064574
        total_loss: 3.7525786558787027
        vf_explained_var: 0.9924448132514954
        vf_loss: 3.7625356316566467
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56129032258064
    gpu_util_percent0: 0.3303225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750752569041134
    mean_env_wait_ms: 1.2057566020276431
    mean_inference_ms: 4.340856778508292
    mean_raw_obs_processing_ms: 0.3819238522540013
  time_since_restore: 3303.054208755493
  time_this_iter_s: 26.071152687072754
  time_total_s: 3303.054208755493
  timers:
    learn_throughput: 8419.936
    learn_time_ms: 19215.347
    sample_throughput: 23789.526
    sample_time_ms: 6800.976
    update_time_ms: 29.793
  timestamp: 1602648576
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    125 |          3303.05 | 20224000 |   286.48 |              331.717 |              107.323 |            793.192 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3319.4840223725896
    time_step_min: 3022
  date: 2020-10-14_04-10-03
  done: false
  episode_len_mean: 793.0779413487446
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.6257325096014
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 172
  episodes_total: 25609
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9976021664879224e-16
        cur_lr: 5.0e-05
        entropy: 0.1566551091770331
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008712213643472447
        total_loss: .inf
        vf_explained_var: 0.9940388798713684
        vf_loss: 2.5338725646336875
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.093333333333337
    gpu_util_percent0: 0.361
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749958875235436
    mean_env_wait_ms: 1.2057428882071815
    mean_inference_ms: 4.340449432698449
    mean_raw_obs_processing_ms: 0.3818973457509795
  time_since_restore: 3329.5058076381683
  time_this_iter_s: 26.45159888267517
  time_total_s: 3329.5058076381683
  timers:
    learn_throughput: 8403.917
    learn_time_ms: 19251.975
    sample_throughput: 23833.347
    sample_time_ms: 6788.472
    update_time_ms: 30.04
  timestamp: 1602648603
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 48d97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    126 |          3329.51 | 20385792 |  286.626 |              331.717 |              107.323 |            793.078 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3318.2404233870966
    time_step_min: 3022
  date: 2020-10-14_04-10-30
  done: false
  episode_len_mean: 792.9347758767516
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.8098848670963
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 25834
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.496403249731884e-16
        cur_lr: 5.0e-05
        entropy: 0.16044227530558905
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036605909505548575
        model: {}
        policy_loss: -0.0072889168513938785
        total_loss: 3.7698705395062766
        vf_explained_var: 0.9928343892097473
        vf_loss: 3.777239739894867
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.990322580645167
    gpu_util_percent0: 0.33225806451612894
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748900618476096
    mean_env_wait_ms: 1.2057223132879535
    mean_inference_ms: 4.339875888125322
    mean_raw_obs_processing_ms: 0.38185955926749526
  time_since_restore: 3355.8570494651794
  time_this_iter_s: 26.35124182701111
  time_total_s: 3355.8570494651794
  timers:
    learn_throughput: 8400.529
    learn_time_ms: 19259.74
    sample_throughput: 23793.158
    sample_time_ms: 6799.938
    update_time_ms: 31.093
  timestamp: 1602648630
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    127 |          3355.86 | 20547584 |   286.81 |              331.717 |              107.323 |            792.935 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3316.9456250240173
    time_step_min: 3022
  date: 2020-10-14_04-10-57
  done: false
  episode_len_mean: 792.7985037406484
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 287.01088963682474
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 26065
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.248201624865942e-16
        cur_lr: 5.0e-05
        entropy: 0.14217434947689375
        entropy_coeff: 0.0005000000000000001
        kl: 0.003887091064825654
        model: {}
        policy_loss: -0.007579348202852998
        total_loss: 3.3648829261461892
        vf_explained_var: 0.9933257699012756
        vf_loss: 3.3725334207216897
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92258064516129
    gpu_util_percent0: 0.3035483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747880639298083
    mean_env_wait_ms: 1.205698630121032
    mean_inference_ms: 4.3393174106104135
    mean_raw_obs_processing_ms: 0.38182387809943097
  time_since_restore: 3382.216087579727
  time_this_iter_s: 26.35903811454773
  time_total_s: 3382.216087579727
  timers:
    learn_throughput: 8405.205
    learn_time_ms: 19249.024
    sample_throughput: 23726.998
    sample_time_ms: 6818.899
    update_time_ms: 29.113
  timestamp: 1602648657
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    128 |          3382.22 | 20709376 |  287.011 |              331.717 |              107.323 |            792.799 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3316.034165521454
    time_step_min: 3022
  date: 2020-10-14_04-11-24
  done: false
  episode_len_mean: 792.6915923469777
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 287.15387929142787
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 173
  episodes_total: 26238
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.124100812432971e-16
        cur_lr: 5.0e-05
        entropy: 0.1430023598174254
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037861804788311324
        model: {}
        policy_loss: -0.008694825228303671
        total_loss: 3.032269318898519
        vf_explained_var: 0.9929423332214355
        vf_loss: 3.0410356521606445
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000003
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474707613944296
    mean_env_wait_ms: 1.2056808113375839
    mean_inference_ms: 4.338914431259194
    mean_raw_obs_processing_ms: 0.3817974805107149
  time_since_restore: 3408.4388477802277
  time_this_iter_s: 26.22276020050049
  time_total_s: 3408.4388477802277
  timers:
    learn_throughput: 8411.853
    learn_time_ms: 19233.813
    sample_throughput: 23722.374
    sample_time_ms: 6820.228
    update_time_ms: 27.225
  timestamp: 1602648684
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    129 |          3408.44 | 20871168 |  287.154 |              331.717 |              107.323 |            792.692 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3314.972470463496
    time_step_min: 3022
  date: 2020-10-14_04-11-51
  done: false
  episode_len_mean: 792.5524763705104
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 287.3213305073418
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 212
  episodes_total: 26450
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.620504062164855e-17
        cur_lr: 5.0e-05
        entropy: 0.15105008085568747
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177333787083626
        model: {}
        policy_loss: -0.009296208234445658
        total_loss: 4.53856639067332
        vf_explained_var: 0.991170346736908
        vf_loss: 4.547938068707784
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403225806451612
    gpu_util_percent0: 0.2861290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746202736284628
    mean_env_wait_ms: 1.2056617202816944
    mean_inference_ms: 4.338422358020948
    mean_raw_obs_processing_ms: 0.38176498469548265
  time_since_restore: 3434.9335494041443
  time_this_iter_s: 26.494701623916626
  time_total_s: 3434.9335494041443
  timers:
    learn_throughput: 8417.284
    learn_time_ms: 19221.403
    sample_throughput: 23690.895
    sample_time_ms: 6829.29
    update_time_ms: 35.606
  timestamp: 1602648711
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    130 |          3434.93 | 21032960 |  287.321 |              331.717 |              107.323 |            792.552 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3313.641835853942
    time_step_min: 3022
  date: 2020-10-14_04-12-18
  done: false
  episode_len_mean: 792.3798568698714
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 287.5304621773204
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 26689
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8102520310824274e-17
        cur_lr: 5.0e-05
        entropy: 0.13767210518320402
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010452359177482625
        total_loss: .inf
        vf_explained_var: 0.9935101866722107
        vf_loss: 3.3806007901827493
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.220000000000002
    gpu_util_percent0: 0.3606666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745149674864516
    mean_env_wait_ms: 1.2056340954169689
    mean_inference_ms: 4.337855172206289
    mean_raw_obs_processing_ms: 0.38172708129528143
  time_since_restore: 3461.1529183387756
  time_this_iter_s: 26.219368934631348
  time_total_s: 3461.1529183387756
  timers:
    learn_throughput: 8410.779
    learn_time_ms: 19236.268
    sample_throughput: 23711.364
    sample_time_ms: 6823.395
    update_time_ms: 36.137
  timestamp: 1602648738
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 48d97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    131 |          3461.15 | 21194752 |   287.53 |              334.747 |              107.323 |             792.38 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3312.7147597211347
    time_step_min: 3022
  date: 2020-10-14_04-12-45
  done: false
  episode_len_mean: 792.2458589242509
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 287.66723065383025
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 26865
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2153780466236426e-17
        cur_lr: 5.0e-05
        entropy: 0.13703347370028496
        entropy_coeff: 0.0005000000000000001
        kl: 0.004063423994618158
        model: {}
        policy_loss: -0.009244664528523572
        total_loss: 3.463837484518687
        vf_explained_var: 0.9920132756233215
        vf_loss: 3.4731505711873374
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.48387096774194
    gpu_util_percent0: 0.3022580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474431545664619
    mean_env_wait_ms: 1.2056155084829172
    mean_inference_ms: 4.337458989847381
    mean_raw_obs_processing_ms: 0.38170149614441345
  time_since_restore: 3487.2503683567047
  time_this_iter_s: 26.097450017929077
  time_total_s: 3487.2503683567047
  timers:
    learn_throughput: 8410.333
    learn_time_ms: 19237.288
    sample_throughput: 23702.386
    sample_time_ms: 6825.979
    update_time_ms: 34.213
  timestamp: 1602648765
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    132 |          3487.25 | 21356544 |  287.667 |              334.747 |              107.323 |            792.246 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3311.64306588244
    time_step_min: 3012
  date: 2020-10-14_04-13-12
  done: false
  episode_len_mean: 792.101458910434
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 287.84218360894255
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 27075
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1076890233118213e-17
        cur_lr: 5.0e-05
        entropy: 0.14753024900952974
        entropy_coeff: 0.0005000000000000001
        kl: 0.004406194901093841
        model: {}
        policy_loss: -0.008399242447922006
        total_loss: 4.08595625559489
        vf_explained_var: 0.9916926026344299
        vf_loss: 4.094429194927216
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35
    gpu_util_percent0: 0.3303333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743511504443252
    mean_env_wait_ms: 1.2055947612878273
    mean_inference_ms: 4.336982210756788
    mean_raw_obs_processing_ms: 0.3816694271688697
  time_since_restore: 3513.684771299362
  time_this_iter_s: 26.43440294265747
  time_total_s: 3513.684771299362
  timers:
    learn_throughput: 8401.543
    learn_time_ms: 19257.415
    sample_throughput: 23665.009
    sample_time_ms: 6836.761
    update_time_ms: 35.111
  timestamp: 1602648792
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    133 |          3513.68 | 21518336 |  287.842 |              334.747 |              107.323 |            792.101 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3310.260375421616
    time_step_min: 3012
  date: 2020-10-14_04-13-38
  done: false
  episode_len_mean: 791.9317666007761
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.04882783468327
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 27318
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0538445116559106e-17
        cur_lr: 5.0e-05
        entropy: 0.13398165752490362
        entropy_coeff: 0.0005000000000000001
        kl: 0.004139222340503086
        model: {}
        policy_loss: -0.008095535122871903
        total_loss: 3.1906695564587912
        vf_explained_var: 0.9938951134681702
        vf_loss: 3.1988320549329123
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.926666666666662
    gpu_util_percent0: 0.35900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742472763448755
    mean_env_wait_ms: 1.2055678785443777
    mean_inference_ms: 4.3364465232565745
    mean_raw_obs_processing_ms: 0.38163336755738625
  time_since_restore: 3539.841105699539
  time_this_iter_s: 26.156334400177002
  time_total_s: 3539.841105699539
  timers:
    learn_throughput: 8406.615
    learn_time_ms: 19245.797
    sample_throughput: 23682.126
    sample_time_ms: 6831.819
    update_time_ms: 35.465
  timestamp: 1602648818
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    134 |          3539.84 | 21680128 |  288.049 |              334.747 |              107.323 |            791.932 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3309.2249016465103
    time_step_min: 3012
  date: 2020-10-14_04-14-05
  done: false
  episode_len_mean: 791.8031206808758
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.2060309944575
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 27494
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.269222558279553e-18
        cur_lr: 5.0e-05
        entropy: 0.1274954273054997
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039248881706347065
        model: {}
        policy_loss: -0.007294766731016959
        total_loss: 2.3881035645802817
        vf_explained_var: 0.9942663311958313
        vf_loss: 2.3954621156056723
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.72903225806452
    gpu_util_percent0: 0.3429032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741683573774164
    mean_env_wait_ms: 1.205547771559018
    mean_inference_ms: 4.336059150960013
    mean_raw_obs_processing_ms: 0.3816083324268077
  time_since_restore: 3566.081512928009
  time_this_iter_s: 26.24040722846985
  time_total_s: 3566.081512928009
  timers:
    learn_throughput: 8395.421
    learn_time_ms: 19271.457
    sample_throughput: 23713.512
    sample_time_ms: 6822.777
    update_time_ms: 35.467
  timestamp: 1602648845
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    135 |          3566.08 | 21841920 |  288.206 |              334.747 |              107.323 |            791.803 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3308.1076283441794
    time_step_min: 3012
  date: 2020-10-14_04-14-32
  done: false
  episode_len_mean: 791.6535629196447
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.3783634482139
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 208
  episodes_total: 27702
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6346112791397766e-18
        cur_lr: 5.0e-05
        entropy: 0.13857279097040495
        entropy_coeff: 0.0005000000000000001
        kl: 0.003999483014922589
        model: {}
        policy_loss: -0.007809690810972825
        total_loss: 3.5393729408582053
        vf_explained_var: 0.992622435092926
        vf_loss: 3.5472520192464194
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206451612903233
    gpu_util_percent0: 0.3467741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740885188188285
    mean_env_wait_ms: 1.205523991411155
    mean_inference_ms: 4.335607389249605
    mean_raw_obs_processing_ms: 0.38157826013173024
  time_since_restore: 3592.441749572754
  time_this_iter_s: 26.360236644744873
  time_total_s: 3592.441749572754
  timers:
    learn_throughput: 8409.596
    learn_time_ms: 19238.973
    sample_throughput: 23629.499
    sample_time_ms: 6847.035
    update_time_ms: 35.557
  timestamp: 1602648872
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 48d97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | RUNNING  | 172.17.0.4:7847 |    136 |          3592.44 | 22003712 |  288.378 |              334.747 |              107.323 |            791.654 |
+-------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_48d97_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3306.7343318880567
    time_step_min: 3012
  date: 2020-10-14_04-14-59
  done: true
  episode_len_mean: 791.4799813946831
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.5891202988416
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 27949
  experiment_id: e21263ee8c2541ba944388e8fa5d7146
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3173056395698883e-18
        cur_lr: 5.0e-05
        entropy: 0.12484266608953476
        entropy_coeff: 0.0005000000000000001
        kl: 0.003520477873583635
        model: {}
        policy_loss: -0.009561487395937244
        total_loss: 3.2520604729652405
        vf_explained_var: 0.9937812685966492
        vf_loss: 3.26168429851532
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15333333333334
    gpu_util_percent0: 0.32099999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7847
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739923654498446
    mean_env_wait_ms: 1.2054970075392224
    mean_inference_ms: 4.3350979246205466
    mean_raw_obs_processing_ms: 0.381543313191463
  time_since_restore: 3618.5323448181152
  time_this_iter_s: 26.090595245361328
  time_total_s: 3618.5323448181152
  timers:
    learn_throughput: 8413.385
    learn_time_ms: 19230.31
    sample_throughput: 23683.984
    sample_time_ms: 6831.283
    update_time_ms: 34.311
  timestamp: 1602648899
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 48d97_00000
  
2020-10-14 04:14:59,852	WARNING util.py:136 -- The `process_trial` operation took 0.514624834060669 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | TERMINATED |       |    137 |          3618.53 | 22165504 |  288.589 |              334.747 |              107.323 |             791.48 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.62 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_48d97_00000 | TERMINATED |       |    137 |          3618.53 | 22165504 |  288.589 |              334.747 |              107.323 |             791.48 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


