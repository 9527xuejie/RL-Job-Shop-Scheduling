2020-10-15 15:41:48,938	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_f0f97_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=17140)[0m 2020-10-15 15:41:51,737	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=17152)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17152)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17141)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17141)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17159)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17159)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17148)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17148)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17136)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17136)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17138)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17138)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17127)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17127)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17106)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17106)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17145)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17145)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17150)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17150)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17081)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17081)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17147)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17147)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17164)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17164)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17137)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17137)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17139)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17139)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17121)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17121)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17171)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17171)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17107)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17107)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17115)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17115)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17100)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17100)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17030)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17030)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17133)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17133)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17054)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17054)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17029)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17029)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17093)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17093)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17068)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17068)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17044)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17044)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17132)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17132)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17063)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17063)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17047)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17047)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17064)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17064)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17111)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17111)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17161)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17161)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17036)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17036)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17116)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17116)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17058)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17058)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17077)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17077)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17038)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17038)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17033)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17033)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17103)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17103)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17032)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17032)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17079)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17079)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17043)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17043)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17059)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17059)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17113)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17113)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17060)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17060)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17078)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17078)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17040)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17040)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17037)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17037)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17123)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17123)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17153)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17153)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17117)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17117)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17089)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17089)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17034)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17034)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17125)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17125)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17050)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17050)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17109)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17109)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17045)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17045)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17035)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17035)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17086)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17086)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17062)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17062)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17110)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17110)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17067)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17067)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17151)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17151)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17129)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17129)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17046)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17046)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17039)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17039)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17095)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17095)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17048)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17048)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17126)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17126)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17118)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17118)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17108)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17108)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17049)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17049)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17105)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17105)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17102)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17102)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17056)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17056)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17112)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17112)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17098)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17098)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=17031)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17031)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4040
    time_step_mean: 3708.8660714285716
    time_step_min: 3400
  date: 2020-10-15_15-42-25
  done: false
  episode_len_mean: 905.6582278481013
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 216.4606188466943
  episode_reward_min: 164.28282828282764
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1647747655709584
        entropy_coeff: 0.0005000000000000001
        kl: 0.00473203028862675
        model: {}
        policy_loss: -0.00807673400170946
        total_loss: 407.51378631591797
        vf_explained_var: 0.5518081784248352
        vf_loss: 407.52150472005206
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.596969696969698
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5515151515151517
    vram_util_percent0: 0.0856788614250077
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17103428677225893
    mean_env_wait_ms: 1.1798386987472398
    mean_inference_ms: 6.1530682686561065
    mean_raw_obs_processing_ms: 0.4600963021325785
  time_since_restore: 28.374234914779663
  time_this_iter_s: 28.374234914779663
  time_total_s: 28.374234914779663
  timers:
    learn_throughput: 8640.528
    learn_time_ms: 18724.781
    sample_throughput: 16891.407
    sample_time_ms: 9578.361
    update_time_ms: 43.52
  timestamp: 1602776545
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      1 |          28.3742 | 161792 |  216.461 |               262.01 |              164.283 |            905.658 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3723.151851851852
    time_step_min: 3400
  date: 2020-10-15_15-42-51
  done: false
  episode_len_mean: 903.9620253164557
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 214.48708605037675
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1318883697191875
        entropy_coeff: 0.0005000000000000001
        kl: 0.009589768092458447
        model: {}
        policy_loss: -0.01046546475845389
        total_loss: 97.49119822184245
        vf_explained_var: 0.8148381114006042
        vf_loss: 97.50126775105794
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.280000000000005
    gpu_util_percent0: 0.335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7533333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16591898141479555
    mean_env_wait_ms: 1.1776816054131638
    mean_inference_ms: 5.788471089801493
    mean_raw_obs_processing_ms: 0.44459570604074894
  time_since_restore: 54.640960454940796
  time_this_iter_s: 26.266725540161133
  time_total_s: 54.640960454940796
  timers:
    learn_throughput: 8639.649
    learn_time_ms: 18726.687
    sample_throughput: 19000.315
    sample_time_ms: 8515.227
    update_time_ms: 41.169
  timestamp: 1602776571
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      2 |           54.641 | 323584 |  214.487 |               262.01 |              131.859 |            903.962 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3720.32476635514
    time_step_min: 3400
  date: 2020-10-15_15-43-17
  done: false
  episode_len_mean: 899.9831223628692
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 214.78020713463707
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.12220632036527
        entropy_coeff: 0.0005000000000000001
        kl: 0.009560395032167435
        model: {}
        policy_loss: -0.011902896052864284
        total_loss: 49.42488066355387
        vf_explained_var: 0.8946843147277832
        vf_loss: 49.43638769785563
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.120000000000005
    gpu_util_percent0: 0.3113333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16264109637574728
    mean_env_wait_ms: 1.178628251854231
    mean_inference_ms: 5.541928771390299
    mean_raw_obs_processing_ms: 0.4342182781500946
  time_since_restore: 80.22493934631348
  time_this_iter_s: 25.58397889137268
  time_total_s: 80.22493934631348
  timers:
    learn_throughput: 8667.263
    learn_time_ms: 18667.024
    sample_throughput: 20297.2
    sample_time_ms: 7971.149
    update_time_ms: 34.986
  timestamp: 1602776597
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      3 |          80.2249 | 485376 |   214.78 |               262.01 |              131.859 |            899.983 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3716.0580204778157
    time_step_min: 3400
  date: 2020-10-15_15-43-43
  done: false
  episode_len_mean: 895.9810126582279
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 215.04304117120526
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1039417584737141
        entropy_coeff: 0.0005000000000000001
        kl: 0.00867797884469231
        model: {}
        policy_loss: -0.013174800493288785
        total_loss: 35.44446245829264
        vf_explained_var: 0.9264928698539734
        vf_loss: 35.45732275644938
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.8
    gpu_util_percent0: 0.3593103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1603973600899105
    mean_env_wait_ms: 1.1798901752401594
    mean_inference_ms: 5.369362352642168
    mean_raw_obs_processing_ms: 0.4265638393863753
  time_since_restore: 105.66246032714844
  time_this_iter_s: 25.43752098083496
  time_total_s: 105.66246032714844
  timers:
    learn_throughput: 8680.42
    learn_time_ms: 18638.73
    sample_throughput: 21072.136
    sample_time_ms: 7678.006
    update_time_ms: 35.66
  timestamp: 1602776623
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      4 |          105.662 | 647168 |  215.043 |               262.01 |              131.859 |            895.981 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3714.5846774193546
    time_step_min: 3400
  date: 2020-10-15_15-44-08
  done: false
  episode_len_mean: 891.0253164556962
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 215.17791842475344
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.082091599702835
        entropy_coeff: 0.0005000000000000001
        kl: 0.0076408466168989735
        model: {}
        policy_loss: -0.012829113693442196
        total_loss: 26.67149782180786
        vf_explained_var: 0.9462361335754395
        vf_loss: 26.68410348892212
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97586206896552
    gpu_util_percent0: 0.39172413793103444
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15873162609201139
    mean_env_wait_ms: 1.1812084356674235
    mean_inference_ms: 5.241683047954716
    mean_raw_obs_processing_ms: 0.4207210146176087
  time_since_restore: 131.14061617851257
  time_this_iter_s: 25.478155851364136
  time_total_s: 131.14061617851257
  timers:
    learn_throughput: 8682.033
    learn_time_ms: 18635.267
    sample_throughput: 21582.815
    sample_time_ms: 7496.335
    update_time_ms: 36.486
  timestamp: 1602776648
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      5 |          131.141 | 808960 |  215.178 |               262.01 |              131.859 |            891.025 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3709.934759358289
    time_step_min: 3400
  date: 2020-10-15_15-44-34
  done: false
  episode_len_mean: 885.1569826707441
  episode_reward_max: 266.2525252525247
  episode_reward_mean: 215.74500355234255
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 981
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0345047016938527
        entropy_coeff: 0.0005000000000000001
        kl: 0.008740813548987111
        model: {}
        policy_loss: -0.011801244584300244
        total_loss: 30.00030755996704
        vf_explained_var: 0.9570322036743164
        vf_loss: 30.011751174926758
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.22758620689655
    gpu_util_percent0: 0.34068965517241384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7586206896551717
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15725038626063925
    mean_env_wait_ms: 1.1834840309546986
    mean_inference_ms: 5.127092503354235
    mean_raw_obs_processing_ms: 0.41542030636088456
  time_since_restore: 156.5421757698059
  time_this_iter_s: 25.401559591293335
  time_total_s: 156.5421757698059
  timers:
    learn_throughput: 8690.573
    learn_time_ms: 18616.954
    sample_throughput: 21929.354
    sample_time_ms: 7377.874
    update_time_ms: 37.426
  timestamp: 1602776674
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      6 |          156.542 | 970752 |  215.745 |              266.253 |              126.556 |            885.157 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3703.7594417077175
    time_step_min: 3393
  date: 2020-10-15_15-44-59
  done: false
  episode_len_mean: 878.3599683544304
  episode_reward_max: 266.2525252525247
  episode_reward_mean: 216.81483346119379
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 283
  episodes_total: 1264
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0508220891157787
        entropy_coeff: 0.0005000000000000001
        kl: 0.007821178723437091
        model: {}
        policy_loss: -0.010197055215636889
        total_loss: 25.470292727152508
        vf_explained_var: 0.9604597687721252
        vf_loss: 25.480233669281006
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.355172413793106
    gpu_util_percent0: 0.35413793103448277
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1557051763605333
    mean_env_wait_ms: 1.1861935992793782
    mean_inference_ms: 5.005836098826805
    mean_raw_obs_processing_ms: 0.4100588651431381
  time_since_restore: 182.21430015563965
  time_this_iter_s: 25.67212438583374
  time_total_s: 182.21430015563965
  timers:
    learn_throughput: 8680.301
    learn_time_ms: 18638.985
    sample_throughput: 22170.899
    sample_time_ms: 7297.494
    update_time_ms: 37.436
  timestamp: 1602776699
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      7 |          182.214 | 1132544 |  216.815 |              266.253 |              126.556 |             878.36 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3693.2005813953488
    time_step_min: 3342
  date: 2020-10-15_15-45-25
  done: false
  episode_len_mean: 874.0970464135021
  episode_reward_max: 270.0404040404039
  episode_reward_mean: 218.1367685291732
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0273421903451283
        entropy_coeff: 0.0005000000000000001
        kl: 0.008351543258565167
        model: {}
        policy_loss: -0.014941895632849386
        total_loss: 16.31189688046773
        vf_explained_var: 0.966367244720459
        vf_loss: 16.326517740885418
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.565517241379315
    gpu_util_percent0: 0.32517241379310347
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15505721067505826
    mean_env_wait_ms: 1.1877969518952673
    mean_inference_ms: 4.954714507014986
    mean_raw_obs_processing_ms: 0.40778387290983337
  time_since_restore: 207.57830286026
  time_this_iter_s: 25.36400270462036
  time_total_s: 207.57830286026
  timers:
    learn_throughput: 8690.125
    learn_time_ms: 18617.913
    sample_throughput: 22352.603
    sample_time_ms: 7238.173
    update_time_ms: 35.684
  timestamp: 1602776725
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      8 |          207.578 | 1294336 |  218.137 |               270.04 |              126.556 |            874.097 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3685.148631029987
    time_step_min: 3342
  date: 2020-10-15_15-45-50
  done: false
  episode_len_mean: 869.632911392405
  episode_reward_max: 271.40404040403985
  episode_reward_mean: 219.33883135148915
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0084790935118992
        entropy_coeff: 0.0005000000000000001
        kl: 0.007280519581399858
        model: {}
        policy_loss: -0.0127268874590906
        total_loss: 15.402397950490316
        vf_explained_var: 0.9677316546440125
        vf_loss: 15.414900859196981
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88275862068966
    gpu_util_percent0: 0.33965517241379317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15449010250782086
    mean_env_wait_ms: 1.1894542313295495
    mean_inference_ms: 4.910314470540144
    mean_raw_obs_processing_ms: 0.40576129130473504
  time_since_restore: 232.9684543609619
  time_this_iter_s: 25.390151500701904
  time_total_s: 232.9684543609619
  timers:
    learn_throughput: 8692.992
    learn_time_ms: 18611.774
    sample_throughput: 22523.918
    sample_time_ms: 7183.12
    update_time_ms: 35.716
  timestamp: 1602776750
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |      9 |          232.968 | 1456128 |  219.339 |              271.404 |              126.556 |            869.633 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3677.5730270906947
    time_step_min: 3342
  date: 2020-10-15_15-46-16
  done: false
  episode_len_mean: 865.266628440367
  episode_reward_max: 271.40404040403985
  episode_reward_mean: 220.30364424057046
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 164
  episodes_total: 1744
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9659438480933508
        entropy_coeff: 0.0005000000000000001
        kl: 0.00674354382014523
        model: {}
        policy_loss: -0.012388948060106486
        total_loss: 21.63013219833374
        vf_explained_var: 0.9617543816566467
        vf_loss: 21.642329851786297
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01379310344828
    gpu_util_percent0: 0.3372413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15397861141851063
    mean_env_wait_ms: 1.1911678209785768
    mean_inference_ms: 4.870079235364043
    mean_raw_obs_processing_ms: 0.4038937898252237
  time_since_restore: 258.5041527748108
  time_this_iter_s: 25.535698413848877
  time_total_s: 258.5041527748108
  timers:
    learn_throughput: 8689.521
    learn_time_ms: 18619.209
    sample_throughput: 22652.011
    sample_time_ms: 7142.501
    update_time_ms: 34.047
  timestamp: 1602776776
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     10 |          258.504 | 1617920 |  220.304 |              271.404 |              126.556 |            865.267 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3664.233416458853
    time_step_min: 3342
  date: 2020-10-15_15-46-41
  done: false
  episode_len_mean: 858.008288639688
  episode_reward_max: 272.61616161616115
  episode_reward_mean: 222.2659998325523
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 307
  episodes_total: 2051
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9442435453335444
        entropy_coeff: 0.0005000000000000001
        kl: 0.0063606204542641836
        model: {}
        policy_loss: -0.00961103243753314
        total_loss: 22.137783527374268
        vf_explained_var: 0.9693326950073242
        vf_loss: 22.147231101989746
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.562068965517245
    gpu_util_percent0: 0.3644827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15318210759447495
    mean_env_wait_ms: 1.1942774998601091
    mean_inference_ms: 4.808648906080042
    mean_raw_obs_processing_ms: 0.4011105870999678
  time_since_restore: 283.74353885650635
  time_this_iter_s: 25.239386081695557
  time_total_s: 283.74353885650635
  timers:
    learn_throughput: 8709.655
    learn_time_ms: 18576.165
    sample_throughput: 23549.346
    sample_time_ms: 6870.339
    update_time_ms: 33.17
  timestamp: 1602776801
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     11 |          283.744 | 1779712 |  222.266 |              272.616 |              126.556 |            858.008 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3657.9593721144965
    time_step_min: 3342
  date: 2020-10-15_15-47-07
  done: false
  episode_len_mean: 854.4254068716094
  episode_reward_max: 272.61616161616115
  episode_reward_mean: 223.03865051966278
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 161
  episodes_total: 2212
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9357274522384008
        entropy_coeff: 0.0005000000000000001
        kl: 0.006479084258899093
        model: {}
        policy_loss: -0.011161864347135028
        total_loss: 13.993338823318481
        vf_explained_var: 0.9730336666107178
        vf_loss: 14.004320621490479
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21666666666666
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1528366057606272
    mean_env_wait_ms: 1.1958138472391198
    mean_inference_ms: 4.781876150294328
    mean_raw_obs_processing_ms: 0.39989535757342365
  time_since_restore: 309.56037044525146
  time_this_iter_s: 25.816831588745117
  time_total_s: 309.56037044525146
  timers:
    learn_throughput: 8721.514
    learn_time_ms: 18550.908
    sample_throughput: 23648.302
    sample_time_ms: 6841.591
    update_time_ms: 33.25
  timestamp: 1602776827
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     12 |           309.56 | 1941504 |  223.039 |              272.616 |              126.556 |            854.425 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3652.6041308089502
    time_step_min: 3283
  date: 2020-10-15_15-47-33
  done: false
  episode_len_mean: 851.2143459915612
  episode_reward_max: 278.97979797979787
  episode_reward_mean: 223.793312875591
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9198300937811533
        entropy_coeff: 0.0005000000000000001
        kl: 0.006205780586848657
        model: {}
        policy_loss: -0.011087854742072523
        total_loss: 15.218324422836304
        vf_explained_var: 0.9686550498008728
        vf_loss: 15.229251782099405
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.610344827586204
    gpu_util_percent0: 0.38068965517241377
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1525308500333247
    mean_env_wait_ms: 1.197337145964827
    mean_inference_ms: 4.758077027900697
    mean_raw_obs_processing_ms: 0.3987922822578853
  time_since_restore: 335.2477216720581
  time_this_iter_s: 25.68735122680664
  time_total_s: 335.2477216720581
  timers:
    learn_throughput: 8716.683
    learn_time_ms: 18561.189
    sample_throughput: 23626.61
    sample_time_ms: 6847.872
    update_time_ms: 34.849
  timestamp: 1602776853
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     13 |          335.248 | 2103296 |  223.793 |               278.98 |              126.556 |            851.214 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3647.3826915442637
    time_step_min: 3283
  date: 2020-10-15_15-47-58
  done: false
  episode_len_mean: 847.929044834308
  episode_reward_max: 278.97979797979787
  episode_reward_mean: 224.61173134857304
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 2565
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8750834216674169
        entropy_coeff: 0.0005000000000000001
        kl: 0.006541201728396118
        model: {}
        policy_loss: -0.011567620793357491
        total_loss: 15.469610452651978
        vf_explained_var: 0.975344181060791
        vf_loss: 15.480961720148722
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.848275862068963
    gpu_util_percent0: 0.30379310344827587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15219901562748814
    mean_env_wait_ms: 1.1992242014990488
    mean_inference_ms: 4.7317648170405056
    mean_raw_obs_processing_ms: 0.39757330542047614
  time_since_restore: 360.5737180709839
  time_this_iter_s: 25.32599639892578
  time_total_s: 360.5737180709839
  timers:
    learn_throughput: 8721.258
    learn_time_ms: 18551.452
    sample_throughput: 23628.816
    sample_time_ms: 6847.233
    update_time_ms: 33.087
  timestamp: 1602776878
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     14 |          360.574 | 2265088 |  224.612 |               278.98 |              126.556 |            847.929 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3638.271459227468
    time_step_min: 3283
  date: 2020-10-15_15-48-23
  done: false
  episode_len_mean: 844.0601688951442
  episode_reward_max: 278.97979797979787
  episode_reward_mean: 225.9424576518169
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 277
  episodes_total: 2842
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.866677130262057
        entropy_coeff: 0.0005000000000000001
        kl: 0.006368907323728005
        model: {}
        policy_loss: -0.013498592539690435
        total_loss: 13.751517534255981
        vf_explained_var: 0.9780821800231934
        vf_loss: 13.764812390009562
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.3896551724138
    gpu_util_percent0: 0.38448275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15177608260653952
    mean_env_wait_ms: 1.2016239427129825
    mean_inference_ms: 4.699164976020883
    mean_raw_obs_processing_ms: 0.3960984993765053
  time_since_restore: 385.63126254081726
  time_this_iter_s: 25.057544469833374
  time_total_s: 385.63126254081726
  timers:
    learn_throughput: 8742.555
    learn_time_ms: 18506.261
    sample_throughput: 23614.787
    sample_time_ms: 6851.3
    update_time_ms: 30.964
  timestamp: 1602776903
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     15 |          385.631 | 2426880 |  225.942 |               278.98 |              126.556 |             844.06 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3633.0091339648175
    time_step_min: 3278
  date: 2020-10-15_15-48-49
  done: false
  episode_len_mean: 842.4756828780813
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 226.62454996332374
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 160
  episodes_total: 3002
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8529908210039139
        entropy_coeff: 0.0005000000000000001
        kl: 0.006265266837241749
        model: {}
        policy_loss: -0.010987085717109343
        total_loss: 13.621409257253012
        vf_explained_var: 0.9727893471717834
        vf_loss: 13.632196267445883
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.33448275862069
    gpu_util_percent0: 0.34034482758620693
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275853
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15156261527442902
    mean_env_wait_ms: 1.2028822511796897
    mean_inference_ms: 4.68255602991036
    mean_raw_obs_processing_ms: 0.39534731011384944
  time_since_restore: 411.04435873031616
  time_this_iter_s: 25.4130961894989
  time_total_s: 411.04435873031616
  timers:
    learn_throughput: 8743.874
    learn_time_ms: 18503.469
    sample_throughput: 23599.023
    sample_time_ms: 6855.877
    update_time_ms: 29.96
  timestamp: 1602776929
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     16 |          411.044 | 2588672 |  226.625 |              279.737 |              126.556 |            842.476 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3627.756262042389
    time_step_min: 3278
  date: 2020-10-15_15-49-15
  done: false
  episode_len_mean: 840.5006329113924
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 227.3260772279756
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8390568047761917
        entropy_coeff: 0.0005000000000000001
        kl: 0.006382488917248945
        model: {}
        policy_loss: -0.010932213355166217
        total_loss: 14.478002707163492
        vf_explained_var: 0.9691622853279114
        vf_loss: 14.488715966542562
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43448275862069
    gpu_util_percent0: 0.41275862068965513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15136591938405536
    mean_env_wait_ms: 1.2041036920293307
    mean_inference_ms: 4.6673524641797615
    mean_raw_obs_processing_ms: 0.39464599745184886
  time_since_restore: 436.67552733421326
  time_this_iter_s: 25.631168603897095
  time_total_s: 436.67552733421326
  timers:
    learn_throughput: 8744.868
    learn_time_ms: 18501.365
    sample_throughput: 23609.891
    sample_time_ms: 6852.721
    update_time_ms: 30.327
  timestamp: 1602776955
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     17 |          436.676 | 2750464 |  227.326 |              279.737 |              126.556 |            840.501 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3621.189189189189
    time_step_min: 3278
  date: 2020-10-15_15-49-40
  done: false
  episode_len_mean: 838.0269557573981
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 228.16246851758098
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 253
  episodes_total: 3413
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8095408777395884
        entropy_coeff: 0.0005000000000000001
        kl: 0.006256838950018088
        model: {}
        policy_loss: -0.011800330537274325
        total_loss: 17.46291907628377
        vf_explained_var: 0.9745703339576721
        vf_loss: 17.474498589833576
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05172413793104
    gpu_util_percent0: 0.3389655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15107932411577718
    mean_env_wait_ms: 1.2059715690222466
    mean_inference_ms: 4.645288035157875
    mean_raw_obs_processing_ms: 0.39364414808530174
  time_since_restore: 462.1448645591736
  time_this_iter_s: 25.469337224960327
  time_total_s: 462.1448645591736
  timers:
    learn_throughput: 8741.695
    learn_time_ms: 18508.081
    sample_throughput: 23605.588
    sample_time_ms: 6853.97
    update_time_ms: 31.559
  timestamp: 1602776980
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     18 |          462.145 | 2912256 |  228.162 |              279.737 |              126.556 |            838.027 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3616.110119877335
    time_step_min: 3278
  date: 2020-10-15_15-50-06
  done: false
  episode_len_mean: 836.1951555188549
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 228.9022679311693
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 3633
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8085831900437673
        entropy_coeff: 0.0005000000000000001
        kl: 0.005845786615585287
        model: {}
        policy_loss: -0.011725900910581307
        total_loss: 13.433278640111288
        vf_explained_var: 0.9770469665527344
        vf_loss: 13.44482429822286
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.056666666666672
    gpu_util_percent0: 0.31100000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15086007418065753
    mean_env_wait_ms: 1.2074296790781736
    mean_inference_ms: 4.628228771679404
    mean_raw_obs_processing_ms: 0.39288073184253564
  time_since_restore: 487.75373220443726
  time_this_iter_s: 25.608867645263672
  time_total_s: 487.75373220443726
  timers:
    learn_throughput: 8738.343
    learn_time_ms: 18515.181
    sample_throughput: 23559.49
    sample_time_ms: 6867.381
    update_time_ms: 31.825
  timestamp: 1602777006
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     19 |          487.754 | 3074048 |  228.902 |              279.737 |              126.556 |            836.195 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3612.632140950347
    time_step_min: 3278
  date: 2020-10-15_15-50-32
  done: false
  episode_len_mean: 835.0163502109705
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 229.41168808336496
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 159
  episodes_total: 3792
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8010916958252589
        entropy_coeff: 0.0005000000000000001
        kl: 0.005972905977008243
        model: {}
        policy_loss: -0.009757631652367612
        total_loss: 13.557638883590698
        vf_explained_var: 0.9725897908210754
        vf_loss: 13.56720002492269
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.51724137931034
    gpu_util_percent0: 0.30241379310344824
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507118370622965
    mean_env_wait_ms: 1.2084325967659575
    mean_inference_ms: 4.616809376370883
    mean_raw_obs_processing_ms: 0.39236958198451155
  time_since_restore: 513.3954031467438
  time_this_iter_s: 25.64167094230652
  time_total_s: 513.3954031467438
  timers:
    learn_throughput: 8736.816
    learn_time_ms: 18518.418
    sample_throughput: 23545.858
    sample_time_ms: 6871.357
    update_time_ms: 33.848
  timestamp: 1602777032
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     20 |          513.395 | 3235840 |  229.412 |              279.737 |              126.556 |            835.016 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3609.6734067059124
    time_step_min: 3278
  date: 2020-10-15_15-50-58
  done: false
  episode_len_mean: 834.0045535036681
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 229.8466501595767
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 161
  episodes_total: 3953
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7751961300770441
        entropy_coeff: 0.0005000000000000001
        kl: 0.006770414882339537
        model: {}
        policy_loss: -0.010245077855264148
        total_loss: 13.407913049062094
        vf_explained_var: 0.9741201400756836
        vf_loss: 13.417869011561075
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.57586206896552
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034476
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1505719816019265
    mean_env_wait_ms: 1.2094098267770121
    mean_inference_ms: 4.60597458545303
    mean_raw_obs_processing_ms: 0.391880397513853
  time_since_restore: 538.9046301841736
  time_this_iter_s: 25.50922703742981
  time_total_s: 538.9046301841736
  timers:
    learn_throughput: 8723.3
    learn_time_ms: 18547.109
    sample_throughput: 23558.602
    sample_time_ms: 6867.64
    update_time_ms: 34.738
  timestamp: 1602777058
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     21 |          538.905 | 3397632 |  229.847 |              279.737 |              126.556 |            834.005 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3604.7112068965516
    time_step_min: 3212
  date: 2020-10-15_15-51-23
  done: false
  episode_len_mean: 832.7110374230223
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 230.63824651058155
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 269
  episodes_total: 4222
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7545836915572485
        entropy_coeff: 0.0005000000000000001
        kl: 0.005760976967091362
        model: {}
        policy_loss: -0.009425222723317953
        total_loss: 14.846301396687826
        vf_explained_var: 0.978369951248169
        vf_loss: 14.855527798334757
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.91666666666667
    gpu_util_percent0: 0.3546666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15035442135241192
    mean_env_wait_ms: 1.2109251189776522
    mean_inference_ms: 4.589263173388879
    mean_raw_obs_processing_ms: 0.3911461538537801
  time_since_restore: 564.4014246463776
  time_this_iter_s: 25.49679446220398
  time_total_s: 564.4014246463776
  timers:
    learn_throughput: 8719.957
    learn_time_ms: 18554.219
    sample_throughput: 23663.145
    sample_time_ms: 6837.299
    update_time_ms: 33.161
  timestamp: 1602777083
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     22 |          564.401 | 3559424 |  230.638 |              289.737 |              126.556 |            832.711 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3600.905643134567
    time_step_min: 3212
  date: 2020-10-15_15-51-49
  done: false
  episode_len_mean: 832.1042278996157
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 231.138554890985
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 4423
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.747990553577741
        entropy_coeff: 0.0005000000000000001
        kl: 0.005494295114961763
        model: {}
        policy_loss: -0.010693619498245729
        total_loss: 12.16190242767334
        vf_explained_var: 0.9790762066841125
        vf_loss: 12.172420819600424
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.0
    gpu_util_percent0: 0.3862068965517242
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1502114810261094
    mean_env_wait_ms: 1.21190830794962
    mean_inference_ms: 4.577939205487168
    mean_raw_obs_processing_ms: 0.39064146867751964
  time_since_restore: 589.8938946723938
  time_this_iter_s: 25.492470026016235
  time_total_s: 589.8938946723938
  timers:
    learn_throughput: 8728.566
    learn_time_ms: 18535.92
    sample_throughput: 23671.389
    sample_time_ms: 6834.918
    update_time_ms: 33.182
  timestamp: 1602777109
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     23 |          589.894 | 3721216 |  231.139 |              289.737 |              126.556 |            832.104 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3597.770061728395
    time_step_min: 3212
  date: 2020-10-15_15-52-15
  done: false
  episode_len_mean: 831.8441728502837
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 231.6387863797288
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 159
  episodes_total: 4582
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7424417585134506
        entropy_coeff: 0.0005000000000000001
        kl: 0.006447711571430166
        model: {}
        policy_loss: -0.01032647981082846
        total_loss: 9.735356251398722
        vf_explained_var: 0.9797441959381104
        vf_loss: 9.74540893236796
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.49655172413793
    gpu_util_percent0: 0.3220689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1501031677371999
    mean_env_wait_ms: 1.212651550305228
    mean_inference_ms: 4.569503506016914
    mean_raw_obs_processing_ms: 0.39027141879306204
  time_since_restore: 615.5059843063354
  time_this_iter_s: 25.61208963394165
  time_total_s: 615.5059843063354
  timers:
    learn_throughput: 8717.305
    learn_time_ms: 18559.864
    sample_throughput: 23669.308
    sample_time_ms: 6835.519
    update_time_ms: 34.598
  timestamp: 1602777135
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     24 |          615.506 | 3883008 |  231.639 |              289.737 |              126.556 |            831.844 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3594.882102575016
    time_step_min: 3212
  date: 2020-10-15_15-52-40
  done: false
  episode_len_mean: 831.540779768177
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 232.10848208108447
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 163
  episodes_total: 4745
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7228637884060541
        entropy_coeff: 0.0005000000000000001
        kl: 0.005764813938488563
        model: {}
        policy_loss: -0.010635872866259888
        total_loss: 10.174961964289347
        vf_explained_var: 0.9803434014320374
        vf_loss: 10.185382525126139
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25862068965517
    gpu_util_percent0: 0.2982758620689656
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275853
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14999863202586894
    mean_env_wait_ms: 1.2133625440242357
    mean_inference_ms: 4.561306080996014
    mean_raw_obs_processing_ms: 0.38991039536739047
  time_since_restore: 641.1668243408203
  time_this_iter_s: 25.660840034484863
  time_total_s: 641.1668243408203
  timers:
    learn_throughput: 8689.905
    learn_time_ms: 18618.385
    sample_throughput: 23671.264
    sample_time_ms: 6834.954
    update_time_ms: 36.731
  timestamp: 1602777160
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     25 |          641.167 | 4044800 |  232.108 |              289.737 |              126.556 |            831.541 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3590.406376109766
    time_step_min: 3212
  date: 2020-10-15_15-53-06
  done: false
  episode_len_mean: 830.890643742503
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 232.71905985080681
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 257
  episodes_total: 5002
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7000831613938013
        entropy_coeff: 0.0005000000000000001
        kl: 0.005935194281240304
        model: {}
        policy_loss: -0.009080819242323438
        total_loss: 15.407442092895508
        vf_explained_var: 0.9771532416343689
        vf_loss: 15.416279872258505
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.843333333333327
    gpu_util_percent0: 0.29766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1498430519906024
    mean_env_wait_ms: 1.2144125735963074
    mean_inference_ms: 4.549236688846651
    mean_raw_obs_processing_ms: 0.3893878995567016
  time_since_restore: 666.8122432231903
  time_this_iter_s: 25.645418882369995
  time_total_s: 666.8122432231903
  timers:
    learn_throughput: 8682.917
    learn_time_ms: 18633.368
    sample_throughput: 23649.46
    sample_time_ms: 6841.256
    update_time_ms: 37.281
  timestamp: 1602777186
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     26 |          666.812 | 4206592 |  232.719 |              289.737 |              126.556 |            830.891 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3586.547706599574
    time_step_min: 3195
  date: 2020-10-15_15-53-32
  done: false
  episode_len_mean: 830.3422213696527
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 233.21004404296139
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 5213
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.69150643547376
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060880950186401606
        model: {}
        policy_loss: -0.01142608958374088
        total_loss: 10.433403253555298
        vf_explained_var: 0.981803834438324
        vf_loss: 10.444566488265991
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.220689655172418
    gpu_util_percent0: 0.3706896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1497278338643381
    mean_env_wait_ms: 1.2151742913049126
    mean_inference_ms: 4.540081065667374
    mean_raw_obs_processing_ms: 0.3889884991501718
  time_since_restore: 692.4670927524567
  time_this_iter_s: 25.654849529266357
  time_total_s: 692.4670927524567
  timers:
    learn_throughput: 8684.001
    learn_time_ms: 18631.044
    sample_throughput: 23636.065
    sample_time_ms: 6845.133
    update_time_ms: 36.934
  timestamp: 1602777212
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     27 |          692.467 | 4368384 |   233.21 |              292.313 |              126.556 |            830.342 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3584.3854675178372
    time_step_min: 3195
  date: 2020-10-15_15-53-57
  done: false
  episode_len_mean: 830.1734921816828
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 233.533121610746
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 159
  episodes_total: 5372
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6903532048066457
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056189208989962935
        model: {}
        policy_loss: -0.011535130285968384
        total_loss: 10.064338286717733
        vf_explained_var: 0.9800041317939758
        vf_loss: 10.075656414031982
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.306896551724137
    gpu_util_percent0: 0.31137931034482763
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14964410479296697
    mean_env_wait_ms: 1.2157207543626376
    mean_inference_ms: 4.533550963883342
    mean_raw_obs_processing_ms: 0.38870634056173536
  time_since_restore: 717.8551223278046
  time_this_iter_s: 25.3880295753479
  time_total_s: 717.8551223278046
  timers:
    learn_throughput: 8683.987
    learn_time_ms: 18631.073
    sample_throughput: 23669.591
    sample_time_ms: 6835.437
    update_time_ms: 37.764
  timestamp: 1602777237
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     28 |          717.855 | 4530176 |  233.533 |              292.313 |              126.556 |            830.173 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3582.0121995630006
    time_step_min: 3195
  date: 2020-10-15_15-54-23
  done: false
  episode_len_mean: 830.0234741784037
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 233.9449150223795
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 166
  episodes_total: 5538
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6738361716270447
        entropy_coeff: 0.0005000000000000001
        kl: 0.005155688966624439
        model: {}
        policy_loss: -0.010013353089258695
        total_loss: 9.565504391988119
        vf_explained_var: 0.9816081523895264
        vf_loss: 9.575339555740356
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.548275862068966
    gpu_util_percent0: 0.3644827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14956028411392966
    mean_env_wait_ms: 1.2162501636984808
    mean_inference_ms: 4.5270274039836345
    mean_raw_obs_processing_ms: 0.388424644591062
  time_since_restore: 743.2669789791107
  time_this_iter_s: 25.411856651306152
  time_total_s: 743.2669789791107
  timers:
    learn_throughput: 8687.79
    learn_time_ms: 18622.917
    sample_throughput: 23711.557
    sample_time_ms: 6823.339
    update_time_ms: 37.562
  timestamp: 1602777263
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     29 |          743.267 | 4691968 |  233.945 |              292.313 |              126.556 |            830.023 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3578.8517096999303
    time_step_min: 3195
  date: 2020-10-15_15-54-49
  done: false
  episode_len_mean: 829.8369678089305
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 234.43644125575554
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 240
  episodes_total: 5778
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6407706787188848
        entropy_coeff: 0.0005000000000000001
        kl: 0.005420890559131901
        model: {}
        policy_loss: -0.009607533769061169
        total_loss: 10.791298151016235
        vf_explained_var: 0.9838590025901794
        vf_loss: 10.800683975219727
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87931034482759
    gpu_util_percent0: 0.3182758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1494460191870015
    mean_env_wait_ms: 1.2169447551507335
    mean_inference_ms: 4.518154829277211
    mean_raw_obs_processing_ms: 0.38803755007510177
  time_since_restore: 768.659631729126
  time_this_iter_s: 25.39265275001526
  time_total_s: 768.659631729126
  timers:
    learn_throughput: 8697.577
    learn_time_ms: 18601.963
    sample_throughput: 23721.343
    sample_time_ms: 6820.524
    update_time_ms: 36.002
  timestamp: 1602777289
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     30 |           768.66 | 4853760 |  234.436 |              292.313 |              126.556 |            829.837 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3575.6610112548296
    time_step_min: 3195
  date: 2020-10-15_15-55-14
  done: false
  episode_len_mean: 829.5845974329055
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 234.90505319910184
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 5999
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6333951304356257
        entropy_coeff: 0.0005000000000000001
        kl: 0.005369040261333187
        model: {}
        policy_loss: -0.011245574724550048
        total_loss: 8.461874643961588
        vf_explained_var: 0.9855525493621826
        vf_loss: 8.472900152206421
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.575862068965524
    gpu_util_percent0: 0.3206896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1493492259967211
    mean_env_wait_ms: 1.2175316502550855
    mean_inference_ms: 4.510550039555216
    mean_raw_obs_processing_ms: 0.38771153089761656
  time_since_restore: 794.0734784603119
  time_this_iter_s: 25.413846731185913
  time_total_s: 794.0734784603119
  timers:
    learn_throughput: 8701.123
    learn_time_ms: 18594.381
    sample_throughput: 23729.538
    sample_time_ms: 6818.169
    update_time_ms: 35.315
  timestamp: 1602777314
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     31 |          794.073 | 5015552 |  234.905 |              292.313 |              126.556 |            829.585 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3572.9936232831915
    time_step_min: 3195
  date: 2020-10-15_15-55-40
  done: false
  episode_len_mean: 829.3567023693606
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 235.2598084053777
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 163
  episodes_total: 6162
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6266387303670248
        entropy_coeff: 0.0005000000000000001
        kl: 0.005494208540767431
        model: {}
        policy_loss: -0.011165471594722476
        total_loss: 7.235161264737447
        vf_explained_var: 0.9854392409324646
        vf_loss: 7.246090531349182
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.383333333333336
    gpu_util_percent0: 0.3373333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14927990383258666
    mean_env_wait_ms: 1.217928673115835
    mean_inference_ms: 4.505215959694546
    mean_raw_obs_processing_ms: 0.38747994668732477
  time_since_restore: 819.5685732364655
  time_this_iter_s: 25.495094776153564
  time_total_s: 819.5685732364655
  timers:
    learn_throughput: 8701.163
    learn_time_ms: 18594.295
    sample_throughput: 23739.895
    sample_time_ms: 6815.195
    update_time_ms: 37.205
  timestamp: 1602777340
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     32 |          819.569 | 5177344 |   235.26 |              292.313 |              126.556 |            829.357 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3570.6457106477797
    time_step_min: 3195
  date: 2020-10-15_15-56-06
  done: false
  episode_len_mean: 829.0303365460578
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 235.6446452197752
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 167
  episodes_total: 6329
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6156158596277237
        entropy_coeff: 0.0005000000000000001
        kl: 0.005225458842081328
        model: {}
        policy_loss: -0.01106348225827484
        total_loss: 9.687271992365519
        vf_explained_var: 0.9811480641365051
        vf_loss: 9.698120911916098
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.966666666666665
    gpu_util_percent0: 0.31299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14921265485634463
    mean_env_wait_ms: 1.2183201448505419
    mean_inference_ms: 4.499967658313575
    mean_raw_obs_processing_ms: 0.38725386156205277
  time_since_restore: 845.3975231647491
  time_this_iter_s: 25.82894992828369
  time_total_s: 845.3975231647491
  timers:
    learn_throughput: 8682.61
    learn_time_ms: 18634.028
    sample_throughput: 23763.906
    sample_time_ms: 6808.308
    update_time_ms: 37.343
  timestamp: 1602777366
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     33 |          845.398 | 5339136 |  235.645 |              292.313 |              126.556 |             829.03 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3567.117845634494
    time_step_min: 3195
  date: 2020-10-15_15-56-31
  done: false
  episode_len_mean: 828.7211641017827
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 236.1450586929785
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 234
  episodes_total: 6563
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5944310575723648
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047600786201655865
        model: {}
        policy_loss: -0.009058705977319429
        total_loss: 11.138667662938436
        vf_explained_var: 0.982389509677887
        vf_loss: 11.147547562917074
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.60689655172414
    gpu_util_percent0: 0.3489655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14912205298060177
    mean_env_wait_ms: 1.2188268278141212
    mean_inference_ms: 4.492943047563793
    mean_raw_obs_processing_ms: 0.38695429147719773
  time_since_restore: 870.8840880393982
  time_this_iter_s: 25.486564874649048
  time_total_s: 870.8840880393982
  timers:
    learn_throughput: 8688.435
    learn_time_ms: 18621.536
    sample_throughput: 23766.687
    sample_time_ms: 6807.512
    update_time_ms: 37.902
  timestamp: 1602777391
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     34 |          870.884 | 5500928 |  236.145 |              292.313 |              126.556 |            828.721 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3563.8186267240103
    time_step_min: 3195
  date: 2020-10-15_15-56-57
  done: false
  episode_len_mean: 828.4669318014435
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 236.62070104491633
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 226
  episodes_total: 6789
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.58115021387736
        entropy_coeff: 0.0005000000000000001
        kl: 0.005653778595539431
        model: {}
        policy_loss: -0.01025580000714399
        total_loss: 9.081565141677856
        vf_explained_var: 0.9846951365470886
        vf_loss: 9.09182890256246
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.041379310344833
    gpu_util_percent0: 0.34310344827586203
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14904267504626847
    mean_env_wait_ms: 1.2192794675956875
    mean_inference_ms: 4.486645023811334
    mean_raw_obs_processing_ms: 0.38668879241176607
  time_since_restore: 896.4625039100647
  time_this_iter_s: 25.578415870666504
  time_total_s: 896.4625039100647
  timers:
    learn_throughput: 8692.293
    learn_time_ms: 18613.27
    sample_throughput: 23774.478
    sample_time_ms: 6805.281
    update_time_ms: 37.546
  timestamp: 1602777417
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     35 |          896.463 | 5662720 |  236.621 |              292.313 |              126.556 |            828.467 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3561.5114393281206
    time_step_min: 3195
  date: 2020-10-15_15-57-23
  done: false
  episode_len_mean: 828.2651035673188
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 236.97882594646086
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 163
  episodes_total: 6952
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5712917496760687
        entropy_coeff: 0.0005000000000000001
        kl: 0.006062560287925105
        model: {}
        policy_loss: -0.009961116263487687
        total_loss: 6.571681658426921
        vf_explained_var: 0.9867879748344421
        vf_loss: 6.581625501314799
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43103448275862
    gpu_util_percent0: 0.31448275862068964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489862469191144
    mean_env_wait_ms: 1.2195805703993603
    mean_inference_ms: 4.4822932509176105
    mean_raw_obs_processing_ms: 0.3865034232451039
  time_since_restore: 921.960746049881
  time_this_iter_s: 25.498242139816284
  time_total_s: 921.960746049881
  timers:
    learn_throughput: 8695.215
    learn_time_ms: 18607.015
    sample_throughput: 23808.26
    sample_time_ms: 6795.625
    update_time_ms: 37.744
  timestamp: 1602777443
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     36 |          921.961 | 5824512 |  236.979 |              292.313 |              126.556 |            828.265 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3558.5696309910927
    time_step_min: 3195
  date: 2020-10-15_15-57-49
  done: false
  episode_len_mean: 827.9617923865711
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 237.40879932915294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 167
  episodes_total: 7119
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5635714183251063
        entropy_coeff: 0.0005000000000000001
        kl: 0.005341902802077432
        model: {}
        policy_loss: -0.011165030511619989
        total_loss: 7.088760217030843
        vf_explained_var: 0.9857079982757568
        vf_loss: 7.099939902623494
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.04666666666667
    gpu_util_percent0: 0.29333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14893178794360057
    mean_env_wait_ms: 1.2198815206590947
    mean_inference_ms: 4.478004857993455
    mean_raw_obs_processing_ms: 0.3863227680602513
  time_since_restore: 947.7535829544067
  time_this_iter_s: 25.792836904525757
  time_total_s: 947.7535829544067
  timers:
    learn_throughput: 8693.81
    learn_time_ms: 18610.023
    sample_throughput: 23774.766
    sample_time_ms: 6805.198
    update_time_ms: 37.96
  timestamp: 1602777469
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     37 |          947.754 | 5986304 |  237.409 |              292.313 |              126.556 |            827.962 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3554.3725731473887
    time_step_min: 3195
  date: 2020-10-15_15-58-14
  done: false
  episode_len_mean: 827.4813858695652
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 238.01187143170804
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 241
  episodes_total: 7360
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5369780013958613
        entropy_coeff: 0.0005000000000000001
        kl: 0.005353876428368191
        model: {}
        policy_loss: -0.007315323096311961
        total_loss: 10.488156159718832
        vf_explained_var: 0.9829527735710144
        vf_loss: 10.495472351710001
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.337931034482757
    gpu_util_percent0: 0.31206896551724134
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488551427111485
    mean_env_wait_ms: 1.2202781687355202
    mean_inference_ms: 4.472035742407449
    mean_raw_obs_processing_ms: 0.38606852415167703
  time_since_restore: 973.1279609203339
  time_this_iter_s: 25.374377965927124
  time_total_s: 973.1279609203339
  timers:
    learn_throughput: 8696.702
    learn_time_ms: 18603.835
    sample_throughput: 23755.508
    sample_time_ms: 6810.715
    update_time_ms: 36.323
  timestamp: 1602777494
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     38 |          973.128 | 6148096 |  238.012 |              292.313 |              126.556 |            827.481 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3550.4837425348373
    time_step_min: 3195
  date: 2020-10-15_15-58-40
  done: false
  episode_len_mean: 826.9894473024667
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 238.60389810251274
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 7581
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5339523702859879
        entropy_coeff: 0.0005000000000000001
        kl: 0.005730630713514984
        model: {}
        policy_loss: -0.009004498375967765
        total_loss: 8.086578289667765
        vf_explained_var: 0.9852219223976135
        vf_loss: 8.095563252766928
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.355172413793102
    gpu_util_percent0: 0.31655172413793103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034476
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14879018520428952
    mean_env_wait_ms: 1.220639730199021
    mean_inference_ms: 4.4669439661993655
    mean_raw_obs_processing_ms: 0.3858607622753211
  time_since_restore: 998.5575618743896
  time_this_iter_s: 25.429600954055786
  time_total_s: 998.5575618743896
  timers:
    learn_throughput: 8700.971
    learn_time_ms: 18594.706
    sample_throughput: 23724.967
    sample_time_ms: 6819.482
    update_time_ms: 37.298
  timestamp: 1602777520
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     39 |          998.558 | 6309888 |  238.604 |              292.313 |              126.556 |            826.989 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3547.675285862786
    time_step_min: 3195
  date: 2020-10-15_15-59-06
  done: false
  episode_len_mean: 826.5464996125032
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 239.03030955381737
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 161
  episodes_total: 7742
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5321356455485026
        entropy_coeff: 0.0005000000000000001
        kl: 0.00547590844022731
        model: {}
        policy_loss: -0.011284213736265277
        total_loss: 8.507766087849935
        vf_explained_var: 0.9816858768463135
        vf_loss: 8.519042531649271
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666665
    gpu_util_percent0: 0.32699999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14874365681953244
    mean_env_wait_ms: 1.2208773602384286
    mean_inference_ms: 4.463335655871363
    mean_raw_obs_processing_ms: 0.3857089162666723
  time_since_restore: 1024.3591663837433
  time_this_iter_s: 25.801604509353638
  time_total_s: 1024.3591663837433
  timers:
    learn_throughput: 8689.123
    learn_time_ms: 18620.06
    sample_throughput: 23677.838
    sample_time_ms: 6833.056
    update_time_ms: 38.617
  timestamp: 1602777546
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     40 |          1024.36 | 6471680 |   239.03 |              292.313 |              126.556 |            826.546 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3544.3294834369844
    time_step_min: 3195
  date: 2020-10-15_15-59-32
  done: false
  episode_len_mean: 826.0644794952682
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 239.5284262180158
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 183
  episodes_total: 7925
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.52472884953022
        entropy_coeff: 0.0005000000000000001
        kl: 0.004711794084869325
        model: {}
        policy_loss: -0.010675423662178218
        total_loss: 8.3647092183431
        vf_explained_var: 0.9835845828056335
        vf_loss: 8.37541139125824
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.034482758620694
    gpu_util_percent0: 0.3337931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14869337741882196
    mean_env_wait_ms: 1.2211473971661915
    mean_inference_ms: 4.459402967831676
    mean_raw_obs_processing_ms: 0.3855456170751924
  time_since_restore: 1049.8851866722107
  time_this_iter_s: 25.526020288467407
  time_total_s: 1049.8851866722107
  timers:
    learn_throughput: 8686.299
    learn_time_ms: 18626.114
    sample_throughput: 23667.86
    sample_time_ms: 6835.937
    update_time_ms: 39.586
  timestamp: 1602777572
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     41 |          1049.89 | 6633472 |  239.528 |              292.313 |              126.556 |            826.064 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3540.2243944423954
    time_step_min: 3192
  date: 2020-10-15_15-59-57
  done: false
  episode_len_mean: 825.4166768553613
  episode_reward_max: 292.76767676767633
  episode_reward_mean: 240.2004221207054
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 254
  episodes_total: 8179
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5037064626812935
        entropy_coeff: 0.0005000000000000001
        kl: 0.005376056535169482
        model: {}
        policy_loss: -0.009181031452802321
        total_loss: 8.43454376856486
        vf_explained_var: 0.9857959747314453
        vf_loss: 8.4438423315684
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.0896551724138
    gpu_util_percent0: 0.3420689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034476
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14862525158215842
    mean_env_wait_ms: 1.2214941590001673
    mean_inference_ms: 4.454183150427378
    mean_raw_obs_processing_ms: 0.38533036618758637
  time_since_restore: 1075.2635481357574
  time_this_iter_s: 25.378361463546753
  time_total_s: 1075.2635481357574
  timers:
    learn_throughput: 8691.056
    learn_time_ms: 18615.919
    sample_throughput: 23677.017
    sample_time_ms: 6833.293
    update_time_ms: 39.495
  timestamp: 1602777597
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     42 |          1075.26 | 6795264 |    240.2 |              292.768 |              126.556 |            825.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3536.559438040346
    time_step_min: 3192
  date: 2020-10-15_16-00-23
  done: false
  episode_len_mean: 825.0002388344877
  episode_reward_max: 292.76767676767633
  episode_reward_mean: 240.70895967074583
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 8374
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.49372132619222003
        entropy_coeff: 0.0005000000000000001
        kl: 0.00568037914733092
        model: {}
        policy_loss: -0.008380183891858906
        total_loss: 7.80752154191335
        vf_explained_var: 0.9841473698616028
        vf_loss: 7.816006541252136
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886666666666667
    gpu_util_percent0: 0.322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14857732375095511
    mean_env_wait_ms: 1.2217471938596307
    mean_inference_ms: 4.4503533275477105
    mean_raw_obs_processing_ms: 0.3851727342687074
  time_since_restore: 1100.9167940616608
  time_this_iter_s: 25.65324592590332
  time_total_s: 1100.9167940616608
  timers:
    learn_throughput: 8707.119
    learn_time_ms: 18581.576
    sample_throughput: 23623.402
    sample_time_ms: 6848.802
    update_time_ms: 39.713
  timestamp: 1602777623
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     43 |          1100.92 | 6957056 |  240.709 |              292.768 |              126.556 |                825 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3533.6711828463713
    time_step_min: 3192
  date: 2020-10-15_16-00-49
  done: false
  episode_len_mean: 824.5495664401219
  episode_reward_max: 292.76767676767633
  episode_reward_mean: 241.11308302144926
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 160
  episodes_total: 8534
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.49470652639865875
        entropy_coeff: 0.0005000000000000001
        kl: 0.005724602417709927
        model: {}
        policy_loss: -0.010496073344256729
        total_loss: 7.473387956619263
        vf_explained_var: 0.9833481311798096
        vf_loss: 7.483988404273987
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.48275862068966
    gpu_util_percent0: 0.31517241379310346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485384110940224
    mean_env_wait_ms: 1.2219480745530034
    mean_inference_ms: 4.447314266688614
    mean_raw_obs_processing_ms: 0.38504818019553866
  time_since_restore: 1126.7074773311615
  time_this_iter_s: 25.790683269500732
  time_total_s: 1126.7074773311615
  timers:
    learn_throughput: 8702.04
    learn_time_ms: 18592.421
    sample_throughput: 23552.831
    sample_time_ms: 6869.323
    update_time_ms: 39.396
  timestamp: 1602777649
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     44 |          1126.71 | 7118848 |  241.113 |              292.768 |              126.556 |             824.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3530.374583093732
    time_step_min: 3186
  date: 2020-10-15_16-01-15
  done: false
  episode_len_mean: 824.0272280059489
  episode_reward_max: 293.6767676767673
  episode_reward_mean: 241.63109645823258
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 8741
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.47812945147355396
        entropy_coeff: 0.0005000000000000001
        kl: 0.005789765544856588
        model: {}
        policy_loss: -0.010132045989545682
        total_loss: 7.940152247746785
        vf_explained_var: 0.9854769110679626
        vf_loss: 7.950378696123759
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.006666666666668
    gpu_util_percent0: 0.3233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484913050178055
    mean_env_wait_ms: 1.2221986461195171
    mean_inference_ms: 4.443539293561549
    mean_raw_obs_processing_ms: 0.38488902449716883
  time_since_restore: 1152.1260406970978
  time_this_iter_s: 25.41856336593628
  time_total_s: 1152.1260406970978
  timers:
    learn_throughput: 8712.482
    learn_time_ms: 18570.139
    sample_throughput: 23528.75
    sample_time_ms: 6876.353
    update_time_ms: 38.446
  timestamp: 1602777675
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     45 |          1152.13 | 7280640 |  241.631 |              293.677 |              126.556 |            824.027 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3525.8327931150106
    time_step_min: 3186
  date: 2020-10-15_16-01-40
  done: false
  episode_len_mean: 823.4236628488825
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 242.30384799849904
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 252
  episodes_total: 8993
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4603457028667132
        entropy_coeff: 0.0005000000000000001
        kl: 0.005457567555519442
        model: {}
        policy_loss: -0.010726391825301107
        total_loss: 7.70986278851827
        vf_explained_var: 0.9865714907646179
        vf_loss: 7.720682978630066
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.220689655172414
    gpu_util_percent0: 0.3641379310344827
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14843402078806162
    mean_env_wait_ms: 1.222496830440605
    mean_inference_ms: 4.439176970060038
    mean_raw_obs_processing_ms: 0.3847154624136825
  time_since_restore: 1177.4749886989594
  time_this_iter_s: 25.348948001861572
  time_total_s: 1177.4749886989594
  timers:
    learn_throughput: 8718.307
    learn_time_ms: 18557.731
    sample_throughput: 23534.807
    sample_time_ms: 6874.584
    update_time_ms: 36.471
  timestamp: 1602777700
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     46 |          1177.47 | 7442432 |  242.304 |              298.525 |              126.556 |            823.424 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3522.869159903488
    time_step_min: 3186
  date: 2020-10-15_16-02-06
  done: false
  episode_len_mean: 822.9783937145352
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 242.75221772504585
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 171
  episodes_total: 9164
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.46012789259354275
        entropy_coeff: 0.0005000000000000001
        kl: 0.005554196114341418
        model: {}
        policy_loss: -0.009548212585893149
        total_loss: 6.0206065972646075
        vf_explained_var: 0.9863595962524414
        vf_loss: 6.030245979626973
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.703448275862073
    gpu_util_percent0: 0.30586206896551715
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14839710177667673
    mean_env_wait_ms: 1.2226770600097208
    mean_inference_ms: 4.4362680231088705
    mean_raw_obs_processing_ms: 0.3845972226038246
  time_since_restore: 1202.8381958007812
  time_this_iter_s: 25.3632071018219
  time_total_s: 1202.8381958007812
  timers:
    learn_throughput: 8730.848
    learn_time_ms: 18531.075
    sample_throughput: 23590.687
    sample_time_ms: 6858.3
    update_time_ms: 35.879
  timestamp: 1602777726
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     47 |          1202.84 | 7604224 |  242.752 |              298.525 |              126.556 |            822.978 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3520.020225927918
    time_step_min: 3186
  date: 2020-10-15_16-02-31
  done: false
  episode_len_mean: 822.4426720907826
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 243.20301721853986
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 177
  episodes_total: 9341
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.46006551136573154
        entropy_coeff: 0.0005000000000000001
        kl: 0.005484122200869024
        model: {}
        policy_loss: -0.010118320487284413
        total_loss: 6.799661954243978
        vf_explained_var: 0.9850449562072754
        vf_loss: 6.8098730246226
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03448275862069
    gpu_util_percent0: 0.356551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14835936698284807
    mean_env_wait_ms: 1.222873576547313
    mean_inference_ms: 4.433354602454379
    mean_raw_obs_processing_ms: 0.38447796544618723
  time_since_restore: 1228.1346597671509
  time_this_iter_s: 25.29646396636963
  time_total_s: 1228.1346597671509
  timers:
    learn_throughput: 8732.043
    learn_time_ms: 18528.539
    sample_throughput: 23609.193
    sample_time_ms: 6852.924
    update_time_ms: 35.127
  timestamp: 1602777751
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     48 |          1228.13 | 7766016 |  243.203 |              298.525 |              126.556 |            822.443 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3515.955126860977
    time_step_min: 3186
  date: 2020-10-15_16-02-57
  done: false
  episode_len_mean: 821.7557387312187
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 243.8023557781484
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 243
  episodes_total: 9584
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.43939946343501407
        entropy_coeff: 0.0005000000000000001
        kl: 0.005502724554389715
        model: {}
        policy_loss: -0.010579032047341267
        total_loss: 8.257195512453714
        vf_explained_var: 0.9856317043304443
        vf_loss: 8.267856558163961
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.57586206896552
    gpu_util_percent0: 0.356551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14831155982879982
    mean_env_wait_ms: 1.223125552796748
    mean_inference_ms: 4.429582785478772
    mean_raw_obs_processing_ms: 0.38432787168693877
  time_since_restore: 1253.60511136055
  time_this_iter_s: 25.470451593399048
  time_total_s: 1253.60511136055
  timers:
    learn_throughput: 8725.557
    learn_time_ms: 18542.312
    sample_throughput: 23637.099
    sample_time_ms: 6844.833
    update_time_ms: 32.934
  timestamp: 1602777777
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     49 |          1253.61 | 7927808 |  243.802 |              298.525 |              126.556 |            821.756 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3512.627205580632
    time_step_min: 3186
  date: 2020-10-15_16-03-23
  done: false
  episode_len_mean: 821.2432101286502
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 244.3643562436698
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 9794
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4198617140452067
        entropy_coeff: 0.0005000000000000001
        kl: 0.004976712826949854
        model: {}
        policy_loss: -0.010781078589692092
        total_loss: 6.021040320396423
        vf_explained_var: 0.9875360131263733
        vf_loss: 6.031906882921855
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94333333333334
    gpu_util_percent0: 0.3553333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14827092563789313
    mean_env_wait_ms: 1.2233440871465087
    mean_inference_ms: 4.426427748677618
    mean_raw_obs_processing_ms: 0.38420098087979165
  time_since_restore: 1279.171174287796
  time_this_iter_s: 25.566062927246094
  time_total_s: 1279.171174287796
  timers:
    learn_throughput: 8737.739
    learn_time_ms: 18516.461
    sample_throughput: 23633.58
    sample_time_ms: 6845.852
    update_time_ms: 31.942
  timestamp: 1602777803
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     50 |          1279.17 | 8089600 |  244.364 |              298.525 |              126.556 |            821.243 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3509.6742684157416
    time_step_min: 3186
  date: 2020-10-15_16-03-49
  done: false
  episode_len_mean: 820.8204098031338
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 244.8066634606406
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 162
  episodes_total: 9956
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.428066464761893
        entropy_coeff: 0.0005000000000000001
        kl: 0.005687459566009541
        model: {}
        policy_loss: -0.011090482973183194
        total_loss: 5.234896739323934
        vf_explained_var: 0.987224280834198
        vf_loss: 5.246130108833313
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.413793103448278
    gpu_util_percent0: 0.34689655172413797
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14824102662503183
    mean_env_wait_ms: 1.2235021632352516
    mean_inference_ms: 4.424053320220716
    mean_raw_obs_processing_ms: 0.3841060592006631
  time_since_restore: 1304.6832253932953
  time_this_iter_s: 25.512051105499268
  time_total_s: 1304.6832253932953
  timers:
    learn_throughput: 8744.221
    learn_time_ms: 18502.734
    sample_throughput: 23582.529
    sample_time_ms: 6860.672
    update_time_ms: 29.209
  timestamp: 1602777829
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     51 |          1304.68 | 8251392 |  244.807 |              298.525 |              126.556 |             820.82 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3506.0282692497776
    time_step_min: 3157
  date: 2020-10-15_16-04-15
  done: false
  episode_len_mean: 820.325396044475
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 245.36338192512514
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 10163
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4246461093425751
        entropy_coeff: 0.0005000000000000001
        kl: 0.005070816182220976
        model: {}
        policy_loss: -0.009410695323216109
        total_loss: 6.691620945930481
        vf_explained_var: 0.9863157272338867
        vf_loss: 6.701180577278137
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.750000000000007
    gpu_util_percent0: 0.30333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482037502049485
    mean_env_wait_ms: 1.2236967660451092
    mean_inference_ms: 4.421125917269202
    mean_raw_obs_processing_ms: 0.38398501186033407
  time_since_restore: 1330.684315919876
  time_this_iter_s: 26.00109052658081
  time_total_s: 1330.684315919876
  timers:
    learn_throughput: 8726.379
    learn_time_ms: 18540.566
    sample_throughput: 23499.512
    sample_time_ms: 6884.909
    update_time_ms: 29.013
  timestamp: 1602777855
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     52 |          1330.68 | 8413184 |  245.363 |              300.646 |              126.556 |            820.325 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3501.8771100607696
    time_step_min: 3157
  date: 2020-10-15_16-04-41
  done: false
  episode_len_mean: 819.7823874003649
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 245.9991997183005
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 250
  episodes_total: 10413
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4001444876194
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047837618427972
        model: {}
        policy_loss: -0.0097249773874258
        total_loss: 6.271246870358785
        vf_explained_var: 0.9883804321289062
        vf_loss: 6.28111207485199
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.960000000000004
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14816033603677203
    mean_env_wait_ms: 1.22392068270348
    mean_inference_ms: 4.417785828794816
    mean_raw_obs_processing_ms: 0.38385450840701607
  time_since_restore: 1356.4709811210632
  time_this_iter_s: 25.786665201187134
  time_total_s: 1356.4709811210632
  timers:
    learn_throughput: 8712.644
    learn_time_ms: 18569.793
    sample_throughput: 23578.591
    sample_time_ms: 6861.818
    update_time_ms: 34.706
  timestamp: 1602777881
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     53 |          1356.47 | 8574976 |  245.999 |              300.646 |              126.556 |            819.782 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3499.0690702087286
    time_step_min: 3157
  date: 2020-10-15_16-05-07
  done: false
  episode_len_mean: 819.409125259777
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 246.4077617283735
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 173
  episodes_total: 10586
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.39603111147880554
        entropy_coeff: 0.0005000000000000001
        kl: 0.005139228499804934
        model: {}
        policy_loss: -0.009373984765261412
        total_loss: 6.07805069287618
        vf_explained_var: 0.986314594745636
        vf_loss: 6.087590495745341
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.33103448275862
    gpu_util_percent0: 0.35586206896551725
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813197532020828
    mean_env_wait_ms: 1.2240731700290879
    mean_inference_ms: 4.41551379510666
    mean_raw_obs_processing_ms: 0.38376293219258
  time_since_restore: 1381.8736851215363
  time_this_iter_s: 25.402704000473022
  time_total_s: 1381.8736851215363
  timers:
    learn_throughput: 8723.978
    learn_time_ms: 18545.667
    sample_throughput: 23633.322
    sample_time_ms: 6845.927
    update_time_ms: 34.497
  timestamp: 1602777907
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     54 |          1381.87 | 8736768 |  246.408 |              300.646 |              126.556 |            819.409 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3496.3686076894364
    time_step_min: 3157
  date: 2020-10-15_16-05-33
  done: false
  episode_len_mean: 819.0632782010779
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 246.8354826841164
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 176
  episodes_total: 10762
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.40235375116268796
        entropy_coeff: 0.0005000000000000001
        kl: 0.005396728714307149
        model: {}
        policy_loss: -0.009094168592127971
        total_loss: 7.081272840499878
        vf_explained_var: 0.9844600558280945
        vf_loss: 7.090534528096517
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.99666666666667
    gpu_util_percent0: 0.35333333333333344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14810317935317283
    mean_env_wait_ms: 1.2242201642484498
    mean_inference_ms: 4.413268268398647
    mean_raw_obs_processing_ms: 0.3836728031469414
  time_since_restore: 1407.621339082718
  time_this_iter_s: 25.74765396118164
  time_total_s: 1407.621339082718
  timers:
    learn_throughput: 8711.117
    learn_time_ms: 18573.049
    sample_throughput: 23643.274
    sample_time_ms: 6843.045
    update_time_ms: 41.775
  timestamp: 1602777933
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     55 |          1407.62 | 8898560 |  246.835 |              300.646 |              126.556 |            819.063 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3492.510138838144
    time_step_min: 3142
  date: 2020-10-15_16-05-58
  done: false
  episode_len_mean: 818.6212479534291
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 247.40125100376113
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 232
  episodes_total: 10994
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3824375569820404
        entropy_coeff: 0.0005000000000000001
        kl: 0.004891937094119688
        model: {}
        policy_loss: -0.008490346993009249
        total_loss: 7.047441482543945
        vf_explained_var: 0.9873548150062561
        vf_loss: 7.056092619895935
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.413793103448278
    gpu_util_percent0: 0.3813793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806884038095805
    mean_env_wait_ms: 1.2243943083385358
    mean_inference_ms: 4.410441884129333
    mean_raw_obs_processing_ms: 0.38356066592436866
  time_since_restore: 1433.2383501529694
  time_this_iter_s: 25.617011070251465
  time_total_s: 1433.2383501529694
  timers:
    learn_throughput: 8700.307
    learn_time_ms: 18596.126
    sample_throughput: 23637.544
    sample_time_ms: 6844.704
    update_time_ms: 43.193
  timestamp: 1602777958
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     56 |          1433.24 | 9060352 |  247.401 |              300.646 |              126.556 |            818.621 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3488.9225604297226
    time_step_min: 3134
  date: 2020-10-15_16-06-24
  done: false
  episode_len_mean: 818.2746968616262
  episode_reward_max: 301.5555555555554
  episode_reward_mean: 247.95323419645783
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 222
  episodes_total: 11216
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3699767291545868
        entropy_coeff: 0.0005000000000000001
        kl: 0.005115050434445341
        model: {}
        policy_loss: -0.007562085253539408
        total_loss: 5.690538167953491
        vf_explained_var: 0.9884591102600098
        vf_loss: 5.698269287745158
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.753333333333334
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14803375620957093
    mean_env_wait_ms: 1.2245658482250084
    mean_inference_ms: 4.407796884510793
    mean_raw_obs_processing_ms: 0.3834553936296381
  time_since_restore: 1458.843517780304
  time_this_iter_s: 25.605167627334595
  time_total_s: 1458.843517780304
  timers:
    learn_throughput: 8686.946
    learn_time_ms: 18624.728
    sample_throughput: 23657.189
    sample_time_ms: 6839.02
    update_time_ms: 43.537
  timestamp: 1602777984
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     57 |          1458.84 | 9222144 |  247.953 |              301.556 |              126.556 |            818.275 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3486.4044299329335
    time_step_min: 3134
  date: 2020-10-15_16-06-50
  done: false
  episode_len_mean: 817.9564071014238
  episode_reward_max: 301.5555555555554
  episode_reward_mean: 248.3344270619711
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 162
  episodes_total: 11378
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.37272237489620846
        entropy_coeff: 0.0005000000000000001
        kl: 0.005328354930194716
        model: {}
        policy_loss: -0.010362972466585537
        total_loss: 5.935941974322001
        vf_explained_var: 0.9858854413032532
        vf_loss: 5.94647475083669
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796551724137927
    gpu_util_percent0: 0.3234482758620689
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.796551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14801049464812294
    mean_env_wait_ms: 1.2246770580225743
    mean_inference_ms: 4.4059189538769825
    mean_raw_obs_processing_ms: 0.3833812335525476
  time_since_restore: 1484.487949371338
  time_this_iter_s: 25.644431591033936
  time_total_s: 1484.487949371338
  timers:
    learn_throughput: 8677.165
    learn_time_ms: 18645.721
    sample_throughput: 23616.431
    sample_time_ms: 6850.823
    update_time_ms: 44.892
  timestamp: 1602778010
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     58 |          1484.49 | 9383936 |  248.334 |              301.556 |              126.556 |            817.956 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3483.210366646442
    time_step_min: 3106
  date: 2020-10-15_16-07-16
  done: false
  episode_len_mean: 817.6424069757403
  episode_reward_max: 305.7979797979797
  episode_reward_mean: 248.84653057380294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 11583
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.36743522187074024
        entropy_coeff: 0.0005000000000000001
        kl: 0.005346729381320377
        model: {}
        policy_loss: -0.007888776114365706
        total_loss: 5.618445793787639
        vf_explained_var: 0.9883816838264465
        vf_loss: 5.6265015204747515
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.493103448275864
    gpu_util_percent0: 0.293448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798200850697732
    mean_env_wait_ms: 1.2248062227508474
    mean_inference_ms: 4.4035968103750776
    mean_raw_obs_processing_ms: 0.38328978686699866
  time_since_restore: 1509.8134834766388
  time_this_iter_s: 25.325534105300903
  time_total_s: 1509.8134834766388
  timers:
    learn_throughput: 8682.412
    learn_time_ms: 18634.454
    sample_throughput: 23634.257
    sample_time_ms: 6845.656
    update_time_ms: 45.884
  timestamp: 1602778036
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     59 |          1509.81 | 9545728 |  248.847 |              305.798 |              126.556 |            817.642 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3479.3053221288515
    time_step_min: 3106
  date: 2020-10-15_16-07-41
  done: false
  episode_len_mean: 817.2589836814069
  episode_reward_max: 305.7979797979797
  episode_reward_mean: 249.43620102265544
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 244
  episodes_total: 11827
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.35027463734149933
        entropy_coeff: 0.0005000000000000001
        kl: 0.005626743387741347
        model: {}
        policy_loss: -0.01029486587503925
        total_loss: 5.235284050305684
        vf_explained_var: 0.9902641773223877
        vf_loss: 5.245736440022786
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.065517241379315
    gpu_util_percent0: 0.3737931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413783
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479478087031841
    mean_env_wait_ms: 1.224964535182266
    mean_inference_ms: 4.401007977200889
    mean_raw_obs_processing_ms: 0.3831888500556066
  time_since_restore: 1535.20157456398
  time_this_iter_s: 25.38809108734131
  time_total_s: 1535.20157456398
  timers:
    learn_throughput: 8680.916
    learn_time_ms: 18637.664
    sample_throughput: 23705.815
    sample_time_ms: 6824.992
    update_time_ms: 45.742
  timestamp: 1602778061
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     60 |           1535.2 | 9707520 |  249.436 |              305.798 |              126.556 |            817.259 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3476.7416819929776
    time_step_min: 3106
  date: 2020-10-15_16-08-07
  done: false
  episode_len_mean: 816.9816788807461
  episode_reward_max: 305.7979797979797
  episode_reward_mean: 249.83815251112017
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 181
  episodes_total: 12008
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3473878701527913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004786956473253667
        model: {}
        policy_loss: -0.008876402610136816
        total_loss: 5.055386622746785
        vf_explained_var: 0.9886049628257751
        vf_loss: 5.064421653747559
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796666666666667
    gpu_util_percent0: 0.33833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14792338858647908
    mean_env_wait_ms: 1.225070991572551
    mean_inference_ms: 4.3990811068739895
    mean_raw_obs_processing_ms: 0.3831123031005487
  time_since_restore: 1560.7740828990936
  time_this_iter_s: 25.572508335113525
  time_total_s: 1560.7740828990936
  timers:
    learn_throughput: 8675.116
    learn_time_ms: 18650.125
    sample_throughput: 23743.078
    sample_time_ms: 6814.281
    update_time_ms: 47.647
  timestamp: 1602778087
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     61 |          1560.77 | 9869312 |  249.838 |              305.798 |              126.556 |            816.982 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3474.062129202373
    time_step_min: 3106
  date: 2020-10-15_16-08-33
  done: false
  episode_len_mean: 816.7635856181251
  episode_reward_max: 306.555555555555
  episode_reward_mean: 250.23456780910365
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 174
  episodes_total: 12182
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3518571654955546
        entropy_coeff: 0.0005000000000000001
        kl: 0.004656291256348292
        model: {}
        policy_loss: -0.010902227833867073
        total_loss: 5.461006919542949
        vf_explained_var: 0.9874956607818604
        vf_loss: 5.47207772731781
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.693103448275863
    gpu_util_percent0: 0.3458620689655173
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790091091036792
    mean_env_wait_ms: 1.2251695524040633
    mean_inference_ms: 4.3972976209991375
    mean_raw_obs_processing_ms: 0.3830427382863043
  time_since_restore: 1586.4201023578644
  time_this_iter_s: 25.646019458770752
  time_total_s: 1586.4201023578644
  timers:
    learn_throughput: 8682.424
    learn_time_ms: 18634.428
    sample_throughput: 23815.363
    sample_time_ms: 6793.598
    update_time_ms: 47.88
  timestamp: 1602778113
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     62 |          1586.42 | 10031104 |  250.235 |              306.556 |              126.556 |            816.764 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3470.7738856079604
    time_step_min: 3106
  date: 2020-10-15_16-08-59
  done: false
  episode_len_mean: 816.4106552752478
  episode_reward_max: 306.555555555555
  episode_reward_mean: 250.72531798194692
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 12407
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3490934396783511
        entropy_coeff: 0.0005000000000000001
        kl: 0.004778989280263583
        model: {}
        policy_loss: -0.008070748135954394
        total_loss: 6.770879467328389
        vf_explained_var: 0.9873373508453369
        vf_loss: 6.779120882352193
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.466666666666672
    gpu_util_percent0: 0.3463333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787560945480357
    mean_env_wait_ms: 1.225283795128141
    mean_inference_ms: 4.395090282330137
    mean_raw_obs_processing_ms: 0.38295462561807614
  time_since_restore: 1611.8705129623413
  time_this_iter_s: 25.45041060447693
  time_total_s: 1611.8705129623413
  timers:
    learn_throughput: 8693.437
    learn_time_ms: 18610.82
    sample_throughput: 23830.183
    sample_time_ms: 6789.373
    update_time_ms: 41.683
  timestamp: 1602778139
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     63 |          1611.87 | 10192896 |  250.725 |              306.556 |              126.556 |            816.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3467.6379091197964
    time_step_min: 3106
  date: 2020-10-15_16-09-25
  done: false
  episode_len_mean: 816.1366946335286
  episode_reward_max: 306.555555555555
  episode_reward_mean: 251.2144246006044
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 227
  episodes_total: 12634
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.33230379472176236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004719143112500508
        model: {}
        policy_loss: -0.008517101462833429
        total_loss: 5.602464318275452
        vf_explained_var: 0.9889903664588928
        vf_loss: 5.611145615577698
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.673333333333332
    gpu_util_percent0: 0.30400000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478448114138741
    mean_env_wait_ms: 1.2254004734112547
    mean_inference_ms: 4.392883773572799
    mean_raw_obs_processing_ms: 0.38287149793704905
  time_since_restore: 1637.6244003772736
  time_this_iter_s: 25.75388741493225
  time_total_s: 1637.6244003772736
  timers:
    learn_throughput: 8677.83
    learn_time_ms: 18644.293
    sample_throughput: 23826.235
    sample_time_ms: 6790.498
    update_time_ms: 41.199
  timestamp: 1602778165
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     64 |          1637.62 | 10354688 |  251.214 |              306.556 |              126.556 |            816.137 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3465.2452756214225
    time_step_min: 3106
  date: 2020-10-15_16-09-51
  done: false
  episode_len_mean: 815.959059301508
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 251.58549397403962
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 165
  episodes_total: 12799
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.33149294306834537
        entropy_coeff: 0.0005000000000000001
        kl: 0.005118549180527528
        model: {}
        policy_loss: -0.011392354687510911
        total_loss: 4.358288009961446
        vf_explained_var: 0.9895090460777283
        vf_loss: 4.369845112164815
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.12758620689655
    gpu_util_percent0: 0.3062068965517242
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478258737255982
    mean_env_wait_ms: 1.225472444135454
    mean_inference_ms: 4.391335458442205
    mean_raw_obs_processing_ms: 0.3828084587376218
  time_since_restore: 1663.0298526287079
  time_this_iter_s: 25.405452251434326
  time_total_s: 1663.0298526287079
  timers:
    learn_throughput: 8685.41
    learn_time_ms: 18628.021
    sample_throughput: 23863.236
    sample_time_ms: 6779.969
    update_time_ms: 33.793
  timestamp: 1602778191
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     65 |          1663.03 | 10516480 |  251.585 |              306.556 |              126.556 |            815.959 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3462.5430315203957
    time_step_min: 3106
  date: 2020-10-15_16-10-16
  done: false
  episode_len_mean: 815.7602001539645
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 251.98010124338037
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 12990
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.33109505474567413
        entropy_coeff: 0.0005000000000000001
        kl: 0.004953681103264292
        model: {}
        policy_loss: -0.009910731421162685
        total_loss: 5.735819061597188
        vf_explained_var: 0.9879274368286133
        vf_loss: 5.7458943128585815
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.386666666666667
    gpu_util_percent0: 0.3363333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780427892536574
    mean_env_wait_ms: 1.2255505232662072
    mean_inference_ms: 4.38958622277356
    mean_raw_obs_processing_ms: 0.38273926361853183
  time_since_restore: 1688.5382418632507
  time_this_iter_s: 25.508389234542847
  time_total_s: 1688.5382418632507
  timers:
    learn_throughput: 8689.293
    learn_time_ms: 18619.697
    sample_throughput: 23871.102
    sample_time_ms: 6777.735
    update_time_ms: 32.752
  timestamp: 1602778216
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     66 |          1688.54 | 10678272 |   251.98 |              306.556 |              126.556 |             815.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3459.4306408797875
    time_step_min: 3104
  date: 2020-10-15_16-10-42
  done: false
  episode_len_mean: 815.5501473811503
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 252.4616347130893
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 241
  episodes_total: 13231
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.31809532394011814
        entropy_coeff: 0.0005000000000000001
        kl: 0.005060969425054888
        model: {}
        policy_loss: -0.00855924459756352
        total_loss: 6.23681366443634
        vf_explained_var: 0.9886898994445801
        vf_loss: 6.245531439781189
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.217241379310348
    gpu_util_percent0: 0.37103448275862067
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777743220195452
    mean_env_wait_ms: 1.2256447325976214
    mean_inference_ms: 4.387475074723523
    mean_raw_obs_processing_ms: 0.38265344417586644
  time_since_restore: 1714.1728868484497
  time_this_iter_s: 25.634644985198975
  time_total_s: 1714.1728868484497
  timers:
    learn_throughput: 8687.547
    learn_time_ms: 18623.439
    sample_throughput: 23878.753
    sample_time_ms: 6775.563
    update_time_ms: 32.708
  timestamp: 1602778242
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     67 |          1714.17 | 10840064 |  252.462 |              306.556 |              126.556 |             815.55 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3456.7052010162906
    time_step_min: 3104
  date: 2020-10-15_16-11-08
  done: false
  episode_len_mean: 815.3170241286863
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 252.86491817188832
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 13428
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3082869350910187
        entropy_coeff: 0.0005000000000000001
        kl: 0.005168662561724584
        model: {}
        policy_loss: -0.010725375924569866
        total_loss: 4.686489701271057
        vf_explained_var: 0.989706814289093
        vf_loss: 4.697368621826172
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.13
    gpu_util_percent0: 0.3403333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477542410637011
    mean_env_wait_ms: 1.2257209900916004
    mean_inference_ms: 4.385753453907019
    mean_raw_obs_processing_ms: 0.3825872677177565
  time_since_restore: 1739.8964335918427
  time_this_iter_s: 25.723546743392944
  time_total_s: 1739.8964335918427
  timers:
    learn_throughput: 8680.578
    learn_time_ms: 18638.391
    sample_throughput: 23910.55
    sample_time_ms: 6766.553
    update_time_ms: 33.153
  timestamp: 1602778268
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     68 |           1739.9 | 11001856 |  252.865 |              306.556 |              126.556 |            815.317 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3454.3622814137093
    time_step_min: 3104
  date: 2020-10-15_16-11-34
  done: false
  episode_len_mean: 815.1194205456284
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 253.2205398347023
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 171
  episodes_total: 13599
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3148614540696144
        entropy_coeff: 0.0005000000000000001
        kl: 0.005381360262011488
        model: {}
        policy_loss: -0.008486275027583664
        total_loss: 5.1140109697977705
        vf_explained_var: 0.9879872798919678
        vf_loss: 5.122654477755229
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.826666666666664
    gpu_util_percent0: 0.35966666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.79
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477355487594432
    mean_env_wait_ms: 1.2257827881805545
    mean_inference_ms: 4.384314203314651
    mean_raw_obs_processing_ms: 0.3825297381544069
  time_since_restore: 1765.7030284404755
  time_this_iter_s: 25.806594848632812
  time_total_s: 1765.7030284404755
  timers:
    learn_throughput: 8659.907
    learn_time_ms: 18682.879
    sample_throughput: 23901.76
    sample_time_ms: 6769.041
    update_time_ms: 32.437
  timestamp: 1602778294
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     69 |           1765.7 | 11163648 |  253.221 |              306.556 |              126.556 |            815.119 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3451.5268622041526
    time_step_min: 3104
  date: 2020-10-15_16-12-00
  done: false
  episode_len_mean: 814.8107091172214
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 253.6615394173278
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 13820
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.30804723252852756
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044756393957262235
        model: {}
        policy_loss: -0.007979019816654423
        total_loss: 6.014165600140889
        vf_explained_var: 0.988081693649292
        vf_loss: 6.022298177083333
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.676666666666666
    gpu_util_percent0: 0.381
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.853333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771412077083898
    mean_env_wait_ms: 1.2258500023367487
    mean_inference_ms: 4.382515525213694
    mean_raw_obs_processing_ms: 0.3824560060444783
  time_since_restore: 1791.440664768219
  time_this_iter_s: 25.73763632774353
  time_total_s: 1791.440664768219
  timers:
    learn_throughput: 8649.64
    learn_time_ms: 18705.056
    sample_throughput: 23899.866
    sample_time_ms: 6769.578
    update_time_ms: 33.902
  timestamp: 1602778320
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     70 |          1791.44 | 11325440 |  253.662 |              306.556 |              126.556 |            814.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3448.369822907741
    time_step_min: 3104
  date: 2020-10-15_16-12-26
  done: false
  episode_len_mean: 814.4635587188612
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 254.14217261583772
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 230
  episodes_total: 14050
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2893260220686595
        entropy_coeff: 0.0005000000000000001
        kl: 0.00460086219633619
        model: {}
        policy_loss: -0.011059359380548509
        total_loss: 4.970375736554463
        vf_explained_var: 0.9899899363517761
        vf_loss: 4.981579661369324
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.055172413793102
    gpu_util_percent0: 0.39931034482758615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768905634302273
    mean_env_wait_ms: 1.225923301959283
    mean_inference_ms: 4.380674766109279
    mean_raw_obs_processing_ms: 0.38238662906087717
  time_since_restore: 1816.7842445373535
  time_this_iter_s: 25.34357976913452
  time_total_s: 1816.7842445373535
  timers:
    learn_throughput: 8650.896
    learn_time_ms: 18702.34
    sample_throughput: 23969.465
    sample_time_ms: 6749.921
    update_time_ms: 34.083
  timestamp: 1602778346
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     71 |          1816.78 | 11487232 |  254.142 |              306.556 |              126.556 |            814.464 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3446.1246648793567
    time_step_min: 3104
  date: 2020-10-15_16-12-52
  done: false
  episode_len_mean: 814.1860759493671
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 254.48115117418882
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 170
  episodes_total: 14220
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.29010538508494693
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044984914517651005
        model: {}
        policy_loss: -0.010564355572569184
        total_loss: 4.877623597780864
        vf_explained_var: 0.988173246383667
        vf_loss: 4.888332962989807
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25172413793104
    gpu_util_percent0: 0.3375862068965518
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476720297414217
    mean_env_wait_ms: 1.2259760887787159
    mean_inference_ms: 4.379344534136533
    mean_raw_obs_processing_ms: 0.38233215089452
  time_since_restore: 1842.1796805858612
  time_this_iter_s: 25.39543604850769
  time_total_s: 1842.1796805858612
  timers:
    learn_throughput: 8657.201
    learn_time_ms: 18688.719
    sample_throughput: 24012.622
    sample_time_ms: 6737.79
    update_time_ms: 33.624
  timestamp: 1602778372
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     72 |          1842.18 | 11649024 |  254.481 |              306.556 |              126.556 |            814.186 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3443.412202588006
    time_step_min: 3104
  date: 2020-10-15_16-13-18
  done: false
  episode_len_mean: 813.8409847434119
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 254.8801888510623
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 14420
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.29735204577445984
        entropy_coeff: 0.0005000000000000001
        kl: 0.00498960407761236
        model: {}
        policy_loss: -0.0081715231644921
        total_loss: 5.799649635950725
        vf_explained_var: 0.9872967600822449
        vf_loss: 5.807969888051351
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.173333333333336
    gpu_util_percent0: 0.32633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765379265341708
    mean_env_wait_ms: 1.2260273371906518
    mean_inference_ms: 4.377832849504399
    mean_raw_obs_processing_ms: 0.3822722234931736
  time_since_restore: 1867.7903907299042
  time_this_iter_s: 25.61071014404297
  time_total_s: 1867.7903907299042
  timers:
    learn_throughput: 8650.516
    learn_time_ms: 18703.162
    sample_throughput: 24010.769
    sample_time_ms: 6738.31
    update_time_ms: 33.789
  timestamp: 1602778398
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     73 |          1867.79 | 11810816 |   254.88 |              306.556 |              126.556 |            813.841 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3440.172949305603
    time_step_min: 3104
  date: 2020-10-15_16-13-44
  done: false
  episode_len_mean: 813.4098751960718
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 255.36681828859383
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 243
  episodes_total: 14663
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.27952173352241516
        entropy_coeff: 0.0005000000000000001
        kl: 0.004244102844192336
        model: {}
        policy_loss: -0.007911291910810784
        total_loss: 5.4590301513671875
        vf_explained_var: 0.9894629120826721
        vf_loss: 5.467081030209859
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.820000000000004
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762982329169833
    mean_env_wait_ms: 1.226097352107626
    mean_inference_ms: 4.376034505321864
    mean_raw_obs_processing_ms: 0.38219884750448857
  time_since_restore: 1893.4583687782288
  time_this_iter_s: 25.667978048324585
  time_total_s: 1893.4583687782288
  timers:
    learn_throughput: 8653.199
    learn_time_ms: 18697.362
    sample_throughput: 24027.245
    sample_time_ms: 6733.689
    update_time_ms: 34.661
  timestamp: 1602778424
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     74 |          1893.46 | 11972608 |  255.367 |              306.556 |              126.556 |             813.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3437.6310955018234
    time_step_min: 3099
  date: 2020-10-15_16-14-09
  done: false
  episode_len_mean: 813.1072582817129
  episode_reward_max: 306.8585858585855
  episode_reward_mean: 255.75442888350204
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 14852
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.27546490480502445
        entropy_coeff: 0.0005000000000000001
        kl: 0.004323257560220857
        model: {}
        policy_loss: -0.010564397797376538
        total_loss: 3.991108854611715
        vf_explained_var: 0.9904248118400574
        vf_loss: 4.001810888449351
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.836666666666666
    gpu_util_percent0: 0.30300000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14761141695110008
    mean_env_wait_ms: 1.2261515816074042
    mean_inference_ms: 4.374659390411527
    mean_raw_obs_processing_ms: 0.382143210640875
  time_since_restore: 1919.1184244155884
  time_this_iter_s: 25.66005563735962
  time_total_s: 1919.1184244155884
  timers:
    learn_throughput: 8648.056
    learn_time_ms: 18708.483
    sample_throughput: 23983.893
    sample_time_ms: 6745.861
    update_time_ms: 35.898
  timestamp: 1602778449
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     75 |          1919.12 | 12134400 |  255.754 |              306.859 |              126.556 |            813.107 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3435.2666355202564
    time_step_min: 3094
  date: 2020-10-15_16-14-35
  done: false
  episode_len_mean: 812.79326635172
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 256.1104464029472
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 177
  episodes_total: 15029
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2823597143093745
        entropy_coeff: 0.0005000000000000001
        kl: 0.00407797460987543
        model: {}
        policy_loss: -0.010367831244366243
        total_loss: 5.212655345598857
        vf_explained_var: 0.9874860644340515
        vf_loss: 5.223164399464925
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.50689655172414
    gpu_util_percent0: 0.34793103448275864
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475947190900503
    mean_env_wait_ms: 1.2261962959456476
    mean_inference_ms: 4.37339774239623
    mean_raw_obs_processing_ms: 0.38209263030240986
  time_since_restore: 1944.5207333564758
  time_this_iter_s: 25.40230894088745
  time_total_s: 1944.5207333564758
  timers:
    learn_throughput: 8650.966
    learn_time_ms: 18702.188
    sample_throughput: 24009.546
    sample_time_ms: 6738.653
    update_time_ms: 37.476
  timestamp: 1602778475
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     76 |          1944.52 | 12296192 |   256.11 |              310.192 |              126.556 |            812.793 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3432.109526938239
    time_step_min: 3094
  date: 2020-10-15_16-15-01
  done: false
  episode_len_mean: 812.3892309707848
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 256.5805070222727
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 237
  episodes_total: 15266
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.2738444482286771
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043979672870288295
        model: {}
        policy_loss: -0.010670721899562826
        total_loss: 5.201486905415853
        vf_explained_var: 0.9896658062934875
        vf_loss: 5.212294499079387
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.40666666666667
    gpu_util_percent0: 0.362
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757548961922182
    mean_env_wait_ms: 1.2262506760334144
    mean_inference_ms: 4.371782510822223
    mean_raw_obs_processing_ms: 0.3820251365410943
  time_since_restore: 1970.0932245254517
  time_this_iter_s: 25.57249116897583
  time_total_s: 1970.0932245254517
  timers:
    learn_throughput: 8660.018
    learn_time_ms: 18682.64
    sample_throughput: 23969.936
    sample_time_ms: 6749.788
    update_time_ms: 39.009
  timestamp: 1602778501
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     77 |          1970.09 | 12457984 |  256.581 |              310.192 |              126.556 |            812.389 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3429.3970454839964
    time_step_min: 3094
  date: 2020-10-15_16-15-27
  done: false
  episode_len_mean: 812.0084625322997
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 257.00416960300646
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 15480
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2587315614024798
        entropy_coeff: 0.0005000000000000001
        kl: 0.00425412164380153
        model: {}
        policy_loss: -0.009416911265968034
        total_loss: 5.0734580755233765
        vf_explained_var: 0.9885712265968323
        vf_loss: 5.083004355430603
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.551724137931036
    gpu_util_percent0: 0.3255172413793104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475555130767403
    mean_env_wait_ms: 1.2263088852975665
    mean_inference_ms: 4.370330051324024
    mean_raw_obs_processing_ms: 0.3819686293588977
  time_since_restore: 1995.6977987289429
  time_this_iter_s: 25.60457420349121
  time_total_s: 1995.6977987289429
  timers:
    learn_throughput: 8663.477
    learn_time_ms: 18675.18
    sample_throughput: 23986.893
    sample_time_ms: 6745.017
    update_time_ms: 39.061
  timestamp: 1602778527
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     78 |           1995.7 | 12619776 |  257.004 |              310.192 |              126.556 |            812.008 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3427.051387198052
    time_step_min: 3094
  date: 2020-10-15_16-15-53
  done: false
  episode_len_mean: 811.671500670798
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 257.35035914630845
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 173
  episodes_total: 15653
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2661385287841161
        entropy_coeff: 0.0005000000000000001
        kl: 0.004082795834013571
        model: {}
        policy_loss: -0.00830440545299401
        total_loss: 4.488071044286092
        vf_explained_var: 0.9884998202323914
        vf_loss: 4.496508518854777
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37666666666667
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754014653265862
    mean_env_wait_ms: 1.2263518579778858
    mean_inference_ms: 4.369185804583504
    mean_raw_obs_processing_ms: 0.381920497371162
  time_since_restore: 2021.0139908790588
  time_this_iter_s: 25.316192150115967
  time_total_s: 2021.0139908790588
  timers:
    learn_throughput: 8688.658
    learn_time_ms: 18621.057
    sample_throughput: 23971.729
    sample_time_ms: 6749.284
    update_time_ms: 38.615
  timestamp: 1602778553
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     79 |          2021.01 | 12781568 |   257.35 |              310.192 |              126.556 |            811.672 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3424.3442633308064
    time_step_min: 3094
  date: 2020-10-15_16-16-19
  done: false
  episode_len_mean: 811.2717021544664
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 257.7709888350555
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 15874
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.262748584151268
        entropy_coeff: 0.0005000000000000001
        kl: 0.004657868994399905
        model: {}
        policy_loss: -0.010715616052038968
        total_loss: 4.881109197934468
        vf_explained_var: 0.9897063374519348
        vf_loss: 4.8919559717178345
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.98965517241379
    gpu_util_percent0: 0.38655172413793104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752311390469142
    mean_env_wait_ms: 1.226395448453163
    mean_inference_ms: 4.367773486051096
    mean_raw_obs_processing_ms: 0.381859939205581
  time_since_restore: 2046.4488010406494
  time_this_iter_s: 25.434810161590576
  time_total_s: 2046.4488010406494
  timers:
    learn_throughput: 8697.481
    learn_time_ms: 18602.167
    sample_throughput: 23978.484
    sample_time_ms: 6747.382
    update_time_ms: 38.359
  timestamp: 1602778579
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     80 |          2046.45 | 12943360 |  257.771 |              310.192 |              126.556 |            811.272 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3421.39697365963
    time_step_min: 3079
  date: 2020-10-15_16-16-45
  done: false
  episode_len_mean: 810.9262340887923
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 258.19645382731346
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 231
  episodes_total: 16105
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.24882828195889792
        entropy_coeff: 0.0005000000000000001
        kl: 0.004502150307719906
        model: {}
        policy_loss: -0.009274985020359358
        total_loss: 5.348036726315816
        vf_explained_var: 0.9888719916343689
        vf_loss: 5.3574360609054565
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.136666666666667
    gpu_util_percent0: 0.3083333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750278591783025
    mean_env_wait_ms: 1.2264476177921195
    mean_inference_ms: 4.3663077423628645
    mean_raw_obs_processing_ms: 0.3818017398481295
  time_since_restore: 2072.0999114513397
  time_this_iter_s: 25.651110410690308
  time_total_s: 2072.0999114513397
  timers:
    learn_throughput: 8687.728
    learn_time_ms: 18623.05
    sample_throughput: 23948.617
    sample_time_ms: 6755.797
    update_time_ms: 37.833
  timestamp: 1602778605
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     81 |           2072.1 | 13105152 |  258.196 |              310.192 |              126.556 |            810.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3419.207257716715
    time_step_min: 3059
  date: 2020-10-15_16-17-10
  done: false
  episode_len_mean: 810.6713767893347
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 258.52902248509514
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 172
  episodes_total: 16277
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.25001314654946327
        entropy_coeff: 0.0005000000000000001
        kl: 0.004377193244484563
        model: {}
        policy_loss: -0.011408900939083347
        total_loss: 3.865146736303965
        vf_explained_var: 0.9900562167167664
        vf_loss: 3.876680632432302
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.953333333333326
    gpu_util_percent0: 0.365
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474886684270876
    mean_env_wait_ms: 1.2264873330326573
    mean_inference_ms: 4.365254271801999
    mean_raw_obs_processing_ms: 0.3817561461422959
  time_since_restore: 2097.547602415085
  time_this_iter_s: 25.447690963745117
  time_total_s: 2097.547602415085
  timers:
    learn_throughput: 8691.419
    learn_time_ms: 18615.142
    sample_throughput: 23909.7
    sample_time_ms: 6766.793
    update_time_ms: 39.328
  timestamp: 1602778630
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     82 |          2097.55 | 13266944 |  258.529 |              312.919 |              126.556 |            810.671 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3416.6888348037724
    time_step_min: 3059
  date: 2020-10-15_16-17-36
  done: false
  episode_len_mean: 810.4195740549724
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 258.90100201088575
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 16481
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.25243983417749405
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041825734078884125
        model: {}
        policy_loss: -0.007913017482981862
        total_loss: 4.611546874046326
        vf_explained_var: 0.9898908734321594
        vf_loss: 4.619586269060771
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.18
    gpu_util_percent0: 0.38066666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747423305896132
    mean_env_wait_ms: 1.2265246018040497
    mean_inference_ms: 4.364038894673646
    mean_raw_obs_processing_ms: 0.3817046722507628
  time_since_restore: 2123.181431055069
  time_this_iter_s: 25.63382863998413
  time_total_s: 2123.181431055069
  timers:
    learn_throughput: 8687.137
    learn_time_ms: 18624.318
    sample_throughput: 23934.495
    sample_time_ms: 6759.783
    update_time_ms: 38.851
  timestamp: 1602778656
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     83 |          2123.18 | 13428736 |  258.901 |              312.919 |              126.556 |             810.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3413.9280830134358
    time_step_min: 3059
  date: 2020-10-15_16-18-02
  done: false
  episode_len_mean: 810.1138294054313
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 259.3166513804146
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 237
  episodes_total: 16718
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2376090536514918
        entropy_coeff: 0.0005000000000000001
        kl: 0.004108809516765177
        model: {}
        policy_loss: -0.007634020197050025
        total_loss: 5.304140488306682
        vf_explained_var: 0.9895066618919373
        vf_loss: 5.3118932247161865
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.32758620689655
    gpu_util_percent0: 0.36413793103448283
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745478153770827
    mean_env_wait_ms: 1.2265711515247102
    mean_inference_ms: 4.362617753755614
    mean_raw_obs_processing_ms: 0.38164652619741857
  time_since_restore: 2148.6201598644257
  time_this_iter_s: 25.43872880935669
  time_total_s: 2148.6201598644257
  timers:
    learn_throughput: 8695.878
    learn_time_ms: 18605.597
    sample_throughput: 23949.605
    sample_time_ms: 6755.519
    update_time_ms: 38.444
  timestamp: 1602778682
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     84 |          2148.62 | 13590528 |  259.317 |              312.919 |              126.556 |            810.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3411.651286917329
    time_step_min: 3059
  date: 2020-10-15_16-18-28
  done: false
  episode_len_mean: 809.8594156612255
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 259.65288799994227
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 16908
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.22809491430719694
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043174341941873235
        model: {}
        policy_loss: -0.009084208014731606
        total_loss: 4.11304239432017
        vf_explained_var: 0.9900930523872375
        vf_loss: 4.122240662574768
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.49655172413794
    gpu_util_percent0: 0.31068965517241376
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743980717372654
    mean_env_wait_ms: 1.2266045703139854
    mean_inference_ms: 4.361520663402273
    mean_raw_obs_processing_ms: 0.38159962464169206
  time_since_restore: 2174.064710378647
  time_this_iter_s: 25.44455051422119
  time_total_s: 2174.064710378647
  timers:
    learn_throughput: 8701.651
    learn_time_ms: 18593.253
    sample_throughput: 23986.986
    sample_time_ms: 6744.991
    update_time_ms: 38.782
  timestamp: 1602778708
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     85 |          2174.06 | 13752320 |  259.653 |              312.919 |              126.556 |            809.859 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3409.515837634913
    time_step_min: 3059
  date: 2020-10-15_16-18-54
  done: false
  episode_len_mean: 809.5757575757576
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 259.98211670938906
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 17094
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.23633441453178725
        entropy_coeff: 0.0005000000000000001
        kl: 0.00409562202791373
        model: {}
        policy_loss: -0.00990123131001989
        total_loss: 4.715577721595764
        vf_explained_var: 0.9888358116149902
        vf_loss: 4.725596904754639
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.213333333333342
    gpu_util_percent0: 0.3993333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742645323599549
    mean_env_wait_ms: 1.2266359673646803
    mean_inference_ms: 4.360454710593854
    mean_raw_obs_processing_ms: 0.3815554118070667
  time_since_restore: 2199.5639460086823
  time_this_iter_s: 25.4992356300354
  time_total_s: 2199.5639460086823
  timers:
    learn_throughput: 8699.525
    learn_time_ms: 18597.797
    sample_throughput: 23972.123
    sample_time_ms: 6749.173
    update_time_ms: 39.241
  timestamp: 1602778734
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     86 |          2199.56 | 13914112 |  259.982 |              312.919 |              126.556 |            809.576 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3406.5946680545917
    time_step_min: 3041
  date: 2020-10-15_16-19-20
  done: false
  episode_len_mean: 809.2171530741723
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 260.430578713656
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 244
  episodes_total: 17338
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.2280901608367761
        entropy_coeff: 0.0005000000000000001
        kl: 0.004126150025210033
        model: {}
        policy_loss: -0.010167474383100247
        total_loss: 4.037740608056386
        vf_explained_var: 0.9916778206825256
        vf_loss: 4.048022071520488
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.675862068965518
    gpu_util_percent0: 0.3662068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740879429179196
    mean_env_wait_ms: 1.226673630623696
    mean_inference_ms: 4.359115133939561
    mean_raw_obs_processing_ms: 0.38149921263539294
  time_since_restore: 2225.02991771698
  time_this_iter_s: 25.46597170829773
  time_total_s: 2225.02991771698
  timers:
    learn_throughput: 8700.911
    learn_time_ms: 18594.836
    sample_throughput: 23994.99
    sample_time_ms: 6742.741
    update_time_ms: 37.351
  timestamp: 1602778760
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     87 |          2225.03 | 14075904 |  260.431 |              315.646 |              126.556 |            809.217 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3404.26346483705
    time_step_min: 3041
  date: 2020-10-15_16-19-46
  done: false
  episode_len_mean: 808.9263800182482
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 260.78137326734463
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 17536
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.21779571970303854
        entropy_coeff: 0.0005000000000000001
        kl: 0.004063838452566415
        model: {}
        policy_loss: -0.00898709589819191
        total_loss: 3.872125267982483
        vf_explained_var: 0.9910173416137695
        vf_loss: 3.881221274534861
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.373333333333335
    gpu_util_percent0: 0.29533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739391485056128
    mean_env_wait_ms: 1.2267074268037117
    mean_inference_ms: 4.358036661292466
    mean_raw_obs_processing_ms: 0.38145438975226464
  time_since_restore: 2250.749566555023
  time_this_iter_s: 25.719648838043213
  time_total_s: 2250.749566555023
  timers:
    learn_throughput: 8701.877
    learn_time_ms: 18592.771
    sample_throughput: 23949.399
    sample_time_ms: 6755.577
    update_time_ms: 37.158
  timestamp: 1602778786
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     88 |          2250.75 | 14237696 |  260.781 |              315.646 |              126.556 |            808.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3402.2683838097932
    time_step_min: 3041
  date: 2020-10-15_16-20-12
  done: false
  episode_len_mean: 808.6676641635142
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 261.09638192095383
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 175
  episodes_total: 17711
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.22601024061441422
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043747064579899115
        model: {}
        policy_loss: -0.009735454455949366
        total_loss: 3.2111634810765586
        vf_explained_var: 0.9917590618133545
        vf_loss: 3.2210118770599365
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.353333333333335
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738117757846633
    mean_env_wait_ms: 1.226733960043803
    mean_inference_ms: 4.357097738493438
    mean_raw_obs_processing_ms: 0.3814129993447764
  time_since_restore: 2276.456791639328
  time_this_iter_s: 25.70722508430481
  time_total_s: 2276.456791639328
  timers:
    learn_throughput: 8689.255
    learn_time_ms: 18619.778
    sample_throughput: 23920.318
    sample_time_ms: 6763.79
    update_time_ms: 38.947
  timestamp: 1602778812
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     89 |          2276.46 | 14399488 |  261.096 |              315.646 |              126.556 |            808.668 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3399.7814351179168
    time_step_min: 3041
  date: 2020-10-15_16-20-38
  done: false
  episode_len_mean: 808.33751393534
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 261.4747812573896
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 229
  episodes_total: 17940
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.2251779946188132
        entropy_coeff: 0.0005000000000000001
        kl: 0.004280702366183202
        model: {}
        policy_loss: -0.009217358068174994
        total_loss: 4.5346901416778564
        vf_explained_var: 0.9908356070518494
        vf_loss: 4.544020175933838
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.990000000000002
    gpu_util_percent0: 0.34833333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473671300118345
    mean_env_wait_ms: 1.2267599551880035
    mean_inference_ms: 4.355917320589147
    mean_raw_obs_processing_ms: 0.3813636438222375
  time_since_restore: 2302.5215022563934
  time_this_iter_s: 26.06471061706543
  time_total_s: 2302.5215022563934
  timers:
    learn_throughput: 8677.151
    learn_time_ms: 18645.751
    sample_throughput: 23795.764
    sample_time_ms: 6799.193
    update_time_ms: 38.541
  timestamp: 1602778838
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     90 |          2302.52 | 14561280 |  261.475 |              315.646 |              126.556 |            808.338 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3397.2853751449234
    time_step_min: 3041
  date: 2020-10-15_16-21-04
  done: false
  episode_len_mean: 808.0633294784955
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 261.849290860029
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 219
  episodes_total: 18159
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.20786967501044273
        entropy_coeff: 0.0005000000000000001
        kl: 0.003977705802147587
        model: {}
        policy_loss: -0.007640881580300629
        total_loss: 3.522599736849467
        vf_explained_var: 0.9922268390655518
        vf_loss: 3.530344545841217
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.29
    gpu_util_percent0: 0.29100000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735129194453347
    mean_env_wait_ms: 1.2267956245063527
    mean_inference_ms: 4.354785657129229
    mean_raw_obs_processing_ms: 0.3813176755384967
  time_since_restore: 2327.9141957759857
  time_this_iter_s: 25.392693519592285
  time_total_s: 2327.9141957759857
  timers:
    learn_throughput: 8691.764
    learn_time_ms: 18614.404
    sample_throughput: 23800.773
    sample_time_ms: 6797.762
    update_time_ms: 37.664
  timestamp: 1602778864
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     91 |          2327.91 | 14723072 |  261.849 |              315.646 |              126.556 |            808.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3395.3088741866695
    time_step_min: 3041
  date: 2020-10-15_16-21-30
  done: false
  episode_len_mean: 807.8764112353423
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 262.13997350103125
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 176
  episodes_total: 18335
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.21612624699870744
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025729528317849
        model: {}
        policy_loss: -0.009867655540195605
        total_loss: 3.189829170703888
        vf_explained_var: 0.99192214012146
        vf_loss: 3.1998048027356467
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.683333333333334
    gpu_util_percent0: 0.3516666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733924165661014
    mean_env_wait_ms: 1.2268165406988016
    mean_inference_ms: 4.3539118923639775
    mean_raw_obs_processing_ms: 0.3812795020556352
  time_since_restore: 2353.826254606247
  time_this_iter_s: 25.91205883026123
  time_total_s: 2353.826254606247
  timers:
    learn_throughput: 8678.038
    learn_time_ms: 18643.845
    sample_throughput: 23775.498
    sample_time_ms: 6804.989
    update_time_ms: 36.168
  timestamp: 1602778890
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     92 |          2353.83 | 14884864 |   262.14 |              315.646 |              126.556 |            807.876 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3393.09152835595
    time_step_min: 3041
  date: 2020-10-15_16-21-57
  done: false
  episode_len_mean: 807.6551798522354
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 262.4695817583696
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 18543
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.2216253305474917
        entropy_coeff: 0.0005000000000000001
        kl: 0.003869050279414902
        model: {}
        policy_loss: -0.008594987156053927
        total_loss: 4.247597336769104
        vf_explained_var: 0.9907740950584412
        vf_loss: 4.256303230921428
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.91666666666667
    gpu_util_percent0: 0.29500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473269068174001
    mean_env_wait_ms: 1.2268332670568085
    mean_inference_ms: 4.352897924440903
    mean_raw_obs_processing_ms: 0.3812364403679977
  time_since_restore: 2379.710329055786
  time_this_iter_s: 25.884074449539185
  time_total_s: 2379.710329055786
  timers:
    learn_throughput: 8681.343
    learn_time_ms: 18636.748
    sample_throughput: 23673.921
    sample_time_ms: 6834.187
    update_time_ms: 37.857
  timestamp: 1602778917
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     93 |          2379.71 | 15046656 |   262.47 |              315.646 |              126.556 |            807.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3390.6616123865456
    time_step_min: 3041
  date: 2020-10-15_16-22-23
  done: false
  episode_len_mean: 807.4059970174691
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 262.8312169414637
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 233
  episodes_total: 18776
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.20370794832706451
        entropy_coeff: 0.0005000000000000001
        kl: 0.004867842265715201
        model: {}
        policy_loss: -0.007855712035961915
        total_loss: 4.6312341292699175
        vf_explained_var: 0.9905586242675781
        vf_loss: 4.639191627502441
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.403448275862072
    gpu_util_percent0: 0.3196551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731118894972448
    mean_env_wait_ms: 1.2268606752448685
    mean_inference_ms: 4.351777528101371
    mean_raw_obs_processing_ms: 0.38118978851844193
  time_since_restore: 2405.3833990097046
  time_this_iter_s: 25.673069953918457
  time_total_s: 2405.3833990097046
  timers:
    learn_throughput: 8678.172
    learn_time_ms: 18643.558
    sample_throughput: 23616.243
    sample_time_ms: 6850.878
    update_time_ms: 36.467
  timestamp: 1602778943
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     94 |          2405.38 | 15208448 |  262.831 |              315.646 |              126.556 |            807.406 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3388.7727320786635
    time_step_min: 3041
  date: 2020-10-15_16-22-48
  done: false
  episode_len_mean: 807.2020883873009
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 263.1109710116667
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 18962
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.20504934092362723
        entropy_coeff: 0.0005000000000000001
        kl: 0.004358735672819118
        model: {}
        policy_loss: -0.007940617130467823
        total_loss: 4.1041334470113116
        vf_explained_var: 0.9900287985801697
        vf_loss: 4.1121765573819475
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.87333333333333
    gpu_util_percent0: 0.2813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472995553453716
    mean_env_wait_ms: 1.2268822817799043
    mean_inference_ms: 4.35090691080921
    mean_raw_obs_processing_ms: 0.38115292046630683
  time_since_restore: 2430.896911382675
  time_this_iter_s: 25.51351237297058
  time_total_s: 2430.896911382675
  timers:
    learn_throughput: 8684.179
    learn_time_ms: 18630.662
    sample_throughput: 23544.721
    sample_time_ms: 6871.689
    update_time_ms: 34.442
  timestamp: 1602778968
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     95 |           2430.9 | 15370240 |  263.111 |              315.646 |              126.556 |            807.202 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3386.904799288219
    time_step_min: 3041
  date: 2020-10-15_16-23-14
  done: false
  episode_len_mean: 806.9766616195897
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 263.4010216507472
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 19153
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.21467343469460806
        entropy_coeff: 0.0005000000000000001
        kl: 0.003984760240806888
        model: {}
        policy_loss: -0.009691227329312824
        total_loss: 4.16459047794342
        vf_explained_var: 0.9902827143669128
        vf_loss: 4.174388984839122
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.360000000000007
    gpu_util_percent0: 0.3223333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472889543984427
    mean_env_wait_ms: 1.22689634960794
    mean_inference_ms: 4.350027439143553
    mean_raw_obs_processing_ms: 0.3811150024895136
  time_since_restore: 2456.524899959564
  time_this_iter_s: 25.627988576889038
  time_total_s: 2456.524899959564
  timers:
    learn_throughput: 8686.699
    learn_time_ms: 18625.256
    sample_throughput: 23479.01
    sample_time_ms: 6890.921
    update_time_ms: 32.753
  timestamp: 1602778994
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     96 |          2456.52 | 15532032 |  263.401 |              315.646 |              126.556 |            806.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3384.5454498475533
    time_step_min: 3041
  date: 2020-10-15_16-23-40
  done: false
  episode_len_mean: 806.6928906531938
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 263.7569425241742
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 244
  episodes_total: 19397
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.2060975357890129
        entropy_coeff: 0.0005000000000000001
        kl: 0.004208495491184294
        model: {}
        policy_loss: -0.009226624349442622
        total_loss: 5.099639892578125
        vf_explained_var: 0.9899895191192627
        vf_loss: 5.108969648679097
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.48666666666667
    gpu_util_percent0: 0.3410000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727394967968
    mean_env_wait_ms: 1.226918347057688
    mean_inference_ms: 4.348939333168665
    mean_raw_obs_processing_ms: 0.3810693550279487
  time_since_restore: 2482.097172021866
  time_this_iter_s: 25.572272062301636
  time_total_s: 2482.097172021866
  timers:
    learn_throughput: 8692.809
    learn_time_ms: 18612.166
    sample_throughput: 23402.76
    sample_time_ms: 6913.373
    update_time_ms: 33.231
  timestamp: 1602779020
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     97 |           2482.1 | 15693824 |  263.757 |              315.646 |              126.556 |            806.693 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3382.5015093374263
    time_step_min: 3041
  date: 2020-10-15_16-24-07
  done: false
  episode_len_mean: 806.4680210300648
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.04889278678223
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 19591
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.19606210912267366
        entropy_coeff: 0.0005000000000000001
        kl: 0.004457082715816796
        model: {}
        policy_loss: -0.009334245774274072
        total_loss: 3.3258522351582847
        vf_explained_var: 0.9921269416809082
        vf_loss: 3.3352845112482705
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.516666666666666
    gpu_util_percent0: 0.3213333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726215758624986
    mean_env_wait_ms: 1.2269372187426935
    mean_inference_ms: 4.348087972563767
    mean_raw_obs_processing_ms: 0.38103289953990244
  time_since_restore: 2508.1302325725555
  time_this_iter_s: 26.033060550689697
  time_total_s: 2508.1302325725555
  timers:
    learn_throughput: 8684.84
    learn_time_ms: 18629.243
    sample_throughput: 23358.348
    sample_time_ms: 6926.517
    update_time_ms: 33.336
  timestamp: 1602779047
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     98 |          2508.13 | 15855616 |  264.049 |              315.646 |              126.556 |            806.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3380.7767413565853
    time_step_min: 3041
  date: 2020-10-15_16-24-33
  done: false
  episode_len_mean: 806.2371029739024
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.3132968364605
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 181
  episodes_total: 19772
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.20596252754330635
        entropy_coeff: 0.0005000000000000001
        kl: 0.004483936936594546
        model: {}
        policy_loss: -0.010501946392954173
        total_loss: 3.6415778398513794
        vf_explained_var: 0.9911496043205261
        vf_loss: 3.6521827578544617
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84333333333333
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725209518134263
    mean_env_wait_ms: 1.2269491277224684
    mean_inference_ms: 4.347296415771641
    mean_raw_obs_processing_ms: 0.38099834813198896
  time_since_restore: 2533.7023260593414
  time_this_iter_s: 25.57209348678589
  time_total_s: 2533.7023260593414
  timers:
    learn_throughput: 8684.766
    learn_time_ms: 18629.402
    sample_throughput: 23395.389
    sample_time_ms: 6915.551
    update_time_ms: 31.578
  timestamp: 1602779073
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |     99 |           2533.7 | 16017408 |  264.313 |              315.646 |              126.556 |            806.237 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3378.5574591724276
    time_step_min: 3041
  date: 2020-10-15_16-24-58
  done: false
  episode_len_mean: 805.9456217512994
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.6598441431505
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 236
  episodes_total: 20008
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.19885142520070076
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046679679459581775
        model: {}
        policy_loss: -0.008300370262683524
        total_loss: 4.468468944231669
        vf_explained_var: 0.9907791614532471
        vf_loss: 4.476868947347005
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.5896551724138
    gpu_util_percent0: 0.3479310344827587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472397414624749
    mean_env_wait_ms: 1.226967663683929
    mean_inference_ms: 4.346313373915764
    mean_raw_obs_processing_ms: 0.3809560775669489
  time_since_restore: 2558.9581475257874
  time_this_iter_s: 25.255821466445923
  time_total_s: 2558.9581475257874
  timers:
    learn_throughput: 8702.351
    learn_time_ms: 18591.757
    sample_throughput: 23537.846
    sample_time_ms: 6873.696
    update_time_ms: 30.187
  timestamp: 1602779098
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    100 |          2558.96 | 16179200 |   264.66 |              315.646 |              126.556 |            805.946 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3376.479304020225
    time_step_min: 3041
  date: 2020-10-15_16-25-25
  done: false
  episode_len_mean: 805.6904891438746
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.96498742806637
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 20219
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.1829805995027224
        entropy_coeff: 0.0005000000000000001
        kl: 0.004511198436375707
        model: {}
        policy_loss: -0.010293429057734707
        total_loss: 3.4685230056444802
        vf_explained_var: 0.9919838309288025
        vf_loss: 3.478907903035482
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.280000000000005
    gpu_util_percent0: 0.2926666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472272501731871
    mean_env_wait_ms: 1.2269824964250973
    mean_inference_ms: 4.34542031256042
    mean_raw_obs_processing_ms: 0.38091940048104417
  time_since_restore: 2584.719925880432
  time_this_iter_s: 25.761778354644775
  time_total_s: 2584.719925880432
  timers:
    learn_throughput: 8690.205
    learn_time_ms: 18617.742
    sample_throughput: 23482.819
    sample_time_ms: 6889.803
    update_time_ms: 31.705
  timestamp: 1602779125
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    101 |          2584.72 | 16340992 |  264.965 |              315.646 |              126.556 |             805.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3374.8642060254583
    time_step_min: 3041
  date: 2020-10-15_16-25-51
  done: false
  episode_len_mean: 805.4894816848919
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 265.2111325583592
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 174
  episodes_total: 20393
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.19006827473640442
        entropy_coeff: 0.0005000000000000001
        kl: 0.004485640830049912
        model: {}
        policy_loss: -0.008435653697233647
        total_loss: 3.467149575551351
        vf_explained_var: 0.9913205504417419
        vf_loss: 3.475680331389109
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57333333333333
    gpu_util_percent0: 0.39
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721723230896563
    mean_env_wait_ms: 1.2269962480618164
    mean_inference_ms: 4.344710105070459
    mean_raw_obs_processing_ms: 0.38088653680893464
  time_since_restore: 2610.398669719696
  time_this_iter_s: 25.678743839263916
  time_total_s: 2610.398669719696
  timers:
    learn_throughput: 8687.505
    learn_time_ms: 18623.528
    sample_throughput: 23552.0
    sample_time_ms: 6869.565
    update_time_ms: 32.301
  timestamp: 1602779151
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    102 |           2610.4 | 16502784 |  265.211 |              315.646 |              126.556 |            805.489 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3372.810063198833
    time_step_min: 3041
  date: 2020-10-15_16-26-16
  done: false
  episode_len_mean: 805.2298214978657
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 265.5199962371088
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 20616
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.19109124317765236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004055134175966184
        model: {}
        policy_loss: -0.00795338962537547
        total_loss: 4.456182678540547
        vf_explained_var: 0.9906907677650452
        vf_loss: 4.464231689771016
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.358620689655172
    gpu_util_percent0: 0.383448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472065766021256
    mean_env_wait_ms: 1.2270058034248696
    mean_inference_ms: 4.343815004661143
    mean_raw_obs_processing_ms: 0.3808478161093101
  time_since_restore: 2635.694925069809
  time_this_iter_s: 25.296255350112915
  time_total_s: 2635.694925069809
  timers:
    learn_throughput: 8702.199
    learn_time_ms: 18592.082
    sample_throughput: 23640.442
    sample_time_ms: 6843.865
    update_time_ms: 29.571
  timestamp: 1602779176
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    103 |          2635.69 | 16664576 |   265.52 |              315.646 |              126.556 |             805.23 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3370.64876430426
    time_step_min: 3033
  date: 2020-10-15_16-26-43
  done: false
  episode_len_mean: 804.9702552293226
  episode_reward_max: 316.8585858585855
  episode_reward_mean: 265.83654187237914
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 228
  episodes_total: 20844
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.17321047807733217
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038343543031563363
        model: {}
        policy_loss: -0.008931881204868356
        total_loss: 4.151850720246633
        vf_explained_var: 0.991041362285614
        vf_loss: 4.1608690818150835
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.926666666666673
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719388693526586
    mean_env_wait_ms: 1.2270208705788292
    mean_inference_ms: 4.342904645460568
    mean_raw_obs_processing_ms: 0.3808109926112282
  time_since_restore: 2661.4636828899384
  time_this_iter_s: 25.768757820129395
  time_total_s: 2661.4636828899384
  timers:
    learn_throughput: 8690.154
    learn_time_ms: 18617.852
    sample_throughput: 23702.642
    sample_time_ms: 6825.906
    update_time_ms: 30.04
  timestamp: 1602779203
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    104 |          2661.46 | 16826368 |  265.837 |              316.859 |              126.556 |             804.97 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3368.9494588280168
    time_step_min: 3033
  date: 2020-10-15_16-27-08
  done: false
  episode_len_mean: 804.738141681336
  episode_reward_max: 316.8585858585855
  episode_reward_mean: 266.08907284943217
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 175
  episodes_total: 21019
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.17105705539385477
        entropy_coeff: 0.0005000000000000001
        kl: 0.004058195103425533
        model: {}
        policy_loss: -0.009294179457356222
        total_loss: 3.1212044954299927
        vf_explained_var: 0.9918803572654724
        vf_loss: 3.130584160486857
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.31379310344828
    gpu_util_percent0: 0.27551724137931033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471846761576861
    mean_env_wait_ms: 1.2270321369799482
    mean_inference_ms: 4.342223926547346
    mean_raw_obs_processing_ms: 0.38078051099615107
  time_since_restore: 2686.942435026169
  time_this_iter_s: 25.47875213623047
  time_total_s: 2686.942435026169
  timers:
    learn_throughput: 8683.058
    learn_time_ms: 18633.067
    sample_throughput: 23779.137
    sample_time_ms: 6803.948
    update_time_ms: 32.265
  timestamp: 1602779228
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    105 |          2686.94 | 16988160 |  266.089 |              316.859 |              126.556 |            804.738 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3366.8982818842633
    time_step_min: 3029
  date: 2020-10-15_16-27-35
  done: false
  episode_len_mean: 804.4678786737
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 266.3871919077736
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 213
  episodes_total: 21232
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17971546202898026
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041747875123595195
        model: {}
        policy_loss: -0.008395435870625079
        total_loss: 4.011339048544566
        vf_explained_var: 0.9910714626312256
        vf_loss: 4.019824266433716
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.14666666666667
    gpu_util_percent0: 0.3936666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717435902392303
    mean_env_wait_ms: 1.2270372296405645
    mean_inference_ms: 4.341409181709857
    mean_raw_obs_processing_ms: 0.3807439728868251
  time_since_restore: 2712.8156225681305
  time_this_iter_s: 25.87318754196167
  time_total_s: 2712.8156225681305
  timers:
    learn_throughput: 8673.919
    learn_time_ms: 18652.698
    sample_throughput: 23769.607
    sample_time_ms: 6806.675
    update_time_ms: 33.309
  timestamp: 1602779255
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    106 |          2712.82 | 17149952 |  266.387 |              317.465 |              126.556 |            804.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3364.65216376453
    time_step_min: 3029
  date: 2020-10-15_16-28-01
  done: false
  episode_len_mean: 804.152792658499
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 266.72214670109076
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 235
  episodes_total: 21467
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.16006832818190256
        entropy_coeff: 0.0005000000000000001
        kl: 0.003974445261216412
        model: {}
        policy_loss: -0.007680329455373188
        total_loss: 3.5257490475972495
        vf_explained_var: 0.9923901557922363
        vf_loss: 3.5335094928741455
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.216666666666665
    gpu_util_percent0: 0.40166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716192132363615
    mean_env_wait_ms: 1.2270505402288427
    mean_inference_ms: 4.3405221021449005
    mean_raw_obs_processing_ms: 0.38070773285933457
  time_since_restore: 2738.395313024521
  time_this_iter_s: 25.57969045639038
  time_total_s: 2738.395313024521
  timers:
    learn_throughput: 8663.633
    learn_time_ms: 18674.844
    sample_throughput: 23849.791
    sample_time_ms: 6783.791
    update_time_ms: 33.348
  timestamp: 1602779281
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    107 |           2738.4 | 17311744 |  266.722 |              317.465 |              126.556 |            804.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3362.985605183985
    time_step_min: 3029
  date: 2020-10-15_16-28-27
  done: false
  episode_len_mean: 803.8867488799593
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 266.9693232729116
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 184
  episodes_total: 21651
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.15804871171712875
        entropy_coeff: 0.0005000000000000001
        kl: 0.003186442811662952
        model: {}
        policy_loss: -0.010185310539479056
        total_loss: 2.83365664879481
        vf_explained_var: 0.9927825927734375
        vf_loss: 2.843920946121216
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333338
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715316222513186
    mean_env_wait_ms: 1.2270637990948738
    mean_inference_ms: 4.339843799906542
    mean_raw_obs_processing_ms: 0.3806774448844124
  time_since_restore: 2764.035732984543
  time_this_iter_s: 25.640419960021973
  time_total_s: 2764.035732984543
  timers:
    learn_throughput: 8671.353
    learn_time_ms: 18658.219
    sample_throughput: 23935.017
    sample_time_ms: 6759.636
    update_time_ms: 33.257
  timestamp: 1602779307
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    108 |          2764.04 | 17473536 |  266.969 |              317.465 |              126.556 |            803.887 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3361.1240772158285
    time_step_min: 3029
  date: 2020-10-15_16-28-53
  done: false
  episode_len_mean: 803.5962479981697
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 267.25517356128165
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 21855
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16283127665519714
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035293440256888666
        model: {}
        policy_loss: -0.009930140882109603
        total_loss: 3.3155870834986367
        vf_explained_var: 0.992332398891449
        vf_loss: 3.32559867699941
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.023333333333333
    gpu_util_percent0: 0.36433333333333345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471434708013914
    mean_env_wait_ms: 1.2270675356622516
    mean_inference_ms: 4.339100409587663
    mean_raw_obs_processing_ms: 0.38064328556029686
  time_since_restore: 2789.8074526786804
  time_this_iter_s: 25.771719694137573
  time_total_s: 2789.8074526786804
  timers:
    learn_throughput: 8674.917
    learn_time_ms: 18650.554
    sample_throughput: 23850.19
    sample_time_ms: 6783.678
    update_time_ms: 35.239
  timestamp: 1602779333
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    109 |          2789.81 | 17635328 |  267.255 |              317.465 |              126.556 |            803.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3358.919040275762
    time_step_min: 3029
  date: 2020-10-15_16-29-19
  done: false
  episode_len_mean: 803.2683986602698
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 267.59086337256844
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 239
  episodes_total: 22094
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1533804809053739
        entropy_coeff: 0.0005000000000000001
        kl: 0.003702786828701695
        model: {}
        policy_loss: -0.00756536432163557
        total_loss: 3.3041084011395774
        vf_explained_var: 0.9929428696632385
        vf_loss: 3.3117505510648093
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85333333333334
    gpu_util_percent0: 0.356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713203062379157
    mean_env_wait_ms: 1.2270811473340815
    mean_inference_ms: 4.338244713387916
    mean_raw_obs_processing_ms: 0.3806079654965666
  time_since_restore: 2815.349534034729
  time_this_iter_s: 25.542081356048584
  time_total_s: 2815.349534034729
  timers:
    learn_throughput: 8667.918
    learn_time_ms: 18665.612
    sample_throughput: 23834.982
    sample_time_ms: 6788.006
    update_time_ms: 35.514
  timestamp: 1602779359
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    110 |          2815.35 | 17797120 |  267.591 |              317.465 |              126.556 |            803.268 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3357.2335297027475
    time_step_min: 3029
  date: 2020-10-15_16-29-45
  done: false
  episode_len_mean: 803.0206435399183
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 267.8546842567393
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 22283
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1497357413172722
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038113862392492592
        model: {}
        policy_loss: -0.008905402617529035
        total_loss: 2.79697593053182
        vf_explained_var: 0.992814302444458
        vf_loss: 2.805956264336904
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.55517241379311
    gpu_util_percent0: 0.32758620689655177
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712314016937197
    mean_env_wait_ms: 1.2270925760517675
    mean_inference_ms: 4.3375861498265476
    mean_raw_obs_processing_ms: 0.38057869700816993
  time_since_restore: 2840.7017843723297
  time_this_iter_s: 25.352250337600708
  time_total_s: 2840.7017843723297
  timers:
    learn_throughput: 8679.626
    learn_time_ms: 18640.434
    sample_throughput: 23886.82
    sample_time_ms: 6773.275
    update_time_ms: 33.461
  timestamp: 1602779385
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    111 |           2840.7 | 17958912 |  267.855 |              317.465 |              126.556 |            803.021 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3355.3479675521485
    time_step_min: 3029
  date: 2020-10-15_16-30-11
  done: false
  episode_len_mean: 802.7871185837558
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 268.1298048539838
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 22482
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.15533562252918878
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034227332022661963
        model: {}
        policy_loss: -0.008599819440860301
        total_loss: 2.7747987310091653
        vf_explained_var: 0.9934248924255371
        vf_loss: 2.783476233482361
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.446666666666665
    gpu_util_percent0: 0.38833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711445467597314
    mean_env_wait_ms: 1.2270941124813692
    mean_inference_ms: 4.336894509421614
    mean_raw_obs_processing_ms: 0.3805473196988501
  time_since_restore: 2866.1768267154694
  time_this_iter_s: 25.47504234313965
  time_total_s: 2866.1768267154694
  timers:
    learn_throughput: 8687.194
    learn_time_ms: 18624.195
    sample_throughput: 23901.159
    sample_time_ms: 6769.212
    update_time_ms: 31.505
  timestamp: 1602779411
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    112 |          2866.18 | 18120704 |   268.13 |              317.465 |              126.556 |            802.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3353.2744354269585
    time_step_min: 3029
  date: 2020-10-15_16-30-37
  done: false
  episode_len_mean: 802.5259265780438
  episode_reward_max: 317.7676767676764
  episode_reward_mean: 268.44471299845856
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 236
  episodes_total: 22718
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.14903269583980241
        entropy_coeff: 0.0005000000000000001
        kl: 0.003485022229142487
        model: {}
        policy_loss: -0.009994654499071961
        total_loss: 3.5449649492899575
        vf_explained_var: 0.9928091168403625
        vf_loss: 3.555034101009369
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.183333333333334
    gpu_util_percent0: 0.302
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710351574447839
    mean_env_wait_ms: 1.2271060420288382
    mean_inference_ms: 4.336092464496262
    mean_raw_obs_processing_ms: 0.380513949878996
  time_since_restore: 2891.6231360435486
  time_this_iter_s: 25.446309328079224
  time_total_s: 2891.6231360435486
  timers:
    learn_throughput: 8680.635
    learn_time_ms: 18638.269
    sample_throughput: 23899.889
    sample_time_ms: 6769.571
    update_time_ms: 31.169
  timestamp: 1602779437
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    113 |          2891.62 | 18282496 |  268.445 |              317.768 |              126.556 |            802.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3351.645337648705
    time_step_min: 3029
  date: 2020-10-15_16-31-03
  done: false
  episode_len_mean: 802.3264076822348
  episode_reward_max: 317.7676767676764
  episode_reward_mean: 268.6938040377583
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 22910
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.1389683187007904
        entropy_coeff: 0.0005000000000000001
        kl: 0.003841286369909843
        model: {}
        policy_loss: -0.008985845527301231
        total_loss: 2.876243789990743
        vf_explained_var: 0.9931276440620422
        vf_loss: 2.8852990667025247
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.613793103448277
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470948348889742
    mean_env_wait_ms: 1.2271139590653364
    mean_inference_ms: 4.335453433304586
    mean_raw_obs_processing_ms: 0.3804854209792388
  time_since_restore: 2917.1889595985413
  time_this_iter_s: 25.565823554992676
  time_total_s: 2917.1889595985413
  timers:
    learn_throughput: 8694.48
    learn_time_ms: 18608.587
    sample_throughput: 23873.18
    sample_time_ms: 6777.145
    update_time_ms: 32.995
  timestamp: 1602779463
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    114 |          2917.19 | 18444288 |  268.694 |              317.768 |              126.556 |            802.326 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3349.990501799887
    time_step_min: 3029
  date: 2020-10-15_16-31-29
  done: false
  episode_len_mean: 802.1040124659136
  episode_reward_max: 317.9191919191917
  episode_reward_mean: 268.9509876062268
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 23103
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.1504321781297525
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044296266278252006
        model: {}
        policy_loss: -0.008044386690016836
        total_loss: 2.621249715487162
        vf_explained_var: 0.9937052726745605
        vf_loss: 2.6293693582216897
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.830000000000002
    gpu_util_percent0: 0.3516666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470870897406186
    mean_env_wait_ms: 1.2271127993502302
    mean_inference_ms: 4.334814267456325
    mean_raw_obs_processing_ms: 0.3804562894268493
  time_since_restore: 2942.7011041641235
  time_this_iter_s: 25.512144565582275
  time_total_s: 2942.7011041641235
  timers:
    learn_throughput: 8693.607
    learn_time_ms: 18610.457
    sample_throughput: 23865.069
    sample_time_ms: 6779.448
    update_time_ms: 30.939
  timestamp: 1602779489
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    115 |           2942.7 | 18606080 |  268.951 |              317.919 |              126.556 |            802.104 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3347.8674507447313
    time_step_min: 3029
  date: 2020-10-15_16-31-55
  done: false
  episode_len_mean: 801.8417941138671
  episode_reward_max: 317.9191919191917
  episode_reward_mean: 269.2704438031513
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 240
  episodes_total: 23343
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.14614903181791306
        entropy_coeff: 0.0005000000000000001
        kl: 0.003426988007655988
        model: {}
        policy_loss: -0.008290602791627558
        total_loss: 3.1870890259742737
        vf_explained_var: 0.9931678175926208
        vf_loss: 3.1954526702562966
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.926666666666666
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470764513312236
    mean_env_wait_ms: 1.2271203314815087
    mean_inference_ms: 4.334039435075982
    mean_raw_obs_processing_ms: 0.38042239996765276
  time_since_restore: 2968.3717341423035
  time_this_iter_s: 25.67062997817993
  time_total_s: 2968.3717341423035
  timers:
    learn_throughput: 8690.054
    learn_time_ms: 18618.066
    sample_throughput: 23971.798
    sample_time_ms: 6749.264
    update_time_ms: 32.201
  timestamp: 1602779515
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    116 |          2968.37 | 18767872 |   269.27 |              317.919 |              126.556 |            801.842 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3346.197914449883
    time_step_min: 3019
  date: 2020-10-15_16-32-21
  done: false
  episode_len_mean: 801.6325559661866
  episode_reward_max: 318.97979797979735
  episode_reward_mean: 269.5193015924502
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 23541
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.13649629428982735
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034725334068449834
        model: {}
        policy_loss: -0.00788314324139113
        total_loss: 2.9185951153437295
        vf_explained_var: 0.9930727481842041
        vf_loss: 2.9265465140342712
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.406896551724138
    gpu_util_percent0: 0.313448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706751944043533
    mean_env_wait_ms: 1.2271269405980683
    mean_inference_ms: 4.33341046624744
    mean_raw_obs_processing_ms: 0.3803948263487166
  time_since_restore: 2993.780005455017
  time_this_iter_s: 25.408271312713623
  time_total_s: 2993.780005455017
  timers:
    learn_throughput: 8696.752
    learn_time_ms: 18603.728
    sample_throughput: 23985.6
    sample_time_ms: 6745.381
    update_time_ms: 31.734
  timestamp: 1602779541
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    117 |          2993.78 | 18929664 |  269.519 |               318.98 |              126.556 |            801.633 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3344.594326480645
    time_step_min: 3019
  date: 2020-10-15_16-32-47
  done: false
  episode_len_mean: 801.403539077312
  episode_reward_max: 318.97979797979735
  episode_reward_mean: 269.763074179758
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 23735
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.14318112283945084
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033448990046357117
        model: {}
        policy_loss: -0.007795179767223696
        total_loss: 3.4445244471232095
        vf_explained_var: 0.9916593432426453
        vf_loss: 3.45239120721817
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.416666666666668
    gpu_util_percent0: 0.295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706041096897457
    mean_env_wait_ms: 1.2271248399935712
    mean_inference_ms: 4.332794062341136
    mean_raw_obs_processing_ms: 0.3803665067461479
  time_since_restore: 3019.5026140213013
  time_this_iter_s: 25.72260856628418
  time_total_s: 3019.5026140213013
  timers:
    learn_throughput: 8693.725
    learn_time_ms: 18610.205
    sample_throughput: 23981.64
    sample_time_ms: 6746.495
    update_time_ms: 32.109
  timestamp: 1602779567
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    118 |           3019.5 | 19091456 |  269.763 |               318.98 |              126.556 |            801.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3342.6826923076924
    time_step_min: 3019
  date: 2020-10-15_16-33-13
  done: false
  episode_len_mean: 801.1335224901944
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.05587418877036
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 231
  episodes_total: 23966
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.14044669270515442
        entropy_coeff: 0.0005000000000000001
        kl: 0.004190241316488634
        model: {}
        policy_loss: -0.0066900534826951725
        total_loss: 3.2881863514582315
        vf_explained_var: 0.9929413199424744
        vf_loss: 3.2949467102686563
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.030000000000005
    gpu_util_percent0: 0.31700000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470507081701091
    mean_env_wait_ms: 1.2271289019949565
    mean_inference_ms: 4.3320883119938705
    mean_raw_obs_processing_ms: 0.38033588555914705
  time_since_restore: 3045.1857681274414
  time_this_iter_s: 25.683154106140137
  time_total_s: 3045.1857681274414
  timers:
    learn_throughput: 8685.77
    learn_time_ms: 18627.249
    sample_throughput: 24072.5
    sample_time_ms: 6721.03
    update_time_ms: 31.606
  timestamp: 1602779593
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    119 |          3045.19 | 19253248 |  270.056 |               318.98 |              126.556 |            801.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3340.914697836359
    time_step_min: 3019
  date: 2020-10-15_16-33-39
  done: false
  episode_len_mean: 800.9029041866622
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.3153026207797
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 24172
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.13026844213406244
        entropy_coeff: 0.0005000000000000001
        kl: 0.004086127446498722
        model: {}
        policy_loss: -0.00882166401182379
        total_loss: 2.5615656773249307
        vf_explained_var: 0.9938321113586426
        vf_loss: 2.5704525113105774
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.493333333333336
    gpu_util_percent0: 0.41300000000000014
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704162987657407
    mean_env_wait_ms: 1.227131530954435
    mean_inference_ms: 4.331458336783855
    mean_raw_obs_processing_ms: 0.3803078005277857
  time_since_restore: 3070.516714334488
  time_this_iter_s: 25.33094620704651
  time_total_s: 3070.516714334488
  timers:
    learn_throughput: 8692.473
    learn_time_ms: 18612.885
    sample_throughput: 24070.913
    sample_time_ms: 6721.473
    update_time_ms: 31.538
  timestamp: 1602779619
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    120 |          3070.52 | 19415040 |  270.315 |               318.98 |              126.556 |            800.903 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3339.3204474420136
    time_step_min: 3019
  date: 2020-10-15_16-34-05
  done: false
  episode_len_mean: 800.6956325424842
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.5538091696039
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 24362
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.1346232071518898
        entropy_coeff: 0.0005000000000000001
        kl: 0.003765755915082991
        model: {}
        policy_loss: -0.006242665423390766
        total_loss: 2.6872461438179016
        vf_explained_var: 0.9934821128845215
        vf_loss: 2.6935561696688333
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78620689655172
    gpu_util_percent0: 0.31862068965517243
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703453129268448
    mean_env_wait_ms: 1.2271281771841158
    mean_inference_ms: 4.3308815144580075
    mean_raw_obs_processing_ms: 0.380281364246919
  time_since_restore: 3096.225128889084
  time_this_iter_s: 25.708414554595947
  time_total_s: 3096.225128889084
  timers:
    learn_throughput: 8673.61
    learn_time_ms: 18653.363
    sample_throughput: 24089.974
    sample_time_ms: 6716.155
    update_time_ms: 32.052
  timestamp: 1602779645
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    121 |          3096.23 | 19576832 |  270.554 |               318.98 |              126.556 |            800.696 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3337.4372835784416
    time_step_min: 3019
  date: 2020-10-15_16-34-31
  done: false
  episode_len_mean: 800.4400845769121
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.83929483095875
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 231
  episodes_total: 24593
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.1360724059243997
        entropy_coeff: 0.0005000000000000001
        kl: 0.004647130689894159
        model: {}
        policy_loss: -0.007822828213344716
        total_loss: 2.812016487121582
        vf_explained_var: 0.9938087463378906
        vf_loss: 2.819907327493032
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.943333333333335
    gpu_util_percent0: 0.3063333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702533032082063
    mean_env_wait_ms: 1.2271297169364623
    mean_inference_ms: 4.330208313701886
    mean_raw_obs_processing_ms: 0.3802517274744264
  time_since_restore: 3121.8594632148743
  time_this_iter_s: 25.634334325790405
  time_total_s: 3121.8594632148743
  timers:
    learn_throughput: 8669.359
    learn_time_ms: 18662.509
    sample_throughput: 24071.322
    sample_time_ms: 6721.359
    update_time_ms: 33.78
  timestamp: 1602779671
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    122 |          3121.86 | 19738624 |  270.839 |               318.98 |              126.556 |             800.44 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3335.7258038455325
    time_step_min: 3019
  date: 2020-10-15_16-34-57
  done: false
  episode_len_mean: 800.2232481251511
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 271.0951210353674
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 209
  episodes_total: 24802
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.12468166587253411
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032854301195281246
        model: {}
        policy_loss: -0.009902211371809244
        total_loss: 2.6866191029548645
        vf_explained_var: 0.9937338829040527
        vf_loss: 2.69658362865448
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05666666666667
    gpu_util_percent0: 0.32300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470169627029193
    mean_env_wait_ms: 1.2271306001307825
    mean_inference_ms: 4.329597564733569
    mean_raw_obs_processing_ms: 0.3802245917876097
  time_since_restore: 3147.5082008838654
  time_this_iter_s: 25.64873766899109
  time_total_s: 3147.5082008838654
  timers:
    learn_throughput: 8662.963
    learn_time_ms: 18676.289
    sample_throughput: 24057.818
    sample_time_ms: 6725.132
    update_time_ms: 35.756
  timestamp: 1602779697
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    123 |          3147.51 | 19900416 |  271.095 |               318.98 |              126.556 |            800.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3334.2120920535644
    time_step_min: 3019
  date: 2020-10-15_16-35-24
  done: false
  episode_len_mean: 800.0065231311029
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 271.32087927457667
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 24988
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.1282355785369873
        entropy_coeff: 0.0005000000000000001
        kl: 0.003834822273347527
        model: {}
        policy_loss: -0.0064158480187567575
        total_loss: 2.2745142579078674
        vf_explained_var: 0.9942635893821716
        vf_loss: 2.280994196732839
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.826666666666664
    gpu_util_percent0: 0.3166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700988440271595
    mean_env_wait_ms: 1.2271287091134562
    mean_inference_ms: 4.3290633056556995
    mean_raw_obs_processing_ms: 0.38019941743722135
  time_since_restore: 3173.1973598003387
  time_this_iter_s: 25.68915891647339
  time_total_s: 3173.1973598003387
  timers:
    learn_throughput: 8657.456
    learn_time_ms: 18688.168
    sample_throughput: 24065.008
    sample_time_ms: 6723.123
    update_time_ms: 36.846
  timestamp: 1602779724
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    124 |           3173.2 | 20062208 |  271.321 |               318.98 |              126.556 |            800.007 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3332.377910210568
    time_step_min: 3006
  date: 2020-10-15_16-35-50
  done: false
  episode_len_mean: 799.7603505710659
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 271.5929776829202
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 228
  episodes_total: 25216
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.1308365762233734
        entropy_coeff: 0.0005000000000000001
        kl: 0.003845173962569485
        model: {}
        policy_loss: -0.00844521870991836
        total_loss: 2.3771212697029114
        vf_explained_var: 0.994819700717926
        vf_loss: 2.3856319387753806
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.603333333333335
    gpu_util_percent0: 0.30066666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700182047113353
    mean_env_wait_ms: 1.2271258712279074
    mean_inference_ms: 4.32842603919261
    mean_raw_obs_processing_ms: 0.38017190260241734
  time_since_restore: 3199.3072118759155
  time_this_iter_s: 26.109852075576782
  time_total_s: 3199.3072118759155
  timers:
    learn_throughput: 8636.583
    learn_time_ms: 18733.334
    sample_throughput: 24025.632
    sample_time_ms: 6734.141
    update_time_ms: 38.926
  timestamp: 1602779750
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    125 |          3199.31 | 20224000 |  271.593 |              320.949 |              126.556 |             799.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3330.7236271960924
    time_step_min: 3006
  date: 2020-10-15_16-36-16
  done: false
  episode_len_mean: 799.5307879836427
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 271.8491310557601
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 25432
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.12018661511441071
        entropy_coeff: 0.0005000000000000001
        kl: 0.003371649499361714
        model: {}
        policy_loss: -0.008376645351139208
        total_loss: 2.056699365377426
        vf_explained_var: 0.9952095150947571
        vf_loss: 2.065136104822159
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.966666666666665
    gpu_util_percent0: 0.32566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469935815271977
    mean_env_wait_ms: 1.2271264056460214
    mean_inference_ms: 4.32782801661197
    mean_raw_obs_processing_ms: 0.3801450543431108
  time_since_restore: 3225.098615169525
  time_this_iter_s: 25.79140329360962
  time_total_s: 3225.098615169525
  timers:
    learn_throughput: 8641.312
    learn_time_ms: 18723.083
    sample_throughput: 23946.284
    sample_time_ms: 6756.455
    update_time_ms: 37.749
  timestamp: 1602779776
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    126 |           3225.1 | 20385792 |  271.849 |              320.949 |              126.556 |            799.531 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3329.2597270558795
    time_step_min: 3006
  date: 2020-10-15_16-36-43
  done: false
  episode_len_mean: 799.3294820250595
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 272.0700707058875
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 187
  episodes_total: 25619
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.12033804319798946
        entropy_coeff: 0.0005000000000000001
        kl: 0.003876735553300629
        model: {}
        policy_loss: -0.007069124665576965
        total_loss: 1.8338869710763295
        vf_explained_var: 0.9953691363334656
        vf_loss: 1.8410162925720215
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.556666666666672
    gpu_util_percent0: 0.33433333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469867420754046
    mean_env_wait_ms: 1.227122533893661
    mean_inference_ms: 4.327311599137322
    mean_raw_obs_processing_ms: 0.38012121088263684
  time_since_restore: 3250.6714901924133
  time_this_iter_s: 25.572875022888184
  time_total_s: 3250.6714901924133
  timers:
    learn_throughput: 8638.834
    learn_time_ms: 18728.453
    sample_throughput: 23914.926
    sample_time_ms: 6765.315
    update_time_ms: 37.134
  timestamp: 1602779803
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    127 |          3250.67 | 20547584 |   272.07 |              320.949 |              126.556 |            799.329 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3327.5211071054773
    time_step_min: 3006
  date: 2020-10-15_16-37-09
  done: false
  episode_len_mean: 799.0939132453663
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 272.33219749247263
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 25843
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.12678717821836472
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034322276478633285
        model: {}
        policy_loss: -0.008822339742134014
        total_loss: 2.5701075394948325
        vf_explained_var: 0.9944183826446533
        vf_loss: 2.578993320465088
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803333333333335
    gpu_util_percent0: 0.341
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469794274142529
    mean_env_wait_ms: 1.2271195267696242
    mean_inference_ms: 4.3267162797808645
    mean_raw_obs_processing_ms: 0.38009529454236407
  time_since_restore: 3276.6029665470123
  time_this_iter_s: 25.931476354599
  time_total_s: 3276.6029665470123
  timers:
    learn_throughput: 8638.829
    learn_time_ms: 18728.464
    sample_throughput: 23845.757
    sample_time_ms: 6784.939
    update_time_ms: 37.082
  timestamp: 1602779829
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    128 |           3276.6 | 20709376 |  272.332 |              320.949 |              126.556 |            799.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3325.8222384503038
    time_step_min: 3006
  date: 2020-10-15_16-37-35
  done: false
  episode_len_mean: 798.8403928790669
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 272.6001869523965
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 26064
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.694065894508601e-22
        cur_lr: 5.0e-05
        entropy: 0.11459821338454883
        entropy_coeff: 0.0005000000000000001
        kl: 0.004690838201592366
        model: {}
        policy_loss: -0.008129160579604408
        total_loss: 2.066160092751185
        vf_explained_var: 0.9952142834663391
        vf_loss: 2.0743466218312583
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.903333333333332
    gpu_util_percent0: 0.306
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469710786721665
    mean_env_wait_ms: 1.2271184490811249
    mean_inference_ms: 4.326133929061194
    mean_raw_obs_processing_ms: 0.3800687342272458
  time_since_restore: 3302.3729541301727
  time_this_iter_s: 25.7699875831604
  time_total_s: 3302.3729541301727
  timers:
    learn_throughput: 8635.417
    learn_time_ms: 18735.864
    sample_throughput: 23843.417
    sample_time_ms: 6785.605
    update_time_ms: 37.033
  timestamp: 1602779855
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    129 |          3302.37 | 20871168 |    272.6 |              320.949 |              126.556 |             798.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3324.338205548983
    time_step_min: 3003
  date: 2020-10-15_16-38-01
  done: false
  episode_len_mean: 798.6358718427368
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 272.8229458284314
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 185
  episodes_total: 26249
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.470329472543005e-23
        cur_lr: 5.0e-05
        entropy: 0.11800270341336727
        entropy_coeff: 0.0005000000000000001
        kl: 0.004974550295931597
        model: {}
        policy_loss: -0.0069315843187117325
        total_loss: 1.7854573428630829
        vf_explained_var: 0.9954321384429932
        vf_loss: 1.7924479246139526
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696454610767967
    mean_env_wait_ms: 1.2271142960710881
    mean_inference_ms: 4.325643077401216
    mean_raw_obs_processing_ms: 0.380046149391486
  time_since_restore: 3327.8938715457916
  time_this_iter_s: 25.520917415618896
  time_total_s: 3327.8938715457916
  timers:
    learn_throughput: 8630.38
    learn_time_ms: 18746.8
    sample_throughput: 23817.782
    sample_time_ms: 6792.908
    update_time_ms: 37.001
  timestamp: 1602779881
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    130 |          3327.89 | 21032960 |  272.823 |              321.404 |              126.556 |            798.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3322.637591765685
    time_step_min: 3003
  date: 2020-10-15_16-38-27
  done: false
  episode_len_mean: 798.3990253853128
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.07911503979005
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 26472
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.12755438623329005
        entropy_coeff: 0.0005000000000000001
        kl: 0.003397385085312029
        model: {}
        policy_loss: -0.007831684575648978
        total_loss: 3.084522763888041
        vf_explained_var: 0.9931008219718933
        vf_loss: 3.0924181739489236
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.940000000000005
    gpu_util_percent0: 0.35666666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695740636183277
    mean_env_wait_ms: 1.2271074358752743
    mean_inference_ms: 4.325077606418608
    mean_raw_obs_processing_ms: 0.3800213192461
  time_since_restore: 3353.5246765613556
  time_this_iter_s: 25.630805015563965
  time_total_s: 3353.5246765613556
  timers:
    learn_throughput: 8638.049
    learn_time_ms: 18730.155
    sample_throughput: 23790.963
    sample_time_ms: 6800.565
    update_time_ms: 36.804
  timestamp: 1602779907
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    131 |          3353.52 | 21194752 |  273.079 |              321.404 |              126.556 |            798.399 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3320.915634616828
    time_step_min: 3003
  date: 2020-10-15_16-38-53
  done: false
  episode_len_mean: 798.1718867076278
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.3368633131854
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 26692
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1175823681357513e-23
        cur_lr: 5.0e-05
        entropy: 0.11687162518501282
        entropy_coeff: 0.0005000000000000001
        kl: 0.004349517015119393
        model: {}
        policy_loss: -0.007446840754710138
        total_loss: 1.96782386302948
        vf_explained_var: 0.9955055713653564
        vf_loss: 1.975329081217448
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25862068965517
    gpu_util_percent0: 0.3168965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694963370367495
    mean_env_wait_ms: 1.227103886040299
    mean_inference_ms: 4.324523604406364
    mean_raw_obs_processing_ms: 0.3799957435753244
  time_since_restore: 3378.9581179618835
  time_this_iter_s: 25.433441400527954
  time_total_s: 3378.9581179618835
  timers:
    learn_throughput: 8645.697
    learn_time_ms: 18713.587
    sample_throughput: 23799.886
    sample_time_ms: 6798.016
    update_time_ms: 34.357
  timestamp: 1602779933
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    132 |          3378.96 | 21356544 |  273.337 |              321.404 |              126.556 |            798.172 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3319.461504062011
    time_step_min: 3003
  date: 2020-10-15_16-39-19
  done: false
  episode_len_mean: 797.9724330357143
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.551457656926
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 188
  episodes_total: 26880
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.11650159396231174
        entropy_coeff: 0.0005000000000000001
        kl: 0.003250186098739505
        model: {}
        policy_loss: -0.008330285796546377
        total_loss: 2.13726536432902
        vf_explained_var: 0.994652271270752
        vf_loss: 2.145653943220774
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416666666666664
    gpu_util_percent0: 0.3316666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694311977431693
    mean_env_wait_ms: 1.2271007128446687
    mean_inference_ms: 4.324049505789062
    mean_raw_obs_processing_ms: 0.3799733328082645
  time_since_restore: 3404.4066598415375
  time_this_iter_s: 25.44854187965393
  time_total_s: 3404.4066598415375
  timers:
    learn_throughput: 8659.58
    learn_time_ms: 18683.584
    sample_throughput: 23796.13
    sample_time_ms: 6799.089
    update_time_ms: 40.853
  timestamp: 1602779959
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    133 |          3404.41 | 21518336 |  273.551 |              321.404 |              126.556 |            797.972 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3317.772246858832
    time_step_min: 3003
  date: 2020-10-15_16-39-46
  done: false
  episode_len_mean: 797.7334907400575
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.81045718753194
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 226
  episodes_total: 27106
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.293955920339378e-24
        cur_lr: 5.0e-05
        entropy: 0.12073460221290588
        entropy_coeff: 0.0005000000000000001
        kl: 0.003983590131004651
        model: {}
        policy_loss: -0.006819716848743458
        total_loss: 2.212758799393972
        vf_explained_var: 0.9949829578399658
        vf_loss: 2.2196388443311057
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.220000000000006
    gpu_util_percent0: 0.293
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693620613332592
    mean_env_wait_ms: 1.2270932872980236
    mean_inference_ms: 4.323499440110382
    mean_raw_obs_processing_ms: 0.379949203615957
  time_since_restore: 3430.138892889023
  time_this_iter_s: 25.73223304748535
  time_total_s: 3430.138892889023
  timers:
    learn_throughput: 8651.062
    learn_time_ms: 18701.981
    sample_throughput: 23842.765
    sample_time_ms: 6785.79
    update_time_ms: 38.945
  timestamp: 1602779986
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    134 |          3430.14 | 21680128 |   273.81 |              321.404 |              126.556 |            797.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3316.07107771261
    time_step_min: 3003
  date: 2020-10-15_16-40-12
  done: false
  episode_len_mean: 797.4768718436653
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.06779830804527
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 27326
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.646977960169689e-24
        cur_lr: 5.0e-05
        entropy: 0.10746520323057969
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035201270366087556
        model: {}
        policy_loss: -0.008008376093736539
        total_loss: 2.132255037625631
        vf_explained_var: 0.9950239062309265
        vf_loss: 2.140317132075628
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.527586206896554
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692880159686714
    mean_env_wait_ms: 1.2270893284954476
    mean_inference_ms: 4.322968529473248
    mean_raw_obs_processing_ms: 0.37992484382525216
  time_since_restore: 3455.777343273163
  time_this_iter_s: 25.638450384140015
  time_total_s: 3455.777343273163
  timers:
    learn_throughput: 8678.012
    learn_time_ms: 18643.902
    sample_throughput: 23808.079
    sample_time_ms: 6795.676
    update_time_ms: 38.534
  timestamp: 1602780012
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    135 |          3455.78 | 21841920 |  274.068 |              321.404 |              126.556 |            797.477 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3314.5969051520115
    time_step_min: 3003
  date: 2020-10-15_16-40-38
  done: false
  episode_len_mean: 797.2722547344698
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.28967402937775
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 185
  episodes_total: 27511
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.10850300639867783
        entropy_coeff: 0.0005000000000000001
        kl: 0.005260383671460052
        model: {}
        policy_loss: -0.00861841247145397
        total_loss: 1.5884666939576466
        vf_explained_var: 0.9958114624023438
        vf_loss: 1.5971393684546153
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.936666666666664
    gpu_util_percent0: 0.321
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692257938731773
    mean_env_wait_ms: 1.2270843584604905
    mean_inference_ms: 4.322520514075189
    mean_raw_obs_processing_ms: 0.3799038619118912
  time_since_restore: 3481.300243616104
  time_this_iter_s: 25.522900342941284
  time_total_s: 3481.300243616104
  timers:
    learn_throughput: 8683.73
    learn_time_ms: 18631.625
    sample_throughput: 23866.114
    sample_time_ms: 6779.151
    update_time_ms: 38.868
  timestamp: 1602780038
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    136 |           3481.3 | 22003712 |   274.29 |              321.404 |              126.556 |            797.272 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3312.8313353072867
    time_step_min: 3003
  date: 2020-10-15_16-41-04
  done: false
  episode_len_mean: 797.0237923576063
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.5610503011364
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 229
  episodes_total: 27740
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.1124781674395005
        entropy_coeff: 0.0005000000000000001
        kl: 0.003990503881747524
        model: {}
        policy_loss: -0.009042980663555985
        total_loss: 1.8409209648768108
        vf_explained_var: 0.9957380890846252
        vf_loss: 1.8500201602776845
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.263333333333335
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691608560521832
    mean_env_wait_ms: 1.227075980191412
    mean_inference_ms: 4.321993797059769
    mean_raw_obs_processing_ms: 0.3798803623757927
  time_since_restore: 3506.901538848877
  time_this_iter_s: 25.601295232772827
  time_total_s: 3506.901538848877
  timers:
    learn_throughput: 8678.407
    learn_time_ms: 18643.052
    sample_throughput: 23893.774
    sample_time_ms: 6771.304
    update_time_ms: 39.204
  timestamp: 1602780064
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    137 |           3506.9 | 22165504 |  274.561 |              321.404 |              126.556 |            797.024 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3311.1892085557665
    time_step_min: 3003
  date: 2020-10-15_16-41-30
  done: false
  episode_len_mean: 796.800515076725
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.81054346447587
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 217
  episodes_total: 27957
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.617444900424223e-25
        cur_lr: 5.0e-05
        entropy: 0.10288584294418494
        entropy_coeff: 0.0005000000000000001
        kl: 0.003140023172212144
        model: {}
        policy_loss: -0.007257028769042033
        total_loss: 2.011667639017105
        vf_explained_var: 0.9952821731567383
        vf_loss: 2.018976092338562
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803333333333338
    gpu_util_percent0: 0.34633333333333344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690884502628268
    mean_env_wait_ms: 1.2270690419310606
    mean_inference_ms: 4.321487169259454
    mean_raw_obs_processing_ms: 0.37985713953974753
  time_since_restore: 3532.4816420078278
  time_this_iter_s: 25.580103158950806
  time_total_s: 3532.4816420078278
  timers:
    learn_throughput: 8687.802
    learn_time_ms: 18622.893
    sample_throughput: 23942.72
    sample_time_ms: 6757.461
    update_time_ms: 36.918
  timestamp: 1602780090
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    138 |          3532.48 | 22327296 |  274.811 |              321.404 |              126.556 |            796.801 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3309.7412627233257
    time_step_min: 3003
  date: 2020-10-15_16-41-56
  done: false
  episode_len_mean: 796.6109295054008
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 275.03205412567934
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 187
  episodes_total: 28144
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3087224502121113e-25
        cur_lr: 5.0e-05
        entropy: 0.10281236097216606
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045992383966222405
        model: {}
        policy_loss: -0.007216975539146612
        total_loss: 1.4275826414426167
        vf_explained_var: 0.9962032437324524
        vf_loss: 1.4348510106404622
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.513333333333335
    gpu_util_percent0: 0.39533333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469029844411344
    mean_env_wait_ms: 1.227063541763679
    mean_inference_ms: 4.321056150994417
    mean_raw_obs_processing_ms: 0.37983709279028494
  time_since_restore: 3557.939308643341
  time_this_iter_s: 25.457666635513306
  time_total_s: 3557.939308643341
  timers:
    learn_throughput: 8704.959
    learn_time_ms: 18586.189
    sample_throughput: 23924.633
    sample_time_ms: 6762.57
    update_time_ms: 36.631
  timestamp: 1602780116
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    139 |          3557.94 | 22489088 |  275.032 |              321.404 |              126.556 |            796.611 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3308.0434659793086
    time_step_min: 3003
  date: 2020-10-15_16-42-22
  done: false
  episode_len_mean: 796.3881623012655
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 275.28469665100215
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 28367
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6543612251060557e-25
        cur_lr: 5.0e-05
        entropy: 0.11996179446578026
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048959210980683565
        model: {}
        policy_loss: -0.007559831162022117
        total_loss: 2.0452216267585754
        vf_explained_var: 0.9951726794242859
        vf_loss: 2.0528414050738015
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.099999999999998
    gpu_util_percent0: 0.334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689700746031972
    mean_env_wait_ms: 1.2270554775442424
    mean_inference_ms: 4.320560296154213
    mean_raw_obs_processing_ms: 0.3798148132920293
  time_since_restore: 3583.220768213272
  time_this_iter_s: 25.28145956993103
  time_total_s: 3583.220768213272
  timers:
    learn_throughput: 8708.815
    learn_time_ms: 18577.959
    sample_throughput: 23982.925
    sample_time_ms: 6746.133
    update_time_ms: 36.766
  timestamp: 1602780142
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    140 |          3583.22 | 22650880 |  275.285 |              321.404 |              126.556 |            796.388 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3306.4316095578447
    time_step_min: 3003
  date: 2020-10-15_16-42-48
  done: false
  episode_len_mean: 796.1704911151533
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 275.52997160636687
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 28588
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.271806125530278e-26
        cur_lr: 5.0e-05
        entropy: 0.10900688295563062
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034270277246832848
        model: {}
        policy_loss: -0.007902056100040985
        total_loss: 1.433708260456721
        vf_explained_var: 0.996518075466156
        vf_loss: 1.4416647851467133
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.124137931034486
    gpu_util_percent0: 0.3372413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468901836450689
    mean_env_wait_ms: 1.2270480947845304
    mean_inference_ms: 4.320067937244528
    mean_raw_obs_processing_ms: 0.3797926667845745
  time_since_restore: 3608.7689402103424
  time_this_iter_s: 25.548171997070312
  time_total_s: 3608.7689402103424
  timers:
    learn_throughput: 8715.67
    learn_time_ms: 18563.345
    sample_throughput: 23977.15
    sample_time_ms: 6747.758
    update_time_ms: 38.484
  timestamp: 1602780168
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    141 |          3608.77 | 22812672 |   275.53 |              321.404 |              126.556 |             796.17 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3305.015036023807
    time_step_min: 3003
  date: 2020-10-15_16-43-15
  done: false
  episode_len_mean: 795.981686763735
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 275.7414215828223
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 28777
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.135903062765139e-26
        cur_lr: 5.0e-05
        entropy: 0.10426144860684872
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036415953266744814
        model: {}
        policy_loss: -0.0069512441696133465
        total_loss: 1.4889361063639324
        vf_explained_var: 0.9960039258003235
        vf_loss: 1.495939463376999
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.880000000000003
    gpu_util_percent0: 0.3429999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468843602038118
    mean_env_wait_ms: 1.227040135939792
    mean_inference_ms: 4.319650401635181
    mean_raw_obs_processing_ms: 0.37977305357106006
  time_since_restore: 3634.352084159851
  time_this_iter_s: 25.583143949508667
  time_total_s: 3634.352084159851
  timers:
    learn_throughput: 8716.805
    learn_time_ms: 18560.929
    sample_throughput: 23930.885
    sample_time_ms: 6760.803
    update_time_ms: 41.071
  timestamp: 1602780195
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    142 |          3634.35 | 22974464 |  275.741 |              321.404 |              126.556 |            795.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3303.3340701087895
    time_step_min: 3003
  date: 2020-10-15_16-43-41
  done: false
  episode_len_mean: 795.7611806489432
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 275.99736163747707
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 29001
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0679515313825696e-26
        cur_lr: 5.0e-05
        entropy: 0.10925706972678502
        entropy_coeff: 0.0005000000000000001
        kl: 0.007478383951820433
        model: {}
        policy_loss: -0.00675231311955334
        total_loss: 1.2908515930175781
        vf_explained_var: 0.9969214797019958
        vf_loss: 1.2976585527261097
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.830000000000002
    gpu_util_percent0: 0.321
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468784202946666
    mean_env_wait_ms: 1.227031070531576
    mean_inference_ms: 4.319168403558189
    mean_raw_obs_processing_ms: 0.3797519021208127
  time_since_restore: 3659.93585395813
  time_this_iter_s: 25.58376979827881
  time_total_s: 3659.93585395813
  timers:
    learn_throughput: 8710.114
    learn_time_ms: 18575.188
    sample_throughput: 23911.485
    sample_time_ms: 6766.288
    update_time_ms: 34.756
  timestamp: 1602780221
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    143 |          3659.94 | 23136256 |  275.997 |              321.404 |              126.556 |            795.761 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3301.802454324204
    time_step_min: 3003
  date: 2020-10-15_16-44-07
  done: false
  episode_len_mean: 795.5614839659125
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 276.2220085795837
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 29219
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0679515313825696e-26
        cur_lr: 5.0e-05
        entropy: 0.12566090313096842
        entropy_coeff: 0.0005000000000000001
        kl: 0.009030882036313415
        model: {}
        policy_loss: -0.00965633497495825
        total_loss: 2.236224055290222
        vf_explained_var: 0.9944090843200684
        vf_loss: 2.245943288008372
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.196666666666665
    gpu_util_percent0: 0.3796666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687203678047883
    mean_env_wait_ms: 1.2270250872017698
    mean_inference_ms: 4.318703691979647
    mean_raw_obs_processing_ms: 0.37973118460536603
  time_since_restore: 3685.5055153369904
  time_this_iter_s: 25.569661378860474
  time_total_s: 3685.5055153369904
  timers:
    learn_throughput: 8725.413
    learn_time_ms: 18542.619
    sample_throughput: 23857.291
    sample_time_ms: 6781.658
    update_time_ms: 35.315
  timestamp: 1602780247
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    144 |          3685.51 | 23298048 |  276.222 |              321.404 |              126.556 |            795.561 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3300.7544785777536
    time_step_min: 3003
  date: 2020-10-15_16-44-33
  done: false
  episode_len_mean: 795.4081542437432
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 276.3850597927036
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 29408
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0679515313825696e-26
        cur_lr: 5.0e-05
        entropy: 0.1240477729588747
        entropy_coeff: 0.0005000000000000001
        kl: 0.004278909803057711
        model: {}
        policy_loss: -0.010010998172219843
        total_loss: 2.2924087246259055
        vf_explained_var: 0.9938840866088867
        vf_loss: 2.302481691042582
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.963333333333342
    gpu_util_percent0: 0.31600000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468663646233892
    mean_env_wait_ms: 1.2270162744205766
    mean_inference_ms: 4.318304054154579
    mean_raw_obs_processing_ms: 0.3797118839152806
  time_since_restore: 3711.211308002472
  time_this_iter_s: 25.705792665481567
  time_total_s: 3711.211308002472
  timers:
    learn_throughput: 8719.581
    learn_time_ms: 18555.019
    sample_throughput: 23904.945
    sample_time_ms: 6768.139
    update_time_ms: 35.305
  timestamp: 1602780273
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    145 |          3711.21 | 23459840 |  276.385 |              321.404 |              126.556 |            795.408 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3299.325773195876
    time_step_min: 3003
  date: 2020-10-15_16-44-59
  done: false
  episode_len_mean: 795.2212885154062
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 276.59862060925093
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 29631
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0339757656912848e-26
        cur_lr: 5.0e-05
        entropy: 0.11945051389435928
        entropy_coeff: 0.0005000000000000001
        kl: 0.00380708147228385
        model: {}
        policy_loss: -0.01014095088855053
        total_loss: 1.93004776040713
        vf_explained_var: 0.9954066872596741
        vf_loss: 1.9402484794457753
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.48275862068966
    gpu_util_percent0: 0.4010344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686072959717797
    mean_env_wait_ms: 1.2270065997326733
    mean_inference_ms: 4.31784192785631
    mean_raw_obs_processing_ms: 0.3796918751010538
  time_since_restore: 3736.696119785309
  time_this_iter_s: 25.484811782836914
  time_total_s: 3736.696119785309
  timers:
    learn_throughput: 8719.097
    learn_time_ms: 18556.049
    sample_throughput: 23913.901
    sample_time_ms: 6765.605
    update_time_ms: 33.169
  timestamp: 1602780299
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    146 |           3736.7 | 23621632 |  276.599 |              321.404 |              126.556 |            795.221 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3297.8451058689307
    time_step_min: 3003
  date: 2020-10-15_16-45-25
  done: false
  episode_len_mean: 795.0472409287365
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 276.8313713067956
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 29847
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.169878828456424e-27
        cur_lr: 5.0e-05
        entropy: 0.10072617294887702
        entropy_coeff: 0.0005000000000000001
        kl: 0.003395519587987413
        model: {}
        policy_loss: -0.009530746858217753
        total_loss: 1.4415943622589111
        vf_explained_var: 0.99647456407547
        vf_loss: 1.4511754512786865
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.696666666666665
    gpu_util_percent0: 0.35900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685460071866446
    mean_env_wait_ms: 1.2269985845763713
    mean_inference_ms: 4.317395478656448
    mean_raw_obs_processing_ms: 0.3796722869782082
  time_since_restore: 3762.1350994110107
  time_this_iter_s: 25.438979625701904
  time_total_s: 3762.1350994110107
  timers:
    learn_throughput: 8729.163
    learn_time_ms: 18534.653
    sample_throughput: 23893.113
    sample_time_ms: 6771.491
    update_time_ms: 32.088
  timestamp: 1602780325
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    147 |          3762.14 | 23783424 |  276.831 |              321.404 |              126.556 |            795.047 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3296.3902040544144
    time_step_min: 3003
  date: 2020-10-15_16-45-51
  done: false
  episode_len_mean: 794.8847792795792
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 277.05244131843733
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 30038
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.584939414228212e-27
        cur_lr: 5.0e-05
        entropy: 0.09415808754662673
        entropy_coeff: 0.0005000000000000001
        kl: 0.00299359776545316
        model: {}
        policy_loss: -0.008002154451484481
        total_loss: 1.0279664099216461
        vf_explained_var: 0.9971506595611572
        vf_loss: 1.0360156446695328
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05666666666667
    gpu_util_percent0: 0.3186666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684918118776155
    mean_env_wait_ms: 1.2269918456868156
    mean_inference_ms: 4.31700476247836
    mean_raw_obs_processing_ms: 0.3796535657860172
  time_since_restore: 3787.6921951770782
  time_this_iter_s: 25.557095766067505
  time_total_s: 3787.6921951770782
  timers:
    learn_throughput: 8731.781
    learn_time_ms: 18529.095
    sample_throughput: 23895.52
    sample_time_ms: 6770.809
    update_time_ms: 35.134
  timestamp: 1602780351
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    148 |          3787.69 | 23945216 |  277.052 |              321.404 |              126.556 |            794.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3294.627858490254
    time_step_min: 3003
  date: 2020-10-15_16-46-17
  done: false
  episode_len_mean: 794.690975779004
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 277.3200804929975
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 30263
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.292469707114106e-27
        cur_lr: 5.0e-05
        entropy: 0.09593602704505126
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032969804015010595
        model: {}
        policy_loss: -0.007113797628941636
        total_loss: 0.8629736403624216
        vf_explained_var: 0.9978658556938171
        vf_loss: 0.8701354165871938
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.263333333333335
    gpu_util_percent0: 0.3546666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468436138515754
    mean_env_wait_ms: 1.22698051927979
    mean_inference_ms: 4.3165614874155125
    mean_raw_obs_processing_ms: 0.3796341490328723
  time_since_restore: 3813.307708978653
  time_this_iter_s: 25.615513801574707
  time_total_s: 3813.307708978653
  timers:
    learn_throughput: 8724.334
    learn_time_ms: 18544.911
    sample_throughput: 23906.583
    sample_time_ms: 6767.676
    update_time_ms: 35.667
  timestamp: 1602780377
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    149 |          3813.31 | 24107008 |   277.32 |              321.404 |              126.556 |            794.691 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3292.998981268485
    time_step_min: 3003
  date: 2020-10-15_16-46-44
  done: false
  episode_len_mean: 794.5247735923349
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 277.57060664394265
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 213
  episodes_total: 30476
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.46234853557053e-28
        cur_lr: 5.0e-05
        entropy: 0.09429628153642018
        entropy_coeff: 0.0005000000000000001
        kl: 0.005168771759296457
        model: {}
        policy_loss: -0.007048836909234524
        total_loss: 1.0725575735171635
        vf_explained_var: 0.997347891330719
        vf_loss: 1.079653576016426
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.0
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683770106766073
    mean_env_wait_ms: 1.2269706968731384
    mean_inference_ms: 4.316137026286915
    mean_raw_obs_processing_ms: 0.3796155424648689
  time_since_restore: 3839.1033210754395
  time_this_iter_s: 25.7956120967865
  time_total_s: 3839.1033210754395
  timers:
    learn_throughput: 8712.672
    learn_time_ms: 18569.734
    sample_throughput: 23825.446
    sample_time_ms: 6790.723
    update_time_ms: 37.61
  timestamp: 1602780404
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    150 |           3839.1 | 24268800 |  277.571 |              321.404 |              126.556 |            794.525 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3291.6494774657085
    time_step_min: 3003
  date: 2020-10-15_16-47-10
  done: false
  episode_len_mean: 794.3958781712646
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 277.7787244386732
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 30666
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.46234853557053e-28
        cur_lr: 5.0e-05
        entropy: 0.10729831581314404
        entropy_coeff: 0.0005000000000000001
        kl: 0.004326682576599221
        model: {}
        policy_loss: -0.007609238295117393
        total_loss: 1.0304337441921234
        vf_explained_var: 0.9971546530723572
        vf_loss: 1.0380966166655223
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.033333333333335
    gpu_util_percent0: 0.36800000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683252582784742
    mean_env_wait_ms: 1.2269617433604316
    mean_inference_ms: 4.315764556582287
    mean_raw_obs_processing_ms: 0.3795977652413687
  time_since_restore: 3864.89360165596
  time_this_iter_s: 25.79028058052063
  time_total_s: 3864.89360165596
  timers:
    learn_throughput: 8699.967
    learn_time_ms: 18596.852
    sample_throughput: 23831.384
    sample_time_ms: 6789.031
    update_time_ms: 37.064
  timestamp: 1602780430
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    151 |          3864.89 | 24430592 |  277.779 |              321.404 |              126.556 |            794.396 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3290.385875028373
    time_step_min: 3003
  date: 2020-10-15_16-47-36
  done: false
  episode_len_mean: 794.2829205115752
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 277.96342737722006
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 219
  episodes_total: 30885
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.231174267785265e-28
        cur_lr: 5.0e-05
        entropy: 0.11525082836548488
        entropy_coeff: 0.0005000000000000001
        kl: 0.004104262489515047
        model: {}
        policy_loss: -0.009941882017301396
        total_loss: 2.079715351263682
        vf_explained_var: 0.9952294826507568
        vf_loss: 2.0897148847579956
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.693333333333335
    gpu_util_percent0: 0.37366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468272529135544
    mean_env_wait_ms: 1.2269483466479885
    mean_inference_ms: 4.315346742537891
    mean_raw_obs_processing_ms: 0.3795786300600136
  time_since_restore: 3890.2637882232666
  time_this_iter_s: 25.37018656730652
  time_total_s: 3890.2637882232666
  timers:
    learn_throughput: 8705.693
    learn_time_ms: 18584.621
    sample_throughput: 23859.664
    sample_time_ms: 6780.984
    update_time_ms: 35.933
  timestamp: 1602780456
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    152 |          3890.26 | 24592384 |  277.963 |              321.404 |              126.556 |            794.283 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3288.943648365803
    time_step_min: 3003
  date: 2020-10-15_16-48-02
  done: false
  episode_len_mean: 794.1698980740169
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 278.18635179810025
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 31101
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6155871338926325e-28
        cur_lr: 5.0e-05
        entropy: 0.09653184562921524
        entropy_coeff: 0.0005000000000000001
        kl: 0.003285205088711033
        model: {}
        policy_loss: -0.008072127825774563
        total_loss: 1.1460022926330566
        vf_explained_var: 0.9971932768821716
        vf_loss: 1.1541227002938588
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.244827586206902
    gpu_util_percent0: 0.3737931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682135373663235
    mean_env_wait_ms: 1.226936237328138
    mean_inference_ms: 4.314932784730333
    mean_raw_obs_processing_ms: 0.37956106881761875
  time_since_restore: 3915.668481826782
  time_this_iter_s: 25.404693603515625
  time_total_s: 3915.668481826782
  timers:
    learn_throughput: 8705.492
    learn_time_ms: 18585.05
    sample_throughput: 23918.349
    sample_time_ms: 6764.346
    update_time_ms: 34.322
  timestamp: 1602780482
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    153 |          3915.67 | 24754176 |  278.186 |              321.404 |              126.556 |             794.17 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3287.552259344598
    time_step_min: 3003
  date: 2020-10-15_16-48-28
  done: false
  episode_len_mean: 794.0543554675018
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 278.40232258031153
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 31294
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.077935669463162e-29
        cur_lr: 5.0e-05
        entropy: 0.08573005286355813
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007286098106609036
        total_loss: .inf
        vf_explained_var: 0.9980301260948181
        vf_loss: 0.730293408036232
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.953333333333333
    gpu_util_percent0: 0.39866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681656746497165
    mean_env_wait_ms: 1.2269268168062972
    mean_inference_ms: 4.314570528955339
    mean_raw_obs_processing_ms: 0.37954409787577503
  time_since_restore: 3940.9289298057556
  time_this_iter_s: 25.26044797897339
  time_total_s: 3940.9289298057556
  timers:
    learn_throughput: 8721.386
    learn_time_ms: 18551.179
    sample_throughput: 23935.881
    sample_time_ms: 6759.392
    update_time_ms: 32.633
  timestamp: 1602780508
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    154 |          3940.93 | 24915968 |  278.402 |              321.404 |              126.556 |            794.054 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3286.0418335558525
    time_step_min: 3003
  date: 2020-10-15_16-48-54
  done: false
  episode_len_mean: 793.9263268156425
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 278.6327617208136
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 31504
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.211690350419474e-28
        cur_lr: 5.0e-05
        entropy: 0.08976890457173188
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036143369507044554
        model: {}
        policy_loss: -0.007865896448493004
        total_loss: 0.7734309186538061
        vf_explained_var: 0.9980778694152832
        vf_loss: 0.7813417017459869
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98965517241379
    gpu_util_percent0: 0.31896551724137934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681128023798404
    mean_env_wait_ms: 1.22691170067055
    mean_inference_ms: 4.314179524847259
    mean_raw_obs_processing_ms: 0.37952587399189314
  time_since_restore: 3966.562959432602
  time_this_iter_s: 25.634029626846313
  time_total_s: 3966.562959432602
  timers:
    learn_throughput: 8722.966
    learn_time_ms: 18547.82
    sample_throughput: 23920.182
    sample_time_ms: 6763.828
    update_time_ms: 32.023
  timestamp: 1602780534
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    155 |          3966.56 | 25077760 |  278.633 |              321.404 |              126.556 |            793.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3284.3764717022823
    time_step_min: 3003
  date: 2020-10-15_16-49-20
  done: false
  episode_len_mean: 793.802345005831
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 278.88076178941964
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 31727
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.05845175209737e-29
        cur_lr: 5.0e-05
        entropy: 0.08573165349662304
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030047891001837948
        model: {}
        policy_loss: -0.0069037986201389385
        total_loss: 0.6351885894934336
        vf_explained_var: 0.9984167218208313
        vf_loss: 0.6421352426211039
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.920000000000005
    gpu_util_percent0: 0.3593333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468059230199769
    mean_env_wait_ms: 1.226896105812638
    mean_inference_ms: 4.313774029811292
    mean_raw_obs_processing_ms: 0.3795088087557288
  time_since_restore: 3991.910928964615
  time_this_iter_s: 25.34796953201294
  time_total_s: 3991.910928964615
  timers:
    learn_throughput: 8729.744
    learn_time_ms: 18533.419
    sample_throughput: 23927.143
    sample_time_ms: 6761.86
    update_time_ms: 33.159
  timestamp: 1602780560
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    156 |          3991.91 | 25239552 |  278.881 |              321.404 |              126.556 |            793.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3282.979168627451
    time_step_min: 3003
  date: 2020-10-15_16-49-46
  done: false
  episode_len_mean: 793.7015444378309
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 279.09510632150864
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 31921
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.029225876048685e-29
        cur_lr: 5.0e-05
        entropy: 0.08296642328302066
        entropy_coeff: 0.0005000000000000001
        kl: 0.003879522264469415
        model: {}
        policy_loss: -0.00842639805826669
        total_loss: 0.6334505826234818
        vf_explained_var: 0.9982810616493225
        vf_loss: 0.6419184406598409
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.386666666666667
    gpu_util_percent0: 0.36266666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468009446918863
    mean_env_wait_ms: 1.2268845050709818
    mean_inference_ms: 4.313425018473102
    mean_raw_obs_processing_ms: 0.37949249116006
  time_since_restore: 4017.6530301570892
  time_this_iter_s: 25.742101192474365
  time_total_s: 4017.6530301570892
  timers:
    learn_throughput: 8725.552
    learn_time_ms: 18542.323
    sample_throughput: 23863.239
    sample_time_ms: 6779.968
    update_time_ms: 34.973
  timestamp: 1602780586
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    157 |          4017.65 | 25401344 |  279.095 |              321.404 |              126.556 |            793.702 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3281.58814727063
    time_step_min: 3003
  date: 2020-10-15_16-50-13
  done: false
  episode_len_mean: 793.6068860318152
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 279.3079495260794
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 32123
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5146129380243426e-29
        cur_lr: 5.0e-05
        entropy: 0.08703182389338811
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035911662271246314
        model: {}
        policy_loss: -0.005694880693530043
        total_loss: 0.7733790675799052
        vf_explained_var: 0.998075008392334
        vf_loss: 0.7791174401839575
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679638279454046
    mean_env_wait_ms: 1.226865837601464
    mean_inference_ms: 4.313058785714503
    mean_raw_obs_processing_ms: 0.37947575858519994
  time_since_restore: 4043.554377555847
  time_this_iter_s: 25.901347398757935
  time_total_s: 4043.554377555847
  timers:
    learn_throughput: 8709.082
    learn_time_ms: 18577.388
    sample_throughput: 23860.579
    sample_time_ms: 6780.724
    update_time_ms: 33.008
  timestamp: 1602780613
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    158 |          4043.55 | 25563136 |  279.308 |              321.404 |              126.556 |            793.607 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3279.921343445287
    time_step_min: 3003
  date: 2020-10-15_16-50-39
  done: false
  episode_len_mean: 793.5010046057309
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 279.5558140834635
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 228
  episodes_total: 32351
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.573064690121713e-30
        cur_lr: 5.0e-05
        entropy: 0.08645101077854633
        entropy_coeff: 0.0005000000000000001
        kl: 0.004002726830852528
        model: {}
        policy_loss: -0.006286975626911347
        total_loss: 0.5288806507984797
        vf_explained_var: 0.998704195022583
        vf_loss: 0.5352108428875605
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.976666666666667
    gpu_util_percent0: 0.3540000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679122866054173
    mean_env_wait_ms: 1.2268489890496037
    mean_inference_ms: 4.312666158328693
    mean_raw_obs_processing_ms: 0.3794582011559683
  time_since_restore: 4069.281332015991
  time_this_iter_s: 25.726954460144043
  time_total_s: 4069.281332015991
  timers:
    learn_throughput: 8700.203
    learn_time_ms: 18596.349
    sample_throughput: 23889.179
    sample_time_ms: 6772.606
    update_time_ms: 33.188
  timestamp: 1602780639
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    159 |          4069.28 | 25724928 |  279.556 |              321.404 |              126.556 |            793.501 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3278.4642670358407
    time_step_min: 3003
  date: 2020-10-15_16-51-05
  done: false
  episode_len_mean: 793.4222604528279
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 279.7740993232372
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 32551
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7865323450608565e-30
        cur_lr: 5.0e-05
        entropy: 0.08268947216371696
        entropy_coeff: 0.0005000000000000001
        kl: 0.006727448237749438
        model: {}
        policy_loss: -0.008967500364330286
        total_loss: 0.3921525826056798
        vf_explained_var: 0.9989423155784607
        vf_loss: 0.40116143723328906
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87333333333333
    gpu_util_percent0: 0.3476666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467860123939014
    mean_env_wait_ms: 1.2268337553250952
    mean_inference_ms: 4.312314567707982
    mean_raw_obs_processing_ms: 0.3794422850610798
  time_since_restore: 4094.647572517395
  time_this_iter_s: 25.36624050140381
  time_total_s: 4094.647572517395
  timers:
    learn_throughput: 8712.72
    learn_time_ms: 18569.632
    sample_throughput: 23942.944
    sample_time_ms: 6757.398
    update_time_ms: 31.46
  timestamp: 1602780665
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: f0f97_00000
  
2020-10-15 16:51:06,466	WARNING util.py:136 -- The `process_trial` operation took 0.5189313888549805 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    160 |          4094.65 | 25886720 |  279.774 |              321.404 |              126.556 |            793.422 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3277.113248516729
    time_step_min: 3003
  date: 2020-10-15_16-51-32
  done: false
  episode_len_mean: 793.3464756902027
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 279.96501664581274
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 32744
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7865323450608565e-30
        cur_lr: 5.0e-05
        entropy: 0.09543908139069875
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842820538518329
        model: {}
        policy_loss: -0.008567489542959569
        total_loss: 0.9882097045580546
        vf_explained_var: 0.9974178671836853
        vf_loss: 0.9968249201774597
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.943333333333335
    gpu_util_percent0: 0.2846666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678172597754668
    mean_env_wait_ms: 1.2268142283800603
    mean_inference_ms: 4.311978013380422
    mean_raw_obs_processing_ms: 0.3794264943306157
  time_since_restore: 4120.63959646225
  time_this_iter_s: 25.992023944854736
  time_total_s: 4120.63959646225
  timers:
    learn_throughput: 8700.897
    learn_time_ms: 18594.865
    sample_throughput: 23963.927
    sample_time_ms: 6751.481
    update_time_ms: 31.645
  timestamp: 1602780692
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    161 |          4120.64 | 26048512 |  279.965 |              321.404 |              126.556 |            793.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3275.698584791059
    time_step_min: 3003
  date: 2020-10-15_16-51-58
  done: false
  episode_len_mean: 793.257354279129
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 280.18045469555705
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 230
  episodes_total: 32974
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8932661725304283e-30
        cur_lr: 5.0e-05
        entropy: 0.09286364167928696
        entropy_coeff: 0.0005000000000000001
        kl: 0.004253874843319257
        model: {}
        policy_loss: -0.010279398595836634
        total_loss: 0.9361542810996374
        vf_explained_var: 0.9978063106536865
        vf_loss: 0.9464801102876663
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.523333333333337
    gpu_util_percent0: 0.37100000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677657860723714
    mean_env_wait_ms: 1.2267940352499072
    mean_inference_ms: 4.311590436336226
    mean_raw_obs_processing_ms: 0.3794097760230278
  time_since_restore: 4146.427523851395
  time_this_iter_s: 25.787927389144897
  time_total_s: 4146.427523851395
  timers:
    learn_throughput: 8682.818
    learn_time_ms: 18633.582
    sample_throughput: 23959.747
    sample_time_ms: 6752.659
    update_time_ms: 32.607
  timestamp: 1602780718
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    162 |          4146.43 | 26210304 |   280.18 |              321.404 |              126.556 |            793.257 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3274.4308268180034
    time_step_min: 3003
  date: 2020-10-15_16-52-25
  done: false
  episode_len_mean: 793.1725198203358
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 280.3739447347799
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 33173
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.466330862652141e-31
        cur_lr: 5.0e-05
        entropy: 0.0834658977886041
        entropy_coeff: 0.0005000000000000001
        kl: 0.003478190415383627
        model: {}
        policy_loss: -0.008572894412888369
        total_loss: 1.0261373817920685
        vf_explained_var: 0.997418224811554
        vf_loss: 1.0347520212332408
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.046666666666667
    gpu_util_percent0: 0.30966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677178077542755
    mean_env_wait_ms: 1.2267755330711163
    mean_inference_ms: 4.311261300047459
    mean_raw_obs_processing_ms: 0.3793945873756015
  time_since_restore: 4171.9862422943115
  time_this_iter_s: 25.55871844291687
  time_total_s: 4171.9862422943115
  timers:
    learn_throughput: 8682.248
    learn_time_ms: 18634.805
    sample_throughput: 23925.893
    sample_time_ms: 6762.214
    update_time_ms: 33.905
  timestamp: 1602780745
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: f0f97_00000
  
2020-10-15 16:52:25,681	WARNING util.py:136 -- The `process_trial` operation took 0.5057611465454102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    163 |          4171.99 | 26372096 |  280.374 |              321.404 |              126.556 |            793.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3273.0888848877685
    time_step_min: 3003
  date: 2020-10-15_16-52-51
  done: false
  episode_len_mean: 793.0833383278393
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 280.5753428804068
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 33370
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.733165431326071e-31
        cur_lr: 5.0e-05
        entropy: 0.07992316596210003
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035171596876656017
        model: {}
        policy_loss: -0.006860955957866584
        total_loss: 0.6228178193171819
        vf_explained_var: 0.9983336925506592
        vf_loss: 0.6297187308470408
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64
    gpu_util_percent0: 0.34600000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676723607322745
    mean_env_wait_ms: 1.2267546937726408
    mean_inference_ms: 4.310925681109695
    mean_raw_obs_processing_ms: 0.37937884338729094
  time_since_restore: 4197.601596593857
  time_this_iter_s: 25.615354299545288
  time_total_s: 4197.601596593857
  timers:
    learn_throughput: 8661.783
    learn_time_ms: 18678.834
    sample_throughput: 23933.821
    sample_time_ms: 6759.974
    update_time_ms: 34.208
  timestamp: 1602780771
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: f0f97_00000
  
2020-10-15 16:52:51,958	WARNING util.py:136 -- The `process_trial` operation took 0.5038559436798096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    164 |           4197.6 | 26533888 |  280.575 |              321.404 |              126.556 |            793.083 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3271.584066048702
    time_step_min: 3003
  date: 2020-10-15_16-53-17
  done: false
  episode_len_mean: 792.9791648063815
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 280.8039101615309
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 227
  episodes_total: 33597
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3665827156630353e-31
        cur_lr: 5.0e-05
        entropy: 0.07954368554055691
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034071299984740713
        model: {}
        policy_loss: -0.005895489632772903
        total_loss: 0.7894210070371628
        vf_explained_var: 0.9981210231781006
        vf_loss: 0.795356273651123
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.746666666666673
    gpu_util_percent0: 0.30033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467627264454371
    mean_env_wait_ms: 1.2267288065146145
    mean_inference_ms: 4.310556839965207
    mean_raw_obs_processing_ms: 0.37936269957373986
  time_since_restore: 4223.034966945648
  time_this_iter_s: 25.433370351791382
  time_total_s: 4223.034966945648
  timers:
    learn_throughput: 8669.92
    learn_time_ms: 18661.303
    sample_throughput: 23968.733
    sample_time_ms: 6750.127
    update_time_ms: 33.027
  timestamp: 1602780797
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: f0f97_00000
  
2020-10-15 16:53:18,149	WARNING util.py:136 -- The `process_trial` operation took 0.5225937366485596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    165 |          4223.03 | 26695680 |  280.804 |              321.404 |              126.556 |            792.979 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3270.2309970969845
    time_step_min: 3003
  date: 2020-10-15_16-53-44
  done: false
  episode_len_mean: 792.8806058454621
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 281.01370198255137
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 33804
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.0800494272261858
        entropy_coeff: 0.0005000000000000001
        kl: 0.005471844342537224
        model: {}
        policy_loss: -0.005980824207654223
        total_loss: 0.572075255215168
        vf_explained_var: 0.9985691905021667
        vf_loss: 0.5780961016813914
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84666666666667
    gpu_util_percent0: 0.2843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675771323581255
    mean_env_wait_ms: 1.226710442614028
    mean_inference_ms: 4.310226981373826
    mean_raw_obs_processing_ms: 0.37934757463854674
  time_since_restore: 4248.9343411922455
  time_this_iter_s: 25.89937424659729
  time_total_s: 4248.9343411922455
  timers:
    learn_throughput: 8651.397
    learn_time_ms: 18701.258
    sample_throughput: 23917.73
    sample_time_ms: 6764.522
    update_time_ms: 32.314
  timestamp: 1602780824
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: f0f97_00000
  
2020-10-15 16:53:44,724	WARNING util.py:136 -- The `process_trial` operation took 0.512570858001709 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    166 |          4248.93 | 26857472 |  281.014 |              321.404 |              126.556 |            792.881 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3268.941115824202
    time_step_min: 3003
  date: 2020-10-15_16-54-10
  done: false
  episode_len_mean: 792.7845207977879
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 281.20503024003597
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 33994
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.07843689993023872
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038517863916543624
        model: {}
        policy_loss: -0.007061794422043022
        total_loss: 0.4724571034312248
        vf_explained_var: 0.9986934065818787
        vf_loss: 0.4795580928524335
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996666666666666
    gpu_util_percent0: 0.40866666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675362398947883
    mean_env_wait_ms: 1.226687731303945
    mean_inference_ms: 4.309916702951025
    mean_raw_obs_processing_ms: 0.3793334234658736
  time_since_restore: 4274.681769609451
  time_this_iter_s: 25.74742841720581
  time_total_s: 4274.681769609451
  timers:
    learn_throughput: 8649.381
    learn_time_ms: 18705.616
    sample_throughput: 23935.562
    sample_time_ms: 6759.482
    update_time_ms: 32.733
  timestamp: 1602780850
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    167 |          4274.68 | 27019264 |  281.205 |              321.404 |              126.556 |            792.785 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3267.4600708161415
    time_step_min: 3003
  date: 2020-10-15_16-54-36
  done: false
  episode_len_mean: 792.6838598439464
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 281.42494851197574
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 34219
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.08927358624835809
        entropy_coeff: 0.0005000000000000001
        kl: 0.004229134957616528
        model: {}
        policy_loss: -0.00626066024415195
        total_loss: 0.7237556676069895
        vf_explained_var: 0.9982532858848572
        vf_loss: 0.7300609350204468
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.790000000000003
    gpu_util_percent0: 0.3390000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674903966023795
    mean_env_wait_ms: 1.226661725958376
    mean_inference_ms: 4.309564675196186
    mean_raw_obs_processing_ms: 0.3793173316140355
  time_since_restore: 4300.231848478317
  time_this_iter_s: 25.550078868865967
  time_total_s: 4300.231848478317
  timers:
    learn_throughput: 8662.871
    learn_time_ms: 18676.486
    sample_throughput: 23964.196
    sample_time_ms: 6751.405
    update_time_ms: 32.243
  timestamp: 1602780876
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    168 |          4300.23 | 27181056 |  281.425 |              321.404 |              126.556 |            792.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3266.1107620709713
    time_step_min: 3003
  date: 2020-10-15_16-55-03
  done: false
  episode_len_mean: 792.5860396212165
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 281.62148528801595
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 34426
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.958228394578794e-32
        cur_lr: 5.0e-05
        entropy: 0.08305402534703414
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035715692404968045
        model: {}
        policy_loss: -0.0072935868726441795
        total_loss: 0.5156462093194326
        vf_explained_var: 0.9986898303031921
        vf_loss: 0.5229813183347384
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.733333333333334
    gpu_util_percent0: 0.3203333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674450693698005
    mean_env_wait_ms: 1.2266390401630796
    mean_inference_ms: 4.309240837488166
    mean_raw_obs_processing_ms: 0.3793030893973874
  time_since_restore: 4325.856154203415
  time_this_iter_s: 25.624305725097656
  time_total_s: 4325.856154203415
  timers:
    learn_throughput: 8675.208
    learn_time_ms: 18649.927
    sample_throughput: 23910.071
    sample_time_ms: 6766.688
    update_time_ms: 32.589
  timestamp: 1602780903
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: f0f97_00000
  
2020-10-15 16:55:03,703	WARNING util.py:136 -- The `process_trial` operation took 0.505507230758667 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    169 |          4325.86 | 27342848 |  281.621 |              321.404 |              126.556 |            792.586 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3264.8830113373438
    time_step_min: 3003
  date: 2020-10-15_16-55-29
  done: false
  episode_len_mean: 792.5007798509619
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 281.80732663122427
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 34622
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479114197289397e-32
        cur_lr: 5.0e-05
        entropy: 0.08075567955772082
        entropy_coeff: 0.0005000000000000001
        kl: 0.004084477802583327
        model: {}
        policy_loss: -0.006035297688261683
        total_loss: 0.5354347676038742
        vf_explained_var: 0.9985582232475281
        vf_loss: 0.5415104577938715
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333335
    gpu_util_percent0: 0.38466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674028822017826
    mean_env_wait_ms: 1.226616353793766
    mean_inference_ms: 4.3089384983116865
    mean_raw_obs_processing_ms: 0.3792886179385251
  time_since_restore: 4351.38649725914
  time_this_iter_s: 25.530343055725098
  time_total_s: 4351.38649725914
  timers:
    learn_throughput: 8672.118
    learn_time_ms: 18656.572
    sample_throughput: 23878.076
    sample_time_ms: 6775.755
    update_time_ms: 31.895
  timestamp: 1602780929
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    170 |          4351.39 | 27504640 |  281.807 |              321.404 |              126.556 |            792.501 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3263.4596217956087
    time_step_min: 3003
  date: 2020-10-15_16-55-55
  done: false
  episode_len_mean: 792.4229952356352
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 282.0147410039777
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 34842
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.395570986446985e-33
        cur_lr: 5.0e-05
        entropy: 0.08660764185090859
        entropy_coeff: 0.0005000000000000001
        kl: 0.003954541132164498
        model: {}
        policy_loss: -0.0067645619734927704
        total_loss: 0.8017760068178177
        vf_explained_var: 0.9981618523597717
        vf_loss: 0.8085838754971822
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89666666666667
    gpu_util_percent0: 0.37099999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673619238979435
    mean_env_wait_ms: 1.226587353443326
    mean_inference_ms: 4.3086019191617595
    mean_raw_obs_processing_ms: 0.3792735544595954
  time_since_restore: 4376.864995956421
  time_this_iter_s: 25.478498697280884
  time_total_s: 4376.864995956421
  timers:
    learn_throughput: 8696.796
    learn_time_ms: 18603.634
    sample_throughput: 23870.078
    sample_time_ms: 6778.026
    update_time_ms: 30.292
  timestamp: 1602780955
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: f0f97_00000
  
2020-10-15 16:55:55,992	WARNING util.py:136 -- The `process_trial` operation took 0.5068309307098389 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    171 |          4376.86 | 27666432 |  282.015 |              321.404 |              126.556 |            792.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3262.2499357234683
    time_step_min: 3003
  date: 2020-10-15_16-56-21
  done: false
  episode_len_mean: 792.3482639582323
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 282.2016011877638
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 209
  episodes_total: 35051
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.697785493223493e-33
        cur_lr: 5.0e-05
        entropy: 0.08354515085617702
        entropy_coeff: 0.0005000000000000001
        kl: 0.004195810935925692
        model: {}
        policy_loss: -0.007593475476217766
        total_loss: 0.7048556605974833
        vf_explained_var: 0.998309314250946
        vf_loss: 0.7124909063180288
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.450000000000003
    gpu_util_percent0: 0.37733333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673160799800875
    mean_env_wait_ms: 1.2265637048774858
    mean_inference_ms: 4.308285616851478
    mean_raw_obs_processing_ms: 0.3792597893811775
  time_since_restore: 4402.659961223602
  time_this_iter_s: 25.794965267181396
  time_total_s: 4402.659961223602
  timers:
    learn_throughput: 8702.516
    learn_time_ms: 18591.405
    sample_throughput: 23821.929
    sample_time_ms: 6791.725
    update_time_ms: 28.034
  timestamp: 1602780981
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    172 |          4402.66 | 27828224 |  282.202 |              321.404 |              126.556 |            792.348 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3261.0132109778965
    time_step_min: 3003
  date: 2020-10-15_16-56-48
  done: false
  episode_len_mean: 792.284757689252
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 282.3875174970676
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 35244
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8488927466117464e-33
        cur_lr: 5.0e-05
        entropy: 0.0730723887681961
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006335571830277331
        total_loss: .inf
        vf_explained_var: 0.9991880059242249
        vf_loss: 0.3078925932447116
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01666666666667
    gpu_util_percent0: 0.31566666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467274817654569
    mean_env_wait_ms: 1.2265402611995408
    mean_inference_ms: 4.307999496082577
    mean_raw_obs_processing_ms: 0.37924610801806663
  time_since_restore: 4428.302681684494
  time_this_iter_s: 25.642720460891724
  time_total_s: 4428.302681684494
  timers:
    learn_throughput: 8703.265
    learn_time_ms: 18589.805
    sample_throughput: 23783.038
    sample_time_ms: 6802.831
    update_time_ms: 27.834
  timestamp: 1602781008
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    173 |           4428.3 | 27990016 |  282.388 |              321.404 |              126.556 |            792.285 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3259.7119252061916
    time_step_min: 3003
  date: 2020-10-15_16-57-14
  done: false
  episode_len_mean: 792.2224541607899
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 282.58275989799216
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 35450
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7733391199176204e-33
        cur_lr: 5.0e-05
        entropy: 0.07364694215357304
        entropy_coeff: 0.0005000000000000001
        kl: 0.003014615853317082
        model: {}
        policy_loss: -0.006805170637865861
        total_loss: 0.3996496647596359
        vf_explained_var: 0.9990543723106384
        vf_loss: 0.4064916620651881
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.846666666666668
    gpu_util_percent0: 0.32966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672383347191625
    mean_env_wait_ms: 1.2265104195209504
    mean_inference_ms: 4.30769630962777
    mean_raw_obs_processing_ms: 0.37923214456791027
  time_since_restore: 4453.72732591629
  time_this_iter_s: 25.424644231796265
  time_total_s: 4453.72732591629
  timers:
    learn_throughput: 8719.308
    learn_time_ms: 18555.6
    sample_throughput: 23739.385
    sample_time_ms: 6815.341
    update_time_ms: 28.903
  timestamp: 1602781034
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: f0f97_00000
  
2020-10-15 16:57:14,799	WARNING util.py:136 -- The `process_trial` operation took 0.5121774673461914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    174 |          4453.73 | 28151808 |  282.583 |              321.404 |              126.556 |            792.222 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3258.3355789473685
    time_step_min: 3003
  date: 2020-10-15_16-57-40
  done: false
  episode_len_mean: 792.1543831123322
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 282.7872325905457
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 35671
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3866695599588102e-33
        cur_lr: 5.0e-05
        entropy: 0.07771228874723117
        entropy_coeff: 0.0005000000000000001
        kl: 0.004042199075532456
        model: {}
        policy_loss: -0.00895907655043023
        total_loss: 0.515070728957653
        vf_explained_var: 0.9987180233001709
        vf_loss: 0.5240686436494192
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.796666666666667
    gpu_util_percent0: 0.3313333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671923060759118
    mean_env_wait_ms: 1.2264828790827442
    mean_inference_ms: 4.307374445102848
    mean_raw_obs_processing_ms: 0.37921825225052547
  time_since_restore: 4479.176945447922
  time_this_iter_s: 25.44961953163147
  time_total_s: 4479.176945447922
  timers:
    learn_throughput: 8724.052
    learn_time_ms: 18545.511
    sample_throughput: 23704.971
    sample_time_ms: 6825.235
    update_time_ms: 36.886
  timestamp: 1602781060
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: f0f97_00000
  
2020-10-15 16:57:40,939	WARNING util.py:136 -- The `process_trial` operation took 0.5279698371887207 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    175 |          4479.18 | 28313600 |  282.787 |              321.404 |              126.556 |            792.154 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3257.2060350602947
    time_step_min: 3003
  date: 2020-10-15_16-58-06
  done: false
  episode_len_mean: 792.098661834402
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 282.95962834365355
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 35870
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.933347799794051e-34
        cur_lr: 5.0e-05
        entropy: 0.07883590459823608
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007883050653617829
        total_loss: .inf
        vf_explained_var: 0.9979570508003235
        vf_loss: 0.8000804881254832
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.206666666666663
    gpu_util_percent0: 0.34133333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671505789384914
    mean_env_wait_ms: 1.2264569051788519
    mean_inference_ms: 4.307092176199521
    mean_raw_obs_processing_ms: 0.3792046828915064
  time_since_restore: 4504.569169282913
  time_this_iter_s: 25.392223834991455
  time_total_s: 4504.569169282913
  timers:
    learn_throughput: 8756.062
    learn_time_ms: 18477.712
    sample_throughput: 23678.646
    sample_time_ms: 6832.823
    update_time_ms: 37.967
  timestamp: 1602781086
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 16:58:07,071	WARNING util.py:136 -- The `process_trial` operation took 0.5266845226287842 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    176 |          4504.57 | 28475392 |   282.96 |              321.404 |              126.556 |            792.099 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3256.059853974071
    time_step_min: 3003
  date: 2020-10-15_16-58-32
  done: false
  episode_len_mean: 792.0382343970942
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 283.1297260177672
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 36067
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0400021699691073e-33
        cur_lr: 5.0e-05
        entropy: 0.07608880971868832
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036455089187559984
        model: {}
        policy_loss: -0.006746840689932772
        total_loss: 0.6389716466267904
        vf_explained_var: 0.9984995722770691
        vf_loss: 0.6457565178473791
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03333333333334
    gpu_util_percent0: 0.31300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671154949444565
    mean_env_wait_ms: 1.2264282558630102
    mean_inference_ms: 4.306810229652268
    mean_raw_obs_processing_ms: 0.37919147060390884
  time_since_restore: 4530.438472509384
  time_this_iter_s: 25.869303226470947
  time_total_s: 4530.438472509384
  timers:
    learn_throughput: 8749.749
    learn_time_ms: 18491.045
    sample_throughput: 23677.96
    sample_time_ms: 6833.021
    update_time_ms: 35.637
  timestamp: 1602781112
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: f0f97_00000
  
2020-10-15 16:58:33,712	WARNING util.py:136 -- The `process_trial` operation took 0.553657054901123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    177 |          4530.44 | 28637184 |   283.13 |              321.404 |              126.556 |            792.038 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3254.7212933840974
    time_step_min: 3003
  date: 2020-10-15_16-58-59
  done: false
  episode_len_mean: 791.97779124876
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 283.3278427947497
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 36292
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.200010849845536e-34
        cur_lr: 5.0e-05
        entropy: 0.07618541891376178
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008402584644500166
        total_loss: .inf
        vf_explained_var: 0.9987444281578064
        vf_loss: 0.537596066792806
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670719772504545
    mean_env_wait_ms: 1.226396231996709
    mean_inference_ms: 4.306498090091862
    mean_raw_obs_processing_ms: 0.37917770011560115
  time_since_restore: 4555.777751922607
  time_this_iter_s: 25.339279413223267
  time_total_s: 4555.777751922607
  timers:
    learn_throughput: 8758.632
    learn_time_ms: 18472.292
    sample_throughput: 23690.463
    sample_time_ms: 6829.415
    update_time_ms: 37.533
  timestamp: 1602781139
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    178 |          4555.78 | 28798976 |  283.328 |              321.404 |              126.556 |            791.978 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3253.6012731163914
    time_step_min: 3003
  date: 2020-10-15_16-59-25
  done: false
  episode_len_mean: 791.9240655486134
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 283.49241842960976
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 36492
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.800016274768305e-34
        cur_lr: 5.0e-05
        entropy: 0.0726606696844101
        entropy_coeff: 0.0005000000000000001
        kl: 0.003194695746060461
        model: {}
        policy_loss: -0.007500521785307986
        total_loss: 0.6184717814127604
        vf_explained_var: 0.9984256625175476
        vf_loss: 0.6260086347659429
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.943333333333335
    gpu_util_percent0: 0.3453333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670318539651322
    mean_env_wait_ms: 1.2263687907288379
    mean_inference_ms: 4.306226311760335
    mean_raw_obs_processing_ms: 0.3791649704982329
  time_since_restore: 4581.390964269638
  time_this_iter_s: 25.61321234703064
  time_total_s: 4581.390964269638
  timers:
    learn_throughput: 8757.868
    learn_time_ms: 18473.903
    sample_throughput: 23699.834
    sample_time_ms: 6826.715
    update_time_ms: 37.074
  timestamp: 1602781165
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    179 |          4581.39 | 28960768 |  283.492 |              321.404 |              126.556 |            791.924 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3252.521752183406
    time_step_min: 3003
  date: 2020-10-15_16-59-51
  done: false
  episode_len_mean: 791.8726217085537
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 283.6627544044265
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 36686
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9000081373841524e-34
        cur_lr: 5.0e-05
        entropy: 0.07033026032149792
        entropy_coeff: 0.0005000000000000001
        kl: 0.003718740442612519
        model: {}
        policy_loss: -0.008817558918963186
        total_loss: 0.5569172178705534
        vf_explained_var: 0.9985693097114563
        vf_loss: 0.565769928197066
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07
    gpu_util_percent0: 0.33866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466994661441724
    mean_env_wait_ms: 1.2263392852755106
    mean_inference_ms: 4.305955259186037
    mean_raw_obs_processing_ms: 0.37915243298531154
  time_since_restore: 4606.817787408829
  time_this_iter_s: 25.426823139190674
  time_total_s: 4606.817787408829
  timers:
    learn_throughput: 8764.031
    learn_time_ms: 18460.912
    sample_throughput: 23724.332
    sample_time_ms: 6819.665
    update_time_ms: 46.23
  timestamp: 1602781191
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: f0f97_00000
  
2020-10-15 16:59:52,060	WARNING util.py:136 -- The `process_trial` operation took 0.5000104904174805 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    180 |          4606.82 | 29122560 |  283.663 |              321.404 |              126.556 |            791.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3251.214895983075
    time_step_min: 3003
  date: 2020-10-15_17-00-17
  done: false
  episode_len_mean: 791.8179060002709
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 283.857563854719
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 229
  episodes_total: 36915
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9500040686920762e-34
        cur_lr: 5.0e-05
        entropy: 0.07386558937529723
        entropy_coeff: 0.0005000000000000001
        kl: 0.003867123683448881
        model: {}
        policy_loss: -0.007623103049506123
        total_loss: 0.6034602224826813
        vf_explained_var: 0.9986987113952637
        vf_loss: 0.6111202736695608
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669533670877863
    mean_env_wait_ms: 1.2263031697376752
    mean_inference_ms: 4.30564620528836
    mean_raw_obs_processing_ms: 0.37913839067265875
  time_since_restore: 4632.566265821457
  time_this_iter_s: 25.748478412628174
  time_total_s: 4632.566265821457
  timers:
    learn_throughput: 8751.742
    learn_time_ms: 18486.834
    sample_throughput: 23754.732
    sample_time_ms: 6810.938
    update_time_ms: 54.317
  timestamp: 1602781217
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: f0f97_00000
  
2020-10-15 17:00:18,546	WARNING util.py:136 -- The `process_trial` operation took 0.5586962699890137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    181 |          4632.57 | 29284352 |  283.858 |              321.404 |              126.556 |            791.818 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3250.076012192161
    time_step_min: 3003
  date: 2020-10-15_17-00-44
  done: false
  episode_len_mean: 791.7651068186104
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 284.0285314961622
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 37119
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.750020343460381e-35
        cur_lr: 5.0e-05
        entropy: 0.06666345335543156
        entropy_coeff: 0.0005000000000000001
        kl: 0.003563488464957724
        model: {}
        policy_loss: -0.008788395822193706
        total_loss: 0.4589664414525032
        vf_explained_var: 0.9988223910331726
        vf_loss: 0.4677881772319476
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06666666666667
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669139170322001
    mean_env_wait_ms: 1.2262746446022048
    mean_inference_ms: 4.305379305073892
    mean_raw_obs_processing_ms: 0.3791260728015894
  time_since_restore: 4658.171093225479
  time_this_iter_s: 25.604827404022217
  time_total_s: 4658.171093225479
  timers:
    learn_throughput: 8756.408
    learn_time_ms: 18476.982
    sample_throughput: 23797.919
    sample_time_ms: 6798.578
    update_time_ms: 56.624
  timestamp: 1602781244
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    182 |          4658.17 | 29446144 |  284.029 |              321.404 |              126.556 |            791.765 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3248.962859596393
    time_step_min: 3003
  date: 2020-10-15_17-01-10
  done: false
  episode_len_mean: 791.7244974537657
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 284.19230498498746
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 37310
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.875010171730191e-35
        cur_lr: 5.0e-05
        entropy: 0.06284111707160871
        entropy_coeff: 0.0005000000000000001
        kl: 0.003446239590023955
        model: {}
        policy_loss: -0.007876161388897648
        total_loss: 0.27952119211355847
        vf_explained_var: 0.9992287755012512
        vf_loss: 0.2874287689725558
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.475862068965515
    gpu_util_percent0: 0.33482758620689657
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466880276664923
    mean_env_wait_ms: 1.2262443590841812
    mean_inference_ms: 4.305124074035848
    mean_raw_obs_processing_ms: 0.37911456894438
  time_since_restore: 4683.299819946289
  time_this_iter_s: 25.128726720809937
  time_total_s: 4683.299819946289
  timers:
    learn_throughput: 8775.133
    learn_time_ms: 18437.555
    sample_throughput: 23836.765
    sample_time_ms: 6787.498
    update_time_ms: 54.869
  timestamp: 1602781270
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: f0f97_00000
  
2020-10-15 17:01:10,766	WARNING util.py:136 -- The `process_trial` operation took 0.5176966190338135 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    183 |           4683.3 | 29607936 |  284.192 |              321.404 |              126.556 |            791.724 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3247.758177258417
    time_step_min: 3003
  date: 2020-10-15_17-01-36
  done: false
  episode_len_mean: 791.6773875506289
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 284.37466274340034
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 37528
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4375050858650953e-35
        cur_lr: 5.0e-05
        entropy: 0.0663527591774861
        entropy_coeff: 0.0005000000000000001
        kl: 0.003655326106430342
        model: {}
        policy_loss: -0.007164638462806276
        total_loss: 0.47021080801884335
        vf_explained_var: 0.9988657832145691
        vf_loss: 0.47740863015254337
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.535483870967738
    gpu_util_percent0: 0.3509677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466843274581882
    mean_env_wait_ms: 1.226208407545538
    mean_inference_ms: 4.3048386356607145
    mean_raw_obs_processing_ms: 0.3791019818323382
  time_since_restore: 4709.144691467285
  time_this_iter_s: 25.844871520996094
  time_total_s: 4709.144691467285
  timers:
    learn_throughput: 8752.585
    learn_time_ms: 18485.053
    sample_throughput: 23883.761
    sample_time_ms: 6774.143
    update_time_ms: 55.508
  timestamp: 1602781296
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: f0f97_00000
  
2020-10-15 17:01:37,386	WARNING util.py:136 -- The `process_trial` operation took 0.5158238410949707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    184 |          4709.14 | 29769728 |  284.375 |              321.404 |              126.556 |            791.677 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3246.6212229738694
    time_step_min: 3003
  date: 2020-10-15_17-02-03
  done: false
  episode_len_mean: 791.6336345088895
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 284.5473545234808
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 213
  episodes_total: 37741
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2187525429325476e-35
        cur_lr: 5.0e-05
        entropy: 0.06498776779820521
        entropy_coeff: 0.0005000000000000001
        kl: 0.00292502073959137
        model: {}
        policy_loss: -0.008328655415728766
        total_loss: 0.4254329353570938
        vf_explained_var: 0.9989354610443115
        vf_loss: 0.4337940861781438
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.263333333333335
    gpu_util_percent0: 0.3723333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668033624381194
    mean_env_wait_ms: 1.2261762007915045
    mean_inference_ms: 4.304562190043038
    mean_raw_obs_processing_ms: 0.37908973608222046
  time_since_restore: 4734.731864690781
  time_this_iter_s: 25.587173223495483
  time_total_s: 4734.731864690781
  timers:
    learn_throughput: 8741.01
    learn_time_ms: 18509.532
    sample_throughput: 23903.258
    sample_time_ms: 6768.617
    update_time_ms: 48.848
  timestamp: 1602781323
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: f0f97_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    185 |          4734.73 | 29931520 |  284.547 |              321.404 |              126.556 |            791.634 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3245.592968007602
    time_step_min: 3003
  date: 2020-10-15_17-02-29
  done: false
  episode_len_mean: 791.5980754020565
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 284.70687896630375
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 37930
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.093762714662738e-36
        cur_lr: 5.0e-05
        entropy: 0.0603233752772212
        entropy_coeff: 0.0005000000000000001
        kl: 0.004140537251563122
        model: {}
        policy_loss: -0.010309185447113123
        total_loss: 0.283783449480931
        vf_explained_var: 0.9992377758026123
        vf_loss: 0.2941227989892165
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.666666666666668
    gpu_util_percent0: 0.2933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667689240805282
    mean_env_wait_ms: 1.2261468292835764
    mean_inference_ms: 4.304322791211757
    mean_raw_obs_processing_ms: 0.3790781455977957
  time_since_restore: 4760.824367284775
  time_this_iter_s: 26.09250259399414
  time_total_s: 4760.824367284775
  timers:
    learn_throughput: 8708.435
    learn_time_ms: 18578.769
    sample_throughput: 23897.408
    sample_time_ms: 6770.274
    update_time_ms: 49.401
  timestamp: 1602781349
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: f0f97_00000
  
2020-10-15 17:02:30,515	WARNING util.py:136 -- The `process_trial` operation took 0.5348126888275146 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    186 |          4760.82 | 30093312 |  284.707 |              321.404 |              126.556 |            791.598 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3244.5820621209336
    time_step_min: 3003
  date: 2020-10-15_17-02-55
  done: false
  episode_len_mean: 791.5667007578737
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 284.8671746177052
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 38133
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.046881357331369e-36
        cur_lr: 5.0e-05
        entropy: 0.06583321467041969
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007623242204620813
        total_loss: .inf
        vf_explained_var: 0.9988188743591309
        vf_loss: 0.5024147828420004
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.980000000000004
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667393241122242
    mean_env_wait_ms: 1.2261111381030412
    mean_inference_ms: 4.304063254626322
    mean_raw_obs_processing_ms: 0.37906620373626165
  time_since_restore: 4786.272682666779
  time_this_iter_s: 25.448315382003784
  time_total_s: 4786.272682666779
  timers:
    learn_throughput: 8718.932
    learn_time_ms: 18556.4
    sample_throughput: 23977.175
    sample_time_ms: 6747.751
    update_time_ms: 49.894
  timestamp: 1602781375
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:02:56,706	WARNING util.py:136 -- The `process_trial` operation took 0.5609502792358398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    187 |          4786.27 | 30255104 |  284.867 |              321.404 |              126.556 |            791.567 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3243.3832319699304
    time_step_min: 3003
  date: 2020-10-15_17-03-22
  done: false
  episode_len_mean: 791.5397189561228
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.0446701812288
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 38357
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.570322035997055e-36
        cur_lr: 5.0e-05
        entropy: 0.06346751035501559
        entropy_coeff: 0.0005000000000000001
        kl: 0.003002208541147411
        model: {}
        policy_loss: -0.007585370369876425
        total_loss: 0.31321701407432556
        vf_explained_var: 0.9992441534996033
        vf_loss: 0.32083411514759064
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.696666666666665
    gpu_util_percent0: 0.2963333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666944267361026
    mean_env_wait_ms: 1.2260743269560512
    mean_inference_ms: 4.303783951377899
    mean_raw_obs_processing_ms: 0.379053812306828
  time_since_restore: 4812.064764022827
  time_this_iter_s: 25.792081356048584
  time_total_s: 4812.064764022827
  timers:
    learn_throughput: 8701.942
    learn_time_ms: 18592.631
    sample_throughput: 23945.168
    sample_time_ms: 6756.77
    update_time_ms: 49.461
  timestamp: 1602781402
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: f0f97_00000
  
2020-10-15 17:03:23,293	WARNING util.py:136 -- The `process_trial` operation took 0.5284793376922607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    188 |          4812.06 | 30416896 |  285.045 |              321.404 |              126.556 |             791.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3242.3719323759315
    time_step_min: 3003
  date: 2020-10-15_17-03-48
  done: false
  episode_len_mean: 791.5122558555754
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.19836840115374
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 38553
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2851610179985277e-36
        cur_lr: 5.0e-05
        entropy: 0.06433871792008479
        entropy_coeff: 0.0005000000000000001
        kl: 0.004996013323155542
        model: {}
        policy_loss: -0.006267695610101025
        total_loss: 0.384463037053744
        vf_explained_var: 0.9990239143371582
        vf_loss: 0.39076292018095654
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.483333333333334
    gpu_util_percent0: 0.3396666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666595537285992
    mean_env_wait_ms: 1.2260426755929337
    mean_inference_ms: 4.303544306159753
    mean_raw_obs_processing_ms: 0.3790423056138793
  time_since_restore: 4837.651093959808
  time_this_iter_s: 25.5863299369812
  time_total_s: 4837.651093959808
  timers:
    learn_throughput: 8697.435
    learn_time_ms: 18602.265
    sample_throughput: 23985.005
    sample_time_ms: 6745.548
    update_time_ms: 47.402
  timestamp: 1602781428
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: f0f97_00000
  
2020-10-15 17:03:49,663	WARNING util.py:136 -- The `process_trial` operation took 0.6019785404205322 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    189 |          4837.65 | 30578688 |  285.198 |              321.404 |              126.556 |            791.512 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3241.363382683513
    time_step_min: 3003
  date: 2020-10-15_17-04-14
  done: false
  episode_len_mean: 791.4859738315828
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.3476612364837
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 38749
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1425805089992638e-36
        cur_lr: 5.0e-05
        entropy: 0.06274431633452575
        entropy_coeff: 0.0005000000000000001
        kl: 0.003541909216437489
        model: {}
        policy_loss: -0.008236637688241899
        total_loss: 0.6590140014886856
        vf_explained_var: 0.9983348250389099
        vf_loss: 0.6672820200522741
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.320689655172416
    gpu_util_percent0: 0.3258620689655173
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666271741069375
    mean_env_wait_ms: 1.2260075624137823
    mean_inference_ms: 4.303303545275615
    mean_raw_obs_processing_ms: 0.3790303908373211
  time_since_restore: 4862.466207027435
  time_this_iter_s: 24.815113067626953
  time_total_s: 4862.466207027435
  timers:
    learn_throughput: 8719.802
    learn_time_ms: 18554.551
    sample_throughput: 24002.405
    sample_time_ms: 6740.658
    update_time_ms: 38.491
  timestamp: 1602781454
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: f0f97_00000
  
2020-10-15 17:04:15,288	WARNING util.py:136 -- The `process_trial` operation took 0.5684347152709961 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    190 |          4862.47 | 30740480 |  285.348 |              321.404 |              126.556 |            791.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3240.2086159062887
    time_step_min: 3003
  date: 2020-10-15_17-04-41
  done: false
  episode_len_mean: 791.4602298968543
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.51920809158923
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 38974
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.712902544996319e-37
        cur_lr: 5.0e-05
        entropy: 0.06740726033846538
        entropy_coeff: 0.0005000000000000001
        kl: 0.005897888331674039
        model: {}
        policy_loss: -0.007097884904699943
        total_loss: 0.4994298368692398
        vf_explained_var: 0.9988844990730286
        vf_loss: 0.5065614283084869
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.519999999999996
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466591753537331
    mean_env_wait_ms: 1.2259661519687484
    mean_inference_ms: 4.30303124851464
    mean_raw_obs_processing_ms: 0.3790183222877824
  time_since_restore: 4888.194422006607
  time_this_iter_s: 25.728214979171753
  time_total_s: 4888.194422006607
  timers:
    learn_throughput: 8718.654
    learn_time_ms: 18556.992
    sample_throughput: 23984.881
    sample_time_ms: 6745.583
    update_time_ms: 30.673
  timestamp: 1602781481
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: f0f97_00000
  
2020-10-15 17:04:41,857	WARNING util.py:136 -- The `process_trial` operation took 0.5793161392211914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    191 |          4888.19 | 30902272 |  285.519 |              321.404 |              126.556 |             791.46 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3239.324789164324
    time_step_min: 3003
  date: 2020-10-15_17-05-07
  done: false
  episode_len_mean: 791.4360067388197
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.64703704391223
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 39176
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.712902544996319e-37
        cur_lr: 5.0e-05
        entropy: 0.07927431414524715
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039291425491683185
        model: {}
        policy_loss: -0.011053793175960891
        total_loss: 1.3150712549686432
        vf_explained_var: 0.996860921382904
        vf_loss: 1.326164702574412
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.523333333333337
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665556750735576
    mean_env_wait_ms: 1.2259330102300117
    mean_inference_ms: 4.302797227078703
    mean_raw_obs_processing_ms: 0.3790070829356821
  time_since_restore: 4913.60830616951
  time_this_iter_s: 25.413884162902832
  time_total_s: 4913.60830616951
  timers:
    learn_throughput: 8721.975
    learn_time_ms: 18549.927
    sample_throughput: 24019.697
    sample_time_ms: 6735.805
    update_time_ms: 28.53
  timestamp: 1602781507
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: f0f97_00000
  
2020-10-15 17:05:07,961	WARNING util.py:136 -- The `process_trial` operation took 0.5093994140625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    192 |          4913.61 | 31064064 |  285.647 |              321.404 |              126.556 |            791.436 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3238.657104346278
    time_step_min: 3003
  date: 2020-10-15_17-05-33
  done: false
  episode_len_mean: 791.417786470902
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.7556431539204
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 39367
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8564512724981596e-37
        cur_lr: 5.0e-05
        entropy: 0.07954894999663036
        entropy_coeff: 0.0005000000000000001
        kl: 0.005001972468259434
        model: {}
        policy_loss: -0.009624527582976347
        total_loss: 1.2438077926635742
        vf_explained_var: 0.9969227910041809
        vf_loss: 1.253472109635671
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92666666666667
    gpu_util_percent0: 0.3186666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665236305994675
    mean_env_wait_ms: 1.2258970447677133
    mean_inference_ms: 4.302568925311171
    mean_raw_obs_processing_ms: 0.37899610022817715
  time_since_restore: 4939.0907163619995
  time_this_iter_s: 25.482410192489624
  time_total_s: 4939.0907163619995
  timers:
    learn_throughput: 8702.952
    learn_time_ms: 18590.475
    sample_throughput: 24040.147
    sample_time_ms: 6730.075
    update_time_ms: 28.238
  timestamp: 1602781533
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: f0f97_00000
  
2020-10-15 17:05:34,232	WARNING util.py:136 -- The `process_trial` operation took 0.6070041656494141 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    193 |          4939.09 | 31225856 |  285.756 |              321.404 |              126.556 |            791.418 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3237.6950254166563
    time_step_min: 3003
  date: 2020-10-15_17-05-59
  done: false
  episode_len_mean: 791.4050824765706
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 285.9065400767975
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 39587
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8564512724981596e-37
        cur_lr: 5.0e-05
        entropy: 0.06623440546294053
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00823872744028146
        total_loss: .inf
        vf_explained_var: 0.9986228346824646
        vf_loss: 0.5990230043729147
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39666666666666
    gpu_util_percent0: 0.3543333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466491099727181
    mean_env_wait_ms: 1.2258559044139814
    mean_inference_ms: 4.30231320861786
    mean_raw_obs_processing_ms: 0.3789846091345477
  time_since_restore: 4964.744471788406
  time_this_iter_s: 25.65375542640686
  time_total_s: 4964.744471788406
  timers:
    learn_throughput: 8711.948
    learn_time_ms: 18571.278
    sample_throughput: 24011.626
    sample_time_ms: 6738.069
    update_time_ms: 27.289
  timestamp: 1602781559
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:06:00,666	WARNING util.py:136 -- The `process_trial` operation took 0.5961999893188477 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    194 |          4964.74 | 31387648 |  285.907 |              321.404 |              126.556 |            791.405 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3236.6639579349903
    time_step_min: 3003
  date: 2020-10-15_17-06-26
  done: false
  episode_len_mean: 791.3940292506408
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.0612827780235
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 39794
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2846769087472375e-37
        cur_lr: 5.0e-05
        entropy: 0.05945739926149448
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038391437847167253
        model: {}
        policy_loss: -0.00787193966001117
        total_loss: 0.42396748811006546
        vf_explained_var: 0.9989607334136963
        vf_loss: 0.43186913679043454
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333333
    gpu_util_percent0: 0.35666666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664538002999497
    mean_env_wait_ms: 1.2258187194540304
    mean_inference_ms: 4.302074094091953
    mean_raw_obs_processing_ms: 0.378973521983467
  time_since_restore: 4990.194588661194
  time_this_iter_s: 25.450116872787476
  time_total_s: 4990.194588661194
  timers:
    learn_throughput: 8715.371
    learn_time_ms: 18563.984
    sample_throughput: 24030.5
    sample_time_ms: 6732.777
    update_time_ms: 25.689
  timestamp: 1602781586
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: f0f97_00000
  
2020-10-15 17:06:26,818	WARNING util.py:136 -- The `process_trial` operation took 0.5188207626342773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    195 |          4990.19 | 31549440 |  286.061 |              321.404 |              126.556 |            791.394 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3235.7165139452204
    time_step_min: 3003
  date: 2020-10-15_17-06-52
  done: false
  episode_len_mean: 791.3813143943183
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.20185474834324
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 39988
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1423384543736187e-37
        cur_lr: 5.0e-05
        entropy: 0.06370018081118663
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057258746431519585
        model: {}
        policy_loss: -0.006009418497342267
        total_loss: 0.400497908393542
        vf_explained_var: 0.9989694952964783
        vf_loss: 0.4065391669670741
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.933333333333334
    gpu_util_percent0: 0.3363333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664217214202813
    mean_env_wait_ms: 1.2257838903728104
    mean_inference_ms: 4.301848968743895
    mean_raw_obs_processing_ms: 0.37896264677804914
  time_since_restore: 5015.711579322815
  time_this_iter_s: 25.516990661621094
  time_total_s: 5015.711579322815
  timers:
    learn_throughput: 8737.205
    learn_time_ms: 18517.592
    sample_throughput: 24077.792
    sample_time_ms: 6719.553
    update_time_ms: 32.189
  timestamp: 1602781612
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: f0f97_00000
  
2020-10-15 17:06:53,030	WARNING util.py:136 -- The `process_trial` operation took 0.5092360973358154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    196 |          5015.71 | 31711232 |  286.202 |              321.404 |              126.556 |            791.381 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3234.781602610407
    time_step_min: 3003
  date: 2020-10-15_17-07-18
  done: false
  episode_len_mean: 791.3656109272758
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.3280773801756
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 40193
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1423384543736187e-37
        cur_lr: 5.0e-05
        entropy: 0.09047373694678147
        entropy_coeff: 0.0005000000000000001
        kl: 0.009895355983947715
        model: {}
        policy_loss: -0.011052656957569221
        total_loss: 1.2139445940653484
        vf_explained_var: 0.9970839619636536
        vf_loss: 1.2250424822171528
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.060000000000002
    gpu_util_percent0: 0.3550000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663925946775758
    mean_env_wait_ms: 1.2257437988351658
    mean_inference_ms: 4.301619489317935
    mean_raw_obs_processing_ms: 0.3789512649799234
  time_since_restore: 5041.524050712585
  time_this_iter_s: 25.812471389770508
  time_total_s: 5041.524050712585
  timers:
    learn_throughput: 8732.418
    learn_time_ms: 18527.744
    sample_throughput: 23978.939
    sample_time_ms: 6747.254
    update_time_ms: 31.508
  timestamp: 1602781638
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: f0f97_00000
  
2020-10-15 17:07:19,550	WARNING util.py:136 -- The `process_trial` operation took 0.5222866535186768 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    197 |          5041.52 | 31873024 |  286.328 |              321.404 |              126.556 |            791.366 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3234.0632974111236
    time_step_min: 3003
  date: 2020-10-15_17-07-44
  done: false
  episode_len_mean: 791.3455989705773
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.43186061200913
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 40411
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1423384543736187e-37
        cur_lr: 5.0e-05
        entropy: 0.08616747210423152
        entropy_coeff: 0.0005000000000000001
        kl: 0.004114967984302591
        model: {}
        policy_loss: -0.010422143386676908
        total_loss: 1.668423593044281
        vf_explained_var: 0.9963147640228271
        vf_loss: 1.678888847430547
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05666666666667
    gpu_util_percent0: 0.37533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663530713992093
    mean_env_wait_ms: 1.2257033752949265
    mean_inference_ms: 4.301370666965565
    mean_raw_obs_processing_ms: 0.37894049044380473
  time_since_restore: 5066.938964128494
  time_this_iter_s: 25.414913415908813
  time_total_s: 5066.938964128494
  timers:
    learn_throughput: 8747.454
    learn_time_ms: 18495.897
    sample_throughput: 24014.856
    sample_time_ms: 6737.163
    update_time_ms: 31.184
  timestamp: 1602781664
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: f0f97_00000
  
2020-10-15 17:07:45,772	WARNING util.py:136 -- The `process_trial` operation took 0.5926566123962402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    198 |          5066.94 | 32034816 |  286.432 |              321.404 |              126.556 |            791.346 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3233.279554251621
    time_step_min: 3003
  date: 2020-10-15_17-08-11
  done: false
  episode_len_mean: 791.3314699436058
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.5512198349635
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 40607
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0711692271868094e-37
        cur_lr: 5.0e-05
        entropy: 0.07165335988005002
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037021689931862056
        model: {}
        policy_loss: -0.009811137317835042
        total_loss: 0.8237588802973429
        vf_explained_var: 0.997944176197052
        vf_loss: 0.8336058209339777
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05666666666667
    gpu_util_percent0: 0.34866666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466322125124724
    mean_env_wait_ms: 1.2256684601403005
    mean_inference_ms: 4.301156730847014
    mean_raw_obs_processing_ms: 0.3789298478409162
  time_since_restore: 5092.677208662033
  time_this_iter_s: 25.73824453353882
  time_total_s: 5092.677208662033
  timers:
    learn_throughput: 8742.744
    learn_time_ms: 18505.861
    sample_throughput: 24009.541
    sample_time_ms: 6738.654
    update_time_ms: 33.046
  timestamp: 1602781691
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: f0f97_00000
  
2020-10-15 17:08:12,332	WARNING util.py:136 -- The `process_trial` operation took 0.5297303199768066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    199 |          5092.68 | 32196608 |  286.551 |              321.404 |              126.556 |            791.331 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3232.372837688627
    time_step_min: 3003
  date: 2020-10-15_17-08-38
  done: false
  episode_len_mean: 791.3268302247494
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.6867949116909
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 40801
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.355846135934047e-38
        cur_lr: 5.0e-05
        entropy: 0.06443341417858998
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038895849720574915
        model: {}
        policy_loss: -0.008340268075698987
        total_loss: 0.4631549268960953
        vf_explained_var: 0.9988529086112976
        vf_loss: 0.47152741998434067
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25806451612903
    gpu_util_percent0: 0.28903225806451616
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466292936090929
    mean_env_wait_ms: 1.225629969664936
    mean_inference_ms: 4.300941029933298
    mean_raw_obs_processing_ms: 0.37891931657417693
  time_since_restore: 5118.497630357742
  time_this_iter_s: 25.82042169570923
  time_total_s: 5118.497630357742
  timers:
    learn_throughput: 8702.228
    learn_time_ms: 18592.021
    sample_throughput: 23970.49
    sample_time_ms: 6749.633
    update_time_ms: 34.527
  timestamp: 1602781718
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: f0f97_00000
  
2020-10-15 17:08:39,066	WARNING util.py:136 -- The `process_trial` operation took 0.5621864795684814 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    200 |           5118.5 | 32358400 |  286.687 |              321.404 |              126.556 |            791.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3231.3383278832544
    time_step_min: 3003
  date: 2020-10-15_17-09-04
  done: false
  episode_len_mean: 791.320641575663
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.84318836768574
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 41024
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6779230679670234e-38
        cur_lr: 5.0e-05
        entropy: 0.06722097347180049
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031346545826333263
        model: {}
        policy_loss: -0.008162334580750516
        total_loss: 0.6046770115693411
        vf_explained_var: 0.9985793232917786
        vf_loss: 0.6128729532162348
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.983333333333338
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662604847446287
    mean_env_wait_ms: 1.2255835767247978
    mean_inference_ms: 4.300699168265554
    mean_raw_obs_processing_ms: 0.37890815705504494
  time_since_restore: 5144.224279642105
  time_this_iter_s: 25.726649284362793
  time_total_s: 5144.224279642105
  timers:
    learn_throughput: 8709.252
    learn_time_ms: 18577.026
    sample_throughput: 23925.77
    sample_time_ms: 6762.249
    update_time_ms: 34.593
  timestamp: 1602781744
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: f0f97_00000
  
2020-10-15 17:09:05,598	WARNING util.py:136 -- The `process_trial` operation took 0.6155223846435547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    201 |          5144.22 | 32520192 |  286.843 |              321.404 |              126.556 |            791.321 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3230.4595454324703
    time_step_min: 3003
  date: 2020-10-15_17-09-31
  done: false
  episode_len_mean: 791.3044290288153
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 286.97901372314345
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 41228
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3389615339835117e-38
        cur_lr: 5.0e-05
        entropy: 0.06638453155755997
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008657409518491477
        total_loss: .inf
        vf_explained_var: 0.9986028671264648
        vf_loss: 0.5648118605216345
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.466666666666672
    gpu_util_percent0: 0.3733333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662257764612213
    mean_env_wait_ms: 1.225547365250659
    mean_inference_ms: 4.30048561426665
    mean_raw_obs_processing_ms: 0.37889780286123376
  time_since_restore: 5169.781305551529
  time_this_iter_s: 25.557025909423828
  time_total_s: 5169.781305551529
  timers:
    learn_throughput: 8708.96
    learn_time_ms: 18577.648
    sample_throughput: 23883.025
    sample_time_ms: 6774.351
    update_time_ms: 34.825
  timestamp: 1602781771
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:09:31,913	WARNING util.py:136 -- The `process_trial` operation took 0.5660560131072998 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    202 |          5169.78 | 32681984 |  286.979 |              321.404 |              126.556 |            791.304 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3229.7412563749303
    time_step_min: 3003
  date: 2020-10-15_17-09-57
  done: false
  episode_len_mean: 791.2748738501654
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.08062200507646
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 41419
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0084423009752674e-38
        cur_lr: 5.0e-05
        entropy: 0.0807157593468825
        entropy_coeff: 0.0005000000000000001
        kl: 0.003873328872335454
        model: {}
        policy_loss: -0.007333730599687745
        total_loss: 1.5704054335753124
        vf_explained_var: 0.996221125125885
        vf_loss: 1.5777794818083446
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.383333333333333
    gpu_util_percent0: 0.33133333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661967816442356
    mean_env_wait_ms: 1.2255086101179602
    mean_inference_ms: 4.300279637807006
    mean_raw_obs_processing_ms: 0.3788878074999519
  time_since_restore: 5195.346874237061
  time_this_iter_s: 25.565568685531616
  time_total_s: 5195.346874237061
  timers:
    learn_throughput: 8710.553
    learn_time_ms: 18574.25
    sample_throughput: 23850.617
    sample_time_ms: 6783.556
    update_time_ms: 36.287
  timestamp: 1602781797
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: f0f97_00000
  
2020-10-15 17:09:58,294	WARNING util.py:136 -- The `process_trial` operation took 0.6262784004211426 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    203 |          5195.35 | 32843776 |  287.081 |              321.404 |              126.556 |            791.275 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3228.9314821492967
    time_step_min: 3003
  date: 2020-10-15_17-10-23
  done: false
  episode_len_mean: 791.2439662832305
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.2087179035614
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 222
  episodes_total: 41641
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0042211504876337e-38
        cur_lr: 5.0e-05
        entropy: 0.0690913653622071
        entropy_coeff: 0.0005000000000000001
        kl: 0.003300030599348247
        model: {}
        policy_loss: -0.008133496060812226
        total_loss: 1.108037938674291
        vf_explained_var: 0.9974381327629089
        vf_loss: 1.116205980380376
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.1
    gpu_util_percent0: 0.33366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466163905544572
    mean_env_wait_ms: 1.2254635807881835
    mean_inference_ms: 4.300045569787674
    mean_raw_obs_processing_ms: 0.37887660211294494
  time_since_restore: 5220.893342494965
  time_this_iter_s: 25.546468257904053
  time_total_s: 5220.893342494965
  timers:
    learn_throughput: 8711.398
    learn_time_ms: 18572.449
    sample_throughput: 23893.773
    sample_time_ms: 6771.304
    update_time_ms: 36.846
  timestamp: 1602781823
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: f0f97_00000
  
2020-10-15 17:10:24,605	WARNING util.py:136 -- The `process_trial` operation took 0.5672566890716553 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    204 |          5220.89 | 33005568 |  287.209 |              321.404 |              126.556 |            791.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3228.0074876800154
    time_step_min: 3003
  date: 2020-10-15_17-10-50
  done: false
  episode_len_mean: 791.2351127891417
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.344822484064
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 41848
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.0211057524381686e-39
        cur_lr: 5.0e-05
        entropy: 0.058379243748883404
        entropy_coeff: 0.0005000000000000001
        kl: 0.002707810723222792
        model: {}
        policy_loss: -0.008417612300642455
        total_loss: 0.4225205183029175
        vf_explained_var: 0.9989780783653259
        vf_loss: 0.4309673185149829
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.474193548387095
    gpu_util_percent0: 0.2961290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466132235024129
    mean_env_wait_ms: 1.2254232910209457
    mean_inference_ms: 4.299830946705019
    mean_raw_obs_processing_ms: 0.37886698381709494
  time_since_restore: 5246.522190332413
  time_this_iter_s: 25.62884783744812
  time_total_s: 5246.522190332413
  timers:
    learn_throughput: 8702.626
    learn_time_ms: 18591.17
    sample_throughput: 23911.665
    sample_time_ms: 6766.237
    update_time_ms: 38.927
  timestamp: 1602781850
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: f0f97_00000
  
2020-10-15 17:10:51,252	WARNING util.py:136 -- The `process_trial` operation took 0.65431809425354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    205 |          5246.52 | 33167360 |  287.345 |              321.404 |              126.556 |            791.235 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3227.1835782154167
    time_step_min: 3003
  date: 2020-10-15_17-11-16
  done: false
  episode_len_mean: 791.2253621637051
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.46787578921976
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 42039
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5105528762190843e-39
        cur_lr: 5.0e-05
        entropy: 0.057509212754666805
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035268287950505814
        model: {}
        policy_loss: -0.007863766804803163
        total_loss: 0.5967360635598501
        vf_explained_var: 0.9984598755836487
        vf_loss: 0.6046285877625147
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.643333333333334
    gpu_util_percent0: 0.3016666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661040821637664
    mean_env_wait_ms: 1.225386594281064
    mean_inference_ms: 4.299631784645602
    mean_raw_obs_processing_ms: 0.3788573353250529
  time_since_restore: 5272.171684026718
  time_this_iter_s: 25.64949369430542
  time_total_s: 5272.171684026718
  timers:
    learn_throughput: 8686.077
    learn_time_ms: 18626.59
    sample_throughput: 23969.786
    sample_time_ms: 6749.831
    update_time_ms: 30.913
  timestamp: 1602781876
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: f0f97_00000
  
2020-10-15 17:11:17,645	WARNING util.py:136 -- The `process_trial` operation took 0.5516769886016846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    206 |          5272.17 | 33329152 |  287.468 |              321.404 |              126.556 |            791.225 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3226.30305471953
    time_step_min: 3003
  date: 2020-10-15_17-11-43
  done: false
  episode_len_mean: 791.2269961887176
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.6016711393455
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 42243
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2552764381095421e-39
        cur_lr: 5.0e-05
        entropy: 0.05760847652951876
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036262277668962875
        model: {}
        policy_loss: -0.007854295773237633
        total_loss: 0.4825543041030566
        vf_explained_var: 0.9988073706626892
        vf_loss: 0.4904374033212662
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000006
    gpu_util_percent0: 0.32133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660756577900333
    mean_env_wait_ms: 1.2253426851555078
    mean_inference_ms: 4.299424330257191
    mean_raw_obs_processing_ms: 0.37884684124259294
  time_since_restore: 5297.821848630905
  time_this_iter_s: 25.65016460418701
  time_total_s: 5297.821848630905
  timers:
    learn_throughput: 8690.602
    learn_time_ms: 18616.892
    sample_throughput: 24005.924
    sample_time_ms: 6739.67
    update_time_ms: 31.841
  timestamp: 1602781903
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: f0f97_00000
  
2020-10-15 17:11:44,102	WARNING util.py:136 -- The `process_trial` operation took 0.6030795574188232 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    207 |          5297.82 | 33490944 |  287.602 |              321.404 |              126.556 |            791.227 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3225.325844021124
    time_step_min: 3003
  date: 2020-10-15_17-12-09
  done: false
  episode_len_mean: 791.2331731901464
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.75091668415064
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 219
  episodes_total: 42462
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.276382190547711e-40
        cur_lr: 5.0e-05
        entropy: 0.06331786202887695
        entropy_coeff: 0.0005000000000000001
        kl: 0.009340556027988592
        model: {}
        policy_loss: -0.007556597527582198
        total_loss: 0.2506932045022647
        vf_explained_var: 0.9993988871574402
        vf_loss: 0.258281455685695
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.029999999999998
    gpu_util_percent0: 0.33833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466040501623691
    mean_env_wait_ms: 1.225299391313411
    mean_inference_ms: 4.299197891846242
    mean_raw_obs_processing_ms: 0.37883694057986045
  time_since_restore: 5323.402921676636
  time_this_iter_s: 25.58107304573059
  time_total_s: 5323.402921676636
  timers:
    learn_throughput: 8685.093
    learn_time_ms: 18628.702
    sample_throughput: 23975.704
    sample_time_ms: 6748.165
    update_time_ms: 30.318
  timestamp: 1602781929
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: f0f97_00000
  
2020-10-15 17:12:10,482	WARNING util.py:136 -- The `process_trial` operation took 0.6032934188842773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    208 |           5323.4 | 33652736 |  287.751 |              321.404 |              126.556 |            791.233 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3224.7060052096776
    time_step_min: 3003
  date: 2020-10-15_17-12-35
  done: false
  episode_len_mean: 791.2284160435078
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.8260714934331
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 42659
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.276382190547711e-40
        cur_lr: 5.0e-05
        entropy: 0.11753381416201591
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012209873646497726
        total_loss: .inf
        vf_explained_var: 0.9940120577812195
        vf_loss: 2.399486780166626
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73666666666667
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660103136606628
    mean_env_wait_ms: 1.2252620066255602
    mean_inference_ms: 4.299005632177343
    mean_raw_obs_processing_ms: 0.37882766400509965
  time_since_restore: 5348.895851135254
  time_this_iter_s: 25.492929458618164
  time_total_s: 5348.895851135254
  timers:
    learn_throughput: 8694.277
    learn_time_ms: 18609.024
    sample_throughput: 24000.285
    sample_time_ms: 6741.253
    update_time_ms: 30.17
  timestamp: 1602781955
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:12:36,810	WARNING util.py:136 -- The `process_trial` operation took 0.627997636795044 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    209 |           5348.9 | 33814528 |  287.826 |              321.404 |              126.556 |            791.228 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3224.8201653973742
    time_step_min: 3003
  date: 2020-10-15_17-13-02
  done: false
  episode_len_mean: 791.2160692616447
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.8023415335088
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 42852
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.41457328582157e-40
        cur_lr: 5.0e-05
        entropy: 0.14654168114066124
        entropy_coeff: 0.0005000000000000001
        kl: 0.005635477447261413
        model: {}
        policy_loss: -0.014700812942464836
        total_loss: 4.4031630754470825
        vf_explained_var: 0.9904267191886902
        vf_loss: 4.417937199274699
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.909677419354843
    gpu_util_percent0: 0.3519354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659845082191492
    mean_env_wait_ms: 1.2252200042827266
    mean_inference_ms: 4.298813958158226
    mean_raw_obs_processing_ms: 0.3788179316805575
  time_since_restore: 5374.691927194595
  time_this_iter_s: 25.79607605934143
  time_total_s: 5374.691927194595
  timers:
    learn_throughput: 8696.34
    learn_time_ms: 18604.607
    sample_throughput: 24000.687
    sample_time_ms: 6741.14
    update_time_ms: 31.838
  timestamp: 1602781982
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: f0f97_00000
  
2020-10-15 17:13:03,363	WARNING util.py:136 -- The `process_trial` operation took 0.5639145374298096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    210 |          5374.69 | 33976320 |  287.802 |              321.404 |              126.556 |            791.216 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3224.7268311953894
    time_step_min: 3003
  date: 2020-10-15_17-13-29
  done: false
  episode_len_mean: 791.2000789265983
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.8327912112438
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 226
  episodes_total: 43078
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.41457328582157e-40
        cur_lr: 5.0e-05
        entropy: 0.11543985580404599
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045726440924530225
        model: {}
        policy_loss: -0.011171201219743429
        total_loss: 3.187137226263682
        vf_explained_var: 0.9931896328926086
        vf_loss: 3.1983662247657776
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.473333333333336
    gpu_util_percent0: 0.335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465952042705691
    mean_env_wait_ms: 1.2251739285881376
    mean_inference_ms: 4.298588042349884
    mean_raw_obs_processing_ms: 0.37880774134213324
  time_since_restore: 5400.566316127777
  time_this_iter_s: 25.874388933181763
  time_total_s: 5400.566316127777
  timers:
    learn_throughput: 8689.874
    learn_time_ms: 18618.451
    sample_throughput: 24005.896
    sample_time_ms: 6739.678
    update_time_ms: 33.029
  timestamp: 1602782009
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: f0f97_00000
  
2020-10-15 17:13:30,148	WARNING util.py:136 -- The `process_trial` operation took 0.6226036548614502 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    211 |          5400.57 | 34138112 |  287.833 |              321.404 |              126.556 |              791.2 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3224.3237266965816
    time_step_min: 3003
  date: 2020-10-15_17-13-55
  done: false
  episode_len_mean: 791.1887939001848
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.8974168673795
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 43280
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.707286642910785e-40
        cur_lr: 5.0e-05
        entropy: 0.08873900460700195
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007326144337033232
        total_loss: .inf
        vf_explained_var: 0.9952406287193298
        vf_loss: 2.08857199549675
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.251612903225805
    gpu_util_percent0: 0.3019354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659222766538968
    mean_env_wait_ms: 1.225136597983349
    mean_inference_ms: 4.298394545893998
    mean_raw_obs_processing_ms: 0.37879869912621045
  time_since_restore: 5426.169779777527
  time_this_iter_s: 25.603463649749756
  time_total_s: 5426.169779777527
  timers:
    learn_throughput: 8684.603
    learn_time_ms: 18629.752
    sample_throughput: 24039.229
    sample_time_ms: 6730.332
    update_time_ms: 34.463
  timestamp: 1602782035
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:13:56,717	WARNING util.py:136 -- The `process_trial` operation took 0.5981521606445312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    212 |          5426.17 | 34299904 |  287.897 |              321.404 |              126.556 |            791.189 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3223.7904748744877
    time_step_min: 3003
  date: 2020-10-15_17-14-22
  done: false
  episode_len_mean: 791.1806616361462
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 287.98749015878815
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 188
  episodes_total: 43468
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.060929964366176e-40
        cur_lr: 5.0e-05
        entropy: 0.07153697932759921
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009129482137116915
        total_loss: .inf
        vf_explained_var: 0.9970440864562988
        vf_loss: 1.174270232518514
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.130000000000003
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658966809884064
    mean_env_wait_ms: 1.2251000280443896
    mean_inference_ms: 4.298209874802578
    mean_raw_obs_processing_ms: 0.37878971341905576
  time_since_restore: 5451.839196920395
  time_this_iter_s: 25.669417142868042
  time_total_s: 5451.839196920395
  timers:
    learn_throughput: 8677.333
    learn_time_ms: 18645.36
    sample_throughput: 24061.37
    sample_time_ms: 6724.139
    update_time_ms: 34.607
  timestamp: 1602782062
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:14:23,163	WARNING util.py:136 -- The `process_trial` operation took 0.5741393566131592 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    213 |          5451.84 | 34461696 |  287.987 |              321.404 |              126.556 |            791.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3223.0275231460264
    time_step_min: 3003
  date: 2020-10-15_17-14-48
  done: false
  episode_len_mean: 791.1837599011035
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.1082511392016
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 43682
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0591394946549265e-39
        cur_lr: 5.0e-05
        entropy: 0.061027352387706436
        entropy_coeff: 0.0005000000000000001
        kl: 0.003084473719354719
        model: {}
        policy_loss: -0.008757521049119532
        total_loss: 0.7956572572390238
        vf_explained_var: 0.9981493949890137
        vf_loss: 0.8044452915589014
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.686666666666667
    gpu_util_percent0: 0.3890000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658687450397642
    mean_env_wait_ms: 1.2250559805302974
    mean_inference_ms: 4.29800867854159
    mean_raw_obs_processing_ms: 0.3787798407154538
  time_since_restore: 5477.31077837944
  time_this_iter_s: 25.47158145904541
  time_total_s: 5477.31077837944
  timers:
    learn_throughput: 8682.152
    learn_time_ms: 18635.012
    sample_throughput: 24053.25
    sample_time_ms: 6726.409
    update_time_ms: 35.907
  timestamp: 1602782088
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: f0f97_00000
  
2020-10-15 17:14:49,427	WARNING util.py:136 -- The `process_trial` operation took 0.5965735912322998 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    214 |          5477.31 | 34623488 |  288.108 |              321.404 |              126.556 |            791.184 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3222.1654466260748
    time_step_min: 3003
  date: 2020-10-15_17-15-15
  done: false
  episode_len_mean: 791.1953664259516
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.236491161702
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 215
  episodes_total: 43897
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.295697473274632e-40
        cur_lr: 5.0e-05
        entropy: 0.06045711878687143
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007151031633839011
        total_loss: .inf
        vf_explained_var: 0.9984343647956848
        vf_loss: 0.6691092352072397
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.549999999999997
    gpu_util_percent0: 0.33666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465837282402857
    mean_env_wait_ms: 1.2250129852991771
    mean_inference_ms: 4.29780184497306
    mean_raw_obs_processing_ms: 0.3787700932670885
  time_since_restore: 5502.962203741074
  time_this_iter_s: 25.6514253616333
  time_total_s: 5502.962203741074
  timers:
    learn_throughput: 8681.934
    learn_time_ms: 18635.478
    sample_throughput: 24044.196
    sample_time_ms: 6728.942
    update_time_ms: 34.436
  timestamp: 1602782115
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:15:15,974	WARNING util.py:136 -- The `process_trial` operation took 0.5831053256988525 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    215 |          5502.96 | 34785280 |  288.236 |              321.404 |              126.556 |            791.195 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3221.439378803015
    time_step_min: 3003
  date: 2020-10-15_17-15-41
  done: false
  episode_len_mean: 791.2002721705602
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.34632787388466
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 44090
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.94354620991195e-40
        cur_lr: 5.0e-05
        entropy: 0.0602851789444685
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010571621067356318
        total_loss: .inf
        vf_explained_var: 0.9987063407897949
        vf_loss: 0.5160163740317026
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.259999999999998
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658108752258292
    mean_env_wait_ms: 1.224975513303132
    mean_inference_ms: 4.297618278446378
    mean_raw_obs_processing_ms: 0.37876162656902324
  time_since_restore: 5528.5115287303925
  time_this_iter_s: 25.549324989318848
  time_total_s: 5528.5115287303925
  timers:
    learn_throughput: 8686.092
    learn_time_ms: 18626.558
    sample_throughput: 24036.825
    sample_time_ms: 6731.005
    update_time_ms: 34.699
  timestamp: 1602782141
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:15:42,311	WARNING util.py:136 -- The `process_trial` operation took 0.5816740989685059 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    216 |          5528.51 | 34947072 |  288.346 |              321.404 |              126.556 |              791.2 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3220.643000836328
    time_step_min: 3003
  date: 2020-10-15_17-16-08
  done: false
  episode_len_mean: 791.2077810644207
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.4659424192013
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 44287
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1915319314867921e-39
        cur_lr: 5.0e-05
        entropy: 0.05645844712853432
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030709061538800597
        model: {}
        policy_loss: -0.006917614979708257
        total_loss: 0.4360609327753385
        vf_explained_var: 0.9989995360374451
        vf_loss: 0.44300677875677746
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09666666666667
    gpu_util_percent0: 0.3203333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465783722745206
    mean_env_wait_ms: 1.2249334996351244
    mean_inference_ms: 4.297436413940051
    mean_raw_obs_processing_ms: 0.37875241360614564
  time_since_restore: 5554.429658412933
  time_this_iter_s: 25.918129682540894
  time_total_s: 5554.429658412933
  timers:
    learn_throughput: 8672.41
    learn_time_ms: 18655.944
    sample_throughput: 24057.217
    sample_time_ms: 6725.3
    update_time_ms: 35.278
  timestamp: 1602782168
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: f0f97_00000
  
2020-10-15 17:16:09,131	WARNING util.py:136 -- The `process_trial` operation took 0.6052310466766357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    217 |          5554.43 | 35108864 |  288.466 |              321.404 |              126.556 |            791.208 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3219.751433905396
    time_step_min: 3003
  date: 2020-10-15_17-16-34
  done: false
  episode_len_mean: 791.2176384675879
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.60080413164286
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 44505
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.957659657433961e-40
        cur_lr: 5.0e-05
        entropy: 0.05804291429618994
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007826625233671317
        total_loss: .inf
        vf_explained_var: 0.999264657497406
        vf_loss: 0.312423899769783
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.746666666666666
    gpu_util_percent0: 0.2923333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657542341083957
    mean_env_wait_ms: 1.2248863930895453
    mean_inference_ms: 4.297233798678794
    mean_raw_obs_processing_ms: 0.3787431288633617
  time_since_restore: 5579.927663087845
  time_this_iter_s: 25.4980046749115
  time_total_s: 5579.927663087845
  timers:
    learn_throughput: 8674.654
    learn_time_ms: 18651.119
    sample_throughput: 24072.622
    sample_time_ms: 6720.996
    update_time_ms: 35.626
  timestamp: 1602782194
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:16:35,450	WARNING util.py:136 -- The `process_trial` operation took 0.6220533847808838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    218 |          5579.93 | 35270656 |  288.601 |              321.404 |              126.556 |            791.218 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3218.963706788465
    time_step_min: 3003
  date: 2020-10-15_17-17-01
  done: false
  episode_len_mean: 791.2244687989264
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.7183148867331
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 44710
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.936489486150942e-40
        cur_lr: 5.0e-05
        entropy: 0.06055643378446499
        entropy_coeff: 0.0005000000000000001
        kl: 0.0029835577006451786
        model: {}
        policy_loss: -0.008259041797524938
        total_loss: 0.5485351632038752
        vf_explained_var: 0.998715877532959
        vf_loss: 0.5568245102961858
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.103225806451608
    gpu_util_percent0: 0.35451612903225815
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657243467593556
    mean_env_wait_ms: 1.2248482783705261
    mean_inference_ms: 4.297050088837364
    mean_raw_obs_processing_ms: 0.37873407778957313
  time_since_restore: 5605.788371801376
  time_this_iter_s: 25.860708713531494
  time_total_s: 5605.788371801376
  timers:
    learn_throughput: 8665.765
    learn_time_ms: 18670.25
    sample_throughput: 24040.567
    sample_time_ms: 6729.958
    update_time_ms: 35.486
  timestamp: 1602782221
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: f0f97_00000
  
2020-10-15 17:17:02,194	WARNING util.py:136 -- The `process_trial` operation took 0.6414244174957275 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    219 |          5605.79 | 35432448 |  288.718 |              321.404 |              126.556 |            791.224 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3218.298659888064
    time_step_min: 3003
  date: 2020-10-15_17-17-27
  done: false
  episode_len_mean: 791.2340676720202
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.8194908792102
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 183
  episodes_total: 44893
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.468244743075471e-40
        cur_lr: 5.0e-05
        entropy: 0.059923452946046986
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008598420749573657
        total_loss: .inf
        vf_explained_var: 0.9987263679504395
        vf_loss: 0.5270209858814875
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.969999999999995
    gpu_util_percent0: 0.3003333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657013044356285
    mean_env_wait_ms: 1.2248079645686687
    mean_inference_ms: 4.296885115302351
    mean_raw_obs_processing_ms: 0.37872583210205335
  time_since_restore: 5631.543437004089
  time_this_iter_s: 25.755065202713013
  time_total_s: 5631.543437004089
  timers:
    learn_throughput: 8670.328
    learn_time_ms: 18660.423
    sample_throughput: 24017.726
    sample_time_ms: 6736.358
    update_time_ms: 33.615
  timestamp: 1602782247
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:17:28,735	WARNING util.py:136 -- The `process_trial` operation took 0.5759549140930176 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    220 |          5631.54 | 35594240 |  288.819 |              321.404 |              126.556 |            791.234 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3217.470985065353
    time_step_min: 3003
  date: 2020-10-15_17-17-54
  done: false
  episode_len_mean: 791.2483539870093
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 288.9427597484964
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 45109
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.702367114613208e-40
        cur_lr: 5.0e-05
        entropy: 0.060996199026703835
        entropy_coeff: 0.0005000000000000001
        kl: 0.0027089756331406534
        model: {}
        policy_loss: -0.008794384232411781
        total_loss: 0.46788734942674637
        vf_explained_var: 0.9988861083984375
        vf_loss: 0.4767122169335683
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.467741935483872
    gpu_util_percent0: 0.352258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656758223199345
    mean_env_wait_ms: 1.2247615071873956
    mean_inference_ms: 4.296691165730501
    mean_raw_obs_processing_ms: 0.37871632769954133
  time_since_restore: 5657.422838687897
  time_this_iter_s: 25.879401683807373
  time_total_s: 5657.422838687897
  timers:
    learn_throughput: 8671.614
    learn_time_ms: 18657.657
    sample_throughput: 24039.562
    sample_time_ms: 6730.239
    update_time_ms: 33.675
  timestamp: 1602782274
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: f0f97_00000
  
2020-10-15 17:17:55,497	WARNING util.py:136 -- The `process_trial` operation took 0.6602132320404053 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    221 |          5657.42 | 35756032 |  288.943 |              321.404 |              126.556 |            791.248 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3216.6367973495308
    time_step_min: 3003
  date: 2020-10-15_17-18-21
  done: false
  episode_len_mean: 791.2643807506454
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.06672670973944
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 45321
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.351183557306604e-40
        cur_lr: 5.0e-05
        entropy: 0.054917944905658565
        entropy_coeff: 0.0005000000000000001
        kl: 0.003121539872760574
        model: {}
        policy_loss: -0.008649900293676183
        total_loss: 0.39647207657496136
        vf_explained_var: 0.9990534782409668
        vf_loss: 0.4051494275530179
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.513333333333335
    gpu_util_percent0: 0.30333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656467117568314
    mean_env_wait_ms: 1.2247174845405515
    mean_inference_ms: 4.296503636843975
    mean_raw_obs_processing_ms: 0.3787073776692504
  time_since_restore: 5683.135587692261
  time_this_iter_s: 25.712749004364014
  time_total_s: 5683.135587692261
  timers:
    learn_throughput: 8667.737
    learn_time_ms: 18666.001
    sample_throughput: 24036.451
    sample_time_ms: 6731.11
    update_time_ms: 33.798
  timestamp: 1602782301
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: f0f97_00000
  
2020-10-15 17:18:22,005	WARNING util.py:136 -- The `process_trial` operation took 0.5850145816802979 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    222 |          5683.14 | 35917824 |  289.067 |              321.404 |              126.556 |            791.264 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3215.8884905535883
    time_step_min: 3003
  date: 2020-10-15_17-18-47
  done: false
  episode_len_mean: 791.2796783336629
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.1805797744096
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 45513
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.675591778653302e-40
        cur_lr: 5.0e-05
        entropy: 0.05562923289835453
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038754314882680774
        model: {}
        policy_loss: -0.0074128941632807255
        total_loss: 0.2956310187776883
        vf_explained_var: 0.9992325305938721
        vf_loss: 0.3030717149376869
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43333333333334
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656218363027793
    mean_env_wait_ms: 1.2246779485664918
    mean_inference_ms: 4.29633282463626
    mean_raw_obs_processing_ms: 0.3786995428977842
  time_since_restore: 5708.830733299255
  time_this_iter_s: 25.69514560699463
  time_total_s: 5708.830733299255
  timers:
    learn_throughput: 8665.781
    learn_time_ms: 18670.216
    sample_throughput: 24046.859
    sample_time_ms: 6728.197
    update_time_ms: 34.5
  timestamp: 1602782327
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: f0f97_00000
  
2020-10-15 17:18:48,594	WARNING util.py:136 -- The `process_trial` operation took 0.5962715148925781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    223 |          5708.83 | 36079616 |  289.181 |              321.404 |              126.556 |             791.28 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3215.136860314888
    time_step_min: 3003
  date: 2020-10-15_17-19-14
  done: false
  episode_len_mean: 791.2977489991906
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.29557911492975
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 45713
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.37795889326651e-41
        cur_lr: 5.0e-05
        entropy: 0.0596417390430967
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038061155937612057
        model: {}
        policy_loss: -0.0070746749406680465
        total_loss: 0.3853044385711352
        vf_explained_var: 0.9991154074668884
        vf_loss: 0.39240892976522446
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.99666666666667
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655957239295492
    mean_env_wait_ms: 1.224632804240196
    mean_inference_ms: 4.296162312783882
    mean_raw_obs_processing_ms: 0.3786907065469909
  time_since_restore: 5734.625686407089
  time_this_iter_s: 25.794953107833862
  time_total_s: 5734.625686407089
  timers:
    learn_throughput: 8649.149
    learn_time_ms: 18706.119
    sample_throughput: 24064.486
    sample_time_ms: 6723.269
    update_time_ms: 33.036
  timestamp: 1602782354
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: f0f97_00000
  
2020-10-15 17:19:15,358	WARNING util.py:136 -- The `process_trial` operation took 0.6718358993530273 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    224 |          5734.63 | 36241408 |  289.296 |              321.404 |              126.556 |            791.298 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3214.3379901052676
    time_step_min: 3003
  date: 2020-10-15_17-19-40
  done: false
  episode_len_mean: 791.3130701735287
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.4193004529824
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 45929
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.188979446633255e-41
        cur_lr: 5.0e-05
        entropy: 0.05616131549080213
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030000416833596923
        model: {}
        policy_loss: -0.007582623429091957
        total_loss: 0.37732118368148804
        vf_explained_var: 0.9991095066070557
        vf_loss: 0.38493189215660095
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.986666666666668
    gpu_util_percent0: 0.25933333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655678090925206
    mean_env_wait_ms: 1.2245850254813588
    mean_inference_ms: 4.295970998193772
    mean_raw_obs_processing_ms: 0.3786821079288545
  time_since_restore: 5760.118783712387
  time_this_iter_s: 25.49309730529785
  time_total_s: 5760.118783712387
  timers:
    learn_throughput: 8650.844
    learn_time_ms: 18702.453
    sample_throughput: 24109.377
    sample_time_ms: 6710.75
    update_time_ms: 34.172
  timestamp: 1602782380
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: f0f97_00000
  
2020-10-15 17:19:41,917	WARNING util.py:136 -- The `process_trial` operation took 0.6541793346405029 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    225 |          5760.12 | 36403200 |  289.419 |              321.404 |              126.556 |            791.313 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3213.5499045304637
    time_step_min: 3003
  date: 2020-10-15_17-20-07
  done: false
  episode_len_mean: 791.3309923267004
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.5366967897201
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 46134
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0944897233166275e-41
        cur_lr: 5.0e-05
        entropy: 0.05439443172266086
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005691985774319619
        total_loss: .inf
        vf_explained_var: 0.9994030594825745
        vf_loss: 0.23959258695443472
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.400000000000002
    gpu_util_percent0: 0.28709677419354834
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655405366054455
    mean_env_wait_ms: 1.2245441609935268
    mean_inference_ms: 4.295801915394226
    mean_raw_obs_processing_ms: 0.37867361548706785
  time_since_restore: 5785.773597717285
  time_this_iter_s: 25.65481400489807
  time_total_s: 5785.773597717285
  timers:
    learn_throughput: 8654.855
    learn_time_ms: 18693.786
    sample_throughput: 24080.276
    sample_time_ms: 6718.86
    update_time_ms: 33.597
  timestamp: 1602782407
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:20:08,454	WARNING util.py:136 -- The `process_trial` operation took 0.6074995994567871 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    226 |          5785.77 | 36564992 |  289.537 |              321.404 |              126.556 |            791.331 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3212.825690452522
    time_step_min: 3003
  date: 2020-10-15_17-20-34
  done: false
  episode_len_mean: 791.352158894646
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.64707960433304
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 46320
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.14173458497494e-41
        cur_lr: 5.0e-05
        entropy: 0.05289359980573257
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046730374451726675
        model: {}
        policy_loss: -0.007919741605292074
        total_loss: 0.2053905346741279
        vf_explained_var: 0.9995108246803284
        vf_loss: 0.21333672727147737
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.11
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655179880697075
    mean_env_wait_ms: 1.2245002556887326
    mean_inference_ms: 4.2956432871286
    mean_raw_obs_processing_ms: 0.37866542334158454
  time_since_restore: 5811.335352897644
  time_this_iter_s: 25.561755180358887
  time_total_s: 5811.335352897644
  timers:
    learn_throughput: 8671.561
    learn_time_ms: 18657.772
    sample_throughput: 24109.896
    sample_time_ms: 6710.606
    update_time_ms: 34.266
  timestamp: 1602782434
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: f0f97_00000
  
2020-10-15 17:20:34,942	WARNING util.py:136 -- The `process_trial` operation took 0.6754419803619385 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    227 |          5811.34 | 36726784 |  289.647 |              321.404 |              126.556 |            791.352 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3212.0271909822313
    time_step_min: 3003
  date: 2020-10-15_17-21-00
  done: false
  episode_len_mean: 791.3732270265624
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.7685839309448
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 46532
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.57086729248747e-41
        cur_lr: 5.0e-05
        entropy: 0.05592020538945993
        entropy_coeff: 0.0005000000000000001
        kl: 0.007435940322466195
        model: {}
        policy_loss: -0.006752988138638709
        total_loss: 0.3169453516602516
        vf_explained_var: 0.9993008971214294
        vf_loss: 0.32372630884250003
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26333333333333
    gpu_util_percent0: 0.31866666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654962904949953
    mean_env_wait_ms: 1.2244527082839478
    mean_inference_ms: 4.295466464930441
    mean_raw_obs_processing_ms: 0.3786569804225858
  time_since_restore: 5837.2379314899445
  time_this_iter_s: 25.902578592300415
  time_total_s: 5837.2379314899445
  timers:
    learn_throughput: 8658.742
    learn_time_ms: 18685.393
    sample_throughput: 24074.942
    sample_time_ms: 6720.348
    update_time_ms: 36.019
  timestamp: 1602782460
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: f0f97_00000
  
2020-10-15 17:21:01,684	WARNING util.py:136 -- The `process_trial` operation took 0.6276447772979736 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    228 |          5837.24 | 36888576 |  289.769 |              321.404 |              126.556 |            791.373 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3211.2685323968994
    time_step_min: 3003
  date: 2020-10-15_17-21-27
  done: false
  episode_len_mean: 791.3978565927954
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.88174873575275
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 46748
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.57086729248747e-41
        cur_lr: 5.0e-05
        entropy: 0.06324194247523944
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00806182799472784
        total_loss: .inf
        vf_explained_var: 0.998730480670929
        vf_loss: 0.5645896544059118
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.122580645161293
    gpu_util_percent0: 0.3441935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465465336851083
    mean_env_wait_ms: 1.2244062475182214
    mean_inference_ms: 4.29528713140337
    mean_raw_obs_processing_ms: 0.3786482225035439
  time_since_restore: 5863.052768468857
  time_this_iter_s: 25.814836978912354
  time_total_s: 5863.052768468857
  timers:
    learn_throughput: 8655.465
    learn_time_ms: 18692.469
    sample_throughput: 24084.23
    sample_time_ms: 6717.757
    update_time_ms: 36.331
  timestamp: 1602782487
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:21:28,562	WARNING util.py:136 -- The `process_trial` operation took 0.6910843849182129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    229 |          5863.05 | 37050368 |  289.882 |              321.404 |              126.556 |            791.398 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3210.678104540317
    time_step_min: 3003
  date: 2020-10-15_17-21-54
  done: false
  episode_len_mean: 791.419157594222
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.9587020039537
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 46937
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3563009387312055e-41
        cur_lr: 5.0e-05
        entropy: 0.08834257225195567
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009969931367474297
        total_loss: .inf
        vf_explained_var: 0.9964148998260498
        vf_loss: 1.4482352534929912
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.160000000000004
    gpu_util_percent0: 0.33033333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654436713411775
    mean_env_wait_ms: 1.2243650299571815
    mean_inference_ms: 4.295129950684602
    mean_raw_obs_processing_ms: 0.37864099309511806
  time_since_restore: 5888.572358846664
  time_this_iter_s: 25.519590377807617
  time_total_s: 5888.572358846664
  timers:
    learn_throughput: 8666.879
    learn_time_ms: 18667.851
    sample_throughput: 24130.733
    sample_time_ms: 6704.811
    update_time_ms: 37.152
  timestamp: 1602782514
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:21:54,924	WARNING util.py:136 -- The `process_trial` operation took 0.619570255279541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    230 |          5888.57 | 37212160 |  289.959 |              321.404 |              126.556 |            791.419 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3210.4894882140584
    time_step_min: 3003
  date: 2020-10-15_17-22-20
  done: false
  episode_len_mean: 791.4555117107943
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 289.9946177662568
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 47136
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5344514080968086e-41
        cur_lr: 5.0e-05
        entropy: 0.09826897084712982
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009980461099379076
        total_loss: .inf
        vf_explained_var: 0.9945312142372131
        vf_loss: 2.5212777256965637
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.549999999999997
    gpu_util_percent0: 0.29800000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654189067212278
    mean_env_wait_ms: 1.224318527719252
    mean_inference_ms: 4.294972671998303
    mean_raw_obs_processing_ms: 0.37863219516294483
  time_since_restore: 5913.859579086304
  time_this_iter_s: 25.287220239639282
  time_total_s: 5913.859579086304
  timers:
    learn_throughput: 8685.689
    learn_time_ms: 18627.423
    sample_throughput: 24161.105
    sample_time_ms: 6696.383
    update_time_ms: 36.458
  timestamp: 1602782540
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:22:21,265	WARNING util.py:136 -- The `process_trial` operation took 0.6822421550750732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    231 |          5913.86 | 37373952 |  289.995 |              321.404 |              126.556 |            791.456 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3209.9598765432097
    time_step_min: 3003
  date: 2020-10-15_17-22-46
  done: false
  episode_len_mean: 791.4804012671594
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.0883449062959
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 47350
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.301677112145212e-41
        cur_lr: 5.0e-05
        entropy: 0.07021985265115897
        entropy_coeff: 0.0005000000000000001
        kl: 0.005270883402166267
        model: {}
        policy_loss: -0.007014880752043003
        total_loss: 0.7674808651208878
        vf_explained_var: 0.9982611536979675
        vf_loss: 0.7745308627684911
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666668
    gpu_util_percent0: 0.3443333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653950488121964
    mean_env_wait_ms: 1.224269316620569
    mean_inference_ms: 4.294795194753497
    mean_raw_obs_processing_ms: 0.3786244385962636
  time_since_restore: 5939.504643917084
  time_this_iter_s: 25.64506483078003
  time_total_s: 5939.504643917084
  timers:
    learn_throughput: 8688.884
    learn_time_ms: 18620.574
    sample_throughput: 24193.619
    sample_time_ms: 6687.383
    update_time_ms: 36.589
  timestamp: 1602782566
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: f0f97_00000
  
2020-10-15 17:22:47,796	WARNING util.py:136 -- The `process_trial` operation took 0.6697990894317627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    232 |           5939.5 | 37535744 |  290.088 |              321.404 |              126.556 |             791.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3209.313778732003
    time_step_min: 3003
  date: 2020-10-15_17-23-13
  done: false
  episode_len_mean: 791.5040164865206
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.1878536383726
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 47554
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.301677112145212e-41
        cur_lr: 5.0e-05
        entropy: 0.06154921433577935
        entropy_coeff: 0.0005000000000000001
        kl: 0.003764911186105261
        model: {}
        policy_loss: -0.007722674952674424
        total_loss: 0.5368046363194784
        vf_explained_var: 0.9986751675605774
        vf_loss: 0.5445580730835596
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.440000000000005
    gpu_util_percent0: 0.3033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653688481874078
    mean_env_wait_ms: 1.224227148014005
    mean_inference_ms: 4.294636583882937
    mean_raw_obs_processing_ms: 0.37861648254731717
  time_since_restore: 5965.354547262192
  time_this_iter_s: 25.849903345108032
  time_total_s: 5965.354547262192
  timers:
    learn_throughput: 8686.333
    learn_time_ms: 18626.041
    sample_throughput: 24162.601
    sample_time_ms: 6695.968
    update_time_ms: 36.592
  timestamp: 1602782593
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: f0f97_00000
  
2020-10-15 17:23:14,554	WARNING util.py:136 -- The `process_trial` operation took 0.6219229698181152 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    233 |          5965.35 | 37697536 |  290.188 |              321.404 |              126.556 |            791.504 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3208.716529324191
    time_step_min: 3003
  date: 2020-10-15_17-23-40
  done: false
  episode_len_mean: 791.5242264909818
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.2798695207723
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 183
  episodes_total: 47737
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.650838556072606e-41
        cur_lr: 5.0e-05
        entropy: 0.058499933841327824
        entropy_coeff: 0.0005000000000000001
        kl: 0.004012892352572332
        model: {}
        policy_loss: -0.0064458615621939925
        total_loss: 0.4806084980567296
        vf_explained_var: 0.9988649487495422
        vf_loss: 0.4870836039384206
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.667741935483875
    gpu_util_percent0: 0.33838709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653481359012815
    mean_env_wait_ms: 1.2241846790525845
    mean_inference_ms: 4.294489165540922
    mean_raw_obs_processing_ms: 0.3786091715011438
  time_since_restore: 5991.098297595978
  time_this_iter_s: 25.74375033378601
  time_total_s: 5991.098297595978
  timers:
    learn_throughput: 8688.586
    learn_time_ms: 18621.212
    sample_throughput: 24161.782
    sample_time_ms: 6696.195
    update_time_ms: 37.175
  timestamp: 1602782620
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: f0f97_00000
  
2020-10-15 17:23:41,322	WARNING util.py:136 -- The `process_trial` operation took 0.6318790912628174 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    234 |           5991.1 | 37859328 |   290.28 |              321.404 |              126.556 |            791.524 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3207.998580642468
    time_step_min: 3003
  date: 2020-10-15_17-24-07
  done: false
  episode_len_mean: 791.545365446773
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.3898572841327
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 47955
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.325419278036303e-41
        cur_lr: 5.0e-05
        entropy: 0.057449838457008205
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033242303179576993
        model: {}
        policy_loss: -0.007023505176524243
        total_loss: 0.5665338337421417
        vf_explained_var: 0.99869704246521
        vf_loss: 0.5735860665639242
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.81666666666667
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653265066856452
    mean_env_wait_ms: 1.2241320819706378
    mean_inference_ms: 4.2943192796381915
    mean_raw_obs_processing_ms: 0.3786004541063588
  time_since_restore: 6017.010036468506
  time_this_iter_s: 25.911738872528076
  time_total_s: 6017.010036468506
  timers:
    learn_throughput: 8678.844
    learn_time_ms: 18642.113
    sample_throughput: 24099.316
    sample_time_ms: 6713.552
    update_time_ms: 37.631
  timestamp: 1602782647
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: f0f97_00000
  
2020-10-15 17:24:08,102	WARNING util.py:136 -- The `process_trial` operation took 0.6450650691986084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    235 |          6017.01 | 38021120 |   290.39 |              321.404 |              126.556 |            791.545 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3207.2664269980464
    time_step_min: 3003
  date: 2020-10-15_17-24-33
  done: false
  episode_len_mean: 791.5661019764159
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.50217483756296
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 213
  episodes_total: 48168
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.627096390181514e-42
        cur_lr: 5.0e-05
        entropy: 0.05551053490489721
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010610295183141716
        total_loss: .inf
        vf_explained_var: 0.9991929531097412
        vf_loss: 0.34291622042655945
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.336666666666662
    gpu_util_percent0: 0.28933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465296139622731
    mean_env_wait_ms: 1.2240855343317296
    mean_inference_ms: 4.2941516979214525
    mean_raw_obs_processing_ms: 0.37859254021308814
  time_since_restore: 6042.8051126003265
  time_this_iter_s: 25.79507613182068
  time_total_s: 6042.8051126003265
  timers:
    learn_throughput: 8667.51
    learn_time_ms: 18666.492
    sample_throughput: 24112.798
    sample_time_ms: 6709.798
    update_time_ms: 39.321
  timestamp: 1602782673
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:24:34,915	WARNING util.py:136 -- The `process_trial` operation took 0.7043628692626953 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    236 |          6042.81 | 38182912 |  290.502 |              321.404 |              126.556 |            791.566 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3206.6081925815533
    time_step_min: 3003
  date: 2020-10-15_17-25-00
  done: false
  episode_len_mean: 791.579635220646
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.60206724175407
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 48358
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.940644585272275e-42
        cur_lr: 5.0e-05
        entropy: 0.05668728270878395
        entropy_coeff: 0.0005000000000000001
        kl: 0.003796237191030135
        model: {}
        policy_loss: -0.007855492896245172
        total_loss: 0.2658596920470397
        vf_explained_var: 0.999310314655304
        vf_loss: 0.27374353259801865
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26451612903226
    gpu_util_percent0: 0.3070967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465276894440313
    mean_env_wait_ms: 1.2240428457288601
    mean_inference_ms: 4.294005028017742
    mean_raw_obs_processing_ms: 0.37858554030895536
  time_since_restore: 6068.512362241745
  time_this_iter_s: 25.707249641418457
  time_total_s: 6068.512362241745
  timers:
    learn_throughput: 8660.457
    learn_time_ms: 18681.694
    sample_throughput: 24104.303
    sample_time_ms: 6712.163
    update_time_ms: 37.527
  timestamp: 1602782700
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: f0f97_00000
  
2020-10-15 17:25:01,524	WARNING util.py:136 -- The `process_trial` operation took 0.678903341293335 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    237 |          6068.51 | 38344704 |  290.602 |              321.404 |              126.556 |             791.58 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3205.878094531363
    time_step_min: 3003
  date: 2020-10-15_17-25-27
  done: false
  episode_len_mean: 791.6002594781606
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.7126540430557
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 48559
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.9703222926361374e-42
        cur_lr: 5.0e-05
        entropy: 0.05432442047943672
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042916887905448675
        model: {}
        policy_loss: -0.006549778957075129
        total_loss: 0.142343882471323
        vf_explained_var: 0.9996342658996582
        vf_loss: 0.14892082661390305
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.713333333333335
    gpu_util_percent0: 0.3043333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652520241771239
    mean_env_wait_ms: 1.2239949404252681
    mean_inference_ms: 4.293853819716883
    mean_raw_obs_processing_ms: 0.3785774962645581
  time_since_restore: 6094.150951385498
  time_this_iter_s: 25.63858914375305
  time_total_s: 6094.150951385498
  timers:
    learn_throughput: 8670.904
    learn_time_ms: 18659.186
    sample_throughput: 24147.815
    sample_time_ms: 6700.068
    update_time_ms: 35.499
  timestamp: 1602782727
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: f0f97_00000
  
2020-10-15 17:25:28,079	WARNING util.py:136 -- The `process_trial` operation took 0.6850450038909912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    238 |          6094.15 | 38506496 |  290.713 |              321.404 |              126.556 |              791.6 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3205.1112319658505
    time_step_min: 3003
  date: 2020-10-15_17-25-53
  done: false
  episode_len_mean: 791.6182724048141
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.8257802017049
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 48773
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4851611463180687e-42
        cur_lr: 5.0e-05
        entropy: 0.05446156983574232
        entropy_coeff: 0.0005000000000000001
        kl: 0.004814083338715136
        model: {}
        policy_loss: -0.0084813464054605
        total_loss: 0.2286884213487307
        vf_explained_var: 0.9994823336601257
        vf_loss: 0.2371969943245252
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.746666666666666
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652291113899885
    mean_env_wait_ms: 1.2239426893105272
    mean_inference_ms: 4.293687060298445
    mean_raw_obs_processing_ms: 0.3785701200920069
  time_since_restore: 6119.808281421661
  time_this_iter_s: 25.65733003616333
  time_total_s: 6119.808281421661
  timers:
    learn_throughput: 8684.725
    learn_time_ms: 18629.491
    sample_throughput: 24128.421
    sample_time_ms: 6705.453
    update_time_ms: 41.81
  timestamp: 1602782753
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: f0f97_00000
  
2020-10-15 17:25:54,613	WARNING util.py:136 -- The `process_trial` operation took 0.6554470062255859 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    239 |          6119.81 | 38668288 |  290.826 |              321.404 |              126.556 |            791.618 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3204.40412008747
    time_step_min: 3003
  date: 2020-10-15_17-26-20
  done: false
  episode_len_mean: 791.6391163199053
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 290.9306951541669
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 48977
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2425805731590344e-42
        cur_lr: 5.0e-05
        entropy: 0.05854980864872535
        entropy_coeff: 0.0005000000000000001
        kl: 0.005727618001401424
        model: {}
        policy_loss: -0.008150885560705015
        total_loss: 0.2500491167108218
        vf_explained_var: 0.9993599057197571
        vf_loss: 0.2582292693356673
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.493333333333332
    gpu_util_percent0: 0.33333333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.873333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465205492399368
    mean_env_wait_ms: 1.2238992510738327
    mean_inference_ms: 4.293539485682126
    mean_raw_obs_processing_ms: 0.37856265990239923
  time_since_restore: 6145.366789102554
  time_this_iter_s: 25.558507680892944
  time_total_s: 6145.366789102554
  timers:
    learn_throughput: 8688.475
    learn_time_ms: 18621.451
    sample_throughput: 24066.954
    sample_time_ms: 6722.579
    update_time_ms: 39.647
  timestamp: 1602782780
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: f0f97_00000
  
2020-10-15 17:26:21,097	WARNING util.py:136 -- The `process_trial` operation took 0.7047226428985596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    240 |          6145.37 | 38830080 |  290.931 |              321.404 |              126.556 |            791.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3203.82291857553
    time_step_min: 3003
  date: 2020-10-15_17-26-46
  done: false
  episode_len_mean: 791.6583941902805
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.01355424502714
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 182
  episodes_total: 49159
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2425805731590344e-42
        cur_lr: 5.0e-05
        entropy: 0.06407811927298705
        entropy_coeff: 0.0005000000000000001
        kl: 0.003470469770642618
        model: {}
        policy_loss: -0.009886164839069048
        total_loss: 0.5019218946496645
        vf_explained_var: 0.9987261295318604
        vf_loss: 0.5118400901556015
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87
    gpu_util_percent0: 0.34900000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651848815631938
    mean_env_wait_ms: 1.223855006647538
    mean_inference_ms: 4.2934019595377935
    mean_raw_obs_processing_ms: 0.37855568822337154
  time_since_restore: 6171.015764951706
  time_this_iter_s: 25.64897584915161
  time_total_s: 6171.015764951706
  timers:
    learn_throughput: 8674.957
    learn_time_ms: 18650.467
    sample_throughput: 24058.124
    sample_time_ms: 6725.046
    update_time_ms: 41.174
  timestamp: 1602782806
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: f0f97_00000
  
2020-10-15 17:26:47,633	WARNING util.py:136 -- The `process_trial` operation took 0.6553144454956055 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    241 |          6171.02 | 38991872 |  291.014 |              321.404 |              126.556 |            791.658 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3203.193355495196
    time_step_min: 3003
  date: 2020-10-15_17-27-13
  done: false
  episode_len_mean: 791.675860672337
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.11131669059927
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 49380
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.212902865795172e-43
        cur_lr: 5.0e-05
        entropy: 0.062345960177481174
        entropy_coeff: 0.0005000000000000001
        kl: 0.003418694056260089
        model: {}
        policy_loss: -0.008521764616792401
        total_loss: 0.5327917312582334
        vf_explained_var: 0.9987661838531494
        vf_loss: 0.5413446774085363
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996666666666666
    gpu_util_percent0: 0.3023333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651619499352286
    mean_env_wait_ms: 1.2238006354519329
    mean_inference_ms: 4.2932399794001395
    mean_raw_obs_processing_ms: 0.37854712891361303
  time_since_restore: 6196.659776687622
  time_this_iter_s: 25.644011735916138
  time_total_s: 6196.659776687622
  timers:
    learn_throughput: 8671.746
    learn_time_ms: 18657.373
    sample_throughput: 24044.545
    sample_time_ms: 6728.844
    update_time_ms: 40.173
  timestamp: 1602782833
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: f0f97_00000
  
2020-10-15 17:27:14,249	WARNING util.py:136 -- The `process_trial` operation took 0.6801652908325195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    242 |          6196.66 | 39153664 |  291.111 |              321.404 |              126.556 |            791.676 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3202.5292562317086
    time_step_min: 3003
  date: 2020-10-15_17-27-39
  done: false
  episode_len_mean: 791.6872618015366
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.20668054585445
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 49591
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.106451432897586e-43
        cur_lr: 5.0e-05
        entropy: 0.05960707304378351
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033160706128304205
        model: {}
        policy_loss: -0.00860478714942777
        total_loss: 0.5410746112465858
        vf_explained_var: 0.9987203478813171
        vf_loss: 0.5497092008590698
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333337
    gpu_util_percent0: 0.31533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651361787944087
    mean_env_wait_ms: 1.2237527039320804
    mean_inference_ms: 4.293084111220837
    mean_raw_obs_processing_ms: 0.37853987320611937
  time_since_restore: 6222.143007040024
  time_this_iter_s: 25.483230352401733
  time_total_s: 6222.143007040024
  timers:
    learn_throughput: 8684.021
    learn_time_ms: 18631.001
    sample_throughput: 24075.591
    sample_time_ms: 6720.167
    update_time_ms: 38.182
  timestamp: 1602782859
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: f0f97_00000
  
2020-10-15 17:27:40,776	WARNING util.py:136 -- The `process_trial` operation took 0.7588582038879395 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    243 |          6222.14 | 39315456 |  291.207 |              321.404 |              126.556 |            791.687 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3201.903365906623
    time_step_min: 3003
  date: 2020-10-15_17-28-06
  done: false
  episode_len_mean: 791.7028726396143
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.3002077829313
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 49780
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.553225716448793e-43
        cur_lr: 5.0e-05
        entropy: 0.054995326635738216
        entropy_coeff: 0.0005000000000000001
        kl: 0.005076525655264656
        model: {}
        policy_loss: -0.007909528429687876
        total_loss: 0.3313940614461899
        vf_explained_var: 0.9991284012794495
        vf_loss: 0.33933108796676
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.293548387096774
    gpu_util_percent0: 0.3087096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651181309502506
    mean_env_wait_ms: 1.2237090831007884
    mean_inference_ms: 4.292947028851065
    mean_raw_obs_processing_ms: 0.3785332099305004
  time_since_restore: 6247.976843833923
  time_this_iter_s: 25.833836793899536
  time_total_s: 6247.976843833923
  timers:
    learn_throughput: 8678.856
    learn_time_ms: 18642.087
    sample_throughput: 24082.485
    sample_time_ms: 6718.244
    update_time_ms: 36.515
  timestamp: 1602782886
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: f0f97_00000
  
2020-10-15 17:28:07,502	WARNING util.py:136 -- The `process_trial` operation took 0.6674394607543945 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    244 |          6247.98 | 39477248 |    291.3 |              321.404 |              126.556 |            791.703 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3201.2695342224356
    time_step_min: 3003
  date: 2020-10-15_17-28-32
  done: false
  episode_len_mean: 791.7215108834827
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.3950617733023
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 49984
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.553225716448793e-43
        cur_lr: 5.0e-05
        entropy: 0.05411679049332937
        entropy_coeff: 0.0005000000000000001
        kl: 0.003311280452180654
        model: {}
        policy_loss: -0.008745847570632273
        total_loss: 0.33714475234349567
        vf_explained_var: 0.9991893768310547
        vf_loss: 0.34591766198476154
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.733333333333338
    gpu_util_percent0: 0.36800000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465096298297341
    mean_env_wait_ms: 1.2236582964314067
    mean_inference_ms: 4.292803512008273
    mean_raw_obs_processing_ms: 0.3785254197784739
  time_since_restore: 6273.458993196487
  time_this_iter_s: 25.482149362564087
  time_total_s: 6273.458993196487
  timers:
    learn_throughput: 8692.562
    learn_time_ms: 18612.695
    sample_throughput: 24121.403
    sample_time_ms: 6707.404
    update_time_ms: 34.719
  timestamp: 1602782912
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: f0f97_00000
  
2020-10-15 17:28:33,954	WARNING util.py:136 -- The `process_trial` operation took 0.7378745079040527 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    245 |          6273.46 | 39639040 |  291.395 |              321.404 |              126.556 |            791.722 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3200.604442494816
    time_step_min: 3003
  date: 2020-10-15_17-28-59
  done: false
  episode_len_mean: 791.7416032511255
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.49968287198806
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 50198
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.766128582243965e-44
        cur_lr: 5.0e-05
        entropy: 0.05470270477235317
        entropy_coeff: 0.0005000000000000001
        kl: 0.004878879990428686
        model: {}
        policy_loss: -0.005708776318821644
        total_loss: 0.23110091810425124
        vf_explained_var: 0.9994547963142395
        vf_loss: 0.23683704311649004
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92666666666667
    gpu_util_percent0: 0.3393333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650727050764326
    mean_env_wait_ms: 1.2236061690936457
    mean_inference_ms: 4.292647051628977
    mean_raw_obs_processing_ms: 0.378518306273844
  time_since_restore: 6299.0427305698395
  time_this_iter_s: 25.58373737335205
  time_total_s: 6299.0427305698395
  timers:
    learn_throughput: 8702.837
    learn_time_ms: 18590.719
    sample_throughput: 24123.318
    sample_time_ms: 6706.872
    update_time_ms: 34.61
  timestamp: 1602782939
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: f0f97_00000
  
2020-10-15 17:29:00,515	WARNING util.py:136 -- The `process_trial` operation took 0.6561882495880127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    246 |          6299.04 | 39800832 |    291.5 |              321.404 |              126.556 |            791.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3199.9393482016962
    time_step_min: 3003
  date: 2020-10-15_17-29-26
  done: false
  episode_len_mean: 791.7625151292684
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.60049391712664
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 50399
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8830642911219824e-44
        cur_lr: 5.0e-05
        entropy: 0.053467778488993645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036580801436988017
        model: {}
        policy_loss: -0.007344892355225359
        total_loss: 0.2113725282251835
        vf_explained_var: 0.9994606971740723
        vf_loss: 0.2187441550195217
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48709677419355
    gpu_util_percent0: 0.2951612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650502114034697
    mean_env_wait_ms: 1.2235614718287915
    mean_inference_ms: 4.292510100684287
    mean_raw_obs_processing_ms: 0.37851137865090206
  time_since_restore: 6324.968678951263
  time_this_iter_s: 25.92594838142395
  time_total_s: 6324.968678951263
  timers:
    learn_throughput: 8701.412
    learn_time_ms: 18593.763
    sample_throughput: 24027.713
    sample_time_ms: 6733.558
    update_time_ms: 34.676
  timestamp: 1602782966
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: f0f97_00000
  
2020-10-15 17:29:27,339	WARNING util.py:136 -- The `process_trial` operation took 0.669945240020752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    247 |          6324.97 | 39962624 |    291.6 |              321.404 |              126.556 |            791.763 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3199.404935291091
    time_step_min: 3003
  date: 2020-10-15_17-29-53
  done: false
  episode_len_mean: 791.7807038355081
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.67988005799344
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 181
  episodes_total: 50580
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9415321455609912e-44
        cur_lr: 5.0e-05
        entropy: 0.05553441091130177
        entropy_coeff: 0.0005000000000000001
        kl: 0.00390920793870464
        model: {}
        policy_loss: -0.007978228110005148
        total_loss: 0.4157225415110588
        vf_explained_var: 0.9990209937095642
        vf_loss: 0.42372853060563404
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.812903225806455
    gpu_util_percent0: 0.2990322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650320203369502
    mean_env_wait_ms: 1.2235168706479636
    mean_inference_ms: 4.292382592333287
    mean_raw_obs_processing_ms: 0.37850475635590974
  time_since_restore: 6351.147259712219
  time_this_iter_s: 26.17858076095581
  time_total_s: 6351.147259712219
  timers:
    learn_throughput: 8692.489
    learn_time_ms: 18612.851
    sample_throughput: 23911.571
    sample_time_ms: 6766.264
    update_time_ms: 36.912
  timestamp: 1602782993
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: f0f97_00000
  
2020-10-15 17:29:54,484	WARNING util.py:136 -- The `process_trial` operation took 0.7268650531768799 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    248 |          6351.15 | 40124416 |   291.68 |              321.404 |              126.556 |            791.781 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3198.6740415303993
    time_step_min: 3003
  date: 2020-10-15_17-30-20
  done: false
  episode_len_mean: 791.804661050311
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.79082693719283
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 50804
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.707660727804956e-45
        cur_lr: 5.0e-05
        entropy: 0.05786617783208688
        entropy_coeff: 0.0005000000000000001
        kl: 0.005581664542357127
        model: {}
        policy_loss: -0.006308064233356466
        total_loss: 0.2813972979784012
        vf_explained_var: 0.9993508458137512
        vf_loss: 0.2877342974146207
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.860000000000003
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650106697882215
    mean_env_wait_ms: 1.2234599158713417
    mean_inference_ms: 4.2922287081552915
    mean_raw_obs_processing_ms: 0.3784967888628791
  time_since_restore: 6377.228069067001
  time_this_iter_s: 26.080809354782104
  time_total_s: 6377.228069067001
  timers:
    learn_throughput: 8678.274
    learn_time_ms: 18643.338
    sample_throughput: 23848.205
    sample_time_ms: 6784.242
    update_time_ms: 30.648
  timestamp: 1602783020
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: f0f97_00000
  
2020-10-15 17:30:21,649	WARNING util.py:136 -- The `process_trial` operation took 0.7592811584472656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    249 |          6377.23 | 40286208 |  291.791 |              321.404 |              126.556 |            791.805 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3198.062084257206
    time_step_min: 3003
  date: 2020-10-15_17-30-47
  done: false
  episode_len_mean: 791.8212276264973
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.87622980377154
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 51009
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.707660727804956e-45
        cur_lr: 5.0e-05
        entropy: 0.06574659173687299
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041131648467853665
        model: {}
        policy_loss: -0.00796141846027846
        total_loss: 0.8365566730499268
        vf_explained_var: 0.9980158805847168
        vf_loss: 0.8445509920517603
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.246875
    gpu_util_percent0: 0.37906249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649858531516866
    mean_env_wait_ms: 1.2234119936977321
    mean_inference_ms: 4.292088383620104
    mean_raw_obs_processing_ms: 0.37848990140297717
  time_since_restore: 6403.338215589523
  time_this_iter_s: 26.110146522521973
  time_total_s: 6403.338215589523
  timers:
    learn_throughput: 8656.947
    learn_time_ms: 18689.268
    sample_throughput: 23798.211
    sample_time_ms: 6798.494
    update_time_ms: 32.468
  timestamp: 1602783047
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: f0f97_00000
  
2020-10-15 17:30:48,914	WARNING util.py:136 -- The `process_trial` operation took 0.73117995262146 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    250 |          6403.34 | 40448000 |  291.876 |              321.404 |              126.556 |            791.821 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3197.5208084915066
    time_step_min: 3003
  date: 2020-10-15_17-31-14
  done: false
  episode_len_mean: 791.8379587133567
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 291.9556711974534
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 51203
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.853830363902478e-45
        cur_lr: 5.0e-05
        entropy: 0.062497165674964585
        entropy_coeff: 0.0005000000000000001
        kl: 0.003791006029738734
        model: {}
        policy_loss: -0.007677286766314258
        total_loss: 0.5421137462059656
        vf_explained_var: 0.9986259937286377
        vf_loss: 0.5498223006725311
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883333333333333
    gpu_util_percent0: 0.40066666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649674965411738
    mean_env_wait_ms: 1.2233659700405115
    mean_inference_ms: 4.29195863484214
    mean_raw_obs_processing_ms: 0.3784834620950253
  time_since_restore: 6428.734813213348
  time_this_iter_s: 25.396597623825073
  time_total_s: 6428.734813213348
  timers:
    learn_throughput: 8684.454
    learn_time_ms: 18630.072
    sample_throughput: 23705.152
    sample_time_ms: 6825.183
    update_time_ms: 30.388
  timestamp: 1602783074
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: f0f97_00000
  
2020-10-15 17:31:15,345	WARNING util.py:136 -- The `process_trial` operation took 0.7398560047149658 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    251 |          6428.73 | 40609792 |  291.956 |              321.404 |              126.556 |            791.838 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3196.916020873087
    time_step_min: 3003
  date: 2020-10-15_17-31-40
  done: false
  episode_len_mean: 791.8552836355148
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.046635721466
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 51404
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.426915181951239e-45
        cur_lr: 5.0e-05
        entropy: 0.05966189317405224
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0081353394780308
        total_loss: .inf
        vf_explained_var: 0.9987649917602539
        vf_loss: 0.5274190406004587
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.916666666666664
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649465847955642
    mean_env_wait_ms: 1.223316256703975
    mean_inference_ms: 4.291827742135087
    mean_raw_obs_processing_ms: 0.3784762034579742
  time_since_restore: 6454.349859714508
  time_this_iter_s: 25.615046501159668
  time_total_s: 6454.349859714508
  timers:
    learn_throughput: 8696.467
    learn_time_ms: 18604.336
    sample_throughput: 23636.705
    sample_time_ms: 6844.947
    update_time_ms: 30.999
  timestamp: 1602783100
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:31:41,862	WARNING util.py:136 -- The `process_trial` operation took 0.6655504703521729 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    252 |          6454.35 | 40771584 |  292.047 |              321.404 |              126.556 |            791.855 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3196.306491643076
    time_step_min: 3003
  date: 2020-10-15_17-32-07
  done: false
  episode_len_mean: 791.8713870592793
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.13903271380946
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 51620
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.640372772926857e-45
        cur_lr: 5.0e-05
        entropy: 0.06241627999891838
        entropy_coeff: 0.0005000000000000001
        kl: 0.004413911956362426
        model: {}
        policy_loss: -0.008414396170943897
        total_loss: 0.6019830902417501
        vf_explained_var: 0.9985876679420471
        vf_loss: 0.6104286958773931
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.740000000000002
    gpu_util_percent0: 0.37600000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649253893059583
    mean_env_wait_ms: 1.2232613679608213
    mean_inference_ms: 4.291681743937993
    mean_raw_obs_processing_ms: 0.3784692081108411
  time_since_restore: 6480.0788242816925
  time_this_iter_s: 25.72896456718445
  time_total_s: 6480.0788242816925
  timers:
    learn_throughput: 8687.061
    learn_time_ms: 18624.48
    sample_throughput: 23633.812
    sample_time_ms: 6845.785
    update_time_ms: 31.121
  timestamp: 1602783127
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: f0f97_00000
  
2020-10-15 17:32:08,605	WARNING util.py:136 -- The `process_trial` operation took 0.6961202621459961 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    253 |          6480.08 | 40933376 |  292.139 |              321.404 |              126.556 |            791.871 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3195.7404152583294
    time_step_min: 3003
  date: 2020-10-15_17-32-34
  done: false
  episode_len_mean: 791.8854325466509
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.2255220427578
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 51821
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8201863864634285e-45
        cur_lr: 5.0e-05
        entropy: 0.06515444877247016
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049497535995518165
        model: {}
        policy_loss: -0.007842508644292442
        total_loss: 0.4233625878890355
        vf_explained_var: 0.998934268951416
        vf_loss: 0.43123766779899597
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32903225806452
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649040600041197
    mean_env_wait_ms: 1.223215536293458
    mean_inference_ms: 4.29155367580435
    mean_raw_obs_processing_ms: 0.3784625778432282
  time_since_restore: 6506.0009615421295
  time_this_iter_s: 25.92213726043701
  time_total_s: 6506.0009615421295
  timers:
    learn_throughput: 8689.206
    learn_time_ms: 18619.883
    sample_throughput: 23589.62
    sample_time_ms: 6858.61
    update_time_ms: 31.873
  timestamp: 1602783154
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: f0f97_00000
  
2020-10-15 17:32:35,478	WARNING util.py:136 -- The `process_trial` operation took 0.6754271984100342 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    254 |             6506 | 41095168 |  292.226 |              321.404 |              126.556 |            791.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3195.2252588629276
    time_step_min: 3003
  date: 2020-10-15_17-33-01
  done: false
  episode_len_mean: 791.8988347050226
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.3006002257785
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 183
  episodes_total: 52004
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.100931932317143e-46
        cur_lr: 5.0e-05
        entropy: 0.06680902031560738
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0109384412644431
        total_loss: .inf
        vf_explained_var: 0.9988085627555847
        vf_loss: 0.48718947917222977
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42258064516129
    gpu_util_percent0: 0.29161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464885389269798
    mean_env_wait_ms: 1.2231692174070654
    mean_inference_ms: 4.291432880341051
    mean_raw_obs_processing_ms: 0.37845602228145214
  time_since_restore: 6531.679399967194
  time_this_iter_s: 25.678438425064087
  time_total_s: 6531.679399967194
  timers:
    learn_throughput: 8686.035
    learn_time_ms: 18626.68
    sample_throughput: 23548.65
    sample_time_ms: 6870.542
    update_time_ms: 31.843
  timestamp: 1602783181
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:33:02,235	WARNING util.py:136 -- The `process_trial` operation took 0.6768631935119629 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    255 |          6531.68 | 41256960 |  292.301 |              321.404 |              126.556 |            791.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3194.654868443746
    time_step_min: 3003
  date: 2020-10-15_17-33-28
  done: false
  episode_len_mean: 791.9121943747726
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.3867813287671
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 52229
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3651397898475713e-45
        cur_lr: 5.0e-05
        entropy: 0.07557731121778488
        entropy_coeff: 0.0005000000000000001
        kl: 0.003877183209018161
        model: {}
        policy_loss: -0.009151474669730911
        total_loss: 0.8546628852685293
        vf_explained_var: 0.9980486035346985
        vf_loss: 0.8638521681229273
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.540000000000003
    gpu_util_percent0: 0.31699999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8866666666666676
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464868015683048
    mean_env_wait_ms: 1.2231112626680496
    mean_inference_ms: 4.291288218549132
    mean_raw_obs_processing_ms: 0.3784485590005879
  time_since_restore: 6557.581837892532
  time_this_iter_s: 25.902437925338745
  time_total_s: 6557.581837892532
  timers:
    learn_throughput: 8673.598
    learn_time_ms: 18653.388
    sample_throughput: 23534.841
    sample_time_ms: 6874.574
    update_time_ms: 31.37
  timestamp: 1602783208
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: f0f97_00000
  
2020-10-15 17:33:29,027	WARNING util.py:136 -- The `process_trial` operation took 0.6533689498901367 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    256 |          6557.58 | 41418752 |  292.387 |              321.404 |              126.556 |            791.912 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3194.1340166828913
    time_step_min: 3003
  date: 2020-10-15_17-33-54
  done: false
  episode_len_mean: 791.925641270144
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.4704487422133
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 52435
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.825698949237857e-46
        cur_lr: 5.0e-05
        entropy: 0.06337448364744584
        entropy_coeff: 0.0005000000000000001
        kl: 0.005226098815910518
        model: {}
        policy_loss: -0.00828116588430324
        total_loss: 0.4313676133751869
        vf_explained_var: 0.9989650845527649
        vf_loss: 0.43968046456575394
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.013333333333335
    gpu_util_percent0: 0.349
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464842055185204
    mean_env_wait_ms: 1.2230630263946602
    mean_inference_ms: 4.291156711587223
    mean_raw_obs_processing_ms: 0.3784418114464758
  time_since_restore: 6583.420914649963
  time_this_iter_s: 25.83907675743103
  time_total_s: 6583.420914649963
  timers:
    learn_throughput: 8668.463
    learn_time_ms: 18664.44
    sample_throughput: 23613.971
    sample_time_ms: 6851.537
    update_time_ms: 33.111
  timestamp: 1602783234
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: f0f97_00000
  
2020-10-15 17:33:55,933	WARNING util.py:136 -- The `process_trial` operation took 0.7110874652862549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    257 |          6583.42 | 41580544 |   292.47 |              321.404 |              126.556 |            791.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3193.603682078396
    time_step_min: 3003
  date: 2020-10-15_17-34-21
  done: false
  episode_len_mean: 791.9394774346794
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.55463134912014
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 52625
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.825698949237857e-46
        cur_lr: 5.0e-05
        entropy: 0.060126399621367455
        entropy_coeff: 0.0005000000000000001
        kl: 0.003812585666310042
        model: {}
        policy_loss: -0.006599199667107314
        total_loss: 0.35030339658260345
        vf_explained_var: 0.9991054534912109
        vf_loss: 0.3569326549768448
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.299999999999997
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648257870538997
    mean_env_wait_ms: 1.223016907508515
    mean_inference_ms: 4.2910378398481415
    mean_raw_obs_processing_ms: 0.3784356077846699
  time_since_restore: 6608.895899057388
  time_this_iter_s: 25.474984407424927
  time_total_s: 6608.895899057388
  timers:
    learn_throughput: 8680.172
    learn_time_ms: 18639.262
    sample_throughput: 23738.089
    sample_time_ms: 6815.713
    update_time_ms: 30.943
  timestamp: 1602783261
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: f0f97_00000
  
2020-10-15 17:34:22,614	WARNING util.py:136 -- The `process_trial` operation took 0.7876784801483154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    258 |           6608.9 | 41742336 |  292.555 |              321.404 |              126.556 |            791.939 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3193.029118672325
    time_step_min: 3003
  date: 2020-10-15_17-34-48
  done: false
  episode_len_mean: 791.9542305508234
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.6445086870976
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 52830
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4128494746189283e-46
        cur_lr: 5.0e-05
        entropy: 0.059510900949438415
        entropy_coeff: 0.0005000000000000001
        kl: 0.004437676126447816
        model: {}
        policy_loss: -0.010416357525779555
        total_loss: 0.34389924506346387
        vf_explained_var: 0.9992349743843079
        vf_loss: 0.3543453539411227
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.341
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464805220544023
    mean_env_wait_ms: 1.2229654469693338
    mean_inference_ms: 4.2909120627997615
    mean_raw_obs_processing_ms: 0.3784285727432345
  time_since_restore: 6634.537575483322
  time_this_iter_s: 25.641676425933838
  time_total_s: 6634.537575483322
  timers:
    learn_throughput: 8693.933
    learn_time_ms: 18609.76
    sample_throughput: 23785.286
    sample_time_ms: 6802.189
    update_time_ms: 28.994
  timestamp: 1602783288
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: f0f97_00000
  
2020-10-15 17:34:49,221	WARNING util.py:136 -- The `process_trial` operation took 0.7267603874206543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    259 |          6634.54 | 41904128 |  292.645 |              321.404 |              126.556 |            791.954 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3192.37320754717
    time_step_min: 3003
  date: 2020-10-15_17-35-14
  done: false
  episode_len_mean: 791.9622591712853
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.739330872347
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 53046
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7064247373094642e-46
        cur_lr: 5.0e-05
        entropy: 0.06240119474629561
        entropy_coeff: 0.0005000000000000001
        kl: 0.004313136295725902
        model: {}
        policy_loss: -0.008447058983923247
        total_loss: 0.603955884774526
        vf_explained_var: 0.9985940456390381
        vf_loss: 0.6124341289202372
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58666666666667
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464784533856699
    mean_env_wait_ms: 1.2229108518461131
    mean_inference_ms: 4.290774293235926
    mean_raw_obs_processing_ms: 0.3784217468609147
  time_since_restore: 6660.042052268982
  time_this_iter_s: 25.50447678565979
  time_total_s: 6660.042052268982
  timers:
    learn_throughput: 8708.101
    learn_time_ms: 18579.481
    sample_throughput: 23884.317
    sample_time_ms: 6773.985
    update_time_ms: 27.053
  timestamp: 1602783314
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: f0f97_00000
  
2020-10-15 17:35:15,730	WARNING util.py:136 -- The `process_trial` operation took 0.7606878280639648 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    260 |          6660.04 | 42065920 |  292.739 |              321.404 |              126.556 |            791.962 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3191.8334179213894
    time_step_min: 3003
  date: 2020-10-15_17-35-41
  done: false
  episode_len_mean: 791.9659874166589
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.82262573144294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 53245
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.532123686547321e-47
        cur_lr: 5.0e-05
        entropy: 0.05990123966087898
        entropy_coeff: 0.0005000000000000001
        kl: 0.003541002282872796
        model: {}
        policy_loss: -0.0076484980624324335
        total_loss: 0.35942038148641586
        vf_explained_var: 0.9990923404693604
        vf_loss: 0.3670988182226817
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.690322580645162
    gpu_util_percent0: 0.31225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647657840978853
    mean_env_wait_ms: 1.222864221033396
    mean_inference_ms: 4.290655746595546
    mean_raw_obs_processing_ms: 0.3784157943288473
  time_since_restore: 6686.11839056015
  time_this_iter_s: 26.076338291168213
  time_total_s: 6686.11839056015
  timers:
    learn_throughput: 8677.388
    learn_time_ms: 18645.242
    sample_throughput: 23854.838
    sample_time_ms: 6782.356
    update_time_ms: 28.326
  timestamp: 1602783341
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: f0f97_00000
  
2020-10-15 17:35:42,735	WARNING util.py:136 -- The `process_trial` operation took 0.6888465881347656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    261 |          6686.12 | 42227712 |  292.823 |              321.404 |              126.556 |            791.966 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3191.2794012963172
    time_step_min: 3003
  date: 2020-10-15_17-36-08
  done: false
  episode_len_mean: 791.9772216815153
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 292.90670555975214
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 183
  episodes_total: 53428
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2660618432736604e-47
        cur_lr: 5.0e-05
        entropy: 0.05991037096828222
        entropy_coeff: 0.0005000000000000001
        kl: 0.005094502354040742
        model: {}
        policy_loss: -0.007205934448090072
        total_loss: 0.1730800693233808
        vf_explained_var: 0.999550998210907
        vf_loss: 0.18031595771511397
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.59677419354839
    gpu_util_percent0: 0.2922580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647483276194764
    mean_env_wait_ms: 1.222817589643499
    mean_inference_ms: 4.290541966308374
    mean_raw_obs_processing_ms: 0.37840952297990205
  time_since_restore: 6712.048088312149
  time_this_iter_s: 25.9296977519989
  time_total_s: 6712.048088312149
  timers:
    learn_throughput: 8670.516
    learn_time_ms: 18660.019
    sample_throughput: 23798.071
    sample_time_ms: 6798.534
    update_time_ms: 28.5
  timestamp: 1602783368
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: f0f97_00000
  
2020-10-15 17:36:09,832	WARNING util.py:136 -- The `process_trial` operation took 0.7401587963104248 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    262 |          6712.05 | 42389504 |  292.907 |              321.404 |              126.556 |            791.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3190.6410117704118
    time_step_min: 3003
  date: 2020-10-15_17-36-35
  done: false
  episode_len_mean: 791.986282732271
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.00407203146875
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 227
  episodes_total: 53655
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2660618432736604e-47
        cur_lr: 5.0e-05
        entropy: 0.06336327890555064
        entropy_coeff: 0.0005000000000000001
        kl: 0.004228046202721695
        model: {}
        policy_loss: -0.008802599312427143
        total_loss: 0.3579979290564855
        vf_explained_var: 0.999213695526123
        vf_loss: 0.366832214097182
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.433333333333337
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647309492251515
    mean_env_wait_ms: 1.2227581997482166
    mean_inference_ms: 4.290407928828489
    mean_raw_obs_processing_ms: 0.37840201823437297
  time_since_restore: 6737.74844622612
  time_this_iter_s: 25.700357913970947
  time_total_s: 6737.74844622612
  timers:
    learn_throughput: 8675.962
    learn_time_ms: 18648.307
    sample_throughput: 23757.674
    sample_time_ms: 6810.094
    update_time_ms: 28.689
  timestamp: 1602783395
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: f0f97_00000
  
2020-10-15 17:36:36,460	WARNING util.py:136 -- The `process_trial` operation took 0.6837120056152344 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    263 |          6737.75 | 42551296 |  293.004 |              321.404 |              126.556 |            791.986 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3190.056624110312
    time_step_min: 3003
  date: 2020-10-15_17-37-02
  done: false
  episode_len_mean: 791.9960636500363
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.08865020969245
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 53857
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1330309216368302e-47
        cur_lr: 5.0e-05
        entropy: 0.05802435179551443
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010097624320527151
        total_loss: .nan
        vf_explained_var: 0.9990975260734558
        vf_loss: 0.3767656609416008
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.845161290322583
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647066735081418
    mean_env_wait_ms: 1.2227104065910983
    mean_inference_ms: 4.290286110551639
    mean_raw_obs_processing_ms: 0.37839576944164
  time_since_restore: 6763.661660909653
  time_this_iter_s: 25.913214683532715
  time_total_s: 6763.661660909653
  timers:
    learn_throughput: 8675.393
    learn_time_ms: 18649.53
    sample_throughput: 23776.133
    sample_time_ms: 6804.807
    update_time_ms: 29.486
  timestamp: 1602783422
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:37:03,523	WARNING util.py:136 -- The `process_trial` operation took 0.7349324226379395 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    264 |          6763.66 | 42713088 |  293.089 |              321.404 |              126.556 |            791.996 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3189.509879446677
    time_step_min: 3003
  date: 2020-10-15_17-37-29
  done: false
  episode_len_mean: 792.0079005310193
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.1722881300651
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 54047
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.199546382455246e-47
        cur_lr: 5.0e-05
        entropy: 0.061193171267708145
        entropy_coeff: 0.0005000000000000001
        kl: 0.006086920349237819
        model: {}
        policy_loss: -0.005661854539842655
        total_loss: 0.2555129254857699
        vf_explained_var: 0.999360978603363
        vf_loss: 0.261205376436313
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.466666666666672
    gpu_util_percent0: 0.3946666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646913246949142
    mean_env_wait_ms: 1.2226637012016264
    mean_inference_ms: 4.2901747266516965
    mean_raw_obs_processing_ms: 0.37838981690952445
  time_since_restore: 6789.495182275772
  time_this_iter_s: 25.833521366119385
  time_total_s: 6789.495182275772
  timers:
    learn_throughput: 8668.911
    learn_time_ms: 18663.475
    sample_throughput: 23780.558
    sample_time_ms: 6803.541
    update_time_ms: 31.281
  timestamp: 1602783449
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: f0f97_00000
  
2020-10-15 17:37:30,328	WARNING util.py:136 -- The `process_trial` operation took 0.7261948585510254 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    265 |           6789.5 | 42874880 |  293.172 |              321.404 |              126.556 |            792.008 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.9229705052294
    time_step_min: 3003
  date: 2020-10-15_17-37-55
  done: false
  episode_len_mean: 792.0151679905638
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.262556265394
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 54259
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.199546382455246e-47
        cur_lr: 5.0e-05
        entropy: 0.07321852756043275
        entropy_coeff: 0.0005000000000000001
        kl: 0.01511737199810644
        model: {}
        policy_loss: -0.008733372977682544
        total_loss: 0.2882388085126877
        vf_explained_var: 0.9992904663085938
        vf_loss: 0.2970087875922521
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.326666666666668
    gpu_util_percent0: 0.3646666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646729349870183
    mean_env_wait_ms: 1.2226091495921851
    mean_inference_ms: 4.290052803351904
    mean_raw_obs_processing_ms: 0.37838286021364115
  time_since_restore: 6815.004501581192
  time_this_iter_s: 25.509319305419922
  time_total_s: 6815.004501581192
  timers:
    learn_throughput: 8685.37
    learn_time_ms: 18628.108
    sample_throughput: 23792.138
    sample_time_ms: 6800.23
    update_time_ms: 30.034
  timestamp: 1602783475
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: f0f97_00000
  
2020-10-15 17:37:56,951	WARNING util.py:136 -- The `process_trial` operation took 0.7686233520507812 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    266 |             6815 | 43036672 |  293.263 |              321.404 |              126.556 |            792.015 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.593926255259
    time_step_min: 3003
  date: 2020-10-15_17-38-22
  done: false
  episode_len_mean: 791.9929511536978
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.29394000581794
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 54477
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.199546382455246e-47
        cur_lr: 5.0e-05
        entropy: 0.1324733110765616
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012530409498140216
        total_loss: .nan
        vf_explained_var: 0.9926699995994568
        vf_loss: 3.1462069948514304
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25483870967742
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464650322953133
    mean_env_wait_ms: 1.2225554569996742
    mean_inference_ms: 4.289923462506317
    mean_raw_obs_processing_ms: 0.3783760147068144
  time_since_restore: 6840.917728424072
  time_this_iter_s: 25.91322684288025
  time_total_s: 6840.917728424072
  timers:
    learn_throughput: 8688.551
    learn_time_ms: 18621.286
    sample_throughput: 23747.994
    sample_time_ms: 6812.87
    update_time_ms: 30.103
  timestamp: 1602783502
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:38:23,881	WARNING util.py:136 -- The `process_trial` operation took 0.7320866584777832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    267 |          6840.92 | 43198464 |  293.294 |              321.404 |              126.556 |            791.993 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.756160070296
    time_step_min: 3003
  date: 2020-10-15_17-38-49
  done: false
  episode_len_mean: 791.9615891132572
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.25680864837966
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 54672
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.799319573682868e-47
        cur_lr: 5.0e-05
        entropy: 0.14815256372094154
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012552107548496375
        total_loss: .nan
        vf_explained_var: 0.9891109466552734
        vf_loss: 4.947665969530742
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333333
    gpu_util_percent0: 0.3273333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646347927144138
    mean_env_wait_ms: 1.2225094872151734
    mean_inference_ms: 4.2898144908702776
    mean_raw_obs_processing_ms: 0.37837037402537227
  time_since_restore: 6866.498405218124
  time_this_iter_s: 25.580676794052124
  time_total_s: 6866.498405218124
  timers:
    learn_throughput: 8687.07
    learn_time_ms: 18624.462
    sample_throughput: 23733.014
    sample_time_ms: 6817.17
    update_time_ms: 32.275
  timestamp: 1602783529
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:38:50,420	WARNING util.py:136 -- The `process_trial` operation took 0.702871561050415 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    268 |           6866.5 | 43360256 |  293.257 |              321.404 |              126.556 |            791.962 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.9065504095147
    time_step_min: 3003
  date: 2020-10-15_17-39-16
  done: false
  episode_len_mean: 791.9283722456121
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.24002008161835
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 54867
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.198979360524302e-47
        cur_lr: 5.0e-05
        entropy: 0.1332733283440272
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057465313002467155
        model: {}
        policy_loss: -0.011206828190552187
        total_loss: 3.9897732535998025
        vf_explained_var: 0.9912099242210388
        vf_loss: 4.001046697298686
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.99677419354839
    gpu_util_percent0: 0.3580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646174538201534
    mean_env_wait_ms: 1.222460876153632
    mean_inference_ms: 4.2897003027158185
    mean_raw_obs_processing_ms: 0.3783636760665959
  time_since_restore: 6892.521723508835
  time_this_iter_s: 26.02331829071045
  time_total_s: 6892.521723508835
  timers:
    learn_throughput: 8669.013
    learn_time_ms: 18663.256
    sample_throughput: 23776.485
    sample_time_ms: 6804.706
    update_time_ms: 34.021
  timestamp: 1602783556
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: f0f97_00000
  
2020-10-15 17:39:17,488	WARNING util.py:136 -- The `process_trial` operation took 0.7809960842132568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    269 |          6892.52 | 43522048 |   293.24 |              321.404 |              126.556 |            791.928 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.8530790190734
    time_step_min: 3003
  date: 2020-10-15_17-39-43
  done: false
  episode_len_mean: 791.899303034703
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.2597682575716
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 229
  episodes_total: 55096
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.198979360524302e-47
        cur_lr: 5.0e-05
        entropy: 0.10916733865936597
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01074708018131787
        total_loss: .nan
        vf_explained_var: 0.9941513538360596
        vf_loss: 2.702947656313578
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.500000000000007
    gpu_util_percent0: 0.3576666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645965234288139
    mean_env_wait_ms: 1.2224045118136266
    mean_inference_ms: 4.289570873840888
    mean_raw_obs_processing_ms: 0.3783569021813647
  time_since_restore: 6918.257988214493
  time_this_iter_s: 25.73626470565796
  time_total_s: 6918.257988214493
  timers:
    learn_throughput: 8673.475
    learn_time_ms: 18653.654
    sample_throughput: 23699.417
    sample_time_ms: 6826.835
    update_time_ms: 34.512
  timestamp: 1602783583
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:39:44,248	WARNING util.py:136 -- The `process_trial` operation took 0.7723290920257568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    270 |          6918.26 | 43683840 |   293.26 |              321.404 |              126.556 |            791.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.5412473756605
    time_step_min: 3003
  date: 2020-10-15_17-40-09
  done: false
  episode_len_mean: 791.8910448840826
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.31396755357787
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 55298
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0798469040786457e-46
        cur_lr: 5.0e-05
        entropy: 0.08124607180555661
        entropy_coeff: 0.0005000000000000001
        kl: 0.003909260713650535
        model: {}
        policy_loss: -0.010752088203541158
        total_loss: 1.2522992591063182
        vf_explained_var: 0.9969494342803955
        vf_loss: 1.2630919764439266
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645792266904853
    mean_env_wait_ms: 1.2223597106915018
    mean_inference_ms: 4.289458922509447
    mean_raw_obs_processing_ms: 0.37835094033337735
  time_since_restore: 6943.754240512848
  time_this_iter_s: 25.496252298355103
  time_total_s: 6943.754240512848
  timers:
    learn_throughput: 8682.626
    learn_time_ms: 18633.995
    sample_throughput: 23847.908
    sample_time_ms: 6784.327
    update_time_ms: 33.017
  timestamp: 1602783609
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: f0f97_00000
  
2020-10-15 17:40:10,807	WARNING util.py:136 -- The `process_trial` operation took 0.7927460670471191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    271 |          6943.75 | 43845632 |  293.314 |              321.404 |              126.556 |            791.891 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3188.096457048021
    time_step_min: 3003
  date: 2020-10-15_17-40-36
  done: false
  episode_len_mean: 791.8958543619323
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.3791884235282
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 182
  episodes_total: 55480
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.399234520393228e-47
        cur_lr: 5.0e-05
        entropy: 0.06915338647862275
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010121257694360489
        total_loss: .nan
        vf_explained_var: 0.9983662962913513
        vf_loss: 0.6431886156400045
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.135483870967743
    gpu_util_percent0: 0.3409677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645618314893982
    mean_env_wait_ms: 1.2223158795021294
    mean_inference_ms: 4.289354592005417
    mean_raw_obs_processing_ms: 0.37834537531613954
  time_since_restore: 6969.56095957756
  time_this_iter_s: 25.806719064712524
  time_total_s: 6969.56095957756
  timers:
    learn_throughput: 8682.864
    learn_time_ms: 18633.483
    sample_throughput: 23928.524
    sample_time_ms: 6761.47
    update_time_ms: 33.376
  timestamp: 1602783636
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:40:37,721	WARNING util.py:136 -- The `process_trial` operation took 0.7973072528839111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    272 |          6969.56 | 44007424 |  293.379 |              321.404 |              126.556 |            791.896 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3187.5259356415186
    time_step_min: 3003
  date: 2020-10-15_17-41-03
  done: false
  episode_len_mean: 791.9002926233776
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.4645817273678
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 55703
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.098851780589841e-47
        cur_lr: 5.0e-05
        entropy: 0.06568159287174542
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006587746805356194
        total_loss: .nan
        vf_explained_var: 0.9988531470298767
        vf_loss: 0.5009665687878927
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78333333333334
    gpu_util_percent0: 0.3483333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645458955711352
    mean_env_wait_ms: 1.2222598709208088
    mean_inference_ms: 4.289231042751752
    mean_raw_obs_processing_ms: 0.37833822299577125
  time_since_restore: 6995.346583604813
  time_this_iter_s: 25.785624027252197
  time_total_s: 6995.346583604813
  timers:
    learn_throughput: 8676.989
    learn_time_ms: 18646.1
    sample_throughput: 23953.919
    sample_time_ms: 6754.302
    update_time_ms: 33.566
  timestamp: 1602783663
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:41:04,683	WARNING util.py:136 -- The `process_trial` operation took 0.8197526931762695 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    273 |          6995.35 | 44169216 |  293.465 |              321.404 |              126.556 |              791.9 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3186.974974939138
    time_step_min: 3003
  date: 2020-10-15_17-41-30
  done: false
  episode_len_mean: 791.9085673403684
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.54743915636374
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 55910
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2148277670884763e-46
        cur_lr: 5.0e-05
        entropy: 0.06303235422819853
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037769193295389414
        model: {}
        policy_loss: -0.0068167134110505385
        total_loss: 0.3429582367340724
        vf_explained_var: 0.9991714358329773
        vf_loss: 0.3498064676920573
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14838709677419
    gpu_util_percent0: 0.34225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645221547222623
    mean_env_wait_ms: 1.2222118540265439
    mean_inference_ms: 4.289117506255324
    mean_raw_obs_processing_ms: 0.3783321157588559
  time_since_restore: 7021.087121486664
  time_this_iter_s: 25.740537881851196
  time_total_s: 7021.087121486664
  timers:
    learn_throughput: 8688.654
    learn_time_ms: 18621.065
    sample_throughput: 23929.747
    sample_time_ms: 6761.125
    update_time_ms: 33.404
  timestamp: 1602783690
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: f0f97_00000
  
2020-10-15 17:41:31,423	WARNING util.py:136 -- The `process_trial` operation took 0.7395191192626953 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    274 |          7021.09 | 44331008 |  293.547 |              321.404 |              126.556 |            791.909 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3186.4685922252156
    time_step_min: 3003
  date: 2020-10-15_17-41-57
  done: false
  episode_len_mean: 791.920034225209
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.6239795052068
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 56099
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.074138835442381e-47
        cur_lr: 5.0e-05
        entropy: 0.06155677450199922
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009977591718779877
        total_loss: .nan
        vf_explained_var: 0.9992919564247131
        vf_loss: 0.28021199504534405
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.569999999999993
    gpu_util_percent0: 0.36033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645090896813048
    mean_env_wait_ms: 1.2221673512281854
    mean_inference_ms: 4.2890165050279245
    mean_raw_obs_processing_ms: 0.3783267933789839
  time_since_restore: 7046.780380010605
  time_this_iter_s: 25.69325852394104
  time_total_s: 7046.780380010605
  timers:
    learn_throughput: 8691.258
    learn_time_ms: 18615.488
    sample_throughput: 23957.161
    sample_time_ms: 6753.388
    update_time_ms: 31.345
  timestamp: 1602783717
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:41:58,204	WARNING util.py:136 -- The `process_trial` operation took 0.8326668739318848 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    275 |          7046.78 | 44492800 |  293.624 |              321.404 |              126.556 |             791.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3185.9668462126465
    time_step_min: 3003
  date: 2020-10-15_17-42-23
  done: false
  episode_len_mean: 791.9279383292776
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.69917240218615
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 56299
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.111208253163573e-47
        cur_lr: 5.0e-05
        entropy: 0.0672562699764967
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008666521724080667
        total_loss: .nan
        vf_explained_var: 0.9987183213233948
        vf_loss: 0.5411611770590147
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.360000000000003
    gpu_util_percent0: 0.3436666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644921201189612
    mean_env_wait_ms: 1.2221189526199243
    mean_inference_ms: 4.288906475503532
    mean_raw_obs_processing_ms: 0.3783201387926303
  time_since_restore: 7072.164923429489
  time_this_iter_s: 25.384543418884277
  time_total_s: 7072.164923429489
  timers:
    learn_throughput: 8698.172
    learn_time_ms: 18600.69
    sample_throughput: 23956.465
    sample_time_ms: 6753.584
    update_time_ms: 30.921
  timestamp: 1602783743
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:42:24,705	WARNING util.py:136 -- The `process_trial` operation took 0.7937252521514893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    276 |          7072.16 | 44654592 |  293.699 |              321.404 |              126.556 |            791.928 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3185.404359296705
    time_step_min: 3003
  date: 2020-10-15_17-42-50
  done: false
  episode_len_mean: 791.9359729667569
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.7863583555951
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 56523
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3666812379745356e-46
        cur_lr: 5.0e-05
        entropy: 0.06501641124486923
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034559111809358
        model: {}
        policy_loss: -0.00856679220062991
        total_loss: 0.2800283320248127
        vf_explained_var: 0.9993317127227783
        vf_loss: 0.28862763196229935
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05161290322581
    gpu_util_percent0: 0.2558064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464471522265959
    mean_env_wait_ms: 1.222064013575126
    mean_inference_ms: 4.2887847953837674
    mean_raw_obs_processing_ms: 0.3783140137301771
  time_since_restore: 7098.279344320297
  time_this_iter_s: 26.114420890808105
  time_total_s: 7098.279344320297
  timers:
    learn_throughput: 8680.929
    learn_time_ms: 18637.636
    sample_throughput: 24018.126
    sample_time_ms: 6736.246
    update_time_ms: 30.413
  timestamp: 1602783770
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: f0f97_00000
  
2020-10-15 17:42:51,968	WARNING util.py:136 -- The `process_trial` operation took 0.7713954448699951 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    277 |          7098.28 | 44816384 |  293.786 |              321.404 |              126.556 |            791.936 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3184.8768020044818
    time_step_min: 3003
  date: 2020-10-15_17-43-17
  done: false
  episode_len_mean: 791.9459087783636
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.86597529091176
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 56719
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.833406189872678e-47
        cur_lr: 5.0e-05
        entropy: 0.05979300197213888
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006964673288166523
        total_loss: .nan
        vf_explained_var: 0.9995889663696289
        vf_loss: 0.16474285970131555
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.258064516129036
    gpu_util_percent0: 0.35516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464455798985571
    mean_env_wait_ms: 1.2220192691394258
    mean_inference_ms: 4.288682580595924
    mean_raw_obs_processing_ms: 0.37830844903876026
  time_since_restore: 7124.019709587097
  time_this_iter_s: 25.740365266799927
  time_total_s: 7124.019709587097
  timers:
    learn_throughput: 8676.857
    learn_time_ms: 18646.384
    sample_throughput: 24022.422
    sample_time_ms: 6735.041
    update_time_ms: 29.717
  timestamp: 1602783797
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:43:18,782	WARNING util.py:136 -- The `process_trial` operation took 0.8100755214691162 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    278 |          7124.02 | 44978176 |  293.866 |              321.404 |              126.556 |            791.946 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3184.403418983802
    time_step_min: 3003
  date: 2020-10-15_17-43-44
  done: false
  episode_len_mean: 791.9510939284772
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 293.9383679515473
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 56905
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0250109284809019e-46
        cur_lr: 5.0e-05
        entropy: 0.05832227102170388
        entropy_coeff: 0.0005000000000000001
        kl: 0.002752711996436119
        model: {}
        policy_loss: -0.006478061972302385
        total_loss: 0.6636315112312635
        vf_explained_var: 0.9983188509941101
        vf_loss: 0.6701387340823809
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096773
    gpu_util_percent0: 0.36193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464438720296594
    mean_env_wait_ms: 1.2219738547750743
    mean_inference_ms: 4.288582221772061
    mean_raw_obs_processing_ms: 0.3783027192339028
  time_since_restore: 7149.848839998245
  time_this_iter_s: 25.82913041114807
  time_total_s: 7149.848839998245
  timers:
    learn_throughput: 8683.619
    learn_time_ms: 18631.864
    sample_throughput: 24010.988
    sample_time_ms: 6738.248
    update_time_ms: 30.67
  timestamp: 1602783824
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: f0f97_00000
  
2020-10-15 17:43:45,858	WARNING util.py:136 -- The `process_trial` operation took 0.8159575462341309 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    279 |          7149.85 | 45139968 |  293.938 |              321.404 |              126.556 |            791.951 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3183.792512657008
    time_step_min: 3003
  date: 2020-10-15_17-44-11
  done: false
  episode_len_mean: 791.9632060774738
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.0296903817352
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 57129
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.125054642404509e-47
        cur_lr: 5.0e-05
        entropy: 0.058607619566222034
        entropy_coeff: 0.0005000000000000001
        kl: 0.004321416257880628
        model: {}
        policy_loss: -0.005669889998292395
        total_loss: 0.16207116221388182
        vf_explained_var: 0.9996455311775208
        vf_loss: 0.16777035097281137
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.360000000000003
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644233068198456
    mean_env_wait_ms: 1.2219172491588233
    mean_inference_ms: 4.288465375293337
    mean_raw_obs_processing_ms: 0.37829613413802565
  time_since_restore: 7175.759289979935
  time_this_iter_s: 25.910449981689453
  time_total_s: 7175.759289979935
  timers:
    learn_throughput: 8664.557
    learn_time_ms: 18672.854
    sample_throughput: 24073.291
    sample_time_ms: 6720.81
    update_time_ms: 32.253
  timestamp: 1602783851
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: f0f97_00000
  
2020-10-15 17:44:12,870	WARNING util.py:136 -- The `process_trial` operation took 0.7347595691680908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    280 |          7175.76 | 45301760 |   294.03 |              321.404 |              126.556 |            791.963 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3183.2326665270216
    time_step_min: 3003
  date: 2020-10-15_17-44-38
  done: false
  episode_len_mean: 791.9771339868141
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.1138080494478
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 57334
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5625273212022546e-47
        cur_lr: 5.0e-05
        entropy: 0.05670665514965852
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007322403728418673
        total_loss: .nan
        vf_explained_var: 0.9997036457061768
        vf_loss: 0.13024427679677805
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.099999999999998
    gpu_util_percent0: 0.29032258064516137
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644006062029458
    mean_env_wait_ms: 1.2218696312581256
    mean_inference_ms: 4.2883582763482915
    mean_raw_obs_processing_ms: 0.3782902649323793
  time_since_restore: 7201.3601331710815
  time_this_iter_s: 25.60084319114685
  time_total_s: 7201.3601331710815
  timers:
    learn_throughput: 8662.79
    learn_time_ms: 18676.663
    sample_throughput: 24039.343
    sample_time_ms: 6730.3
    update_time_ms: 32.98
  timestamp: 1602783878
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:44:39,757	WARNING util.py:136 -- The `process_trial` operation took 0.8445608615875244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    281 |          7201.36 | 45463552 |  294.114 |              321.404 |              126.556 |            791.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3182.7326281359824
    time_step_min: 3003
  date: 2020-10-15_17-45-04
  done: false
  episode_len_mean: 791.9843891245393
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.18776387756236
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 57524
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.843790981803382e-47
        cur_lr: 5.0e-05
        entropy: 0.06361776652435462
        entropy_coeff: 0.0005000000000000001
        kl: 0.005110125833501418
        model: {}
        policy_loss: -0.006766456914192531
        total_loss: 0.2866070345044136
        vf_explained_var: 0.9992968440055847
        vf_loss: 0.2934052919348081
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64
    gpu_util_percent0: 0.35766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643875748193616
    mean_env_wait_ms: 1.221824453418796
    mean_inference_ms: 4.288260965237364
    mean_raw_obs_processing_ms: 0.37828522761444255
  time_since_restore: 7226.531241893768
  time_this_iter_s: 25.171108722686768
  time_total_s: 7226.531241893768
  timers:
    learn_throughput: 8687.54
    learn_time_ms: 18623.453
    sample_throughput: 24047.556
    sample_time_ms: 6728.002
    update_time_ms: 33.113
  timestamp: 1602783904
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: f0f97_00000
  
2020-10-15 17:45:06,203	WARNING util.py:136 -- The `process_trial` operation took 0.8365123271942139 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    282 |          7226.53 | 45625344 |  294.188 |              321.404 |              126.556 |            791.984 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3182.2229464997745
    time_step_min: 3003
  date: 2020-10-15_17-45-32
  done: false
  episode_len_mean: 791.9971071230599
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.2639728773316
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 57728
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.843790981803382e-47
        cur_lr: 5.0e-05
        entropy: 0.0637574599434932
        entropy_coeff: 0.0005000000000000001
        kl: 0.00486303586512804
        model: {}
        policy_loss: -0.007661503938531193
        total_loss: 0.4605453635255496
        vf_explained_var: 0.9989061951637268
        vf_loss: 0.46823875854412716
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709685
    gpu_util_percent0: 0.2964516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464370529151402
    mean_env_wait_ms: 1.221774025273463
    mean_inference_ms: 4.28815647721161
    mean_raw_obs_processing_ms: 0.3782783189022224
  time_since_restore: 7252.527693271637
  time_this_iter_s: 25.996451377868652
  time_total_s: 7252.527693271637
  timers:
    learn_throughput: 8681.537
    learn_time_ms: 18636.332
    sample_throughput: 24027.593
    sample_time_ms: 6733.592
    update_time_ms: 34.717
  timestamp: 1602783932
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: f0f97_00000
  
2020-10-15 17:45:33,468	WARNING util.py:136 -- The `process_trial` operation took 0.8301272392272949 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    283 |          7252.53 | 45787136 |  294.264 |              321.404 |              126.556 |            791.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3181.6769201540615
    time_step_min: 3003
  date: 2020-10-15_17-45-59
  done: false
  episode_len_mean: 792.008024851152
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.344922170187
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 217
  episodes_total: 57945
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.921895490901691e-47
        cur_lr: 5.0e-05
        entropy: 0.06287674605846405
        entropy_coeff: 0.0005000000000000001
        kl: 0.004183895148647328
        model: {}
        policy_loss: -0.00800537963611229
        total_loss: 0.3267648344238599
        vf_explained_var: 0.999258816242218
        vf_loss: 0.33480163911978406
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.180000000000007
    gpu_util_percent0: 0.3643333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643516742310514
    mean_env_wait_ms: 1.2217211898864646
    mean_inference_ms: 4.28804491848075
    mean_raw_obs_processing_ms: 0.3782728645547466
  time_since_restore: 7278.133830070496
  time_this_iter_s: 25.606136798858643
  time_total_s: 7278.133830070496
  timers:
    learn_throughput: 8677.72
    learn_time_ms: 18644.528
    sample_throughput: 24104.359
    sample_time_ms: 6712.147
    update_time_ms: 34.76
  timestamp: 1602783959
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: f0f97_00000
  
2020-10-15 17:46:00,236	WARNING util.py:136 -- The `process_trial` operation took 0.8087749481201172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    284 |          7278.13 | 45948928 |  294.345 |              321.404 |              126.556 |            792.008 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3181.1821295786285
    time_step_min: 3003
  date: 2020-10-15_17-46-26
  done: false
  episode_len_mean: 792.0160813181521
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.4171233159911
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 58142
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.609477454508456e-48
        cur_lr: 5.0e-05
        entropy: 0.06566774845123291
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051942401720831794
        model: {}
        policy_loss: -0.007774918194627389
        total_loss: 0.4100962479909261
        vf_explained_var: 0.9989750981330872
        vf_loss: 0.41790398706992465
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806453
    gpu_util_percent0: 0.32677419354838705
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643363500244821
    mean_env_wait_ms: 1.2216756448515553
    mean_inference_ms: 4.287947284778789
    mean_raw_obs_processing_ms: 0.37826749760057854
  time_since_restore: 7303.951328754425
  time_this_iter_s: 25.817498683929443
  time_total_s: 7303.951328754425
  timers:
    learn_throughput: 8682.729
    learn_time_ms: 18633.773
    sample_throughput: 24073.868
    sample_time_ms: 6720.648
    update_time_ms: 35.42
  timestamp: 1602783986
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: f0f97_00000
  
2020-10-15 17:46:27,159	WARNING util.py:136 -- The `process_trial` operation took 0.8394386768341064 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    285 |          7303.95 | 46110720 |  294.417 |              321.404 |              126.556 |            792.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3180.7291319938918
    time_step_min: 3003
  date: 2020-10-15_17-46-52
  done: false
  episode_len_mean: 792.0252533045312
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.4835366990893
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 187
  episodes_total: 58329
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.609477454508456e-48
        cur_lr: 5.0e-05
        entropy: 0.063668812935551
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035072601749561727
        model: {}
        policy_loss: -0.00953459104251427
        total_loss: 0.2969442879160245
        vf_explained_var: 0.9992637634277344
        vf_loss: 0.3065107042590777
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.270000000000003
    gpu_util_percent0: 0.29333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643206014448737
    mean_env_wait_ms: 1.2216296725630937
    mean_inference_ms: 4.287853606711433
    mean_raw_obs_processing_ms: 0.3782619125489679
  time_since_restore: 7329.616112470627
  time_this_iter_s: 25.664783716201782
  time_total_s: 7329.616112470627
  timers:
    learn_throughput: 8669.59
    learn_time_ms: 18662.013
    sample_throughput: 24080.251
    sample_time_ms: 6718.867
    update_time_ms: 37.742
  timestamp: 1602784012
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: f0f97_00000
  
2020-10-15 17:46:53,952	WARNING util.py:136 -- The `process_trial` operation took 0.7690169811248779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    286 |          7329.62 | 46272512 |  294.484 |              321.404 |              126.556 |            792.025 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3180.2204580413604
    time_step_min: 3003
  date: 2020-10-15_17-47-19
  done: false
  episode_len_mean: 792.0348213675798
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.56362397801314
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 227
  episodes_total: 58556
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.804738727254228e-48
        cur_lr: 5.0e-05
        entropy: 0.06372366504122813
        entropy_coeff: 0.0005000000000000001
        kl: 0.004013514670077711
        model: {}
        policy_loss: -0.007482810959724399
        total_loss: 0.5016263127326965
        vf_explained_var: 0.9989170432090759
        vf_loss: 0.5091409633557001
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45483870967743
    gpu_util_percent0: 0.34548387096774186
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643041106928906
    mean_env_wait_ms: 1.221572575451025
    mean_inference_ms: 4.287742005076219
    mean_raw_obs_processing_ms: 0.37825568156090694
  time_since_restore: 7355.405994176865
  time_this_iter_s: 25.789881706237793
  time_total_s: 7355.405994176865
  timers:
    learn_throughput: 8686.655
    learn_time_ms: 18625.352
    sample_throughput: 24068.27
    sample_time_ms: 6722.211
    update_time_ms: 38.596
  timestamp: 1602784039
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: f0f97_00000
  
2020-10-15 17:47:20,757	WARNING util.py:136 -- The `process_trial` operation took 0.740227460861206 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    287 |          7355.41 | 46434304 |  294.564 |              321.404 |              126.556 |            792.035 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3179.7707283008585
    time_step_min: 3003
  date: 2020-10-15_17-47-46
  done: false
  episode_len_mean: 792.0447598624868
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.6328747497434
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 58758
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.402369363627114e-48
        cur_lr: 5.0e-05
        entropy: 0.06156160278866688
        entropy_coeff: 0.0005000000000000001
        kl: 0.00418394790419067
        model: {}
        policy_loss: -0.007642833411712975
        total_loss: 0.4747292846441269
        vf_explained_var: 0.9988923668861389
        vf_loss: 0.48240288843711215
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.31
    gpu_util_percent0: 0.37033333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464284043435722
    mean_env_wait_ms: 1.2215266462131473
    mean_inference_ms: 4.287640507987633
    mean_raw_obs_processing_ms: 0.37825039714435565
  time_since_restore: 7381.056595087051
  time_this_iter_s: 25.650600910186768
  time_total_s: 7381.056595087051
  timers:
    learn_throughput: 8689.54
    learn_time_ms: 18619.167
    sample_throughput: 24051.349
    sample_time_ms: 6726.941
    update_time_ms: 37.361
  timestamp: 1602784066
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: f0f97_00000
  
2020-10-15 17:47:47,603	WARNING util.py:136 -- The `process_trial` operation took 0.8248839378356934 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    288 |          7381.06 | 46596096 |  294.633 |              321.404 |              126.556 |            792.045 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3179.2985756243315
    time_step_min: 3003
  date: 2020-10-15_17-48-13
  done: false
  episode_len_mean: 792.0569305671004
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.70444080150725
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 58949
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.201184681813557e-48
        cur_lr: 5.0e-05
        entropy: 0.06097612498948971
        entropy_coeff: 0.0005000000000000001
        kl: 0.005607279754864673
        model: {}
        policy_loss: -0.006587157224809441
        total_loss: 0.3060295606652896
        vf_explained_var: 0.9992205500602722
        vf_loss: 0.31264720608790714
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30322580645161
    gpu_util_percent0: 0.2967741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464270848344228
    mean_env_wait_ms: 1.2214802697612288
    mean_inference_ms: 4.287547435712963
    mean_raw_obs_processing_ms: 0.37824528475164115
  time_since_restore: 7406.738215684891
  time_this_iter_s: 25.681620597839355
  time_total_s: 7406.738215684891
  timers:
    learn_throughput: 8701.55
    learn_time_ms: 18593.469
    sample_throughput: 24040.919
    sample_time_ms: 6729.859
    update_time_ms: 34.922
  timestamp: 1602784093
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: f0f97_00000
  
2020-10-15 17:48:14,395	WARNING util.py:136 -- The `process_trial` operation took 0.833519458770752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    289 |          7406.74 | 46757888 |  294.704 |              321.404 |              126.556 |            792.057 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3178.8362516706425
    time_step_min: 3003
  date: 2020-10-15_17-48-39
  done: false
  episode_len_mean: 792.0642211140225
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.76925966622474
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 59155
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.201184681813557e-48
        cur_lr: 5.0e-05
        entropy: 0.07004105920592944
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01136403410055209
        total_loss: .nan
        vf_explained_var: 0.9985410571098328
        vf_loss: 0.6202274958292643
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.593333333333337
    gpu_util_percent0: 0.308
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642563307308168
    mean_env_wait_ms: 1.2214285914214222
    mean_inference_ms: 4.287449080159725
    mean_raw_obs_processing_ms: 0.3782386929124655
  time_since_restore: 7432.263925790787
  time_this_iter_s: 25.525710105895996
  time_total_s: 7432.263925790787
  timers:
    learn_throughput: 8716.788
    learn_time_ms: 18560.965
    sample_throughput: 24057.765
    sample_time_ms: 6725.147
    update_time_ms: 32.98
  timestamp: 1602784119
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:48:40,971	WARNING util.py:136 -- The `process_trial` operation took 0.7726497650146484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    290 |          7432.26 | 46919680 |  294.769 |              321.404 |              126.556 |            792.064 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3178.385577506574
    time_step_min: 3003
  date: 2020-10-15_17-49-06
  done: false
  episode_len_mean: 792.0723934647128
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.834229102546
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 215
  episodes_total: 59370
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.801777022720335e-48
        cur_lr: 5.0e-05
        entropy: 0.06774183176457882
        entropy_coeff: 0.0005000000000000001
        kl: 0.004167658606699358
        model: {}
        policy_loss: -0.008677542277534181
        total_loss: 0.8021297504504522
        vf_explained_var: 0.9981705546379089
        vf_loss: 0.8108411629994711
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.219354838709684
    gpu_util_percent0: 0.28806451612903233
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642363069965095
    mean_env_wait_ms: 1.2213773758379523
    mean_inference_ms: 4.28734261298294
    mean_raw_obs_processing_ms: 0.37823367883459535
  time_since_restore: 7458.186842441559
  time_this_iter_s: 25.922916650772095
  time_total_s: 7458.186842441559
  timers:
    learn_throughput: 8702.731
    learn_time_ms: 18590.945
    sample_throughput: 24050.544
    sample_time_ms: 6727.166
    update_time_ms: 32.849
  timestamp: 1602784146
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: f0f97_00000
  
2020-10-15 17:49:08,182	WARNING util.py:136 -- The `process_trial` operation took 0.8294763565063477 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    291 |          7458.19 | 47081472 |  294.834 |              321.404 |              126.556 |            792.072 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3177.9348644198785
    time_step_min: 3003
  date: 2020-10-15_17-49-33
  done: false
  episode_len_mean: 792.0795393499866
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.90264195134205
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 59568
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.008885113601675e-49
        cur_lr: 5.0e-05
        entropy: 0.06672019883990288
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008765857644903008
        total_loss: .nan
        vf_explained_var: 0.9989103674888611
        vf_loss: 0.43189992010593414
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.906666666666663
    gpu_util_percent0: 0.31566666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642217492749457
    mean_env_wait_ms: 1.221331295015155
    mean_inference_ms: 4.287252168976529
    mean_raw_obs_processing_ms: 0.37822867022805096
  time_since_restore: 7483.770982503891
  time_this_iter_s: 25.584140062332153
  time_total_s: 7483.770982503891
  timers:
    learn_throughput: 8678.12
    learn_time_ms: 18643.67
    sample_throughput: 24086.965
    sample_time_ms: 6716.994
    update_time_ms: 30.611
  timestamp: 1602784173
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:49:34,839	WARNING util.py:136 -- The `process_trial` operation took 0.7932980060577393 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    292 |          7483.77 | 47243264 |  294.903 |              321.404 |              126.556 |             792.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3177.510559547137
    time_step_min: 3003
  date: 2020-10-15_17-50-00
  done: false
  episode_len_mean: 792.0915404568656
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 294.9683040766627
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 187
  episodes_total: 59755
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3513327670402512e-48
        cur_lr: 5.0e-05
        entropy: 0.06162620925654968
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040559336193837225
        model: {}
        policy_loss: -0.0064278502444115775
        total_loss: 0.1928547869126002
        vf_explained_var: 0.9995333552360535
        vf_loss: 0.19931344936291376
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82
    gpu_util_percent0: 0.32833333333333325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642072669066547
    mean_env_wait_ms: 1.2212862184863487
    mean_inference_ms: 4.2871618247079555
    mean_raw_obs_processing_ms: 0.3782230159076649
  time_since_restore: 7509.4463703632355
  time_this_iter_s: 25.675387859344482
  time_total_s: 7509.4463703632355
  timers:
    learn_throughput: 8687.225
    learn_time_ms: 18624.128
    sample_throughput: 24127.048
    sample_time_ms: 6705.835
    update_time_ms: 28.48
  timestamp: 1602784200
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: f0f97_00000
  
2020-10-15 17:50:01,672	WARNING util.py:136 -- The `process_trial` operation took 0.7931876182556152 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    293 |          7509.45 | 47405056 |  294.968 |              321.404 |              126.556 |            792.092 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3176.954499190763
    time_step_min: 3003
  date: 2020-10-15_17-50-27
  done: false
  episode_len_mean: 792.1055369379283
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.0505111132325
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 59979
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.756663835201256e-49
        cur_lr: 5.0e-05
        entropy: 0.05739949829876423
        entropy_coeff: 0.0005000000000000001
        kl: 0.004451337154023349
        model: {}
        policy_loss: -0.005594792019110173
        total_loss: 0.23249359180529913
        vf_explained_var: 0.9994910359382629
        vf_loss: 0.23811707769831023
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.635483870967747
    gpu_util_percent0: 0.2993548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641914337825984
    mean_env_wait_ms: 1.221229381267129
    mean_inference_ms: 4.287055995425955
    mean_raw_obs_processing_ms: 0.3782171119136832
  time_since_restore: 7535.123775482178
  time_this_iter_s: 25.67740511894226
  time_total_s: 7535.123775482178
  timers:
    learn_throughput: 8696.781
    learn_time_ms: 18603.665
    sample_throughput: 24057.485
    sample_time_ms: 6725.225
    update_time_ms: 34.51
  timestamp: 1602784227
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: f0f97_00000
  
2020-10-15 17:50:28,499	WARNING util.py:136 -- The `process_trial` operation took 0.8578469753265381 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    294 |          7535.12 | 47566848 |  295.051 |              321.404 |              126.556 |            792.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3176.4680635237382
    time_step_min: 3003
  date: 2020-10-15_17-50-54
  done: false
  episode_len_mean: 792.1171466077334
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.12379137749224
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 60181
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.378331917600628e-49
        cur_lr: 5.0e-05
        entropy: 0.054872218829890095
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036710223260646067
        model: {}
        policy_loss: -0.007569453948841935
        total_loss: 0.18517287572224936
        vf_explained_var: 0.9995307922363281
        vf_loss: 0.1927697646121184
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55806451612903
    gpu_util_percent0: 0.3667741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641729631628386
    mean_env_wait_ms: 1.221183159178865
    mean_inference_ms: 4.286960215513821
    mean_raw_obs_processing_ms: 0.37821213887923494
  time_since_restore: 7560.995462656021
  time_this_iter_s: 25.871687173843384
  time_total_s: 7560.995462656021
  timers:
    learn_throughput: 8686.049
    learn_time_ms: 18626.65
    sample_throughput: 24082.916
    sample_time_ms: 6718.123
    update_time_ms: 36.076
  timestamp: 1602784254
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: f0f97_00000
  
2020-10-15 17:50:55,600	WARNING util.py:136 -- The `process_trial` operation took 0.777764081954956 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    295 |             7561 | 47728640 |  295.124 |              321.404 |              126.556 |            792.117 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3176.0303688354743
    time_step_min: 3003
  date: 2020-10-15_17-51-21
  done: false
  episode_len_mean: 792.12623610674
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.1902000241264
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 60371
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.689165958800314e-49
        cur_lr: 5.0e-05
        entropy: 0.0570521072174112
        entropy_coeff: 0.0005000000000000001
        kl: 0.005172056999678413
        model: {}
        policy_loss: -0.008590368592801193
        total_loss: 0.1743697995940844
        vf_explained_var: 0.9995273947715759
        vf_loss: 0.18298868586619696
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.439999999999998
    gpu_util_percent0: 0.37666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641593431449415
    mean_env_wait_ms: 1.2211375409880971
    mean_inference_ms: 4.286872941561035
    mean_raw_obs_processing_ms: 0.37820736047357867
  time_since_restore: 7586.951310157776
  time_this_iter_s: 25.95584750175476
  time_total_s: 7586.951310157776
  timers:
    learn_throughput: 8678.118
    learn_time_ms: 18643.674
    sample_throughput: 24042.969
    sample_time_ms: 6729.285
    update_time_ms: 35.974
  timestamp: 1602784281
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: f0f97_00000
  
2020-10-15 17:51:22,720	WARNING util.py:136 -- The `process_trial` operation took 0.7947998046875 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    296 |          7586.95 | 47890432 |   295.19 |              321.404 |              126.556 |            792.126 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3175.5363101727953
    time_step_min: 3003
  date: 2020-10-15_17-51-48
  done: false
  episode_len_mean: 792.1315450643776
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.2635124770313
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 209
  episodes_total: 60580
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.689165958800314e-49
        cur_lr: 5.0e-05
        entropy: 0.056316351518034935
        entropy_coeff: 0.0005000000000000001
        kl: 0.003963885654229671
        model: {}
        policy_loss: -0.007367694401182234
        total_loss: 0.2175512077907721
        vf_explained_var: 0.9994816184043884
        vf_loss: 0.22494706263144812
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36451612903226
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641457818563325
    mean_env_wait_ms: 1.221085352594274
    mean_inference_ms: 4.286777113030668
    mean_raw_obs_processing_ms: 0.3782007360880362
  time_since_restore: 7612.659677028656
  time_this_iter_s: 25.708366870880127
  time_total_s: 7612.659677028656
  timers:
    learn_throughput: 8680.039
    learn_time_ms: 18639.547
    sample_throughput: 24055.35
    sample_time_ms: 6725.822
    update_time_ms: 33.844
  timestamp: 1602784308
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: f0f97_00000
  
2020-10-15 17:51:49,514	WARNING util.py:136 -- The `process_trial` operation took 0.7960662841796875 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    297 |          7612.66 | 48052224 |  295.264 |              321.404 |              126.556 |            792.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3175.028609053498
    time_step_min: 3003
  date: 2020-10-15_17-52-15
  done: false
  episode_len_mean: 792.1396637936706
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.33821320647706
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 60796
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.44582979400157e-50
        cur_lr: 5.0e-05
        entropy: 0.05656698625534773
        entropy_coeff: 0.0005000000000000001
        kl: 0.004010701920681943
        model: {}
        policy_loss: -0.006363558878850502
        total_loss: 0.2974345460534096
        vf_explained_var: 0.9992851614952087
        vf_loss: 0.30382638424634933
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34
    gpu_util_percent0: 0.3449999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464125711518175
    mean_env_wait_ms: 1.221033771959549
    mean_inference_ms: 4.286675753264897
    mean_raw_obs_processing_ms: 0.37819590995392277
  time_since_restore: 7638.35999584198
  time_this_iter_s: 25.700318813323975
  time_total_s: 7638.35999584198
  timers:
    learn_throughput: 8674.489
    learn_time_ms: 18651.473
    sample_throughput: 24074.879
    sample_time_ms: 6720.366
    update_time_ms: 34.147
  timestamp: 1602784335
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: f0f97_00000
  
2020-10-15 17:52:16,370	WARNING util.py:136 -- The `process_trial` operation took 0.7825212478637695 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    298 |          7638.36 | 48214016 |  295.338 |              321.404 |              126.556 |             792.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3174.566428747231
    time_step_min: 3003
  date: 2020-10-15_17-52-42
  done: false
  episode_len_mean: 792.1456280434818
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.4058777342371
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 60991
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.222914897000785e-50
        cur_lr: 5.0e-05
        entropy: 0.05701581947505474
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031978714008194706
        model: {}
        policy_loss: -0.008528329276790222
        total_loss: 0.22493603204687437
        vf_explained_var: 0.9994192719459534
        vf_loss: 0.23349286864201227
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.912903225806456
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641129178442952
    mean_env_wait_ms: 1.2209879155006835
    mean_inference_ms: 4.286591272841292
    mean_raw_obs_processing_ms: 0.3781913154403797
  time_since_restore: 7664.322165489197
  time_this_iter_s: 25.962169647216797
  time_total_s: 7664.322165489197
  timers:
    learn_throughput: 8659.609
    learn_time_ms: 18683.522
    sample_throughput: 24070.096
    sample_time_ms: 6721.702
    update_time_ms: 35.739
  timestamp: 1602784362
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: f0f97_00000
  
2020-10-15 17:52:43,500	WARNING util.py:136 -- The `process_trial` operation took 0.8889009952545166 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    299 |          7664.32 | 48375808 |  295.406 |              321.404 |              126.556 |            792.146 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3174.1808456694203
    time_step_min: 3003
  date: 2020-10-15_17-53-09
  done: false
  episode_len_mean: 792.147692911198
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.4644042622985
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 61181
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1114574485003925e-50
        cur_lr: 5.0e-05
        entropy: 0.05877668348451456
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037319547651956477
        model: {}
        policy_loss: -0.008812254253522648
        total_loss: 0.8817840019861857
        vf_explained_var: 0.9979982376098633
        vf_loss: 0.8906256705522537
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.861290322580643
    gpu_util_percent0: 0.3283870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464097228932893
    mean_env_wait_ms: 1.2209421159734746
    mean_inference_ms: 4.286503288935018
    mean_raw_obs_processing_ms: 0.37818596251373493
  time_since_restore: 7690.011832237244
  time_this_iter_s: 25.689666748046875
  time_total_s: 7690.011832237244
  timers:
    learn_throughput: 8651.549
    learn_time_ms: 18700.93
    sample_throughput: 24087.204
    sample_time_ms: 6716.927
    update_time_ms: 37.917
  timestamp: 1602784389
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: f0f97_00000
  
2020-10-15 17:53:10,526	WARNING util.py:136 -- The `process_trial` operation took 0.8655343055725098 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    300 |          7690.01 | 48537600 |  295.464 |              321.404 |              126.556 |            792.148 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3173.692692545875
    time_step_min: 3003
  date: 2020-10-15_17-53-36
  done: false
  episode_len_mean: 792.1543284262636
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.535490555634
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 227
  episodes_total: 61408
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0557287242501962e-50
        cur_lr: 5.0e-05
        entropy: 0.05824820945660273
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009842537925578654
        total_loss: .nan
        vf_explained_var: 0.998711347579956
        vf_loss: 0.568788044154644
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640812716744886
    mean_env_wait_ms: 1.220884945461156
    mean_inference_ms: 4.28640188560993
    mean_raw_obs_processing_ms: 0.3781801406801052
  time_since_restore: 7715.82753777504
  time_this_iter_s: 25.81570553779602
  time_total_s: 7715.82753777504
  timers:
    learn_throughput: 8663.08
    learn_time_ms: 18676.038
    sample_throughput: 24039.539
    sample_time_ms: 6730.245
    update_time_ms: 37.34
  timestamp: 1602784416
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:53:37,432	WARNING util.py:136 -- The `process_trial` operation took 0.8007998466491699 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    301 |          7715.83 | 48699392 |  295.535 |              321.404 |              126.556 |            792.154 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3173.242438517267
    time_step_min: 3003
  date: 2020-10-15_17-54-03
  done: false
  episode_len_mean: 792.1625438254772
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.6011979947503
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 61608
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5835930863752944e-50
        cur_lr: 5.0e-05
        entropy: 0.05848261869202057
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006654852962431808
        total_loss: .nan
        vf_explained_var: 0.9992919564247131
        vf_loss: 0.29615307847658795
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.987096774193553
    gpu_util_percent0: 0.29290322580645156
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464066220516371
    mean_env_wait_ms: 1.2208397723356728
    mean_inference_ms: 4.286312618985709
    mean_raw_obs_processing_ms: 0.37817542167872403
  time_since_restore: 7742.008690834045
  time_this_iter_s: 26.181153059005737
  time_total_s: 7742.008690834045
  timers:
    learn_throughput: 8651.374
    learn_time_ms: 18701.307
    sample_throughput: 23928.266
    sample_time_ms: 6761.543
    update_time_ms: 39.398
  timestamp: 1602784443
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:54:04,791	WARNING util.py:136 -- The `process_trial` operation took 0.8966677188873291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    302 |          7742.01 | 48861184 |  295.601 |              321.404 |              126.556 |            792.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3172.8465150926286
    time_step_min: 3003
  date: 2020-10-15_17-54-30
  done: false
  episode_len_mean: 792.1709440434965
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.6626071714257
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 61798
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3753896295629414e-50
        cur_lr: 5.0e-05
        entropy: 0.05961554187039534
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008593263026947776
        total_loss: .nan
        vf_explained_var: 0.9992561936378479
        vf_loss: 0.2940200095375379
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.319354838709682
    gpu_util_percent0: 0.33483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464053128198169
    mean_env_wait_ms: 1.2207929940688553
    mean_inference_ms: 4.286228722555844
    mean_raw_obs_processing_ms: 0.37817064052339777
  time_since_restore: 7768.05717086792
  time_this_iter_s: 26.04848003387451
  time_total_s: 7768.05717086792
  timers:
    learn_throughput: 8648.21
    learn_time_ms: 18708.149
    sample_throughput: 23834.196
    sample_time_ms: 6788.23
    update_time_ms: 41.887
  timestamp: 1602784470
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:54:31,956	WARNING util.py:136 -- The `process_trial` operation took 0.8247385025024414 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    303 |          7768.06 | 49022976 |  295.663 |              321.404 |              126.556 |            792.171 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3172.3962200200135
    time_step_min: 3003
  date: 2020-10-15_17-54-57
  done: false
  episode_len_mean: 792.1812302432102
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.7301542617969
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 62004
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.563084444344412e-50
        cur_lr: 5.0e-05
        entropy: 0.05668934962401787
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037792231111476817
        model: {}
        policy_loss: -0.007967918849317357
        total_loss: 0.36571842183669406
        vf_explained_var: 0.9991349577903748
        vf_loss: 0.37371468047300976
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.87741935483871
    gpu_util_percent0: 0.2996774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464038920574114
    mean_env_wait_ms: 1.2207419574833596
    mean_inference_ms: 4.28614081208021
    mean_raw_obs_processing_ms: 0.378164546041922
  time_since_restore: 7794.01938867569
  time_this_iter_s: 25.962217807769775
  time_total_s: 7794.01938867569
  timers:
    learn_throughput: 8640.99
    learn_time_ms: 18723.78
    sample_throughput: 23799.777
    sample_time_ms: 6798.047
    update_time_ms: 35.379
  timestamp: 1602784497
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: f0f97_00000
  
2020-10-15 17:54:59,048	WARNING util.py:136 -- The `process_trial` operation took 0.8238525390625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    304 |          7794.02 | 49184768 |   295.73 |              321.404 |              126.556 |            792.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3171.92045290059
    time_step_min: 3003
  date: 2020-10-15_17-55-24
  done: false
  episode_len_mean: 792.188772640342
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.802798731249
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 219
  episodes_total: 62223
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.781542222172206e-50
        cur_lr: 5.0e-05
        entropy: 0.059184350073337555
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058232120936736465
        model: {}
        policy_loss: -0.008975113790559893
        total_loss: 0.32209520787000656
        vf_explained_var: 0.9992207884788513
        vf_loss: 0.3310999224583308
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48
    gpu_util_percent0: 0.3183333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640199034788698
    mean_env_wait_ms: 1.2206899588352835
    mean_inference_ms: 4.286044063849982
    mean_raw_obs_processing_ms: 0.37815974331478425
  time_since_restore: 7819.856110572815
  time_this_iter_s: 25.836721897125244
  time_total_s: 7819.856110572815
  timers:
    learn_throughput: 8646.791
    learn_time_ms: 18711.219
    sample_throughput: 23762.482
    sample_time_ms: 6808.716
    update_time_ms: 33.542
  timestamp: 1602784524
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: f0f97_00000
  
2020-10-15 17:55:26,145	WARNING util.py:136 -- The `process_trial` operation took 0.8672513961791992 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    305 |          7819.86 | 49346560 |  295.803 |              321.404 |              126.556 |            792.189 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3171.507479677404
    time_step_min: 3003
  date: 2020-10-15_17-55-51
  done: false
  episode_len_mean: 792.1963630537531
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.86416920951837
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 62415
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.781542222172206e-50
        cur_lr: 5.0e-05
        entropy: 0.058855935310324035
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006889000307031286
        total_loss: .nan
        vf_explained_var: 0.9991669654846191
        vf_loss: 0.3356520930926005
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.332258064516132
    gpu_util_percent0: 0.3016129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464007734351679
    mean_env_wait_ms: 1.2206446422181647
    mean_inference_ms: 4.285963120599501
    mean_raw_obs_processing_ms: 0.3781553730632344
  time_since_restore: 7845.6189041137695
  time_this_iter_s: 25.76279354095459
  time_total_s: 7845.6189041137695
  timers:
    learn_throughput: 8661.434
    learn_time_ms: 18679.585
    sample_throughput: 23719.983
    sample_time_ms: 6820.916
    update_time_ms: 33.253
  timestamp: 1602784551
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:55:53,089	WARNING util.py:136 -- The `process_trial` operation took 0.885326623916626 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    306 |          7845.62 | 49508352 |  295.864 |              321.404 |              126.556 |            792.196 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3171.117689369116
    time_step_min: 3003
  date: 2020-10-15_17-56-18
  done: false
  episode_len_mean: 792.2039962305739
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.9225552656364
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 62609
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.672313333258309e-50
        cur_lr: 5.0e-05
        entropy: 0.05876246094703674
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033511869066084423
        model: {}
        policy_loss: -0.010236643992053965
        total_loss: 0.4476107532779376
        vf_explained_var: 0.9989115595817566
        vf_loss: 0.4578767791390419
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44838709677419
    gpu_util_percent0: 0.2719354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639939526068804
    mean_env_wait_ms: 1.2205981523375247
    mean_inference_ms: 4.285881552189438
    mean_raw_obs_processing_ms: 0.3781500741411257
  time_since_restore: 7871.311334609985
  time_this_iter_s: 25.69243049621582
  time_total_s: 7871.311334609985
  timers:
    learn_throughput: 8671.183
    learn_time_ms: 18658.585
    sample_throughput: 23686.211
    sample_time_ms: 6830.641
    update_time_ms: 33.236
  timestamp: 1602784578
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: f0f97_00000
  
2020-10-15 17:56:19,909	WARNING util.py:136 -- The `process_trial` operation took 0.8155021667480469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    307 |          7871.31 | 49670144 |  295.923 |              321.404 |              126.556 |            792.204 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3170.691093555888
    time_step_min: 3003
  date: 2020-10-15_17-56-45
  done: false
  episode_len_mean: 792.2146199388847
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 295.9929127987278
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 62832
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3361566666291545e-50
        cur_lr: 5.0e-05
        entropy: 0.05686310182015101
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00779075946775265
        total_loss: .nan
        vf_explained_var: 0.9994289875030518
        vf_loss: 0.25449249520897865
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639776721757636
    mean_env_wait_ms: 1.2205415962797965
    mean_inference_ms: 4.285785805789512
    mean_raw_obs_processing_ms: 0.37814458088694125
  time_since_restore: 7897.084060907364
  time_this_iter_s: 25.77272629737854
  time_total_s: 7897.084060907364
  timers:
    learn_throughput: 8672.892
    learn_time_ms: 18654.907
    sample_throughput: 23673.451
    sample_time_ms: 6834.323
    update_time_ms: 34.649
  timestamp: 1602784605
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:56:46,843	WARNING util.py:136 -- The `process_trial` operation took 0.8687255382537842 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    308 |          7897.08 | 49831936 |  295.993 |              321.404 |              126.556 |            792.215 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3170.2598037659163
    time_step_min: 3003
  date: 2020-10-15_17-57-12
  done: false
  episode_len_mean: 792.225964589415
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.0587516233532
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 63032
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0042349999437313e-50
        cur_lr: 5.0e-05
        entropy: 0.060039603461821876
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006458762907035028
        total_loss: .nan
        vf_explained_var: 0.9995037913322449
        vf_loss: 0.20107349753379822
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.548387096774192
    gpu_util_percent0: 0.29129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639628631399368
    mean_env_wait_ms: 1.2204964234846087
    mean_inference_ms: 4.2857030478016815
    mean_raw_obs_processing_ms: 0.37814000774333206
  time_since_restore: 7922.786796092987
  time_this_iter_s: 25.70273518562317
  time_total_s: 7922.786796092987
  timers:
    learn_throughput: 8687.605
    learn_time_ms: 18623.314
    sample_throughput: 23679.038
    sample_time_ms: 6832.71
    update_time_ms: 32.876
  timestamp: 1602784632
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:57:13,735	WARNING util.py:136 -- The `process_trial` operation took 0.816206693649292 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    309 |          7922.79 | 49993728 |  296.059 |              321.404 |              126.556 |            792.226 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3170.024708359055
    time_step_min: 3003
  date: 2020-10-15_17-57-39
  done: false
  episode_len_mean: 792.2241747465321
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.0839540718219
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 63223
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0063524999155985e-50
        cur_lr: 5.0e-05
        entropy: 0.0988117145995299
        entropy_coeff: 0.0005000000000000001
        kl: 0.005162670199448864
        model: {}
        policy_loss: -0.011306127397498736
        total_loss: 2.2980920871098838
        vf_explained_var: 0.9944363236427307
        vf_loss: 2.3094475467999778
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.459999999999997
    gpu_util_percent0: 0.2936666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639501041008995
    mean_env_wait_ms: 1.2204504634245972
    mean_inference_ms: 4.285622615927051
    mean_raw_obs_processing_ms: 0.3781354089902725
  time_since_restore: 7948.404012918472
  time_this_iter_s: 25.61721682548523
  time_total_s: 7948.404012918472
  timers:
    learn_throughput: 8698.999
    learn_time_ms: 18598.922
    sample_throughput: 23652.093
    sample_time_ms: 6840.494
    update_time_ms: 32.855
  timestamp: 1602784659
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: f0f97_00000
  
2020-10-15 17:57:40,499	WARNING util.py:136 -- The `process_trial` operation took 0.8537521362304688 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    310 |           7948.4 | 50155520 |  296.084 |              321.404 |              126.556 |            792.224 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3170.2141786688962
    time_step_min: 3003
  date: 2020-10-15_17-58-06
  done: false
  episode_len_mean: 792.1892586345508
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.0500002945731
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 63437
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0063524999155985e-50
        cur_lr: 5.0e-05
        entropy: 0.11862651755412419
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010884785534775196
        total_loss: .nan
        vf_explained_var: 0.9919130802154541
        vf_loss: 4.056018888950348
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793548387096774
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639340279323948
    mean_env_wait_ms: 1.2203974004576779
    mean_inference_ms: 4.285534546033113
    mean_raw_obs_processing_ms: 0.3781293060115964
  time_since_restore: 7974.388760328293
  time_this_iter_s: 25.984747409820557
  time_total_s: 7974.388760328293
  timers:
    learn_throughput: 8690.933
    learn_time_ms: 18616.183
    sample_throughput: 23687.0
    sample_time_ms: 6830.413
    update_time_ms: 33.723
  timestamp: 1602784686
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:58:07,681	WARNING util.py:136 -- The `process_trial` operation took 0.894566535949707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    311 |          7974.39 | 50317312 |   296.05 |              321.404 |              126.556 |            792.189 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3170.2429211539975
    time_step_min: 3003
  date: 2020-10-15_17-58-33
  done: false
  episode_len_mean: 792.167899954439
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.0523355818633
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 63651
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5095287498733965e-50
        cur_lr: 5.0e-05
        entropy: 0.08942437544465065
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011224364046938717
        total_loss: .nan
        vf_explained_var: 0.9953483939170837
        vf_loss: 2.2146849632263184
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59666666666667
    gpu_util_percent0: 0.336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463918239533369
    mean_env_wait_ms: 1.2203494674310142
    mean_inference_ms: 4.285446260206549
    mean_raw_obs_processing_ms: 0.3781249636078513
  time_since_restore: 8000.028383016586
  time_this_iter_s: 25.639622688293457
  time_total_s: 8000.028383016586
  timers:
    learn_throughput: 8703.144
    learn_time_ms: 18590.064
    sample_throughput: 23783.31
    sample_time_ms: 6802.754
    update_time_ms: 31.838
  timestamp: 1602784713
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:58:34,461	WARNING util.py:136 -- The `process_trial` operation took 0.8368847370147705 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    312 |          8000.03 | 50479104 |  296.052 |              321.404 |              126.556 |            792.168 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3170.016489803596
    time_step_min: 3003
  date: 2020-10-15_17-59-00
  done: false
  episode_len_mean: 792.1643406481525
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.0933136638691
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 63843
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.764293124810095e-50
        cur_lr: 5.0e-05
        entropy: 0.06441434348622958
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010811050044139847
        total_loss: .nan
        vf_explained_var: 0.9976771473884583
        vf_loss: 0.9552405426899592
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53548387096774
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639068907680086
    mean_env_wait_ms: 1.220304286771494
    mean_inference_ms: 4.285367917239227
    mean_raw_obs_processing_ms: 0.37812059250541386
  time_since_restore: 8025.774102926254
  time_this_iter_s: 25.74571990966797
  time_total_s: 8025.774102926254
  timers:
    learn_throughput: 8706.06
    learn_time_ms: 18583.837
    sample_throughput: 23864.88
    sample_time_ms: 6779.502
    update_time_ms: 29.524
  timestamp: 1602784740
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:59:01,571	WARNING util.py:136 -- The `process_trial` operation took 0.8919172286987305 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    313 |          8025.77 | 50640896 |  296.093 |              321.404 |              126.556 |            792.164 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3169.6110286419676
    time_step_min: 3003
  date: 2020-10-15_17-59-27
  done: false
  episode_len_mean: 792.1727901566135
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.15733589348145
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 64043
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0146439687215144e-49
        cur_lr: 5.0e-05
        entropy: 0.05866290535777807
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007602370703049625
        total_loss: .nan
        vf_explained_var: 0.9992884993553162
        vf_loss: 0.2867494548360507
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81290322580646
    gpu_util_percent0: 0.29354838709677417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638929174091536
    mean_env_wait_ms: 1.2202568917866932
    mean_inference_ms: 4.285288557948046
    mean_raw_obs_processing_ms: 0.37811517109996085
  time_since_restore: 8051.764162540436
  time_this_iter_s: 25.99005961418152
  time_total_s: 8051.764162540436
  timers:
    learn_throughput: 8695.981
    learn_time_ms: 18605.376
    sample_throughput: 23902.947
    sample_time_ms: 6768.705
    update_time_ms: 29.213
  timestamp: 1602784767
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 17:59:28,871	WARNING util.py:136 -- The `process_trial` operation took 0.8372511863708496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    314 |          8051.76 | 50802688 |  296.157 |              321.404 |              126.556 |            792.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3169.1697575559397
    time_step_min: 3003
  date: 2020-10-15_17-59-54
  done: false
  episode_len_mean: 792.1788787402555
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.2242412611648
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 64267
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5219659530822713e-49
        cur_lr: 5.0e-05
        entropy: 0.05972290846208731
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031189473423485956
        model: {}
        policy_loss: -0.0064851961069507524
        total_loss: 0.5613763655225436
        vf_explained_var: 0.9986872673034668
        vf_loss: 0.5678914139668146
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.176666666666666
    gpu_util_percent0: 0.364
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638755416152602
    mean_env_wait_ms: 1.2202033544299933
    mean_inference_ms: 4.285194278125077
    mean_raw_obs_processing_ms: 0.3781103835668549
  time_since_restore: 8077.146810054779
  time_this_iter_s: 25.38264751434326
  time_total_s: 8077.146810054779
  timers:
    learn_throughput: 8707.035
    learn_time_ms: 18581.756
    sample_throughput: 23983.469
    sample_time_ms: 6745.98
    update_time_ms: 29.107
  timestamp: 1602784794
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: f0f97_00000
  
2020-10-15 17:59:55,490	WARNING util.py:136 -- The `process_trial` operation took 0.9432003498077393 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    315 |          8077.15 | 50964480 |  296.224 |              321.404 |              126.556 |            792.179 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3168.768399646056
    time_step_min: 3003
  date: 2020-10-15_18-00-21
  done: false
  episode_len_mean: 792.1893179032934
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.28456383326574
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 64463
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.609829765411357e-50
        cur_lr: 5.0e-05
        entropy: 0.060463933274149895
        entropy_coeff: 0.0005000000000000001
        kl: 0.005799735779874027
        model: {}
        policy_loss: -0.008598623549914919
        total_loss: 0.304004792124033
        vf_explained_var: 0.9992507100105286
        vf_loss: 0.3126336485147476
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.187096774193552
    gpu_util_percent0: 0.3212903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638630041258377
    mean_env_wait_ms: 1.2201594203293156
    mean_inference_ms: 4.285119564772204
    mean_raw_obs_processing_ms: 0.3781058546391395
  time_since_restore: 8103.284902095795
  time_this_iter_s: 26.138092041015625
  time_total_s: 8103.284902095795
  timers:
    learn_throughput: 8699.028
    learn_time_ms: 18598.86
    sample_throughput: 23908.608
    sample_time_ms: 6767.102
    update_time_ms: 27.553
  timestamp: 1602784821
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: f0f97_00000
  
2020-10-15 18:00:22,784	WARNING util.py:136 -- The `process_trial` operation took 0.855426549911499 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    316 |          8103.28 | 51126272 |  296.285 |              321.404 |              126.556 |            792.189 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3168.3800554102368
    time_step_min: 3003
  date: 2020-10-15_18-00-48
  done: false
  episode_len_mean: 792.1994895986389
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.3408120334107
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 64655
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.609829765411357e-50
        cur_lr: 5.0e-05
        entropy: 0.0643371194601059
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009769651425206879
        total_loss: .nan
        vf_explained_var: 0.9988670945167542
        vf_loss: 0.4647126719355583
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.225806451612907
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8709677419354844
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638494978612207
    mean_env_wait_ms: 1.2201144921391431
    mean_inference_ms: 4.285042311479086
    mean_raw_obs_processing_ms: 0.3781010170441651
  time_since_restore: 8129.220320701599
  time_this_iter_s: 25.935418605804443
  time_total_s: 8129.220320701599
  timers:
    learn_throughput: 8697.801
    learn_time_ms: 18601.484
    sample_throughput: 23826.188
    sample_time_ms: 6790.511
    update_time_ms: 27.216
  timestamp: 1602784848
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:00:50,054	WARNING util.py:136 -- The `process_trial` operation took 0.9721856117248535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    317 |          8129.22 | 51288064 |  296.341 |              321.404 |              126.556 |            792.199 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3168.0759321824717
    time_step_min: 3003
  date: 2020-10-15_18-01-16
  done: false
  episode_len_mean: 792.2056207316509
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.39132207268494
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 64867
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1414744648117035e-49
        cur_lr: 5.0e-05
        entropy: 0.06054007665564617
        entropy_coeff: 0.0005000000000000001
        kl: 0.004026286556230237
        model: {}
        policy_loss: -0.010287197364959866
        total_loss: 0.649862473209699
        vf_explained_var: 0.998542845249176
        vf_loss: 0.6601799527804056
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.999999999999996
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638356043524203
    mean_env_wait_ms: 1.2200621798239533
    mean_inference_ms: 4.284959802901331
    mean_raw_obs_processing_ms: 0.3780954464160308
  time_since_restore: 8155.279357671738
  time_this_iter_s: 26.05903697013855
  time_total_s: 8155.279357671738
  timers:
    learn_throughput: 8704.417
    learn_time_ms: 18587.345
    sample_throughput: 23704.585
    sample_time_ms: 6825.346
    update_time_ms: 27.429
  timestamp: 1602784876
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: f0f97_00000
  
2020-10-15 18:01:17,285	WARNING util.py:136 -- The `process_trial` operation took 0.8728022575378418 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    318 |          8155.28 | 51449856 |  296.391 |              321.404 |              126.556 |            792.206 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3167.6504067541177
    time_step_min: 3003
  date: 2020-10-15_18-01-43
  done: false
  episode_len_mean: 792.2183086687259
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.4547852784443
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 65073
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.707372324058518e-50
        cur_lr: 5.0e-05
        entropy: 0.057289816749592624
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007920075230989218
        total_loss: .nan
        vf_explained_var: 0.9992222189903259
        vf_loss: 0.3406900341312091
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.035483870967745
    gpu_util_percent0: 0.2887096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638201181615912
    mean_env_wait_ms: 1.2200163347697575
    mean_inference_ms: 4.284879260323432
    mean_raw_obs_processing_ms: 0.37809136878519467
  time_since_restore: 8181.468315124512
  time_this_iter_s: 26.188957452774048
  time_total_s: 8181.468315124512
  timers:
    learn_throughput: 8692.605
    learn_time_ms: 18612.603
    sample_throughput: 23634.422
    sample_time_ms: 6845.608
    update_time_ms: 27.743
  timestamp: 1602784903
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:01:44,668	WARNING util.py:136 -- The `process_trial` operation took 0.8923711776733398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    319 |          8181.47 | 51611648 |  296.455 |              321.404 |              126.556 |            792.218 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3167.255734085613
    time_step_min: 3003
  date: 2020-10-15_18-02-10
  done: false
  episode_len_mean: 792.2225065114142
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.51540531715136
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 65270
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.561058486087777e-50
        cur_lr: 5.0e-05
        entropy: 0.05439667248477539
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031757160128715136
        model: {}
        policy_loss: -0.004385798509853582
        total_loss: 0.21850077559550604
        vf_explained_var: 0.9994440674781799
        vf_loss: 0.22291377435127893
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.506451612903227
    gpu_util_percent0: 0.2961290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638092720159554
    mean_env_wait_ms: 1.2199706456365833
    mean_inference_ms: 4.284802313942123
    mean_raw_obs_processing_ms: 0.3780873037083594
  time_since_restore: 8207.306791305542
  time_this_iter_s: 25.838476181030273
  time_total_s: 8207.306791305542
  timers:
    learn_throughput: 8680.965
    learn_time_ms: 18637.56
    sample_throughput: 23609.191
    sample_time_ms: 6852.924
    update_time_ms: 25.639
  timestamp: 1602784930
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: f0f97_00000
  
2020-10-15 18:02:11,965	WARNING util.py:136 -- The `process_trial` operation took 0.9789106845855713 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    320 |          8207.31 | 51773440 |  296.515 |              321.404 |              126.556 |            792.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3166.8499915931707
    time_step_min: 3003
  date: 2020-10-15_18-02-37
  done: false
  episode_len_mean: 792.2225938994028
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.57396815610576
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 65469
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2805292430438885e-50
        cur_lr: 5.0e-05
        entropy: 0.057801676293214165
        entropy_coeff: 0.0005000000000000001
        kl: 0.003534657007548958
        model: {}
        policy_loss: -0.008518537176617732
        total_loss: 0.3597800259788831
        vf_explained_var: 0.9991292357444763
        vf_loss: 0.36832745869954425
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.622580645161293
    gpu_util_percent0: 0.3803225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463795139555648
    mean_env_wait_ms: 1.2199231482294646
    mean_inference_ms: 4.284727691359949
    mean_raw_obs_processing_ms: 0.37808214161995124
  time_since_restore: 8233.300285339355
  time_this_iter_s: 25.993494033813477
  time_total_s: 8233.300285339355
  timers:
    learn_throughput: 8687.376
    learn_time_ms: 18623.806
    sample_throughput: 23563.353
    sample_time_ms: 6866.255
    update_time_ms: 24.792
  timestamp: 1602784957
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: f0f97_00000
  
2020-10-15 18:02:39,165	WARNING util.py:136 -- The `process_trial` operation took 0.8968100547790527 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    321 |           8233.3 | 51935232 |  296.574 |              321.404 |              126.556 |            792.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3166.4182167177005
    time_step_min: 3003
  date: 2020-10-15_18-03-05
  done: false
  episode_len_mean: 792.2252736378998
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.6407434727239
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 65689
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1402646215219443e-50
        cur_lr: 5.0e-05
        entropy: 0.056577293202281
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053895063465461135
        model: {}
        policy_loss: -0.004688860865220097
        total_loss: 0.19640816748142242
        vf_explained_var: 0.9995219111442566
        vf_loss: 0.20112531756361327
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.413333333333338
    gpu_util_percent0: 0.35966666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637794683978592
    mean_env_wait_ms: 1.2198711985728008
    mean_inference_ms: 4.2846421834904485
    mean_raw_obs_processing_ms: 0.37807752046300097
  time_since_restore: 8259.17269706726
  time_this_iter_s: 25.872411727905273
  time_total_s: 8259.17269706726
  timers:
    learn_throughput: 8691.976
    learn_time_ms: 18613.949
    sample_throughput: 23457.38
    sample_time_ms: 6897.275
    update_time_ms: 26.243
  timestamp: 1602784985
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: f0f97_00000
  
2020-10-15 18:03:06,315	WARNING util.py:136 -- The `process_trial` operation took 0.8749051094055176 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    322 |          8259.17 | 52097024 |  296.641 |              321.404 |              126.556 |            792.225 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3166.0366027763434
    time_step_min: 3003
  date: 2020-10-15_18-03-32
  done: false
  episode_len_mean: 792.2365984701312
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.699875301092
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 65888
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1402646215219443e-50
        cur_lr: 5.0e-05
        entropy: 0.055943792685866356
        entropy_coeff: 0.0005000000000000001
        kl: 0.005098474017965297
        model: {}
        policy_loss: -0.007721367374567005
        total_loss: 0.30876027047634125
        vf_explained_var: 0.9992799162864685
        vf_loss: 0.3165096069375674
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806456
    gpu_util_percent0: 0.332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637667303819468
    mean_env_wait_ms: 1.219826556278972
    mean_inference_ms: 4.284572643922819
    mean_raw_obs_processing_ms: 0.37807317554750797
  time_since_restore: 8285.099781513214
  time_this_iter_s: 25.92708444595337
  time_total_s: 8285.099781513214
  timers:
    learn_throughput: 8697.026
    learn_time_ms: 18603.142
    sample_throughput: 23370.806
    sample_time_ms: 6922.825
    update_time_ms: 28.375
  timestamp: 1602785012
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: f0f97_00000
  
2020-10-15 18:03:33,516	WARNING util.py:136 -- The `process_trial` operation took 0.8797378540039062 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    323 |           8285.1 | 52258816 |    296.7 |              321.404 |              126.556 |            792.237 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3165.6781657933793
    time_step_min: 3003
  date: 2020-10-15_18-03-59
  done: false
  episode_len_mean: 792.2439921307506
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.74903315846063
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 66080
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1402646215219443e-50
        cur_lr: 5.0e-05
        entropy: 0.063273584159712
        entropy_coeff: 0.0005000000000000001
        kl: 0.00416794167055438
        model: {}
        policy_loss: -0.010563866114049839
        total_loss: 0.7455651511748632
        vf_explained_var: 0.9981672167778015
        vf_loss: 0.7561606615781784
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23870967741935
    gpu_util_percent0: 0.28548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637537824810687
    mean_env_wait_ms: 1.2197821173381258
    mean_inference_ms: 4.284499079215725
    mean_raw_obs_processing_ms: 0.37806860722037455
  time_since_restore: 8311.16011929512
  time_this_iter_s: 26.060337781906128
  time_total_s: 8311.16011929512
  timers:
    learn_throughput: 8700.451
    learn_time_ms: 18595.818
    sample_throughput: 23317.1
    sample_time_ms: 6938.77
    update_time_ms: 27.224
  timestamp: 1602785039
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: f0f97_00000
  
2020-10-15 18:04:00,926	WARNING util.py:136 -- The `process_trial` operation took 0.9498903751373291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    324 |          8311.16 | 52420608 |  296.749 |              321.404 |              126.556 |            792.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3165.346842359476
    time_step_min: 3003
  date: 2020-10-15_18-04-27
  done: false
  episode_len_mean: 792.2488913692721
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.7994840254481
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 66298
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0701323107609721e-50
        cur_lr: 5.0e-05
        entropy: 0.06569098184506099
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010982486904443553
        total_loss: .nan
        vf_explained_var: 0.9986534118652344
        vf_loss: 0.5906192064285278
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.654838709677417
    gpu_util_percent0: 0.31870967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463741276202813
    mean_env_wait_ms: 1.2197288165909494
    mean_inference_ms: 4.284418500068132
    mean_raw_obs_processing_ms: 0.3780632370767493
  time_since_restore: 8337.3270778656
  time_this_iter_s: 26.166958570480347
  time_total_s: 8337.3270778656
  timers:
    learn_throughput: 8684.928
    learn_time_ms: 18629.054
    sample_throughput: 23170.787
    sample_time_ms: 6982.585
    update_time_ms: 27.595
  timestamp: 1602785067
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:04:28,432	WARNING util.py:136 -- The `process_trial` operation took 0.945357084274292 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    325 |          8337.33 | 52582400 |  296.799 |              321.404 |              126.556 |            792.249 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3165.0129862312842
    time_step_min: 3003
  date: 2020-10-15_18-04-54
  done: false
  episode_len_mean: 792.2564923835732
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.850618332009
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 66501
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6051984661414583e-50
        cur_lr: 5.0e-05
        entropy: 0.060672661289572716
        entropy_coeff: 0.0005000000000000001
        kl: 0.004166066025694211
        model: {}
        policy_loss: -0.009566113755378561
        total_loss: 0.4292627026637395
        vf_explained_var: 0.9989707469940186
        vf_loss: 0.43885914981365204
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00645161290323
    gpu_util_percent0: 0.28612903225806446
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637279826839233
    mean_env_wait_ms: 1.2196837961628522
    mean_inference_ms: 4.284344540560023
    mean_raw_obs_processing_ms: 0.3780593296680644
  time_since_restore: 8363.025340080261
  time_this_iter_s: 25.698262214660645
  time_total_s: 8363.025340080261
  timers:
    learn_throughput: 8694.263
    learn_time_ms: 18609.053
    sample_throughput: 23295.898
    sample_time_ms: 6945.085
    update_time_ms: 29.81
  timestamp: 1602785094
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: f0f97_00000
  
2020-10-15 18:04:55,324	WARNING util.py:136 -- The `process_trial` operation took 0.8501060009002686 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    326 |          8363.03 | 52744192 |  296.851 |              321.404 |              126.556 |            792.256 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3164.742535185908
    time_step_min: 3003
  date: 2020-10-15_18-05-21
  done: false
  episode_len_mean: 792.2602411083789
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.8912893403531
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 66692
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.025992330707292e-51
        cur_lr: 5.0e-05
        entropy: 0.07062442228198051
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056070439362277584
        model: {}
        policy_loss: -0.009953916713129729
        total_loss: 0.8335456599791845
        vf_explained_var: 0.997920036315918
        vf_loss: 0.8435348769028982
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.386666666666667
    gpu_util_percent0: 0.34299999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637162404872878
    mean_env_wait_ms: 1.2196400133946161
    mean_inference_ms: 4.284273493911211
    mean_raw_obs_processing_ms: 0.37805529998877574
  time_since_restore: 8388.736829042435
  time_this_iter_s: 25.711488962173462
  time_total_s: 8388.736829042435
  timers:
    learn_throughput: 8689.146
    learn_time_ms: 18620.011
    sample_throughput: 23388.323
    sample_time_ms: 6917.64
    update_time_ms: 30.189
  timestamp: 1602785121
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: f0f97_00000
  
2020-10-15 18:05:22,262	WARNING util.py:136 -- The `process_trial` operation took 0.9174299240112305 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    327 |          8388.74 | 52905984 |  296.891 |              321.404 |              126.556 |             792.26 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3164.4355487741027
    time_step_min: 3003
  date: 2020-10-15_18-05-48
  done: false
  episode_len_mean: 792.2643097391434
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 296.9376468927252
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 66895
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.025992330707292e-51
        cur_lr: 5.0e-05
        entropy: 0.06497499626129866
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00984216941287741
        total_loss: .nan
        vf_explained_var: 0.9983925819396973
        vf_loss: 0.7131046156088511
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97096774193549
    gpu_util_percent0: 0.3348387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637026906255676
    mean_env_wait_ms: 1.2195919819537844
    mean_inference_ms: 4.284201902081149
    mean_raw_obs_processing_ms: 0.3780501559156837
  time_since_restore: 8414.83925151825
  time_this_iter_s: 26.10242247581482
  time_total_s: 8414.83925151825
  timers:
    learn_throughput: 8681.765
    learn_time_ms: 18635.841
    sample_throughput: 23400.05
    sample_time_ms: 6914.173
    update_time_ms: 30.469
  timestamp: 1602785148
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:05:49,626	WARNING util.py:136 -- The `process_trial` operation took 0.9458301067352295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    328 |          8414.84 | 53067776 |  296.938 |              321.404 |              126.556 |            792.264 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3164.055196886788
    time_step_min: 3003
  date: 2020-10-15_18-06-15
  done: false
  episode_len_mean: 792.2750204872234
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.0001949014087
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 67115
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2038988496060937e-50
        cur_lr: 5.0e-05
        entropy: 0.05298697389662266
        entropy_coeff: 0.0005000000000000001
        kl: 0.005234879596779744
        model: {}
        policy_loss: -0.0071036332519724965
        total_loss: 0.21915041158596674
        vf_explained_var: 0.9994659423828125
        vf_loss: 0.22628053526083627
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.016129032258064
    gpu_util_percent0: 0.3090322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636870901940754
    mean_env_wait_ms: 1.2195409182256347
    mean_inference_ms: 4.284120405911595
    mean_raw_obs_processing_ms: 0.37804568896901714
  time_since_restore: 8440.840709209442
  time_this_iter_s: 26.001457691192627
  time_total_s: 8440.840709209442
  timers:
    learn_throughput: 8689.902
    learn_time_ms: 18618.391
    sample_throughput: 23376.795
    sample_time_ms: 6921.051
    update_time_ms: 32.105
  timestamp: 1602785175
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: f0f97_00000
  
2020-10-15 18:06:16,815	WARNING util.py:136 -- The `process_trial` operation took 0.8713827133178711 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    329 |          8440.84 | 53229568 |      297 |              321.404 |              126.556 |            792.275 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3163.6669887757375
    time_step_min: 3003
  date: 2020-10-15_18-06-42
  done: false
  episode_len_mean: 792.284856858463
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.05834833605877
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 67311
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2038988496060937e-50
        cur_lr: 5.0e-05
        entropy: 0.052736232367654644
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040414338776220875
        model: {}
        policy_loss: -0.008714454151534786
        total_loss: 0.15655551850795746
        vf_explained_var: 0.9995970726013184
        vf_loss: 0.16529634470740953
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.145161290322584
    gpu_util_percent0: 0.30838709677419357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636757430066547
    mean_env_wait_ms: 1.2194968162690951
    mean_inference_ms: 4.2840548993314735
    mean_raw_obs_processing_ms: 0.378041407050282
  time_since_restore: 8466.927708864212
  time_this_iter_s: 26.086999654769897
  time_total_s: 8466.927708864212
  timers:
    learn_throughput: 8680.524
    learn_time_ms: 18638.505
    sample_throughput: 23366.099
    sample_time_ms: 6924.22
    update_time_ms: 32.327
  timestamp: 1602785202
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: f0f97_00000
  
2020-10-15 18:06:44,187	WARNING util.py:136 -- The `process_trial` operation took 0.9167885780334473 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    330 |          8466.93 | 53391360 |  297.058 |              321.404 |              126.556 |            792.285 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3163.299401108838
    time_step_min: 3003
  date: 2020-10-15_18-07-10
  done: false
  episode_len_mean: 792.2957306233704
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.11207027611925
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 67504
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.019494248030468e-51
        cur_lr: 5.0e-05
        entropy: 0.055189876506725945
        entropy_coeff: 0.0005000000000000001
        kl: 0.00402466671463723
        model: {}
        policy_loss: -0.007926473462703143
        total_loss: 0.28499940906961757
        vf_explained_var: 0.9992856979370117
        vf_loss: 0.2929534713427226
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.054838709677416
    gpu_util_percent0: 0.3238709677419356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636635542083978
    mean_env_wait_ms: 1.2194522216047916
    mean_inference_ms: 4.283986556523595
    mean_raw_obs_processing_ms: 0.3780370754390012
  time_since_restore: 8492.814484357834
  time_this_iter_s: 25.886775493621826
  time_total_s: 8492.814484357834
  timers:
    learn_throughput: 8676.19
    learn_time_ms: 18647.817
    sample_throughput: 23432.487
    sample_time_ms: 6904.602
    update_time_ms: 32.14
  timestamp: 1602785230
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: f0f97_00000
  
2020-10-15 18:07:11,361	WARNING util.py:136 -- The `process_trial` operation took 0.9666252136230469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    331 |          8492.81 | 53553152 |  297.112 |              321.404 |              126.556 |            792.296 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3162.880295530107
    time_step_min: 3003
  date: 2020-10-15_18-07-37
  done: false
  episode_len_mean: 792.3056068280149
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.17544294557274
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 217
  episodes_total: 67721
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.009747124015234e-51
        cur_lr: 5.0e-05
        entropy: 0.05602567084133625
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008176830500209084
        total_loss: .nan
        vf_explained_var: 0.9993364810943604
        vf_loss: 0.2958836182951927
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.719354838709677
    gpu_util_percent0: 0.2822580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636507526546827
    mean_env_wait_ms: 1.219399732982866
    mean_inference_ms: 4.283909631077591
    mean_raw_obs_processing_ms: 0.3780318962422212
  time_since_restore: 8518.836498737335
  time_this_iter_s: 26.022014379501343
  time_total_s: 8518.836498737335
  timers:
    learn_throughput: 8668.615
    learn_time_ms: 18664.111
    sample_throughput: 23466.274
    sample_time_ms: 6894.661
    update_time_ms: 30.459
  timestamp: 1602785257
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:07:38,584	WARNING util.py:136 -- The `process_trial` operation took 0.885535717010498 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    332 |          8518.84 | 53714944 |  297.175 |              321.404 |              126.556 |            792.306 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3162.545410350466
    time_step_min: 3003
  date: 2020-10-15_18-08-04
  done: false
  episode_len_mean: 792.3129536119658
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.22444430466226
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 67927
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5146206860228507e-51
        cur_lr: 5.0e-05
        entropy: 0.07035206692914169
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010697330707140887
        total_loss: .nan
        vf_explained_var: 0.9981864094734192
        vf_loss: 0.7466409107049307
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216666666666665
    gpu_util_percent0: 0.3573333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463637601946765
    mean_env_wait_ms: 1.2193551757667935
    mean_inference_ms: 4.283839498021345
    mean_raw_obs_processing_ms: 0.37802826037000153
  time_since_restore: 8544.559520244598
  time_this_iter_s: 25.723021507263184
  time_total_s: 8544.559520244598
  timers:
    learn_throughput: 8665.795
    learn_time_ms: 18670.186
    sample_throughput: 23549.184
    sample_time_ms: 6870.387
    update_time_ms: 29.154
  timestamp: 1602785284
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:08:05,497	WARNING util.py:136 -- The `process_trial` operation took 0.8770787715911865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    333 |          8544.56 | 53876736 |  297.224 |              321.404 |              126.556 |            792.313 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3162.426989070396
    time_step_min: 3003
  date: 2020-10-15_18-08-31
  done: false
  episode_len_mean: 792.3052497137321
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.2360080146122
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 68118
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.771931029034276e-51
        cur_lr: 5.0e-05
        entropy: 0.07543136055270831
        entropy_coeff: 0.0005000000000000001
        kl: 0.004089793947059661
        model: {}
        policy_loss: -0.010882105290268859
        total_loss: 1.4807318846384685
        vf_explained_var: 0.9966067671775818
        vf_loss: 1.4916517039140065
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56451612903226
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636267092752606
    mean_env_wait_ms: 1.2193117941117577
    mean_inference_ms: 4.28377179075606
    mean_raw_obs_processing_ms: 0.378024229421162
  time_since_restore: 8570.396100282669
  time_this_iter_s: 25.83658003807068
  time_total_s: 8570.396100282669
  timers:
    learn_throughput: 8674.449
    learn_time_ms: 18651.559
    sample_throughput: 23607.619
    sample_time_ms: 6853.381
    update_time_ms: 31.023
  timestamp: 1602785311
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: f0f97_00000
  
2020-10-15 18:08:32,602	WARNING util.py:136 -- The `process_trial` operation took 0.9193053245544434 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    334 |           8570.4 | 54038528 |  297.236 |              321.404 |              126.556 |            792.305 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3162.2185458593317
    time_step_min: 3003
  date: 2020-10-15_18-08-58
  done: false
  episode_len_mean: 792.3088553864169
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.2776232760859
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 68320
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.385965514517138e-51
        cur_lr: 5.0e-05
        entropy: 0.06059476795295874
        entropy_coeff: 0.0005000000000000001
        kl: 0.004124866003015389
        model: {}
        policy_loss: -0.009755545373385152
        total_loss: 0.5383115212122599
        vf_explained_var: 0.998728334903717
        vf_loss: 0.5480973472197851
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.138709677419357
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636141514534942
    mean_env_wait_ms: 1.2192642452828317
    mean_inference_ms: 4.283704119777136
    mean_raw_obs_processing_ms: 0.3780193623671718
  time_since_restore: 8596.416640281677
  time_this_iter_s: 26.02053999900818
  time_total_s: 8596.416640281677
  timers:
    learn_throughput: 8670.345
    learn_time_ms: 18660.387
    sample_throughput: 23731.34
    sample_time_ms: 6817.651
    update_time_ms: 32.494
  timestamp: 1602785338
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: f0f97_00000
  
2020-10-15 18:08:59,909	WARNING util.py:136 -- The `process_trial` operation took 0.9367122650146484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    335 |          8596.42 | 54200320 |  297.278 |              321.404 |              126.556 |            792.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3161.8478260869565
    time_step_min: 3003
  date: 2020-10-15_18-09-25
  done: false
  episode_len_mean: 792.3166326232857
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.33415199559
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 68540
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.692982757258569e-51
        cur_lr: 5.0e-05
        entropy: 0.055644393898546696
        entropy_coeff: 0.0005000000000000001
        kl: 0.004579993585745494
        model: {}
        policy_loss: -0.007469985575880855
        total_loss: 0.21236507842938104
        vf_explained_var: 0.9994866251945496
        vf_loss: 0.21986288204789162
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333332
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635981906593948
    mean_env_wait_ms: 1.219214072375574
    mean_inference_ms: 4.28362754826945
    mean_raw_obs_processing_ms: 0.3780149844175939
  time_since_restore: 8622.102865457535
  time_this_iter_s: 25.686225175857544
  time_total_s: 8622.102865457535
  timers:
    learn_throughput: 8666.744
    learn_time_ms: 18668.141
    sample_throughput: 23728.771
    sample_time_ms: 6818.389
    update_time_ms: 32.129
  timestamp: 1602785365
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: f0f97_00000
  
2020-10-15 18:09:26,912	WARNING util.py:136 -- The `process_trial` operation took 0.8968539237976074 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    336 |           8622.1 | 54362112 |  297.334 |              321.404 |              126.556 |            792.317 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3161.469545216328
    time_step_min: 3003
  date: 2020-10-15_18-09-52
  done: false
  episode_len_mean: 792.3242602345136
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.3889241567522
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 68738
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.464913786292845e-52
        cur_lr: 5.0e-05
        entropy: 0.0531109015767773
        entropy_coeff: 0.0005000000000000001
        kl: 0.004313295745911698
        model: {}
        policy_loss: -0.006750732345002082
        total_loss: 0.23714041834076247
        vf_explained_var: 0.9994089603424072
        vf_loss: 0.2439177098373572
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03548387096774
    gpu_util_percent0: 0.3090322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463588625101613
    mean_env_wait_ms: 1.2191698088594203
    mean_inference_ms: 4.283563484611871
    mean_raw_obs_processing_ms: 0.3780111918437996
  time_since_restore: 8647.804530620575
  time_this_iter_s: 25.70166516304016
  time_total_s: 8647.804530620575
  timers:
    learn_throughput: 8671.464
    learn_time_ms: 18657.979
    sample_throughput: 23734.918
    sample_time_ms: 6816.624
    update_time_ms: 32.492
  timestamp: 1602785392
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: f0f97_00000
  
2020-10-15 18:09:53,814	WARNING util.py:136 -- The `process_trial` operation took 0.8898770809173584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    337 |           8647.8 | 54523904 |  297.389 |              321.404 |              126.556 |            792.324 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3161.1253593101246
    time_step_min: 3003
  date: 2020-10-15_18-10-19
  done: false
  episode_len_mean: 792.3293581708449
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.43898288244515
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 68928
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.232456893146422e-52
        cur_lr: 5.0e-05
        entropy: 0.0546182319521904
        entropy_coeff: 0.0005000000000000001
        kl: 0.004269148300712307
        model: {}
        policy_loss: -0.010044910304713994
        total_loss: 0.4795935849348704
        vf_explained_var: 0.9988396167755127
        vf_loss: 0.48966579635938007
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.123333333333342
    gpu_util_percent0: 0.329
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146357687412057
    mean_env_wait_ms: 1.219126442201335
    mean_inference_ms: 4.283499781477125
    mean_raw_obs_processing_ms: 0.3780068235877344
  time_since_restore: 8673.380937337875
  time_this_iter_s: 25.576406717300415
  time_total_s: 8673.380937337875
  timers:
    learn_throughput: 8679.594
    learn_time_ms: 18640.503
    sample_throughput: 23850.455
    sample_time_ms: 6783.602
    update_time_ms: 29.92
  timestamp: 1602785419
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: f0f97_00000
  
2020-10-15 18:10:20,775	WARNING util.py:136 -- The `process_trial` operation took 0.9656658172607422 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    338 |          8673.38 | 54685696 |  297.439 |              321.404 |              126.556 |            792.329 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3160.7933634337646
    time_step_min: 3003
  date: 2020-10-15_18-10-46
  done: false
  episode_len_mean: 792.3356423902354
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.4900750140376
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 69148
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.116228446573211e-52
        cur_lr: 5.0e-05
        entropy: 0.06041483674198389
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011338808894834074
        total_loss: .nan
        vf_explained_var: 0.9989799857139587
        vf_loss: 0.4716704189777374
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48387096774194
    gpu_util_percent0: 0.3258064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635625517868106
    mean_env_wait_ms: 1.2190746400733805
    mean_inference_ms: 4.283425352512624
    mean_raw_obs_processing_ms: 0.37800174478630705
  time_since_restore: 8699.212547779083
  time_this_iter_s: 25.831610441207886
  time_total_s: 8699.212547779083
  timers:
    learn_throughput: 8676.674
    learn_time_ms: 18646.777
    sample_throughput: 23928.972
    sample_time_ms: 6761.344
    update_time_ms: 28.06
  timestamp: 1602785446
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:10:47,989	WARNING util.py:136 -- The `process_trial` operation took 0.9983439445495605 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    339 |          8699.21 | 54847488 |   297.49 |              321.404 |              126.556 |            792.336 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3160.481257034023
    time_step_min: 3003
  date: 2020-10-15_18-11-13
  done: false
  episode_len_mean: 792.3428451955243
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.5383744294945
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 69352
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1743426698598175e-52
        cur_lr: 5.0e-05
        entropy: 0.05885839741677046
        entropy_coeff: 0.0005000000000000001
        kl: 0.003774490013408164
        model: {}
        policy_loss: -0.010197216470260173
        total_loss: 0.5690183987220129
        vf_explained_var: 0.9986574053764343
        vf_loss: 0.5792450507481893
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16451612903226
    gpu_util_percent0: 0.3158064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635511254106676
    mean_env_wait_ms: 1.219029913720389
    mean_inference_ms: 4.283359082190255
    mean_raw_obs_processing_ms: 0.37799832151764273
  time_since_restore: 8725.057363986969
  time_this_iter_s: 25.844816207885742
  time_total_s: 8725.057363986969
  timers:
    learn_throughput: 8684.309
    learn_time_ms: 18630.382
    sample_throughput: 23967.097
    sample_time_ms: 6750.588
    update_time_ms: 29.554
  timestamp: 1602785473
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: f0f97_00000
  
2020-10-15 18:11:15,169	WARNING util.py:136 -- The `process_trial` operation took 1.0138311386108398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    340 |          8725.06 | 55009280 |  297.538 |              321.404 |              126.556 |            792.343 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3160.1739668470127
    time_step_min: 3003
  date: 2020-10-15_18-11-41
  done: false
  episode_len_mean: 792.3505938857094
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.5854629235027
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 69542
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5871713349299087e-52
        cur_lr: 5.0e-05
        entropy: 0.05684075721849998
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007212287494136642
        total_loss: .nan
        vf_explained_var: 0.9991159439086914
        vf_loss: 0.35255765666564304
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.854838709677423
    gpu_util_percent0: 0.27645161290322584
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463540806909794
    mean_env_wait_ms: 1.2189867466783577
    mean_inference_ms: 4.283294246548784
    mean_raw_obs_processing_ms: 0.37799442927957616
  time_since_restore: 8751.039776802063
  time_this_iter_s: 25.982412815093994
  time_total_s: 8751.039776802063
  timers:
    learn_throughput: 8692.076
    learn_time_ms: 18613.735
    sample_throughput: 23883.362
    sample_time_ms: 6774.256
    update_time_ms: 30.103
  timestamp: 1602785501
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:11:42,514	WARNING util.py:136 -- The `process_trial` operation took 1.0254113674163818 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    341 |          8751.04 | 55171072 |  297.585 |              321.404 |              126.556 |            792.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3159.797555341951
    time_step_min: 3003
  date: 2020-10-15_18-12-08
  done: false
  episode_len_mean: 792.3633887224189
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.6428438711907
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 69749
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.380757002394863e-52
        cur_lr: 5.0e-05
        entropy: 0.054802728506426014
        entropy_coeff: 0.0005000000000000001
        kl: 0.004544676863588393
        model: {}
        policy_loss: -0.008483146488288185
        total_loss: 0.1844262182712555
        vf_explained_var: 0.9995551109313965
        vf_loss: 0.192936759442091
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870963
    gpu_util_percent0: 0.26129032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635279030566134
    mean_env_wait_ms: 1.218938378015339
    mean_inference_ms: 4.283227584552779
    mean_raw_obs_processing_ms: 0.3779895206605452
  time_since_restore: 8776.977588653564
  time_this_iter_s: 25.937811851501465
  time_total_s: 8776.977588653564
  timers:
    learn_throughput: 8692.92
    learn_time_ms: 18611.927
    sample_throughput: 23885.646
    sample_time_ms: 6773.608
    update_time_ms: 30.188
  timestamp: 1602785528
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: f0f97_00000
  
2020-10-15 18:12:09,878	WARNING util.py:136 -- The `process_trial` operation took 0.9510881900787354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    342 |          8776.98 | 55332864 |  297.643 |              321.404 |              126.556 |            792.363 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3159.4268367682603
    time_step_min: 3003
  date: 2020-10-15_18-12-35
  done: false
  episode_len_mean: 792.3762881440721
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.69761792873294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 69965
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1903785011974314e-52
        cur_lr: 5.0e-05
        entropy: 0.058846744087835155
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010274229677937305
        total_loss: .nan
        vf_explained_var: 0.9991998076438904
        vf_loss: 0.36120834201574326
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.406666666666677
    gpu_util_percent0: 0.3343333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635135802982263
    mean_env_wait_ms: 1.2188904522201838
    mean_inference_ms: 4.283157254394778
    mean_raw_obs_processing_ms: 0.37798551063635333
  time_since_restore: 8802.646322011948
  time_this_iter_s: 25.66873335838318
  time_total_s: 8802.646322011948
  timers:
    learn_throughput: 8694.65
    learn_time_ms: 18608.225
    sample_throughput: 23895.379
    sample_time_ms: 6770.849
    update_time_ms: 30.914
  timestamp: 1602785555
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:12:36,829	WARNING util.py:136 -- The `process_trial` operation took 0.9658513069152832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    343 |          8802.65 | 55494656 |  297.698 |              321.404 |              126.556 |            792.376 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3159.1262871160043
    time_step_min: 3003
  date: 2020-10-15_18-13-02
  done: false
  episode_len_mean: 792.382817399236
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.745687160643
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 70164
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.785567751796147e-52
        cur_lr: 5.0e-05
        entropy: 0.05960126624753078
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008511906247197961
        total_loss: .nan
        vf_explained_var: 0.9989349246025085
        vf_loss: 0.46397731949885684
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.556666666666672
    gpu_util_percent0: 0.33433333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635040600441038
    mean_env_wait_ms: 1.2188459234119373
    mean_inference_ms: 4.283094870191178
    mean_raw_obs_processing_ms: 0.3779818451760109
  time_since_restore: 8828.120929479599
  time_this_iter_s: 25.474607467651367
  time_total_s: 8828.120929479599
  timers:
    learn_throughput: 8701.104
    learn_time_ms: 18594.421
    sample_throughput: 23933.427
    sample_time_ms: 6760.085
    update_time_ms: 28.758
  timestamp: 1602785582
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:13:03,732	WARNING util.py:136 -- The `process_trial` operation took 0.9906716346740723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    344 |          8828.12 | 55656448 |  297.746 |              321.404 |              126.556 |            792.383 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3158.826620014792
    time_step_min: 3003
  date: 2020-10-15_18-13-29
  done: false
  episode_len_mean: 792.3880802797282
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.7925725113655
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 70354
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6783516276942212e-52
        cur_lr: 5.0e-05
        entropy: 0.0575551874935627
        entropy_coeff: 0.0005000000000000001
        kl: 0.003804949422677358
        model: {}
        policy_loss: -0.007577516498713521
        total_loss: 0.3796651363372803
        vf_explained_var: 0.9990484714508057
        vf_loss: 0.3872714415192604
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03870967741935
    gpu_util_percent0: 0.33774193548387094
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634924418816736
    mean_env_wait_ms: 1.2188031554764036
    mean_inference_ms: 4.283035240874681
    mean_raw_obs_processing_ms: 0.3779776567478235
  time_since_restore: 8854.041654348373
  time_this_iter_s: 25.920724868774414
  time_total_s: 8854.041654348373
  timers:
    learn_throughput: 8707.022
    learn_time_ms: 18581.784
    sample_throughput: 23888.339
    sample_time_ms: 6772.844
    update_time_ms: 28.308
  timestamp: 1602785609
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: f0f97_00000
  
2020-10-15 18:13:31,058	WARNING util.py:136 -- The `process_trial` operation took 1.0782678127288818 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    345 |          8854.04 | 55818240 |  297.793 |              321.404 |              126.556 |            792.388 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3158.45110884238
    time_step_min: 3003
  date: 2020-10-15_18-13-56
  done: false
  episode_len_mean: 792.3972367861697
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.8481620799173
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 70570
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3391758138471106e-52
        cur_lr: 5.0e-05
        entropy: 0.057789191914101444
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00804866643253869
        total_loss: .nan
        vf_explained_var: 0.9992757439613342
        vf_loss: 0.32420752694209415
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.887096774193548
    gpu_util_percent0: 0.34548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634795811049064
    mean_env_wait_ms: 1.218752380499144
    mean_inference_ms: 4.2829657216726575
    mean_raw_obs_processing_ms: 0.37797294847429486
  time_since_restore: 8879.9630920887
  time_this_iter_s: 25.921437740325928
  time_total_s: 8879.9630920887
  timers:
    learn_throughput: 8696.583
    learn_time_ms: 18604.087
    sample_throughput: 23888.477
    sample_time_ms: 6772.805
    update_time_ms: 28.244
  timestamp: 1602785636
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:13:58,291	WARNING util.py:136 -- The `process_trial` operation took 0.9851596355438232 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    346 |          8879.96 | 55980032 |  297.848 |              321.404 |              126.556 |            792.397 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3158.0883937281737
    time_step_min: 3003
  date: 2020-10-15_18-14-24
  done: false
  episode_len_mean: 792.4050441540091
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.8997149167401
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 70775
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0087637207706658e-52
        cur_lr: 5.0e-05
        entropy: 0.05817755435903867
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009797519317847522
        total_loss: .nan
        vf_explained_var: 0.9992275834083557
        vf_loss: 0.32462821900844574
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43225806451613
    gpu_util_percent0: 0.3196774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634681227819962
    mean_env_wait_ms: 1.2187078613425018
    mean_inference_ms: 4.282903511069769
    mean_raw_obs_processing_ms: 0.3779695859257891
  time_since_restore: 8905.882987499237
  time_this_iter_s: 25.91989541053772
  time_total_s: 8905.882987499237
  timers:
    learn_throughput: 8692.188
    learn_time_ms: 18613.495
    sample_throughput: 23818.747
    sample_time_ms: 6792.633
    update_time_ms: 29.579
  timestamp: 1602785664
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:14:25,571	WARNING util.py:136 -- The `process_trial` operation took 1.0241782665252686 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    347 |          8905.88 | 56141824 |    297.9 |              321.404 |              126.556 |            792.405 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3157.7891627421673
    time_step_min: 3003
  date: 2020-10-15_18-14-51
  done: false
  episode_len_mean: 792.4121294104385
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.94347516422204
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 70968
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.013145581155999e-52
        cur_lr: 5.0e-05
        entropy: 0.054940115660429
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043917447328567505
        model: {}
        policy_loss: -0.009139632786779353
        total_loss: 0.335384505490462
        vf_explained_var: 0.9991660118103027
        vf_loss: 0.3445515955487887
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30967741935485
    gpu_util_percent0: 0.3129032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634588590049558
    mean_env_wait_ms: 1.2186646499416003
    mean_inference_ms: 4.282841270172654
    mean_raw_obs_processing_ms: 0.37796588298724054
  time_since_restore: 8931.872514724731
  time_this_iter_s: 25.989527225494385
  time_total_s: 8931.872514724731
  timers:
    learn_throughput: 8683.877
    learn_time_ms: 18631.31
    sample_throughput: 23744.023
    sample_time_ms: 6814.01
    update_time_ms: 31.698
  timestamp: 1602785691
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: f0f97_00000
  
2020-10-15 18:14:52,909	WARNING util.py:136 -- The `process_trial` operation took 0.9305813312530518 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    348 |          8931.87 | 56303616 |  297.943 |              321.404 |              126.556 |            792.412 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3157.4477154505835
    time_step_min: 3003
  date: 2020-10-15_18-15-18
  done: false
  episode_len_mean: 792.4231763515792
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 297.99897494104755
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 71176
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5065727905779994e-52
        cur_lr: 5.0e-05
        entropy: 0.05492916082342466
        entropy_coeff: 0.0005000000000000001
        kl: 0.006228593993000686
        model: {}
        policy_loss: -0.005319665739079937
        total_loss: 0.16256814450025558
        vf_explained_var: 0.999612033367157
        vf_loss: 0.16791526849071184
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.167741935483868
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634457036330584
    mean_env_wait_ms: 1.2186163497700513
    mean_inference_ms: 4.282777821177618
    mean_raw_obs_processing_ms: 0.37796100855177156
  time_since_restore: 8957.783200025558
  time_this_iter_s: 25.910685300827026
  time_total_s: 8957.783200025558
  timers:
    learn_throughput: 8680.45
    learn_time_ms: 18638.666
    sample_throughput: 23746.867
    sample_time_ms: 6813.194
    update_time_ms: 31.833
  timestamp: 1602785718
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: f0f97_00000
  
2020-10-15 18:15:20,129	WARNING util.py:136 -- The `process_trial` operation took 0.9623451232910156 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    349 |          8957.78 | 56465408 |  297.999 |              321.404 |              126.556 |            792.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3157.0815659999157
    time_step_min: 3003
  date: 2020-10-15_18-15-45
  done: false
  episode_len_mean: 792.4324176670823
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.05301222685284
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 71387
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5065727905779994e-52
        cur_lr: 5.0e-05
        entropy: 0.05604497125993172
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009243974869605154
        total_loss: .nan
        vf_explained_var: 0.9992147088050842
        vf_loss: 0.3498604396979014
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.112903225806452
    gpu_util_percent0: 0.3012903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463432253162358
    mean_env_wait_ms: 1.2185697696522033
    mean_inference_ms: 4.282711257992794
    mean_raw_obs_processing_ms: 0.3779573162143909
  time_since_restore: 8983.56934261322
  time_this_iter_s: 25.786142587661743
  time_total_s: 8983.56934261322
  timers:
    learn_throughput: 8686.73
    learn_time_ms: 18625.19
    sample_throughput: 23745.704
    sample_time_ms: 6813.527
    update_time_ms: 30.103
  timestamp: 1602785745
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:15:47,187	WARNING util.py:136 -- The `process_trial` operation took 0.9291331768035889 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    350 |          8983.57 | 56627200 |  298.053 |              321.404 |              126.556 |            792.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3156.7808778305844
    time_step_min: 3003
  date: 2020-10-15_18-16-12
  done: false
  episode_len_mean: 792.4346659961445
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.09828779793526
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 71586
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2598591858669987e-52
        cur_lr: 5.0e-05
        entropy: 0.05640492122620344
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008986756653333336
        total_loss: .nan
        vf_explained_var: 0.9990091323852539
        vf_loss: 0.4068640520175298
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490000000000002
    gpu_util_percent0: 0.34966666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463423132044795
    mean_env_wait_ms: 1.2185256045993818
    mean_inference_ms: 4.282651922774211
    mean_raw_obs_processing_ms: 0.377953787445903
  time_since_restore: 9009.136061668396
  time_this_iter_s: 25.56671905517578
  time_total_s: 9009.136061668396
  timers:
    learn_throughput: 8688.945
    learn_time_ms: 18620.443
    sample_throughput: 23844.276
    sample_time_ms: 6785.36
    update_time_ms: 29.554
  timestamp: 1602785772
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:16:14,086	WARNING util.py:136 -- The `process_trial` operation took 0.9079172611236572 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    351 |          9009.14 | 56788992 |  298.098 |              321.404 |              126.556 |            792.435 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3156.465950120586
    time_step_min: 3003
  date: 2020-10-15_18-16-39
  done: false
  episode_len_mean: 792.4408253110241
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.14359071566554
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 71779
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.389788778800497e-52
        cur_lr: 5.0e-05
        entropy: 0.05623631738126278
        entropy_coeff: 0.0005000000000000001
        kl: 0.004563139984384179
        model: {}
        policy_loss: -0.010380275110946968
        total_loss: 0.308787743250529
        vf_explained_var: 0.9992507100105286
        vf_loss: 0.3191961422562599
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.145161290322584
    gpu_util_percent0: 0.27451612903225814
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463412458461503
    mean_env_wait_ms: 1.2184823092176236
    mean_inference_ms: 4.282594526528435
    mean_raw_obs_processing_ms: 0.37794975639693423
  time_since_restore: 9035.004538297653
  time_this_iter_s: 25.868476629257202
  time_total_s: 9035.004538297653
  timers:
    learn_throughput: 8683.413
    learn_time_ms: 18632.305
    sample_throughput: 23904.377
    sample_time_ms: 6768.3
    update_time_ms: 29.468
  timestamp: 1602785799
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: f0f97_00000
  
2020-10-15 18:16:41,261	WARNING util.py:136 -- The `process_trial` operation took 0.9816045761108398 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    352 |             9035 | 56950784 |  298.144 |              321.404 |              126.556 |            792.441 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3156.134059733437
    time_step_min: 3003
  date: 2020-10-15_18-17-07
  done: false
  episode_len_mean: 792.4478812205725
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.1969461977651
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 71999
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6948943894002486e-52
        cur_lr: 5.0e-05
        entropy: 0.05336391615370909
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007605333555450973
        total_loss: .nan
        vf_explained_var: 0.9994052052497864
        vf_loss: 0.27260714893539745
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44193548387097
    gpu_util_percent0: 0.32451612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463399585155902
    mean_env_wait_ms: 1.2184316317832087
    mean_inference_ms: 4.282527684294462
    mean_raw_obs_processing_ms: 0.37794519311976454
  time_since_restore: 9060.76220369339
  time_this_iter_s: 25.757665395736694
  time_total_s: 9060.76220369339
  timers:
    learn_throughput: 8685.08
    learn_time_ms: 18628.728
    sample_throughput: 23871.501
    sample_time_ms: 6777.621
    update_time_ms: 29.882
  timestamp: 1602785827
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:17:08,394	WARNING util.py:136 -- The `process_trial` operation took 1.037611722946167 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    353 |          9060.76 | 57112576 |  298.197 |              321.404 |              126.556 |            792.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3155.8048533732467
    time_step_min: 3003
  date: 2020-10-15_18-17-34
  done: false
  episode_len_mean: 792.455555247777
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.2479874784515
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 72202
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.542341584100373e-52
        cur_lr: 5.0e-05
        entropy: 0.05329290529092153
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008793924629571848
        total_loss: .nan
        vf_explained_var: 0.9995098114013672
        vf_loss: 0.19996861120065054
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.254838709677422
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633879592469726
    mean_env_wait_ms: 1.2183874052676167
    mean_inference_ms: 4.282467703347183
    mean_raw_obs_processing_ms: 0.3779418402359738
  time_since_restore: 9086.863228082657
  time_this_iter_s: 26.101024389266968
  time_total_s: 9086.863228082657
  timers:
    learn_throughput: 8673.225
    learn_time_ms: 18654.191
    sample_throughput: 23756.221
    sample_time_ms: 6810.511
    update_time_ms: 31.527
  timestamp: 1602785854
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:17:35,858	WARNING util.py:136 -- The `process_trial` operation took 0.9369339942932129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    354 |          9086.86 | 57274368 |  298.248 |              321.404 |              126.556 |            792.456 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3155.4964199817546
    time_step_min: 3003
  date: 2020-10-15_18-18-01
  done: false
  episode_len_mean: 792.463145098906
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.2956544949992
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 72392
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.813512376150559e-52
        cur_lr: 5.0e-05
        entropy: 0.05493502008418242
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008298754614467422
        total_loss: .nan
        vf_explained_var: 0.999393880367279
        vf_loss: 0.23894042894244194
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05483870967742
    gpu_util_percent0: 0.33354838709677426
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633790507046157
    mean_env_wait_ms: 1.2183455488609178
    mean_inference_ms: 4.2824099341756785
    mean_raw_obs_processing_ms: 0.3779380901202255
  time_since_restore: 9112.860333919525
  time_this_iter_s: 25.997105836868286
  time_total_s: 9112.860333919525
  timers:
    learn_throughput: 8675.11
    learn_time_ms: 18650.139
    sample_throughput: 23716.226
    sample_time_ms: 6821.996
    update_time_ms: 29.957
  timestamp: 1602785881
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:18:03,254	WARNING util.py:136 -- The `process_trial` operation took 0.9850444793701172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    355 |          9112.86 | 57436160 |  298.296 |              321.404 |              126.556 |            792.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3155.1689269274234
    time_step_min: 3003
  date: 2020-10-15_18-18-29
  done: false
  episode_len_mean: 792.4686105448735
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.34683761197397
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 72604
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.720268564225839e-52
        cur_lr: 5.0e-05
        entropy: 0.058816962564984955
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007627915319365759
        total_loss: .nan
        vf_explained_var: 0.999233067035675
        vf_loss: 0.3381854047377904
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.567741935483873
    gpu_util_percent0: 0.33612903225806456
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633665467107926
    mean_env_wait_ms: 1.218296732363117
    mean_inference_ms: 4.282346100077546
    mean_raw_obs_processing_ms: 0.37793352133139435
  time_since_restore: 9138.699100971222
  time_this_iter_s: 25.838767051696777
  time_total_s: 9138.699100971222
  timers:
    learn_throughput: 8685.597
    learn_time_ms: 18627.621
    sample_throughput: 23671.556
    sample_time_ms: 6834.87
    update_time_ms: 29.773
  timestamp: 1602785909
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:18:30,485	WARNING util.py:136 -- The `process_trial` operation took 0.9434890747070312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    356 |           9138.7 | 57597952 |  298.347 |              321.404 |              126.556 |            792.469 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3154.824830960365
    time_step_min: 3003
  date: 2020-10-15_18-18-56
  done: false
  episode_len_mean: 792.476486746326
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.39684650376813
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 72810
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.580402846338757e-52
        cur_lr: 5.0e-05
        entropy: 0.055856712783376374
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006649386409359674
        total_loss: .nan
        vf_explained_var: 0.9993448257446289
        vf_loss: 0.3698228659729163
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.248387096774195
    gpu_util_percent0: 0.34935483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633542304594585
    mean_env_wait_ms: 1.2182518125433048
    mean_inference_ms: 4.282286565166165
    mean_raw_obs_processing_ms: 0.37792998324797894
  time_since_restore: 9164.427038669586
  time_this_iter_s: 25.727937698364258
  time_total_s: 9164.427038669586
  timers:
    learn_throughput: 8693.585
    learn_time_ms: 18610.504
    sample_throughput: 23710.601
    sample_time_ms: 6823.614
    update_time_ms: 36.463
  timestamp: 1602785936
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:18:57,547	WARNING util.py:136 -- The `process_trial` operation took 0.9954817295074463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    357 |          9164.43 | 57759744 |  298.397 |              321.404 |              126.556 |            792.476 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3154.5147129366937
    time_step_min: 3003
  date: 2020-10-15_18-19-23
  done: false
  episode_len_mean: 792.482926762454
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.4449614693963
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 73009
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2870604269508135e-51
        cur_lr: 5.0e-05
        entropy: 0.060949272476136684
        entropy_coeff: 0.0005000000000000001
        kl: 0.006833333444471161
        model: {}
        policy_loss: -0.007381927563983481
        total_loss: 0.264026856670777
        vf_explained_var: 0.9993147850036621
        vf_loss: 0.2714392642180125
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77741935483871
    gpu_util_percent0: 0.35709677419354846
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633455933270015
    mean_env_wait_ms: 1.2182075051940644
    mean_inference_ms: 4.282229966866471
    mean_raw_obs_processing_ms: 0.37792653508653234
  time_since_restore: 9190.486750125885
  time_this_iter_s: 26.059711456298828
  time_total_s: 9190.486750125885
  timers:
    learn_throughput: 8683.131
    learn_time_ms: 18632.911
    sample_throughput: 23793.729
    sample_time_ms: 6799.775
    update_time_ms: 42.408
  timestamp: 1602785963
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: f0f97_00000
  
2020-10-15 18:19:24,890	WARNING util.py:136 -- The `process_trial` operation took 0.9432964324951172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    358 |          9190.49 | 57921536 |  298.445 |              321.404 |              126.556 |            792.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3154.239997812983
    time_step_min: 3003
  date: 2020-10-15_18-19-50
  done: false
  episode_len_mean: 792.4827812307902
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.48519689070145
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 73205
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2870604269508135e-51
        cur_lr: 5.0e-05
        entropy: 0.07554723074038823
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009258137899450958
        total_loss: .nan
        vf_explained_var: 0.9981512427330017
        vf_loss: 0.7585083891948065
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.490322580645167
    gpu_util_percent0: 0.3045161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633338016774575
    mean_env_wait_ms: 1.2181643291554147
    mean_inference_ms: 4.282173470567336
    mean_raw_obs_processing_ms: 0.3779224614502119
  time_since_restore: 9216.44449543953
  time_this_iter_s: 25.95774531364441
  time_total_s: 9216.44449543953
  timers:
    learn_throughput: 8685.57
    learn_time_ms: 18627.678
    sample_throughput: 23795.234
    sample_time_ms: 6799.345
    update_time_ms: 42.988
  timestamp: 1602785990
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:19:52,302	WARNING util.py:136 -- The `process_trial` operation took 1.02817702293396 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    359 |          9216.44 | 58083328 |  298.485 |              321.404 |              126.556 |            792.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3154.0537225561143
    time_step_min: 3003
  date: 2020-10-15_18-20-17
  done: false
  episode_len_mean: 792.472481375046
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.5154752240265
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 218
  episodes_total: 73423
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9305906404262206e-51
        cur_lr: 5.0e-05
        entropy: 0.07135861366987228
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010361608874518424
        total_loss: .nan
        vf_explained_var: 0.9972687363624573
        vf_loss: 1.2115241090456645
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.563333333333333
    gpu_util_percent0: 0.341
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633211670278465
    mean_env_wait_ms: 1.2181149528157642
    mean_inference_ms: 4.282110590192449
    mean_raw_obs_processing_ms: 0.37791810461802583
  time_since_restore: 9241.955559492111
  time_this_iter_s: 25.511064052581787
  time_total_s: 9241.955559492111
  timers:
    learn_throughput: 8688.65
    learn_time_ms: 18621.074
    sample_throughput: 23838.737
    sample_time_ms: 6786.937
    update_time_ms: 43.032
  timestamp: 1602786017
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:20:19,231	WARNING util.py:136 -- The `process_trial` operation took 0.9885766506195068 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    360 |          9241.96 | 58245120 |  298.515 |              321.404 |              126.556 |            792.472 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3153.715811646395
    time_step_min: 3003
  date: 2020-10-15_18-20-45
  done: false
  episode_len_mean: 792.4799473048037
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.56595919400934
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 73631
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.89588596063933e-51
        cur_lr: 5.0e-05
        entropy: 0.053908866830170155
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008512383841055756
        total_loss: .nan
        vf_explained_var: 0.9995100498199463
        vf_loss: 0.19993364438414574
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94193548387097
    gpu_util_percent0: 0.35225806451612907
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633102528351566
    mean_env_wait_ms: 1.218070359543137
    mean_inference_ms: 4.282053012669011
    mean_raw_obs_processing_ms: 0.377914861127775
  time_since_restore: 9267.782697200775
  time_this_iter_s: 25.82713770866394
  time_total_s: 9267.782697200775
  timers:
    learn_throughput: 8685.023
    learn_time_ms: 18628.851
    sample_throughput: 23793.8
    sample_time_ms: 6799.754
    update_time_ms: 44.693
  timestamp: 1602786045
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:20:46,455	WARNING util.py:136 -- The `process_trial` operation took 1.059629201889038 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    361 |          9267.78 | 58406912 |  298.566 |              321.404 |              126.556 |             792.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3153.3979098044
    time_step_min: 3003
  date: 2020-10-15_18-21-12
  done: false
  episode_len_mean: 792.4872864709628
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.61414521814913
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 188
  episodes_total: 73819
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.343828940958997e-51
        cur_lr: 5.0e-05
        entropy: 0.050067685854931675
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036413221387192607
        model: {}
        policy_loss: -0.007248167699193194
        total_loss: 0.11848485097289085
        vf_explained_var: 0.9996898770332336
        vf_loss: 0.12575804814696312
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.525806451612905
    gpu_util_percent0: 0.31064516129032255
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463301029021964
    mean_env_wait_ms: 1.2180290279725245
    mean_inference_ms: 4.28199765730137
    mean_raw_obs_processing_ms: 0.377911105609172
  time_since_restore: 9293.572401046753
  time_this_iter_s: 25.789703845977783
  time_total_s: 9293.572401046753
  timers:
    learn_throughput: 8691.941
    learn_time_ms: 18614.024
    sample_throughput: 23783.316
    sample_time_ms: 6802.752
    update_time_ms: 46.524
  timestamp: 1602786072
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: f0f97_00000
  
2020-10-15 18:21:13,556	WARNING util.py:136 -- The `process_trial` operation took 0.9633426666259766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    362 |          9293.57 | 58568704 |  298.614 |              321.404 |              126.556 |            792.487 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3153.0584609561915
    time_step_min: 3003
  date: 2020-10-15_18-21-39
  done: false
  episode_len_mean: 792.4983046726195
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.6665605082934
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 74027
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1719144704794984e-51
        cur_lr: 5.0e-05
        entropy: 0.05233504343777895
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005300047487253323
        total_loss: .nan
        vf_explained_var: 0.9994260668754578
        vf_loss: 0.24497950449585915
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303225806451614
    gpu_util_percent0: 0.2987096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632903486569127
    mean_env_wait_ms: 1.2179816568673456
    mean_inference_ms: 4.281938334297121
    mean_raw_obs_processing_ms: 0.37790678623310825
  time_since_restore: 9319.49895143509
  time_this_iter_s: 25.92655038833618
  time_total_s: 9319.49895143509
  timers:
    learn_throughput: 8693.144
    learn_time_ms: 18611.449
    sample_throughput: 23756.1
    sample_time_ms: 6810.546
    update_time_ms: 46.066
  timestamp: 1602786099
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:21:40,790	WARNING util.py:136 -- The `process_trial` operation took 0.9665064811706543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    363 |           9319.5 | 58730496 |  298.667 |              321.404 |              126.556 |            792.498 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3152.73222180289
    time_step_min: 3003
  date: 2020-10-15_18-22-06
  done: false
  episode_len_mean: 792.5052803146637
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.7168047565278
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 74238
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2578717057192464e-51
        cur_lr: 5.0e-05
        entropy: 0.051326097610096134
        entropy_coeff: 0.0005000000000000001
        kl: 0.004402499781766285
        model: {}
        policy_loss: -0.009198545003679707
        total_loss: 0.24051625902454057
        vf_explained_var: 0.9994524121284485
        vf_loss: 0.2497404652337233
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.232258064516135
    gpu_util_percent0: 0.28741935483870973
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632780609498655
    mean_env_wait_ms: 1.2179363649040806
    mean_inference_ms: 4.281880166558632
    mean_raw_obs_processing_ms: 0.3779034023627319
  time_since_restore: 9345.495757341385
  time_this_iter_s: 25.996805906295776
  time_total_s: 9345.495757341385
  timers:
    learn_throughput: 8687.747
    learn_time_ms: 18623.009
    sample_throughput: 23862.117
    sample_time_ms: 6780.287
    update_time_ms: 45.813
  timestamp: 1602786126
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: f0f97_00000
  
2020-10-15 18:22:08,216	WARNING util.py:136 -- The `process_trial` operation took 1.057464361190796 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    364 |           9345.5 | 58892288 |  298.717 |              321.404 |              126.556 |            792.505 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3152.4329788807186
    time_step_min: 3003
  date: 2020-10-15_18-22-34
  done: false
  episode_len_mean: 792.5099485443285
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.76124063577157
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 74433
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6289358528596232e-51
        cur_lr: 5.0e-05
        entropy: 0.053092213037113346
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008726711268536747
        total_loss: .nan
        vf_explained_var: 0.9992449283599854
        vf_loss: 0.30505092938741046
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.980645161290326
    gpu_util_percent0: 0.30548387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632698001583092
    mean_env_wait_ms: 1.2178936341102247
    mean_inference_ms: 4.281826638150731
    mean_raw_obs_processing_ms: 0.3779000592658877
  time_since_restore: 9371.799571275711
  time_this_iter_s: 26.303813934326172
  time_total_s: 9371.799571275711
  timers:
    learn_throughput: 8677.834
    learn_time_ms: 18644.284
    sample_throughput: 23833.663
    sample_time_ms: 6788.382
    update_time_ms: 46.132
  timestamp: 1602786154
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:22:36,077	WARNING util.py:136 -- The `process_trial` operation took 1.0936784744262695 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    365 |           9371.8 | 59054080 |  298.761 |              321.404 |              126.556 |             792.51 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3152.1226670599594
    time_step_min: 3003
  date: 2020-10-15_18-23-02
  done: false
  episode_len_mean: 792.5165885032828
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.8075725769007
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 74630
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4434037792894356e-51
        cur_lr: 5.0e-05
        entropy: 0.05395437156160673
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006171074443651984
        total_loss: .nan
        vf_explained_var: 0.9991724491119385
        vf_loss: 0.3513864502310753
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.671875
    gpu_util_percent0: 0.3421875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632574959139605
    mean_env_wait_ms: 1.2178504902198481
    mean_inference_ms: 4.281772753761921
    mean_raw_obs_processing_ms: 0.37789589531591117
  time_since_restore: 9398.007792711258
  time_this_iter_s: 26.208221435546875
  time_total_s: 9398.007792711258
  timers:
    learn_throughput: 8667.663
    learn_time_ms: 18666.162
    sample_throughput: 23815.224
    sample_time_ms: 6793.638
    update_time_ms: 46.498
  timestamp: 1602786182
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:23:03,648	WARNING util.py:136 -- The `process_trial` operation took 1.012397289276123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    366 |          9398.01 | 59215872 |  298.808 |              321.404 |              126.556 |            792.517 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3151.7765226458478
    time_step_min: 3003
  date: 2020-10-15_18-23-29
  done: false
  episode_len_mean: 792.5280828323313
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.86112629298947
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 74850
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.665105668934153e-51
        cur_lr: 5.0e-05
        entropy: 0.052829196055730186
        entropy_coeff: 0.0005000000000000001
        kl: 0.004551078425720334
        model: {}
        policy_loss: -0.004192593555975084
        total_loss: 0.24923432370026907
        vf_explained_var: 0.9994763731956482
        vf_loss: 0.2534533279637496
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.716129032258067
    gpu_util_percent0: 0.34225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632465693595456
    mean_env_wait_ms: 1.217801394627671
    mean_inference_ms: 4.281713337731556
    mean_raw_obs_processing_ms: 0.37789186722725626
  time_since_restore: 9424.021364927292
  time_this_iter_s: 26.013572216033936
  time_total_s: 9424.021364927292
  timers:
    learn_throughput: 8655.212
    learn_time_ms: 18693.014
    sample_throughput: 23809.522
    sample_time_ms: 6795.265
    update_time_ms: 38.314
  timestamp: 1602786209
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: f0f97_00000
  
2020-10-15 18:23:31,024	WARNING util.py:136 -- The `process_trial` operation took 0.9720406532287598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    367 |          9424.02 | 59377664 |  298.861 |              321.404 |              126.556 |            792.528 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3151.4547308917836
    time_step_min: 3003
  date: 2020-10-15_18-23-56
  done: false
  episode_len_mean: 792.5364875487988
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.9081839405871
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 75053
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8325528344670766e-51
        cur_lr: 5.0e-05
        entropy: 0.053433461425205074
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038607497699558735
        model: {}
        policy_loss: -0.006617364056486015
        total_loss: 0.2620653746028741
        vf_explained_var: 0.9993886947631836
        vf_loss: 0.2687094608942668
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.746666666666666
    gpu_util_percent0: 0.28966666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632353234492884
    mean_env_wait_ms: 1.217757614482541
    mean_inference_ms: 4.281659911905229
    mean_raw_obs_processing_ms: 0.37788888337910564
  time_since_restore: 9449.729744195938
  time_this_iter_s: 25.70837926864624
  time_total_s: 9449.729744195938
  timers:
    learn_throughput: 8671.501
    learn_time_ms: 18657.9
    sample_throughput: 23782.639
    sample_time_ms: 6802.945
    update_time_ms: 30.293
  timestamp: 1602786236
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: f0f97_00000
  
2020-10-15 18:23:58,231	WARNING util.py:136 -- The `process_trial` operation took 1.0467731952667236 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    368 |          9449.73 | 59539456 |  298.908 |              321.404 |              126.556 |            792.536 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3151.1537407909786
    time_step_min: 3003
  date: 2020-10-15_18-24-23
  done: false
  episode_len_mean: 792.5449736856095
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 298.953946728998
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 75244
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.162764172335383e-52
        cur_lr: 5.0e-05
        entropy: 0.05292397396018108
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008073312948302677
        total_loss: .nan
        vf_explained_var: 0.9994640946388245
        vf_loss: 0.20921503379940987
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.161290322580644
    gpu_util_percent0: 0.3367741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8709677419354844
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632261118150286
    mean_env_wait_ms: 1.2177160624553434
    mean_inference_ms: 4.281606969263771
    mean_raw_obs_processing_ms: 0.377885109474626
  time_since_restore: 9475.397031545639
  time_this_iter_s: 25.667287349700928
  time_total_s: 9475.397031545639
  timers:
    learn_throughput: 8683.046
    learn_time_ms: 18633.093
    sample_throughput: 23812.023
    sample_time_ms: 6794.551
    update_time_ms: 29.208
  timestamp: 1602786263
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:24:25,317	WARNING util.py:136 -- The `process_trial` operation took 1.0618970394134521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    369 |           9475.4 | 59701248 |  298.954 |              321.404 |              126.556 |            792.545 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3150.838938546004
    time_step_min: 3003
  date: 2020-10-15_18-24-51
  done: false
  episode_len_mean: 792.5511848592482
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.0023067712587
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 75452
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3744146258503074e-51
        cur_lr: 5.0e-05
        entropy: 0.05427538541456064
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0055177128039455665
        total_loss: .nan
        vf_explained_var: 0.9996085166931152
        vf_loss: 0.16505272189776102
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.183870967741935
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463214988676231
    mean_env_wait_ms: 1.2176692909778273
    mean_inference_ms: 4.28154957383426
    mean_raw_obs_processing_ms: 0.37788084372871034
  time_since_restore: 9501.332875490189
  time_this_iter_s: 25.93584394454956
  time_total_s: 9501.332875490189
  timers:
    learn_throughput: 8672.978
    learn_time_ms: 18654.723
    sample_throughput: 23776.592
    sample_time_ms: 6804.676
    update_time_ms: 37.127
  timestamp: 1602786291
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:24:52,663	WARNING util.py:136 -- The `process_trial` operation took 1.0609405040740967 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    370 |          9501.33 | 59863040 |  299.002 |              321.404 |              126.556 |            792.551 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3150.500634803079
    time_step_min: 3003
  date: 2020-10-15_18-25-18
  done: false
  episode_len_mean: 792.5599920697858
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.05241551117786
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 75660
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0616219387754606e-51
        cur_lr: 5.0e-05
        entropy: 0.052117022685706615
        entropy_coeff: 0.0005000000000000001
        kl: 0.006997420297314723
        model: {}
        policy_loss: -0.007195000827778131
        total_loss: 0.14089245659609637
        vf_explained_var: 0.999664843082428
        vf_loss: 0.14811351522803307
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.264516129032256
    gpu_util_percent0: 0.30032258064516126
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463204468297033
    mean_env_wait_ms: 1.2176243972342957
    mean_inference_ms: 4.281496349068704
    mean_raw_obs_processing_ms: 0.37787758505156827
  time_since_restore: 9527.298211097717
  time_this_iter_s: 25.965335607528687
  time_total_s: 9527.298211097717
  timers:
    learn_throughput: 8666.546
    learn_time_ms: 18668.567
    sample_throughput: 23767.648
    sample_time_ms: 6807.237
    update_time_ms: 36.815
  timestamp: 1602786318
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: f0f97_00000
  
2020-10-15 18:25:20,110	WARNING util.py:136 -- The `process_trial` operation took 1.0252511501312256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    371 |           9527.3 | 60024832 |  299.052 |              321.404 |              126.556 |             792.56 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3150.253403155173
    time_step_min: 3003
  date: 2020-10-15_18-25-45
  done: false
  episode_len_mean: 792.5612064647104
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.08846792691554
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 75858
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0616219387754606e-51
        cur_lr: 5.0e-05
        entropy: 0.06792409966389339
        entropy_coeff: 0.0005000000000000001
        kl: 0.006544571369886398
        model: {}
        policy_loss: -0.010824671170363823
        total_loss: 0.6936303327480952
        vf_explained_var: 0.9982423186302185
        vf_loss: 0.7044889777898788
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.032258064516128
    gpu_util_percent0: 0.3090322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463195986524808
    mean_env_wait_ms: 1.2175824141860347
    mean_inference_ms: 4.28144402485006
    mean_raw_obs_processing_ms: 0.3778743231127906
  time_since_restore: 9553.061882257462
  time_this_iter_s: 25.763671159744263
  time_total_s: 9553.061882257462
  timers:
    learn_throughput: 8665.662
    learn_time_ms: 18670.471
    sample_throughput: 23776.948
    sample_time_ms: 6804.574
    update_time_ms: 35.15
  timestamp: 1602786345
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: f0f97_00000
  
2020-10-15 18:25:47,206	WARNING util.py:136 -- The `process_trial` operation took 0.9765164852142334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    372 |          9553.06 | 60186624 |  299.088 |              321.404 |              126.556 |            792.561 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3150.059503762564
    time_step_min: 3003
  date: 2020-10-15_18-26-13
  done: false
  episode_len_mean: 792.5613479186936
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.1187748531087
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 76058
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0616219387754606e-51
        cur_lr: 5.0e-05
        entropy: 0.07298683747649193
        entropy_coeff: 0.0005000000000000001
        kl: 0.007556640271407862
        model: {}
        policy_loss: -0.011769280888984213
        total_loss: 0.7878047724564871
        vf_explained_var: 0.998127281665802
        vf_loss: 0.7996105204025904
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94838709677419
    gpu_util_percent0: 0.30709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631842484314853
    mean_env_wait_ms: 1.2175387688928507
    mean_inference_ms: 4.281391553888183
    mean_raw_obs_processing_ms: 0.37787053064121334
  time_since_restore: 9579.025075674057
  time_this_iter_s: 25.96319341659546
  time_total_s: 9579.025075674057
  timers:
    learn_throughput: 8660.894
    learn_time_ms: 18680.75
    sample_throughput: 23796.084
    sample_time_ms: 6799.102
    update_time_ms: 33.752
  timestamp: 1602786373
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: f0f97_00000
  
2020-10-15 18:26:14,600	WARNING util.py:136 -- The `process_trial` operation took 1.078805923461914 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    373 |          9579.03 | 60348416 |  299.119 |              321.404 |              126.556 |            792.561 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3149.813469939262
    time_step_min: 3003
  date: 2020-10-15_18-26-40
  done: false
  episode_len_mean: 792.559331366765
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.152170674294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 217
  episodes_total: 76275
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0616219387754606e-51
        cur_lr: 5.0e-05
        entropy: 0.0760252084583044
        entropy_coeff: 0.0005000000000000001
        kl: 0.006911210133694112
        model: {}
        policy_loss: -0.011867774141137488
        total_loss: 1.1740022997061412
        vf_explained_var: 0.9972290992736816
        vf_loss: 1.1859080990155537
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.396774193548385
    gpu_util_percent0: 0.3029032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463173006551313
    mean_env_wait_ms: 1.2174913614865868
    mean_inference_ms: 4.281334437060021
    mean_raw_obs_processing_ms: 0.3778667212410208
  time_since_restore: 9605.175561666489
  time_this_iter_s: 26.15048599243164
  time_total_s: 9605.175561666489
  timers:
    learn_throughput: 8654.525
    learn_time_ms: 18694.497
    sample_throughput: 23758.527
    sample_time_ms: 6809.85
    update_time_ms: 32.487
  timestamp: 1602786400
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: f0f97_00000
  
2020-10-15 18:26:42,238	WARNING util.py:136 -- The `process_trial` operation took 0.9749767780303955 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    374 |          9605.18 | 60510208 |  299.152 |              321.404 |              126.556 |            792.559 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3149.5971006528766
    time_step_min: 3003
  date: 2020-10-15_18-27-08
  done: false
  episode_len_mean: 792.5611098761719
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.1893797078749
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 76477
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0616219387754606e-51
        cur_lr: 5.0e-05
        entropy: 0.06440660202254851
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010535890406269269
        total_loss: .nan
        vf_explained_var: 0.9987160563468933
        vf_loss: 0.5377080539862314
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91612903225806
    gpu_util_percent0: 0.29225806451612907
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631636439696033
    mean_env_wait_ms: 1.2174489092410332
    mean_inference_ms: 4.281283404714026
    mean_raw_obs_processing_ms: 0.3778635401348237
  time_since_restore: 9631.220017194748
  time_this_iter_s: 26.044455528259277
  time_total_s: 9631.220017194748
  timers:
    learn_throughput: 8658.138
    learn_time_ms: 18686.696
    sample_throughput: 23824.249
    sample_time_ms: 6791.064
    update_time_ms: 32.068
  timestamp: 1602786428
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:27:09,815	WARNING util.py:136 -- The `process_trial` operation took 1.0838422775268555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    375 |          9631.22 | 60672000 |  299.189 |              321.404 |              126.556 |            792.561 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3149.341368239716
    time_step_min: 3003
  date: 2020-10-15_18-27-35
  done: false
  episode_len_mean: 792.5646145819748
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.2312198547357
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 76670
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.092432908163191e-51
        cur_lr: 5.0e-05
        entropy: 0.05515686081101497
        entropy_coeff: 0.0005000000000000001
        kl: 0.004454289912246168
        model: {}
        policy_loss: -0.00844779007214432
        total_loss: 0.29114509870608646
        vf_explained_var: 0.9992506504058838
        vf_loss: 0.29962046444416046
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.34375
    gpu_util_percent0: 0.29531250000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631539909923422
    mean_env_wait_ms: 1.2174078655046667
    mean_inference_ms: 4.2812338683760585
    mean_raw_obs_processing_ms: 0.3778598899799097
  time_since_restore: 9657.403402328491
  time_this_iter_s: 26.183385133743286
  time_total_s: 9657.403402328491
  timers:
    learn_throughput: 8652.805
    learn_time_ms: 18698.214
    sample_throughput: 23881.207
    sample_time_ms: 6774.867
    update_time_ms: 32.365
  timestamp: 1602786455
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: f0f97_00000
  
2020-10-15 18:27:37,400	WARNING util.py:136 -- The `process_trial` operation took 1.0259950160980225 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    376 |           9657.4 | 60833792 |  299.231 |              321.404 |              126.556 |            792.565 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3149.0363506214617
    time_step_min: 3003
  date: 2020-10-15_18-28-03
  done: false
  episode_len_mean: 792.571974870254
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.27984452950255
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 76881
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5462164540815955e-51
        cur_lr: 5.0e-05
        entropy: 0.05293136773010095
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004803699538266907
        total_loss: .nan
        vf_explained_var: 0.9997014999389648
        vf_loss: 0.13681752607226372
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.970967741935482
    gpu_util_percent0: 0.29580645161290325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631427589625834
    mean_env_wait_ms: 1.2173609584640879
    mean_inference_ms: 4.2811771320779926
    mean_raw_obs_processing_ms: 0.37785583937359474
  time_since_restore: 9683.474336385727
  time_this_iter_s: 26.070934057235718
  time_total_s: 9683.474336385727
  timers:
    learn_throughput: 8651.317
    learn_time_ms: 18701.43
    sample_throughput: 23881.479
    sample_time_ms: 6774.79
    update_time_ms: 32.753
  timestamp: 1602786483
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:28:04,899	WARNING util.py:136 -- The `process_trial` operation took 1.0697169303894043 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    377 |          9683.47 | 60995584 |   299.28 |              321.404 |              126.556 |            792.572 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3148.7206437796094
    time_step_min: 3003
  date: 2020-10-15_18-28-30
  done: false
  episode_len_mean: 792.5800417688187
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.32725826188044
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 77091
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3193246811223936e-51
        cur_lr: 5.0e-05
        entropy: 0.04859623654435078
        entropy_coeff: 0.0005000000000000001
        kl: 0.003503958481208732
        model: {}
        policy_loss: -0.009172110721313706
        total_loss: 0.22804311911265054
        vf_explained_var: 0.9994699954986572
        vf_loss: 0.23723952348033586
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4258064516129
    gpu_util_percent0: 0.25870967741935486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631328744652514
    mean_env_wait_ms: 1.2173166996836433
    mean_inference_ms: 4.281126383204949
    mean_raw_obs_processing_ms: 0.3778526179693229
  time_since_restore: 9709.394586801529
  time_this_iter_s: 25.920250415802002
  time_total_s: 9709.394586801529
  timers:
    learn_throughput: 8647.829
    learn_time_ms: 18708.972
    sample_throughput: 23840.385
    sample_time_ms: 6786.468
    update_time_ms: 34.192
  timestamp: 1602786510
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: f0f97_00000
  
2020-10-15 18:28:32,437	WARNING util.py:136 -- The `process_trial` operation took 1.078784704208374 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    378 |          9709.39 | 61157376 |  299.327 |              321.404 |              126.556 |             792.58 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3148.4647721387882
    time_step_min: 3003
  date: 2020-10-15_18-28-58
  done: false
  episode_len_mean: 792.5852935848666
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.36849003974953
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 77286
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1596623405611968e-51
        cur_lr: 5.0e-05
        entropy: 0.05737603362649679
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008665977336931974
        total_loss: .nan
        vf_explained_var: 0.9994136691093445
        vf_loss: 0.24294686317443848
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.903225806451616
    gpu_util_percent0: 0.2706451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631239308682198
    mean_env_wait_ms: 1.2172761145003073
    mean_inference_ms: 4.281076353073949
    mean_raw_obs_processing_ms: 0.37784945621322247
  time_since_restore: 9735.671023607254
  time_this_iter_s: 26.276436805725098
  time_total_s: 9735.671023607254
  timers:
    learn_throughput: 8628.523
    learn_time_ms: 18750.834
    sample_throughput: 23737.713
    sample_time_ms: 6815.821
    update_time_ms: 36.508
  timestamp: 1602786538
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:29:00,110	WARNING util.py:136 -- The `process_trial` operation took 1.032754898071289 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    379 |          9735.67 | 61319168 |  299.368 |              321.404 |              126.556 |            792.585 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3148.26082072207
    time_step_min: 3003
  date: 2020-10-15_18-29-26
  done: false
  episode_len_mean: 792.5817266744097
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.39456899619444
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 77490
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7394935108417953e-51
        cur_lr: 5.0e-05
        entropy: 0.08763067610561848
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012306498868080476
        total_loss: .nan
        vf_explained_var: 0.9962198138237
        vf_loss: 1.5462182660897572
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.745161290322585
    gpu_util_percent0: 0.3545161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631131110240847
    mean_env_wait_ms: 1.2172312993200975
    mean_inference_ms: 4.281025288100662
    mean_raw_obs_processing_ms: 0.3778453947874255
  time_since_restore: 9761.56558895111
  time_this_iter_s: 25.89456534385681
  time_total_s: 9761.56558895111
  timers:
    learn_throughput: 8635.227
    learn_time_ms: 18736.276
    sample_throughput: 23677.121
    sample_time_ms: 6833.263
    update_time_ms: 28.964
  timestamp: 1602786566
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:29:27,443	WARNING util.py:136 -- The `process_trial` operation took 1.0075962543487549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    380 |          9761.57 | 61480960 |  299.395 |              321.404 |              126.556 |            792.582 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3148.2551188605867
    time_step_min: 3003
  date: 2020-10-15_18-29-53
  done: false
  episode_len_mean: 792.5622007722008
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.3927927927922
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 77700
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6092402662626926e-51
        cur_lr: 5.0e-05
        entropy: 0.09055117890238762
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011522971052424206
        total_loss: .nan
        vf_explained_var: 0.995897114276886
        vf_loss: 1.8350715637207031
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.916129032258066
    gpu_util_percent0: 0.3190322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463102465318659
    mean_env_wait_ms: 1.2171872653373919
    mean_inference_ms: 4.28097342128711
    mean_raw_obs_processing_ms: 0.37784222890266034
  time_since_restore: 9787.627836704254
  time_this_iter_s: 26.06224775314331
  time_total_s: 9787.627836704254
  timers:
    learn_throughput: 8642.233
    learn_time_ms: 18721.088
    sample_throughput: 23590.501
    sample_time_ms: 6858.354
    update_time_ms: 27.383
  timestamp: 1602786593
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:29:54,972	WARNING util.py:136 -- The `process_trial` operation took 1.007584810256958 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    381 |          9787.63 | 61642752 |  299.393 |              321.404 |              126.556 |            792.562 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3148.1415654139587
    time_step_min: 3003
  date: 2020-10-15_18-30-20
  done: false
  episode_len_mean: 792.5601509550215
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.41737616460296
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 77904
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9138603993940395e-51
        cur_lr: 5.0e-05
        entropy: 0.06764100740353267
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032879554006891945
        model: {}
        policy_loss: -0.00892715837835567
        total_loss: 0.737944687406222
        vf_explained_var: 0.9982158541679382
        vf_loss: 0.7469056695699692
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529032258064515
    gpu_util_percent0: 0.33677419354838706
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463093810341149
    mean_env_wait_ms: 1.2171452254750577
    mean_inference_ms: 4.280923919977181
    mean_raw_obs_processing_ms: 0.3778391803511114
  time_since_restore: 9813.179948568344
  time_this_iter_s: 25.552111864089966
  time_total_s: 9813.179948568344
  timers:
    learn_throughput: 8654.485
    learn_time_ms: 18694.585
    sample_throughput: 23594.986
    sample_time_ms: 6857.05
    update_time_ms: 27.075
  timestamp: 1602786620
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: f0f97_00000
  
2020-10-15 18:30:21,912	WARNING util.py:136 -- The `process_trial` operation took 1.0231645107269287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    382 |          9813.18 | 61804544 |  299.417 |              321.404 |              126.556 |             792.56 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3147.8721220740285
    time_step_min: 3003
  date: 2020-10-15_18-30-47
  done: false
  episode_len_mean: 792.5689078965902
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.4577506113539
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 78097
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9569301996970198e-51
        cur_lr: 5.0e-05
        entropy: 0.05512436262021462
        entropy_coeff: 0.0005000000000000001
        kl: 0.004234588084121545
        model: {}
        policy_loss: -0.0070840482755253715
        total_loss: 0.25193534915645915
        vf_explained_var: 0.9993699193000793
        vf_loss: 0.2590469593803088
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806457
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630844637294138
    mean_env_wait_ms: 1.2171047274433635
    mean_inference_ms: 4.280877331376869
    mean_raw_obs_processing_ms: 0.3778355746336309
  time_since_restore: 9839.165502548218
  time_this_iter_s: 25.985553979873657
  time_total_s: 9839.165502548218
  timers:
    learn_throughput: 8651.45
    learn_time_ms: 18701.142
    sample_throughput: 23606.934
    sample_time_ms: 6853.579
    update_time_ms: 34.871
  timestamp: 1602786647
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: f0f97_00000
  
2020-10-15 18:30:49,328	WARNING util.py:136 -- The `process_trial` operation took 1.06331467628479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    383 |          9839.17 | 61966336 |  299.458 |              321.404 |              126.556 |            792.569 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3147.5734364019677
    time_step_min: 3003
  date: 2020-10-15_18-31-15
  done: false
  episode_len_mean: 792.5794588244308
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.5047735724519
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 78311
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.784650998485099e-52
        cur_lr: 5.0e-05
        entropy: 0.05402525793761015
        entropy_coeff: 0.0005000000000000001
        kl: 0.00321013800567016
        model: {}
        policy_loss: -0.007351272332016379
        total_loss: 0.20236371705929437
        vf_explained_var: 0.9995319843292236
        vf_loss: 0.20974200094739595
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206451612903226
    gpu_util_percent0: 0.32354838709677425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463073559047182
    mean_env_wait_ms: 1.2170583911695596
    mean_inference_ms: 4.28082523447245
    mean_raw_obs_processing_ms: 0.37783201877187594
  time_since_restore: 9865.188742399216
  time_this_iter_s: 26.023239850997925
  time_total_s: 9865.188742399216
  timers:
    learn_throughput: 8659.795
    learn_time_ms: 18683.121
    sample_throughput: 23591.584
    sample_time_ms: 6858.039
    update_time_ms: 34.759
  timestamp: 1602786675
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: f0f97_00000
  
2020-10-15 18:31:16,922	WARNING util.py:136 -- The `process_trial` operation took 1.1211540699005127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    384 |          9865.19 | 62128128 |  299.505 |              321.404 |              126.556 |            792.579 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3147.27801707659
    time_step_min: 3003
  date: 2020-10-15_18-31-43
  done: false
  episode_len_mean: 792.5888736053798
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.5457142879191
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 78516
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.8923254992425494e-52
        cur_lr: 5.0e-05
        entropy: 0.0562267933661739
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009175094251986593
        total_loss: .nan
        vf_explained_var: 0.9992611408233643
        vf_loss: 0.31875797361135483
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.575000000000003
    gpu_util_percent0: 0.33218749999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630635315412524
    mean_env_wait_ms: 1.2170162969765967
    mean_inference_ms: 4.280776544000464
    mean_raw_obs_processing_ms: 0.3778288770093505
  time_since_restore: 9891.358866214752
  time_this_iter_s: 26.1701238155365
  time_total_s: 9891.358866214752
  timers:
    learn_throughput: 8657.195
    learn_time_ms: 18688.733
    sample_throughput: 23609.43
    sample_time_ms: 6852.855
    update_time_ms: 36.799
  timestamp: 1602786703
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:31:44,504	WARNING util.py:136 -- The `process_trial` operation took 1.0351452827453613 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    385 |          9891.36 | 62289920 |  299.546 |              321.404 |              126.556 |            792.589 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3146.9921055642426
    time_step_min: 3003
  date: 2020-10-15_18-32-10
  done: false
  episode_len_mean: 792.5991817962367
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.58712138857936
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 78709
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.338488248863823e-52
        cur_lr: 5.0e-05
        entropy: 0.05686013028025627
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005840249862861431
        total_loss: .nan
        vf_explained_var: 0.9993463158607483
        vf_loss: 0.25986749803026515
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06129032258064
    gpu_util_percent0: 0.3074193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630553640544286
    mean_env_wait_ms: 1.2169764365358806
    mean_inference_ms: 4.280729277458316
    mean_raw_obs_processing_ms: 0.37782583507438416
  time_since_restore: 9917.428413629532
  time_this_iter_s: 26.069547414779663
  time_total_s: 9917.428413629532
  timers:
    learn_throughput: 8675.484
    learn_time_ms: 18649.335
    sample_throughput: 23515.461
    sample_time_ms: 6880.239
    update_time_ms: 34.882
  timestamp: 1602786730
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:32:12,082	WARNING util.py:136 -- The `process_trial` operation took 1.1439073085784912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    386 |          9917.43 | 62451712 |  299.587 |              321.404 |              126.556 |            792.599 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3146.812400152149
    time_step_min: 3003
  date: 2020-10-15_18-32-37
  done: false
  episode_len_mean: 792.5984844644939
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.60936459224456
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 78916
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1007732373295737e-51
        cur_lr: 5.0e-05
        entropy: 0.08694071943561237
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012260072010879716
        total_loss: .nan
        vf_explained_var: 0.9965264201164246
        vf_loss: 1.500173270702362
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42258064516129
    gpu_util_percent0: 0.28709677419354834
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146304590204597
    mean_env_wait_ms: 1.2169324730237365
    mean_inference_ms: 4.280678848486422
    mean_raw_obs_processing_ms: 0.37782201935810356
  time_since_restore: 9943.23051571846
  time_this_iter_s: 25.802102088928223
  time_total_s: 9943.23051571846
  timers:
    learn_throughput: 8683.554
    learn_time_ms: 18632.002
    sample_throughput: 23547.398
    sample_time_ms: 6870.908
    update_time_ms: 34.084
  timestamp: 1602786757
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:32:39,387	WARNING util.py:136 -- The `process_trial` operation took 1.1199982166290283 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    387 |          9943.23 | 62613504 |  299.609 |              321.404 |              126.556 |            792.598 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3146.786120735439
    time_step_min: 3003
  date: 2020-10-15_18-33-05
  done: false
  episode_len_mean: 792.593203417248
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.61104217792064
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 79128
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6511598559943604e-51
        cur_lr: 5.0e-05
        entropy: 0.08184480418761571
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012907411088235676
        total_loss: .nan
        vf_explained_var: 0.9967021942138672
        vf_loss: 1.5725018084049225
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.225806451612904
    gpu_util_percent0: 0.27935483870967737
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630338632828174
    mean_env_wait_ms: 1.216888757948455
    mean_inference_ms: 4.280629684320743
    mean_raw_obs_processing_ms: 0.377818975048958
  time_since_restore: 9969.086024522781
  time_this_iter_s: 25.85550880432129
  time_total_s: 9969.086024522781
  timers:
    learn_throughput: 8683.649
    learn_time_ms: 18631.798
    sample_throughput: 23617.655
    sample_time_ms: 6850.469
    update_time_ms: 34.659
  timestamp: 1602786785
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:33:06,744	WARNING util.py:136 -- The `process_trial` operation took 1.0629651546478271 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    388 |          9969.09 | 62775296 |  299.611 |              321.404 |              126.556 |            792.593 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3146.6095281341054
    time_step_min: 3003
  date: 2020-10-15_18-33-32
  done: false
  episode_len_mean: 792.6017875376606
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.64424559994734
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 79327
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4767397839915402e-51
        cur_lr: 5.0e-05
        entropy: 0.06357830421378215
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008764316218730528
        total_loss: .nan
        vf_explained_var: 0.9992439150810242
        vf_loss: 0.31640147666136426
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.109677419354846
    gpu_util_percent0: 0.3103225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630261505056746
    mean_env_wait_ms: 1.2168482150307542
    mean_inference_ms: 4.2805832848640435
    mean_raw_obs_processing_ms: 0.37781615614578773
  time_since_restore: 9994.895441055298
  time_this_iter_s: 25.80941653251648
  time_total_s: 9994.895441055298
  timers:
    learn_throughput: 8700.761
    learn_time_ms: 18595.155
    sample_throughput: 23695.552
    sample_time_ms: 6827.948
    update_time_ms: 34.892
  timestamp: 1602786812
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:33:34,072	WARNING util.py:136 -- The `process_trial` operation took 1.0622310638427734 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    389 |           9994.9 | 62937088 |  299.644 |              321.404 |              126.556 |            792.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3146.308958228485
    time_step_min: 3003
  date: 2020-10-15_18-33-59
  done: false
  episode_len_mean: 792.6125166612177
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.6876682474972
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 79526
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.71510967598731e-51
        cur_lr: 5.0e-05
        entropy: 0.055372412937382855
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006294631602941081
        total_loss: .nan
        vf_explained_var: 0.9997520446777344
        vf_loss: 0.10106299072504044
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.519999999999996
    gpu_util_percent0: 0.2736666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630162495611848
    mean_env_wait_ms: 1.2168069399068222
    mean_inference_ms: 4.280537330736137
    mean_raw_obs_processing_ms: 0.37781239328783944
  time_since_restore: 10020.628943681717
  time_this_iter_s: 25.733502626419067
  time_total_s: 10020.628943681717
  timers:
    learn_throughput: 8701.804
    learn_time_ms: 18592.925
    sample_throughput: 23753.849
    sample_time_ms: 6811.191
    update_time_ms: 36.355
  timestamp: 1602786839
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:34:01,355	WARNING util.py:136 -- The `process_trial` operation took 1.0919959545135498 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    390 |          10020.6 | 63098880 |  299.688 |              321.404 |              126.556 |            792.613 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3146.0075289865986
    time_step_min: 3003
  date: 2020-10-15_18-34-27
  done: false
  episode_len_mean: 792.622639143194
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.7341833646601
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 212
  episodes_total: 79738
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.572664513980966e-51
        cur_lr: 5.0e-05
        entropy: 0.05485953235377868
        entropy_coeff: 0.0005000000000000001
        kl: 0.005380135805656512
        model: {}
        policy_loss: -0.00523774169657069
        total_loss: 0.17974493031700453
        vf_explained_var: 0.9995567798614502
        vf_loss: 0.18501010164618492
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.265625
    gpu_util_percent0: 0.316875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630063992923095
    mean_env_wait_ms: 1.21676171974414
    mean_inference_ms: 4.280488633821225
    mean_raw_obs_processing_ms: 0.3778091809632848
  time_since_restore: 10047.073932170868
  time_this_iter_s: 26.444988489151
  time_total_s: 10047.073932170868
  timers:
    learn_throughput: 8674.893
    learn_time_ms: 18650.605
    sample_throughput: 23867.122
    sample_time_ms: 6778.865
    update_time_ms: 45.383
  timestamp: 1602786867
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: f0f97_00000
  
2020-10-15 18:34:29,244	WARNING util.py:136 -- The `process_trial` operation took 1.0732953548431396 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    391 |          10047.1 | 63260672 |  299.734 |              321.404 |              126.556 |            792.623 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3145.720738988397
    time_step_min: 3003
  date: 2020-10-15_18-34-55
  done: false
  episode_len_mean: 792.632144510189
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.7776203344944
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 79939
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.572664513980966e-51
        cur_lr: 5.0e-05
        entropy: 0.050417449014882244
        entropy_coeff: 0.0005000000000000001
        kl: 0.003339407208841294
        model: {}
        policy_loss: -0.004130319384557879
        total_loss: 0.13992643232146898
        vf_explained_var: 0.9997290968894958
        vf_loss: 0.14408196260531744
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.467741935483875
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462997083481614
    mean_env_wait_ms: 1.2167220731186543
    mean_inference_ms: 4.280443136045136
    mean_raw_obs_processing_ms: 0.377806275666932
  time_since_restore: 10073.031103372574
  time_this_iter_s: 25.957171201705933
  time_total_s: 10073.031103372574
  timers:
    learn_throughput: 8658.343
    learn_time_ms: 18686.255
    sample_throughput: 23849.578
    sample_time_ms: 6783.852
    update_time_ms: 47.896
  timestamp: 1602786895
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: f0f97_00000
  
2020-10-15 18:34:56,784	WARNING util.py:136 -- The `process_trial` operation took 1.1100895404815674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    392 |            10073 | 63422464 |  299.778 |              321.404 |              126.556 |            792.632 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3145.4213936995093
    time_step_min: 3003
  date: 2020-10-15_18-35-22
  done: false
  episode_len_mean: 792.6423660073626
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.820925924875
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 80135
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.786332256990483e-51
        cur_lr: 5.0e-05
        entropy: 0.05320949635157982
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005434562954178546
        total_loss: .nan
        vf_explained_var: 0.9998180866241455
        vf_loss: 0.09664525340000789
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.551612903225806
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629885463348086
    mean_env_wait_ms: 1.2166820793772992
    mean_inference_ms: 4.280398050410639
    mean_raw_obs_processing_ms: 0.37780320516287375
  time_since_restore: 10098.996895313263
  time_this_iter_s: 25.965791940689087
  time_total_s: 10098.996895313263
  timers:
    learn_throughput: 8658.421
    learn_time_ms: 18686.087
    sample_throughput: 23837.913
    sample_time_ms: 6787.171
    update_time_ms: 42.276
  timestamp: 1602786922
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:35:24,395	WARNING util.py:136 -- The `process_trial` operation took 1.1478245258331299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    393 |            10099 | 63584256 |  299.821 |              321.404 |              126.556 |            792.642 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3145.1685057414
    time_step_min: 3003
  date: 2020-10-15_18-35-50
  done: false
  episode_len_mean: 792.6463903410505
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.85367491192676
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 80340
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.179498385485725e-51
        cur_lr: 5.0e-05
        entropy: 0.07335259144504865
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011134808407708382
        total_loss: .nan
        vf_explained_var: 0.9979879260063171
        vf_loss: 0.8615238020817438
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.693548387096783
    gpu_util_percent0: 0.33387096774193553
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462979386008299
    mean_env_wait_ms: 1.2166385615200184
    mean_inference_ms: 4.28034981287976
    mean_raw_obs_processing_ms: 0.3777995544055865
  time_since_restore: 10124.689253807068
  time_this_iter_s: 25.69235849380493
  time_total_s: 10124.689253807068
  timers:
    learn_throughput: 8671.613
    learn_time_ms: 18657.658
    sample_throughput: 23864.103
    sample_time_ms: 6779.723
    update_time_ms: 44.571
  timestamp: 1602786950
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:35:51,659	WARNING util.py:136 -- The `process_trial` operation took 1.1362018585205078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    394 |          10124.7 | 63746048 |  299.854 |              321.404 |              126.556 |            792.646 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3145.043626787245
    time_step_min: 3003
  date: 2020-10-15_18-36-17
  done: false
  episode_len_mean: 792.6453747501458
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.872734195092
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 80547
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.269247578228588e-51
        cur_lr: 5.0e-05
        entropy: 0.06803226284682751
        entropy_coeff: 0.0005000000000000001
        kl: 0.00518248505735149
        model: {}
        policy_loss: -0.00974761513498379
        total_loss: 1.022634228070577
        vf_explained_var: 0.9976904988288879
        vf_loss: 1.032415856917699
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.1
    gpu_util_percent0: 0.3464516129032259
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629692766599522
    mean_env_wait_ms: 1.2165966176950727
    mean_inference_ms: 4.280303245316165
    mean_raw_obs_processing_ms: 0.37779669857155557
  time_since_restore: 10150.478659391403
  time_this_iter_s: 25.789405584335327
  time_total_s: 10150.478659391403
  timers:
    learn_throughput: 8685.19
    learn_time_ms: 18628.492
    sample_throughput: 23858.6
    sample_time_ms: 6781.286
    update_time_ms: 42.607
  timestamp: 1602786977
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: f0f97_00000
  
2020-10-15 18:36:18,895	WARNING util.py:136 -- The `process_trial` operation took 1.0835535526275635 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    395 |          10150.5 | 63907840 |  299.873 |              321.404 |              126.556 |            792.645 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3144.792835547185
    time_step_min: 3003
  date: 2020-10-15_18-36-44
  done: false
  episode_len_mean: 792.6546873065015
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.91313131313075
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 80750
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.269247578228588e-51
        cur_lr: 5.0e-05
        entropy: 0.05445646091053883
        entropy_coeff: 0.0005000000000000001
        kl: 0.00454838698108991
        model: {}
        policy_loss: -0.007802866476898392
        total_loss: 0.19908771912256876
        vf_explained_var: 0.9995015263557434
        vf_loss: 0.20691781242688498
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53548387096775
    gpu_util_percent0: 0.32935483870967747
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629608622096557
    mean_env_wait_ms: 1.2165559040361065
    mean_inference_ms: 4.280257977895984
    mean_raw_obs_processing_ms: 0.37779391685239694
  time_since_restore: 10176.349715471268
  time_this_iter_s: 25.871056079864502
  time_total_s: 10176.349715471268
  timers:
    learn_throughput: 8675.543
    learn_time_ms: 18649.206
    sample_throughput: 23963.528
    sample_time_ms: 6751.593
    update_time_ms: 42.23
  timestamp: 1602787004
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: f0f97_00000
  
2020-10-15 18:36:46,412	WARNING util.py:136 -- The `process_trial` operation took 1.1631407737731934 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    396 |          10176.3 | 64069632 |  299.913 |              321.404 |              126.556 |            792.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3144.506408909435
    time_step_min: 3003
  date: 2020-10-15_18-37-12
  done: false
  episode_len_mean: 792.6656907435546
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.95615720635124
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 80949
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.134623789114294e-51
        cur_lr: 5.0e-05
        entropy: 0.05104814376682043
        entropy_coeff: 0.0005000000000000001
        kl: 0.00524917570874095
        model: {}
        policy_loss: -0.006167035306134494
        total_loss: 0.13195153263707957
        vf_explained_var: 0.9996556639671326
        vf_loss: 0.13814409263432026
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4875
    gpu_util_percent0: 0.31843750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629509313953715
    mean_env_wait_ms: 1.2165151286323646
    mean_inference_ms: 4.280214018791531
    mean_raw_obs_processing_ms: 0.37779012290090613
  time_since_restore: 10202.25286746025
  time_this_iter_s: 25.903151988983154
  time_total_s: 10202.25286746025
  timers:
    learn_throughput: 8673.495
    learn_time_ms: 18653.61
    sample_throughput: 23913.398
    sample_time_ms: 6765.747
    update_time_ms: 43.839
  timestamp: 1602787032
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: f0f97_00000
  
2020-10-15 18:37:14,012	WARNING util.py:136 -- The `process_trial` operation took 1.1326284408569336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    397 |          10202.3 | 64231424 |  299.956 |              321.404 |              126.556 |            792.666 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3144.2291371296833
    time_step_min: 3003
  date: 2020-10-15_18-37-40
  done: false
  episode_len_mean: 792.6753163543168
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 299.9993454673891
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 81159
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.134623789114294e-51
        cur_lr: 5.0e-05
        entropy: 0.05236568736533324
        entropy_coeff: 0.0005000000000000001
        kl: 0.00476196159919103
        model: {}
        policy_loss: -0.009772694951000934
        total_loss: 0.09876949650545915
        vf_explained_var: 0.9997522234916687
        vf_loss: 0.10856837406754494
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180645161290318
    gpu_util_percent0: 0.31838709677419347
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629412902469213
    mean_env_wait_ms: 1.2164707898409952
    mean_inference_ms: 4.280167933783384
    mean_raw_obs_processing_ms: 0.3777870869186831
  time_since_restore: 10228.3568546772
  time_this_iter_s: 26.103987216949463
  time_total_s: 10228.3568546772
  timers:
    learn_throughput: 8657.421
    learn_time_ms: 18688.245
    sample_throughput: 23909.167
    sample_time_ms: 6766.944
    update_time_ms: 44.112
  timestamp: 1602787060
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: f0f97_00000
  
2020-10-15 18:37:41,788	WARNING util.py:136 -- The `process_trial` operation took 1.1280593872070312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    398 |          10228.4 | 64393216 |  299.999 |              321.404 |              126.556 |            792.675 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3143.9703502385755
    time_step_min: 3003
  date: 2020-10-15_18-38-07
  done: false
  episode_len_mean: 792.6838942012241
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.0387764471483
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 203
  episodes_total: 81362
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.567311894557147e-51
        cur_lr: 5.0e-05
        entropy: 0.05297144129872322
        entropy_coeff: 0.0005000000000000001
        kl: 0.005107741958151261
        model: {}
        policy_loss: -0.008630693882878404
        total_loss: 0.29800666868686676
        vf_explained_var: 0.9992664456367493
        vf_loss: 0.3066638360420863
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.977419354838716
    gpu_util_percent0: 0.3303225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462932290941185
    mean_env_wait_ms: 1.2164311713658698
    mean_inference_ms: 4.280123660566163
    mean_raw_obs_processing_ms: 0.37778419191865903
  time_since_restore: 10254.261958360672
  time_this_iter_s: 25.90510368347168
  time_total_s: 10254.261958360672
  timers:
    learn_throughput: 8648.989
    learn_time_ms: 18706.463
    sample_throughput: 23901.195
    sample_time_ms: 6769.201
    update_time_ms: 43.014
  timestamp: 1602787087
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: f0f97_00000
  
2020-10-15 18:38:09,346	WARNING util.py:136 -- The `process_trial` operation took 1.1698451042175293 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    399 |          10254.3 | 64555008 |  300.039 |              321.404 |              126.556 |            792.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3143.7090576731975
    time_step_min: 3003
  date: 2020-10-15_18-38-35
  done: false
  episode_len_mean: 792.6913692264305
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.07909607248666
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 81557
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.567311894557147e-51
        cur_lr: 5.0e-05
        entropy: 0.049137952737510204
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0064324817309776945
        total_loss: .nan
        vf_explained_var: 0.9994590878486633
        vf_loss: 0.23465092107653618
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00967741935484
    gpu_util_percent0: 0.334516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629252664973744
    mean_env_wait_ms: 1.2163914690024575
    mean_inference_ms: 4.2800805357941565
    mean_raw_obs_processing_ms: 0.3777811500273625
  time_since_restore: 10280.113340377808
  time_this_iter_s: 25.85138201713562
  time_total_s: 10280.113340377808
  timers:
    learn_throughput: 8640.63
    learn_time_ms: 18724.56
    sample_throughput: 23925.439
    sample_time_ms: 6762.342
    update_time_ms: 43.132
  timestamp: 1602787115
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:38:36,875	WARNING util.py:136 -- The `process_trial` operation took 1.1659846305847168 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    400 |          10280.1 | 64716800 |  300.079 |              321.404 |              126.556 |            792.691 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3143.457996304409
    time_step_min: 3003
  date: 2020-10-15_18-39-02
  done: false
  episode_len_mean: 792.6967895798936
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.11932015069
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 81765
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3509678418357207e-51
        cur_lr: 5.0e-05
        entropy: 0.05209474886457125
        entropy_coeff: 0.0005000000000000001
        kl: 0.003714629953416685
        model: {}
        policy_loss: -0.008870139533731466
        total_loss: 0.23970882718761763
        vf_explained_var: 0.9993976950645447
        vf_loss: 0.2486050066848596
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.353125
    gpu_util_percent0: 0.281875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629146240827115
    mean_env_wait_ms: 1.2163482625239903
    mean_inference_ms: 4.280033744259803
    mean_raw_obs_processing_ms: 0.377777601440623
  time_since_restore: 10305.875064134598
  time_this_iter_s: 25.76172375679016
  time_total_s: 10305.875064134598
  timers:
    learn_throughput: 8667.585
    learn_time_ms: 18666.33
    sample_throughput: 23921.981
    sample_time_ms: 6763.32
    update_time_ms: 34.113
  timestamp: 1602787142
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: f0f97_00000
  
2020-10-15 18:39:04,326	WARNING util.py:136 -- The `process_trial` operation took 1.1322424411773682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    401 |          10305.9 | 64878592 |  300.119 |              321.404 |              126.556 |            792.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3143.227961819203
    time_step_min: 3003
  date: 2020-10-15_18-39-29
  done: false
  episode_len_mean: 792.7004464939248
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.15409400697007
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 81972
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1754839209178604e-51
        cur_lr: 5.0e-05
        entropy: 0.05695107920716206
        entropy_coeff: 0.0005000000000000001
        kl: 0.004210324725136161
        model: {}
        policy_loss: -0.009299258342556035
        total_loss: 0.5387405802806219
        vf_explained_var: 0.9987483024597168
        vf_loss: 0.5480683197577795
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354845
    gpu_util_percent0: 0.2803225806451612
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629049457332485
    mean_env_wait_ms: 1.216306612345804
    mean_inference_ms: 4.279989789217158
    mean_raw_obs_processing_ms: 0.3777748724051174
  time_since_restore: 10331.544804096222
  time_this_iter_s: 25.669739961624146
  time_total_s: 10331.544804096222
  timers:
    learn_throughput: 8678.102
    learn_time_ms: 18643.709
    sample_throughput: 23920.265
    sample_time_ms: 6763.805
    update_time_ms: 32.009
  timestamp: 1602787169
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: f0f97_00000
  
2020-10-15 18:39:31,768	WARNING util.py:136 -- The `process_trial` operation took 1.1950271129608154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    402 |          10331.5 | 65040384 |  300.154 |              321.404 |              126.556 |              792.7 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3143.0150980153417
    time_step_min: 3003
  date: 2020-10-15_18-39-57
  done: false
  episode_len_mean: 792.7076275311526
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.1905806015759
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 82176
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.877419604589302e-52
        cur_lr: 5.0e-05
        entropy: 0.05483014260729154
        entropy_coeff: 0.0005000000000000001
        kl: 0.003967152248757581
        model: {}
        policy_loss: -0.009799453910545708
        total_loss: 0.41765280067920685
        vf_explained_var: 0.9989669322967529
        vf_loss: 0.4274796744187673
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.932258064516127
    gpu_util_percent0: 0.33677419354838706
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628968305288312
    mean_env_wait_ms: 1.216266744713422
    mean_inference_ms: 4.279945049411652
    mean_raw_obs_processing_ms: 0.37777218924532296
  time_since_restore: 10357.336063623428
  time_this_iter_s: 25.79125952720642
  time_total_s: 10357.336063623428
  timers:
    learn_throughput: 8685.447
    learn_time_ms: 18627.941
    sample_throughput: 23949.412
    sample_time_ms: 6755.573
    update_time_ms: 29.272
  timestamp: 1602787197
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: f0f97_00000
  
2020-10-15 18:39:59,009	WARNING util.py:136 -- The `process_trial` operation took 1.0658936500549316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    403 |          10357.3 | 65202176 |  300.191 |              321.404 |              126.556 |            792.708 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3142.769766170665
    time_step_min: 3003
  date: 2020-10-15_18-40-25
  done: false
  episode_len_mean: 792.7147903995339
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.22801419887713
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 82371
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.938709802294651e-52
        cur_lr: 5.0e-05
        entropy: 0.056200288236141205
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007121229282347485
        total_loss: .nan
        vf_explained_var: 0.9993414282798767
        vf_loss: 0.28964517389734584
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.258064516129036
    gpu_util_percent0: 0.32064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628874803520492
    mean_env_wait_ms: 1.216226975273656
    mean_inference_ms: 4.279904509325646
    mean_raw_obs_processing_ms: 0.3777684421279519
  time_since_restore: 10383.66987323761
  time_this_iter_s: 26.33380961418152
  time_total_s: 10383.66987323761
  timers:
    learn_throughput: 8666.894
    learn_time_ms: 18667.818
    sample_throughput: 23903.254
    sample_time_ms: 6768.618
    update_time_ms: 28.84
  timestamp: 1602787225
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:40:26,871	WARNING util.py:136 -- The `process_trial` operation took 1.1306078433990479 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    404 |          10383.7 | 65363968 |  300.228 |              321.404 |              126.556 |            792.715 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3142.515423572744
    time_step_min: 3003
  date: 2020-10-15_18-40-53
  done: false
  episode_len_mean: 792.7230873555981
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.26608239279227
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 82582
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.408064703441975e-52
        cur_lr: 5.0e-05
        entropy: 0.057537942193448544
        entropy_coeff: 0.0005000000000000001
        kl: 0.004646885907277465
        model: {}
        policy_loss: -0.009486758940814374
        total_loss: 0.42136458059151966
        vf_explained_var: 0.9990007281303406
        vf_loss: 0.4308801045020421
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.977419354838712
    gpu_util_percent0: 0.3274193548387098
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628780656294305
    mean_env_wait_ms: 1.2161830421591602
    mean_inference_ms: 4.279860234899813
    mean_raw_obs_processing_ms: 0.37776549084496175
  time_since_restore: 10409.839797496796
  time_this_iter_s: 26.16992425918579
  time_total_s: 10409.839797496796
  timers:
    learn_throughput: 8662.379
    learn_time_ms: 18677.548
    sample_throughput: 23813.388
    sample_time_ms: 6794.161
    update_time_ms: 29.425
  timestamp: 1602787253
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: f0f97_00000
  
2020-10-15 18:40:54,632	WARNING util.py:136 -- The `process_trial` operation took 1.103959560394287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    405 |          10409.8 | 65525760 |  300.266 |              321.404 |              126.556 |            792.723 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3142.266213831613
    time_step_min: 3003
  date: 2020-10-15_18-41-20
  done: false
  episode_len_mean: 792.7325328565906
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.3034837161716
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 202
  episodes_total: 82784
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2040323517209874e-52
        cur_lr: 5.0e-05
        entropy: 0.057408420058588185
        entropy_coeff: 0.0005000000000000001
        kl: 0.004947985367228587
        model: {}
        policy_loss: -0.007110711274435744
        total_loss: 0.6092614481846491
        vf_explained_var: 0.9985727667808533
        vf_loss: 0.6164008726676306
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03548387096775
    gpu_util_percent0: 0.2787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628693201944146
    mean_env_wait_ms: 1.216144192958251
    mean_inference_ms: 4.279817897032032
    mean_raw_obs_processing_ms: 0.3777628303749813
  time_since_restore: 10435.720121860504
  time_this_iter_s: 25.880324363708496
  time_total_s: 10435.720121860504
  timers:
    learn_throughput: 8669.177
    learn_time_ms: 18662.903
    sample_throughput: 23753.772
    sample_time_ms: 6811.213
    update_time_ms: 29.868
  timestamp: 1602787280
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: f0f97_00000
  
2020-10-15 18:41:22,064	WARNING util.py:136 -- The `process_trial` operation took 1.0773937702178955 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    406 |          10435.7 | 65687552 |  300.303 |              321.404 |              126.556 |            792.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3142.020473865075
    time_step_min: 3003
  date: 2020-10-15_18-41-47
  done: false
  episode_len_mean: 792.7415793976935
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.3401495462203
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 82981
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1020161758604937e-52
        cur_lr: 5.0e-05
        entropy: 0.05334185188015302
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038618206550988057
        model: {}
        policy_loss: -0.006150262095616199
        total_loss: 0.27925581485033035
        vf_explained_var: 0.9993195533752441
        vf_loss: 0.2854327435294787
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.845161290322583
    gpu_util_percent0: 0.31451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628626946969311
    mean_env_wait_ms: 1.2161046591678015
    mean_inference_ms: 4.279776607318958
    mean_raw_obs_processing_ms: 0.3777597363556646
  time_since_restore: 10461.610885858536
  time_this_iter_s: 25.890763998031616
  time_total_s: 10461.610885858536
  timers:
    learn_throughput: 8670.6
    learn_time_ms: 18659.839
    sample_throughput: 23749.831
    sample_time_ms: 6812.343
    update_time_ms: 28.246
  timestamp: 1602787307
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: f0f97_00000
  
2020-10-15 18:41:49,624	WARNING util.py:136 -- The `process_trial` operation took 1.1876165866851807 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    407 |          10461.6 | 65849344 |   300.34 |              321.404 |              126.556 |            792.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.755920380059
    time_step_min: 3003
  date: 2020-10-15_18-42-15
  done: false
  episode_len_mean: 792.7518241155894
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.379719955623
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 83191
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.510080879302468e-53
        cur_lr: 5.0e-05
        entropy: 0.062312846072018147
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008985950514518967
        total_loss: .nan
        vf_explained_var: 0.9993221759796143
        vf_loss: 0.2807250457505385
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32580645161291
    gpu_util_percent0: 0.35161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628521805464478
    mean_env_wait_ms: 1.2160617119442358
    mean_inference_ms: 4.279730840476517
    mean_raw_obs_processing_ms: 0.37775632188908115
  time_since_restore: 10487.451220989227
  time_this_iter_s: 25.84033513069153
  time_total_s: 10487.451220989227
  timers:
    learn_throughput: 8692.389
    learn_time_ms: 18613.064
    sample_throughput: 23672.605
    sample_time_ms: 6834.567
    update_time_ms: 25.725
  timestamp: 1602787335
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:42:17,004	WARNING util.py:136 -- The `process_trial` operation took 1.1000502109527588 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    408 |          10487.5 | 66011136 |   300.38 |              321.404 |              126.556 |            792.752 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.6230548656854
    time_step_min: 3003
  date: 2020-10-15_18-42-43
  done: false
  episode_len_mean: 792.7533305354038
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.38794867555526
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 83395
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.265121318953704e-53
        cur_lr: 5.0e-05
        entropy: 0.10399781229595344
        entropy_coeff: 0.0005000000000000001
        kl: 0.00595229200553149
        model: {}
        policy_loss: -0.013848885311745107
        total_loss: 2.0347795685132346
        vf_explained_var: 0.9953298568725586
        vf_loss: 2.0486804445584617
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956249999999997
    gpu_util_percent0: 0.2921875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628439034755153
    mean_env_wait_ms: 1.216021147708413
    mean_inference_ms: 4.2796903464125755
    mean_raw_obs_processing_ms: 0.37775368457209524
  time_since_restore: 10513.491572141647
  time_this_iter_s: 26.040351152420044
  time_total_s: 10513.491572141647
  timers:
    learn_throughput: 8699.487
    learn_time_ms: 18597.878
    sample_throughput: 23590.272
    sample_time_ms: 6858.42
    update_time_ms: 24.513
  timestamp: 1602787363
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: f0f97_00000
  
2020-10-15 18:42:44,726	WARNING util.py:136 -- The `process_trial` operation took 1.1719067096710205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    409 |          10513.5 | 66172928 |  300.388 |              321.404 |              126.556 |            792.753 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.768056968464
    time_step_min: 3003
  date: 2020-10-15_18-43-10
  done: false
  episode_len_mean: 792.7506010693652
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.36327244164414
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 83601
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.265121318953704e-53
        cur_lr: 5.0e-05
        entropy: 0.11878848572572072
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014496540611920258
        total_loss: .nan
        vf_explained_var: 0.9929294586181641
        vf_loss: 3.310334841410319
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.516129032258064
    gpu_util_percent0: 0.32000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628355460341205
    mean_env_wait_ms: 1.2159812781436037
    mean_inference_ms: 4.279648224674516
    mean_raw_obs_processing_ms: 0.3777509178964003
  time_since_restore: 10539.495251655579
  time_this_iter_s: 26.003679513931274
  time_total_s: 10539.495251655579
  timers:
    learn_throughput: 8702.702
    learn_time_ms: 18591.007
    sample_throughput: 23509.763
    sample_time_ms: 6881.907
    update_time_ms: 22.625
  timestamp: 1602787390
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:43:12,441	WARNING util.py:136 -- The `process_trial` operation took 1.1916847229003906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    410 |          10539.5 | 66334720 |  300.363 |              321.404 |              126.556 |            792.751 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.8056905424282
    time_step_min: 3003
  date: 2020-10-15_18-43-38
  done: false
  episode_len_mean: 792.749543550639
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.3642182032253
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 83799
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2397681978430555e-52
        cur_lr: 5.0e-05
        entropy: 0.08807703231771787
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012527203187346458
        total_loss: .nan
        vf_explained_var: 0.9957948327064514
        vf_loss: 1.9041349490483601
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.981250000000003
    gpu_util_percent0: 0.2875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628263865350324
    mean_env_wait_ms: 1.215941717757306
    mean_inference_ms: 4.279607186459868
    mean_raw_obs_processing_ms: 0.3777476123549904
  time_since_restore: 10565.352796554565
  time_this_iter_s: 25.857544898986816
  time_total_s: 10565.352796554565
  timers:
    learn_throughput: 8698.369
    learn_time_ms: 18600.268
    sample_throughput: 23514.734
    sample_time_ms: 6880.452
    update_time_ms: 22.834
  timestamp: 1602787418
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:43:40,048	WARNING util.py:136 -- The `process_trial` operation took 1.1861984729766846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    411 |          10565.4 | 66496512 |  300.364 |              321.404 |              126.556 |             792.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.615525701491
    time_step_min: 3003
  date: 2020-10-15_18-44-06
  done: false
  episode_len_mean: 792.7555291036781
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.39722483735045
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 84010
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8596522967645833e-52
        cur_lr: 5.0e-05
        entropy: 0.058042130743463836
        entropy_coeff: 0.0005000000000000001
        kl: 0.00538248293257008
        model: {}
        policy_loss: -0.007374917971901596
        total_loss: 0.3593207448720932
        vf_explained_var: 0.9991438984870911
        vf_loss: 0.36672468731800717
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.359375
    gpu_util_percent0: 0.29062499999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628163240879877
    mean_env_wait_ms: 1.21589936471597
    mean_inference_ms: 4.279564476321075
    mean_raw_obs_processing_ms: 0.37774469215880235
  time_since_restore: 10591.674415826797
  time_this_iter_s: 26.321619272232056
  time_total_s: 10591.674415826797
  timers:
    learn_throughput: 8681.012
    learn_time_ms: 18637.459
    sample_throughput: 23473.248
    sample_time_ms: 6892.612
    update_time_ms: 24.205
  timestamp: 1602787446
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: f0f97_00000
  
2020-10-15 18:44:07,858	WARNING util.py:136 -- The `process_trial` operation took 1.0897910594940186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    412 |          10591.7 | 66658304 |  300.397 |              321.404 |              126.556 |            792.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.3729548614
    time_step_min: 3003
  date: 2020-10-15_18-44-33
  done: false
  episode_len_mean: 792.7636475911125
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.4331677880344
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 84209
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8596522967645833e-52
        cur_lr: 5.0e-05
        entropy: 0.04888278618454933
        entropy_coeff: 0.0005000000000000001
        kl: 0.004130092352473487
        model: {}
        policy_loss: -0.00785520647574837
        total_loss: 0.28593894218405086
        vf_explained_var: 0.9993786811828613
        vf_loss: 0.2938185880581538
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5741935483871
    gpu_util_percent0: 0.3038709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462808447655337
    mean_env_wait_ms: 1.2158616315056585
    mean_inference_ms: 4.2795265805103675
    mean_raw_obs_processing_ms: 0.37774208895502304
  time_since_restore: 10617.689207077026
  time_this_iter_s: 26.014791250228882
  time_total_s: 10617.689207077026
  timers:
    learn_throughput: 8675.925
    learn_time_ms: 18648.387
    sample_throughput: 23410.72
    sample_time_ms: 6911.022
    update_time_ms: 24.545
  timestamp: 1602787473
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: f0f97_00000
  
2020-10-15 18:44:35,473	WARNING util.py:136 -- The `process_trial` operation took 1.109299898147583 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    413 |          10617.7 | 66820096 |  300.433 |              321.404 |              126.556 |            792.764 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3141.1130234818584
    time_step_min: 3003
  date: 2020-10-15_18-45-01
  done: false
  episode_len_mean: 792.7719911383857
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.47178127757155
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 84409
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.298261483822916e-53
        cur_lr: 5.0e-05
        entropy: 0.04753857168058554
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007296255779995893
        total_loss: .nan
        vf_explained_var: 0.9993754029273987
        vf_loss: 0.2513362740476926
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.315625
    gpu_util_percent0: 0.28625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628012859559755
    mean_env_wait_ms: 1.2158225513596341
    mean_inference_ms: 4.279485785047012
    mean_raw_obs_processing_ms: 0.37773888671625094
  time_since_restore: 10643.8546936512
  time_this_iter_s: 26.165486574172974
  time_total_s: 10643.8546936512
  timers:
    learn_throughput: 8688.432
    learn_time_ms: 18621.542
    sample_throughput: 23379.305
    sample_time_ms: 6920.308
    update_time_ms: 24.564
  timestamp: 1602787501
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:45:03,207	WARNING util.py:136 -- The `process_trial` operation took 1.1721315383911133 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    414 |          10643.9 | 66981888 |  300.472 |              321.404 |              126.556 |            792.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3140.848076718419
    time_step_min: 3003
  date: 2020-10-15_18-45-29
  done: false
  episode_len_mean: 792.781220823731
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.5116687169509
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 84615
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3947392225734377e-52
        cur_lr: 5.0e-05
        entropy: 0.04703567580630382
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036328027490526438
        model: {}
        policy_loss: -0.009545068469985077
        total_loss: 0.20453819756706557
        vf_explained_var: 0.9994995594024658
        vf_loss: 0.21410678327083588
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01875
    gpu_util_percent0: 0.294375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627920289501925
    mean_env_wait_ms: 1.2157812092216977
    mean_inference_ms: 4.279443643433262
    mean_raw_obs_processing_ms: 0.3777357838176914
  time_since_restore: 10670.308584928513
  time_this_iter_s: 26.453891277313232
  time_total_s: 10670.308584928513
  timers:
    learn_throughput: 8684.067
    learn_time_ms: 18630.902
    sample_throughput: 23350.186
    sample_time_ms: 6928.938
    update_time_ms: 25.661
  timestamp: 1602787529
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: f0f97_00000
  
2020-10-15 18:45:31,240	WARNING util.py:136 -- The `process_trial` operation took 1.180600881576538 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    415 |          10670.3 | 67143680 |  300.512 |              321.404 |              126.556 |            792.781 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3140.5932549278664
    time_step_min: 3003
  date: 2020-10-15_18-45-57
  done: false
  episode_len_mean: 792.7881370919251
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.54965064645626
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 84819
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.973696112867188e-53
        cur_lr: 5.0e-05
        entropy: 0.048286571788291134
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034349335667987666
        model: {}
        policy_loss: -0.008739796020866683
        total_loss: 0.29919125884771347
        vf_explained_var: 0.9992663264274597
        vf_loss: 0.30795519798994064
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196875
    gpu_util_percent0: 0.2840625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627836963094795
    mean_env_wait_ms: 1.2157414316388797
    mean_inference_ms: 4.279404658418196
    mean_raw_obs_processing_ms: 0.3777332030880685
  time_since_restore: 10696.511466503143
  time_this_iter_s: 26.202881574630737
  time_total_s: 10696.511466503143
  timers:
    learn_throughput: 8670.787
    learn_time_ms: 18659.436
    sample_throughput: 23353.798
    sample_time_ms: 6927.867
    update_time_ms: 27.164
  timestamp: 1602787557
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: f0f97_00000
  
2020-10-15 18:45:59,174	WARNING util.py:136 -- The `process_trial` operation took 1.1590263843536377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    416 |          10696.5 | 67305472 |   300.55 |              321.404 |              126.556 |            792.788 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3140.3400567212307
    time_step_min: 3003
  date: 2020-10-15_18-46-25
  done: false
  episode_len_mean: 792.7951730708161
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.5869602485453
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 85023
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.486848056433594e-53
        cur_lr: 5.0e-05
        entropy: 0.047541823548575245
        entropy_coeff: 0.0005000000000000001
        kl: 0.003343467270800223
        model: {}
        policy_loss: -0.009276717669005544
        total_loss: 0.2856380318601926
        vf_explained_var: 0.9993031024932861
        vf_loss: 0.29493851959705353
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309677419354845
    gpu_util_percent0: 0.3258064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627760795206646
    mean_env_wait_ms: 1.215702632200831
    mean_inference_ms: 4.2793651947384
    mean_raw_obs_processing_ms: 0.3777307035770926
  time_since_restore: 10722.683087825775
  time_this_iter_s: 26.171621322631836
  time_total_s: 10722.683087825775
  timers:
    learn_throughput: 8664.164
    learn_time_ms: 18673.699
    sample_throughput: 23322.832
    sample_time_ms: 6937.065
    update_time_ms: 29.737
  timestamp: 1602787585
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: f0f97_00000
  
2020-10-15 18:46:27,033	WARNING util.py:136 -- The `process_trial` operation took 1.1446480751037598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    417 |          10722.7 | 67467264 |  300.587 |              321.404 |              126.556 |            792.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3140.1145406516
    time_step_min: 3003
  date: 2020-10-15_18-46-53
  done: false
  episode_len_mean: 792.8018680841576
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.62347450994565
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 85221
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.743424028216797e-53
        cur_lr: 5.0e-05
        entropy: 0.04465730985005697
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007976765962666832
        total_loss: .nan
        vf_explained_var: 0.9995720386505127
        vf_loss: 0.17815477028489113
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.828125
    gpu_util_percent0: 0.266875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627665996639594
    mean_env_wait_ms: 1.215663549690099
    mean_inference_ms: 4.279326444993185
    mean_raw_obs_processing_ms: 0.3777272894369902
  time_since_restore: 10748.844841957092
  time_this_iter_s: 26.16175413131714
  time_total_s: 10748.844841957092
  timers:
    learn_throughput: 8653.953
    learn_time_ms: 18695.734
    sample_throughput: 23330.347
    sample_time_ms: 6934.83
    update_time_ms: 31.503
  timestamp: 1602787613
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: f0f97_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 18:46:54,713	WARNING util.py:136 -- The `process_trial` operation took 1.1125373840332031 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    418 |          10748.8 | 67629056 |  300.623 |              321.404 |              126.556 |            792.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3139.8385506329855
    time_step_min: 3003
  date: 2020-10-15_18-47-20
  done: false
  episode_len_mean: 792.8094457774916
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.6653750000733
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 85435
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6151360423251957e-53
        cur_lr: 5.0e-05
        entropy: 0.048006682035823665
        entropy_coeff: 0.0005000000000000001
        kl: 0.005567550930815439
        model: {}
        policy_loss: -0.007219504002326478
        total_loss: 0.2724554439385732
        vf_explained_var: 0.9993523955345154
        vf_loss: 0.2796989555160205
    num_steps_sampled: 67790848
    num_steps_trained: 67790848
  iterations_since_restore: 419
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73225806451613
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627577014957327
    mean_env_wait_ms: 1.2156213610374536
    mean_inference_ms: 4.279284184621438
    mean_raw_obs_processing_ms: 0.3777246712619981
  time_since_restore: 10774.801379442215
  time_this_iter_s: 25.95653748512268
  time_total_s: 10774.801379442215
  timers:
    learn_throughput: 8649.329
    learn_time_ms: 18705.728
    sample_throughput: 23382.67
    sample_time_ms: 6919.313
    update_time_ms: 33.474
  timestamp: 1602787640
  timesteps_since_restore: 0
  timesteps_total: 67790848
  training_iteration: 419
  trial_id: f0f97_00000
  
2020-10-15 18:47:22,380	WARNING util.py:136 -- The `process_trial` operation took 1.2134506702423096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | RUNNING  | 172.17.0.4:17140 |    419 |          10774.8 | 67790848 |  300.665 |              321.404 |              126.556 |            792.809 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f0f97_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3139.609020272244
    time_step_min: 3003
  date: 2020-10-15_18-47-48
  done: true
  episode_len_mean: 792.8146815989536
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 300.6988334607883
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 196
  episodes_total: 85631
  experiment_id: 08906739d7694b56b11d00a51d46c02f
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6151360423251957e-53
        cur_lr: 5.0e-05
        entropy: 0.05669149166593949
        entropy_coeff: 0.0005000000000000001
        kl: 0.004951652178230385
        model: {}
        policy_loss: -0.009259281663010674
        total_loss: 0.4931090647975604
        vf_explained_var: 0.9987780451774597
        vf_loss: 0.5023966804146767
    num_steps_sampled: 67952640
    num_steps_trained: 67952640
  iterations_since_restore: 420
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28125
    gpu_util_percent0: 0.2959375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 17140
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627501133393822
    mean_env_wait_ms: 1.2155846768357599
    mean_inference_ms: 4.279249360220029
    mean_raw_obs_processing_ms: 0.3777223255323367
  time_since_restore: 10800.932337284088
  time_this_iter_s: 26.13095784187317
  time_total_s: 10800.932337284088
  timers:
    learn_throughput: 8647.976
    learn_time_ms: 18708.655
    sample_throughput: 23363.205
    sample_time_ms: 6925.077
    update_time_ms: 36.575
  timestamp: 1602787668
  timesteps_since_restore: 0
  timesteps_total: 67952640
  training_iteration: 420
  trial_id: f0f97_00000
  
2020-10-15 18:47:50,363	WARNING util.py:136 -- The `process_trial` operation took 1.387068271636963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | TERMINATED |       |    420 |          10800.9 | 67952640 |  300.699 |              321.404 |              126.556 |            792.815 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f0f97_00000 | TERMINATED |       |    420 |          10800.9 | 67952640 |  300.699 |              321.404 |              126.556 |            792.815 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


