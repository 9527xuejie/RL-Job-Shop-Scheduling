2020-10-11 03:47:23,760	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
== Status ==
Memory usage on this node: 32.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_79b7c_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=3236)[0m 2020-10-11 03:47:26,692	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=3267)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3267)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3292)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3292)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3266)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3266)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3273)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3273)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3264)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3264)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3278)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3278)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3259)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3259)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3228)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3228)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3231)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3231)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3242)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3242)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3241)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3241)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3172)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3172)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3280)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3280)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3232)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3232)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3239)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3239)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3195)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3195)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3235)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3235)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3299)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3299)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3287)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3287)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3237)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3237)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3218)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3218)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3247)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3247)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3160)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3160)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3258)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3258)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3251)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3251)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3225)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3225)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3245)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3245)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3253)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3253)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3249)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3249)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3276)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3276)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3215)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3215)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3234)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3234)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3279)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3279)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3183)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3183)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3262)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3262)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3167)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3167)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3186)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3186)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3252)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3252)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3197)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3197)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3164)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3164)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3163)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3163)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3169)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3169)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3158)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3158)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3213)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3213)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3176)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3176)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3184)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3184)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3161)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3161)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3174)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3174)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3166)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3166)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3156)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3156)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3233)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3233)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3170)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3170)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3270)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3270)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3192)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3192)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3181)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3181)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3193)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3193)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3178)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3178)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3199)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3199)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3157)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3157)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3189)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3189)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3188)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3188)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3255)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3255)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3159)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3159)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3219)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3219)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3290)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3290)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3243)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3243)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3223)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3223)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3165)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3165)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3240)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3240)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3221)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3221)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3230)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3230)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3238)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3238)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3283)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3283)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3226)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3226)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3217)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3217)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3256)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3256)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3268)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3268)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3171)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3171)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=3173)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3173)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_03-48-09
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.176694427217756
        entropy_coeff: 0.00010000000000000002
        kl: 0.012481516027556998
        model: {}
        policy_loss: -0.014165030118809747
        total_loss: 9.352353164127894
        vf_explained_var: 0.7688503861427307
        vf_loss: 9.364139488765172
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46363636363636
    gpu_util_percent0: 0.31409090909090914
    gpu_util_percent1: 0.00022727272727272727
    gpu_util_percent2: 0.00022727272727272727
    ram_util_percent: 6.299999999999998
    vram_util_percent0: 0.19370359879543217
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17370945658717635
    mean_env_wait_ms: 1.2064911959236808
    mean_inference_ms: 5.575072632118594
    mean_raw_obs_processing_ms: 0.46125821251676347
  time_since_restore: 37.131871700286865
  time_this_iter_s: 37.131871700286865
  time_total_s: 37.131871700286865
  timers:
    learn_throughput: 5758.127
    learn_time_ms: 28098.025
    sample_throughput: 18194.892
    sample_time_ms: 8892.166
    update_time_ms: 105.527
  timestamp: 1602388089
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      1 |          37.1319 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3623.3958333333335
    time_step_min: 3341
  date: 2020-10-11_03-48-45
  done: false
  episode_len_mean: 882.8259493670886
  episode_reward_max: 263.59595959595873
  episode_reward_mean: 215.91999104973772
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.1397526945386613
        entropy_coeff: 0.00010000000000000002
        kl: 0.013301880364971501
        model: {}
        policy_loss: -0.01733198407704809
        total_loss: 8.738042558942523
        vf_explained_var: 0.8962677121162415
        vf_loss: 8.752828189304896
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.604761904761904
    gpu_util_percent0: 0.27404761904761904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.476190476190476
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16852749947014117
    mean_env_wait_ms: 1.2027934955725683
    mean_inference_ms: 5.385285227846363
    mean_raw_obs_processing_ms: 0.4501009274374952
  time_since_restore: 72.72512865066528
  time_this_iter_s: 35.59325695037842
  time_total_s: 72.72512865066528
  timers:
    learn_throughput: 5799.985
    learn_time_ms: 27895.244
    sample_throughput: 19451.463
    sample_time_ms: 8317.729
    update_time_ms: 104.45
  timestamp: 1602388125
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      2 |          72.7251 | 323584 |   215.92 |              263.596 |              71.9293 |            882.826 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3608.2892376681616
    time_step_min: 3273
  date: 2020-10-11_03-49-20
  done: false
  episode_len_mean: 872.7383966244726
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 219.78858202275904
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.1184341737202235
        entropy_coeff: 0.00010000000000000002
        kl: 0.013849153649061918
        model: {}
        policy_loss: -0.020254456205293536
        total_loss: 6.3926516601017545
        vf_explained_var: 0.9488757848739624
        vf_loss: 6.410248143332345
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.841463414634145
    gpu_util_percent0: 0.3824390243902439
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.49268292682927
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1654249561862434
    mean_env_wait_ms: 1.2035668539261462
    mean_inference_ms: 5.234661411418543
    mean_raw_obs_processing_ms: 0.4418284083184267
  time_since_restore: 107.91638207435608
  time_this_iter_s: 35.191253423690796
  time_total_s: 107.91638207435608
  timers:
    learn_throughput: 5806.975
    learn_time_ms: 27861.668
    sample_throughput: 20328.522
    sample_time_ms: 7958.867
    update_time_ms: 103.903
  timestamp: 1602388160
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      3 |          107.916 | 485376 |  219.789 |              274.051 |              71.9293 |            872.738 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3601.844370860927
    time_step_min: 3273
  date: 2020-10-11_03-49-55
  done: false
  episode_len_mean: 863.9303797468355
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 219.7200965349698
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.0788760100092207
        entropy_coeff: 0.00010000000000000002
        kl: 0.012313432525843382
        model: {}
        policy_loss: -0.019407800930951322
        total_loss: 6.789063521793911
        vf_explained_var: 0.9661949276924133
        vf_loss: 6.8061166150229315
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.630000000000003
    gpu_util_percent0: 0.362
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.494999999999999
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16321843758359073
    mean_env_wait_ms: 1.2060459279802918
    mean_inference_ms: 5.122172275081175
    mean_raw_obs_processing_ms: 0.4351219475722825
  time_since_restore: 142.70891404151917
  time_this_iter_s: 34.792531967163086
  time_total_s: 142.70891404151917
  timers:
    learn_throughput: 5816.199
    learn_time_ms: 27817.482
    sample_throughput: 20950.127
    sample_time_ms: 7722.722
    update_time_ms: 88.216
  timestamp: 1602388195
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      4 |          142.709 | 647168 |   219.72 |              274.051 |              71.9293 |             863.93 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3601.1823529411763
    time_step_min: 3273
  date: 2020-10-11_03-50-30
  done: false
  episode_len_mean: 854.2380410022779
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 220.21810358712392
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 246
  episodes_total: 878
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.0312368614333016
        entropy_coeff: 0.00010000000000000002
        kl: 0.013365275226533413
        model: {}
        policy_loss: -0.01905492363896753
        total_loss: 8.937845025743757
        vf_explained_var: 0.9795241951942444
        vf_loss: 8.954329899379186
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.895
    gpu_util_percent0: 0.38075000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4825
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16089401443103238
    mean_env_wait_ms: 1.2106849969913862
    mean_inference_ms: 5.003094967013789
    mean_raw_obs_processing_ms: 0.4276901328889427
  time_since_restore: 177.65599274635315
  time_this_iter_s: 34.947078704833984
  time_total_s: 177.65599274635315
  timers:
    learn_throughput: 5821.656
    learn_time_ms: 27791.405
    sample_throughput: 21259.5
    sample_time_ms: 7610.339
    update_time_ms: 78.623
  timestamp: 1602388230
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      5 |          177.656 | 808960 |  220.218 |              274.051 |              71.9293 |            854.238 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3598.9322820037105
    time_step_min: 3273
  date: 2020-10-11_03-51-05
  done: false
  episode_len_mean: 847.9122965641953
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 220.81745118453966
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 228
  episodes_total: 1106
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.0264701672962733
        entropy_coeff: 0.00010000000000000002
        kl: 0.012899184399949653
        model: {}
        policy_loss: -0.020378501753189733
        total_loss: 5.687450170516968
        vf_explained_var: 0.9852557182312012
        vf_loss: 5.705351488930838
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.165000000000003
    gpu_util_percent0: 0.3415
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.489999999999999
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15942432847743243
    mean_env_wait_ms: 1.2139069461398324
    mean_inference_ms: 4.925712241681351
    mean_raw_obs_processing_ms: 0.4231076018262806
  time_since_restore: 212.39793252944946
  time_this_iter_s: 34.74193978309631
  time_total_s: 212.39793252944946
  timers:
    learn_throughput: 5829.353
    learn_time_ms: 27754.709
    sample_throughput: 21509.574
    sample_time_ms: 7521.86
    update_time_ms: 72.275
  timestamp: 1602388265
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      6 |          212.398 | 970752 |  220.817 |              274.051 |              71.9293 |            847.912 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3591.847087378641
    time_step_min: 3273
  date: 2020-10-11_03-51-40
  done: false
  episode_len_mean: 843.5632911392405
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 221.74025859864454
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.9784489784921918
        entropy_coeff: 0.00010000000000000002
        kl: 0.013751964750034469
        model: {}
        policy_loss: -0.018733240198343992
        total_loss: 4.1398260082517355
        vf_explained_var: 0.9893166422843933
        vf_loss: 4.155906575066703
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.6390243902439
    gpu_util_percent0: 0.3965853658536585
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.502439024390244
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15862473842430982
    mean_env_wait_ms: 1.2157987163592778
    mean_inference_ms: 4.8831272706172655
    mean_raw_obs_processing_ms: 0.42048212133894164
  time_since_restore: 247.29025530815125
  time_this_iter_s: 34.89232277870178
  time_total_s: 247.29025530815125
  timers:
    learn_throughput: 5829.755
    learn_time_ms: 27752.796
    sample_throughput: 21691.529
    sample_time_ms: 7458.764
    update_time_ms: 64.892
  timestamp: 1602388300
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      7 |           247.29 | 1132544 |   221.74 |              274.051 |              71.9293 |            843.563 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3588.0014347202296
    time_step_min: 3273
  date: 2020-10-11_03-52-15
  done: false
  episode_len_mean: 840.4880450070324
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 222.49742147210492
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.9355012476444244
        entropy_coeff: 0.00010000000000000002
        kl: 0.012841540854424238
        model: {}
        policy_loss: -0.019386535310851678
        total_loss: 4.082937700407846
        vf_explained_var: 0.9904271364212036
        vf_loss: 4.0998493773596625
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.1390243902439
    gpu_util_percent0: 0.32268292682926825
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.497560975609756
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1579256877898227
    mean_env_wait_ms: 1.2175024188182477
    mean_inference_ms: 4.845952106590638
    mean_raw_obs_processing_ms: 0.4181199685804222
  time_since_restore: 282.3161885738373
  time_this_iter_s: 35.025933265686035
  time_total_s: 282.3161885738373
  timers:
    learn_throughput: 5825.692
    learn_time_ms: 27772.15
    sample_throughput: 21852.317
    sample_time_ms: 7403.883
    update_time_ms: 61.518
  timestamp: 1602388335
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      8 |          282.316 | 1294336 |  222.497 |              274.051 |              71.9293 |            840.488 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3581.479904019196
    time_step_min: 3273
  date: 2020-10-11_03-52-50
  done: false
  episode_len_mean: 836.0731563421829
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 223.44873513900058
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 273
  episodes_total: 1695
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.8865315445831844
        entropy_coeff: 0.00010000000000000002
        kl: 0.011193592101335526
        model: {}
        policy_loss: -0.01807991456839123
        total_loss: 5.319871323449271
        vf_explained_var: 0.9926224946975708
        vf_loss: 5.335801226752145
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.04
    gpu_util_percent0: 0.31975000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4799999999999995
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15696513526709316
    mean_env_wait_ms: 1.2202858167914379
    mean_inference_ms: 4.793382729156925
    mean_raw_obs_processing_ms: 0.41483269040732684
  time_since_restore: 317.2618134021759
  time_this_iter_s: 34.94562482833862
  time_total_s: 317.2618134021759
  timers:
    learn_throughput: 5826.174
    learn_time_ms: 27769.854
    sample_throughput: 21958.998
    sample_time_ms: 7367.914
    update_time_ms: 59.09
  timestamp: 1602388370
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |      9 |          317.262 | 1456128 |  223.449 |              274.051 |              71.9293 |            836.073 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3578.9255888650964
    time_step_min: 3273
  date: 2020-10-11_03-53-25
  done: false
  episode_len_mean: 833.8037974683544
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 223.8084484081319
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 201
  episodes_total: 1896
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.8546519705227443
        entropy_coeff: 0.00010000000000000002
        kl: 0.012178398402673858
        model: {}
        policy_loss: -0.01978121032672269
        total_loss: 3.266998972211565
        vf_explained_var: 0.9939385652542114
        vf_loss: 3.2844299418585643
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.732499999999998
    gpu_util_percent0: 0.44175000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.5025
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15640371480060666
    mean_env_wait_ms: 1.222026769192169
    mean_inference_ms: 4.7618990280586155
    mean_raw_obs_processing_ms: 0.41286621894847825
  time_since_restore: 352.23164463043213
  time_this_iter_s: 34.969831228256226
  time_total_s: 352.23164463043213
  timers:
    learn_throughput: 5823.655
    learn_time_ms: 27781.867
    sample_throughput: 22072.548
    sample_time_ms: 7330.01
    update_time_ms: 56.939
  timestamp: 1602388405
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     10 |          352.232 | 1617920 |  223.808 |              274.051 |              71.9293 |            833.804 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3576.5483711747283
    time_step_min: 3273
  date: 2020-10-11_03-54-00
  done: false
  episode_len_mean: 831.6426484907497
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 224.26759808405373
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.824029803276062
        entropy_coeff: 0.00010000000000000002
        kl: 0.011623075364955835
        model: {}
        policy_loss: -0.01997815810942224
        total_loss: 2.6984662328447615
        vf_explained_var: 0.9948481917381287
        vf_loss: 2.7162021228245328
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.390243902439025
    gpu_util_percent0: 0.38658536585365855
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.497560975609756
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15601524768101657
    mean_env_wait_ms: 1.223302231284329
    mean_inference_ms: 4.740058845727592
    mean_raw_obs_processing_ms: 0.41149551549947166
  time_since_restore: 387.2815799713135
  time_this_iter_s: 35.04993534088135
  time_total_s: 387.2815799713135
  timers:
    learn_throughput: 5831.606
    learn_time_ms: 27743.986
    sample_throughput: 22592.275
    sample_time_ms: 7161.386
    update_time_ms: 50.693
  timestamp: 1602388440
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     11 |          387.282 | 1779712 |  224.268 |              274.051 |              71.9293 |            831.643 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3571.9574175824177
    time_step_min: 3273
  date: 2020-10-11_03-54-35
  done: false
  episode_len_mean: 830.007233273056
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 225.00726980473812
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.7896963102476937
        entropy_coeff: 0.00010000000000000002
        kl: 0.011191743958209242
        model: {}
        policy_loss: -0.01974713908774512
        total_loss: 2.4721424068723405
        vf_explained_var: 0.9954738616943359
        vf_loss: 2.4897301367350986
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.614634146341466
    gpu_util_percent0: 0.2626829268292683
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.5
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15566487754817834
    mean_env_wait_ms: 1.2245758058930682
    mean_inference_ms: 4.720337177805598
    mean_raw_obs_processing_ms: 0.4102309023228298
  time_since_restore: 422.6193723678589
  time_this_iter_s: 35.33779239654541
  time_total_s: 422.6193723678589
  timers:
    learn_throughput: 5827.259
    learn_time_ms: 27764.684
    sample_throughput: 22723.097
    sample_time_ms: 7120.156
    update_time_ms: 45.249
  timestamp: 1602388475
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     12 |          422.619 | 1941504 |  225.007 |              274.051 |              71.9293 |            830.007 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3562.8761061946902
    time_step_min: 3273
  date: 2020-10-11_03-55-11
  done: false
  episode_len_mean: 827.3802704852824
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 226.4989151659796
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 302
  episodes_total: 2514
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.7577954913888659
        entropy_coeff: 0.00010000000000000002
        kl: 0.01014237358633961
        model: {}
        policy_loss: -0.016304380642915412
        total_loss: 3.2584745713642667
        vf_explained_var: 0.9956944584846497
        vf_loss: 3.2728262969425748
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.37073170731707
    gpu_util_percent0: 0.27536585365853655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.487804878048781
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15508501588119877
    mean_env_wait_ms: 1.2269158485605087
    mean_inference_ms: 4.687951753187695
    mean_raw_obs_processing_ms: 0.4081984849242406
  time_since_restore: 457.9715666770935
  time_this_iter_s: 35.35219430923462
  time_total_s: 457.9715666770935
  timers:
    learn_throughput: 5824.944
    learn_time_ms: 27775.717
    sample_throughput: 22689.93
    sample_time_ms: 7130.564
    update_time_ms: 39.487
  timestamp: 1602388511
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     13 |          457.972 | 2103296 |  226.499 |              274.051 |              71.9293 |             827.38 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3558.252445447705
    time_step_min: 3273
  date: 2020-10-11_03-55-46
  done: false
  episode_len_mean: 826.1090841399852
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 227.1116075122032
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 172
  episodes_total: 2686
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.725023478269577
        entropy_coeff: 0.00010000000000000002
        kl: 0.010252352271761214
        model: {}
        policy_loss: -0.01882441621273756
        total_loss: 2.2354869161333357
        vf_explained_var: 0.9959235787391663
        vf_loss: 2.252333334514073
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.990000000000002
    gpu_util_percent0: 0.339
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4975000000000005
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1547987430134466
    mean_env_wait_ms: 1.2280884013390903
    mean_inference_ms: 4.6720229544864695
    mean_raw_obs_processing_ms: 0.4071999360334582
  time_since_restore: 492.95544266700745
  time_this_iter_s: 34.98387598991394
  time_total_s: 492.95544266700745
  timers:
    learn_throughput: 5822.126
    learn_time_ms: 27789.161
    sample_throughput: 22672.483
    sample_time_ms: 7136.051
    update_time_ms: 39.306
  timestamp: 1602388546
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     14 |          492.955 | 2265088 |  227.112 |              274.051 |              71.9293 |            826.109 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3553.8160511363635
    time_step_min: 3269
  date: 2020-10-11_03-56-21
  done: false
  episode_len_mean: 824.8066104078762
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 227.82383966244726
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.7060970451150622
        entropy_coeff: 0.00010000000000000002
        kl: 0.010213323536195926
        model: {}
        policy_loss: -0.019373546753610884
        total_loss: 1.972534954547882
        vf_explained_var: 0.9960017800331116
        vf_loss: 1.9899364624704634
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.656097560975606
    gpu_util_percent0: 0.2621951219512196
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.507317073170732
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1545581667932923
    mean_env_wait_ms: 1.229116423837859
    mean_inference_ms: 4.65855257208741
    mean_raw_obs_processing_ms: 0.406344796681222
  time_since_restore: 528.2388994693756
  time_this_iter_s: 35.283456802368164
  time_total_s: 528.2388994693756
  timers:
    learn_throughput: 5819.957
    learn_time_ms: 27799.516
    sample_throughput: 22600.227
    sample_time_ms: 7158.866
    update_time_ms: 39.784
  timestamp: 1602388581
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     15 |          528.239 | 2426880 |  227.824 |              274.051 |              71.9293 |            824.807 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3549.0687813021705
    time_step_min: 3269
  date: 2020-10-11_03-56-56
  done: false
  episode_len_mean: 823.327158451869
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 228.61555682528228
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 179
  episodes_total: 3023
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6788246035575867
        entropy_coeff: 0.00010000000000000002
        kl: 0.01008287597713726
        model: {}
        policy_loss: -0.017681216787812964
        total_loss: 2.14350163936615
        vf_explained_var: 0.9964456558227539
        vf_loss: 2.1592341491154263
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.224390243902437
    gpu_util_percent0: 0.28707317073170724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4975609756097565
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15430196251535763
    mean_env_wait_ms: 1.2302478008378992
    mean_inference_ms: 4.64430942311832
    mean_raw_obs_processing_ms: 0.4054213606923527
  time_since_restore: 563.1548008918762
  time_this_iter_s: 34.91590142250061
  time_total_s: 563.1548008918762
  timers:
    learn_throughput: 5813.723
    learn_time_ms: 27829.327
    sample_throughput: 22648.521
    sample_time_ms: 7143.601
    update_time_ms: 39.745
  timestamp: 1602388616
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     16 |          563.155 | 2588672 |  228.616 |              274.051 |              71.9293 |            823.327 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3541.536474164134
    time_step_min: 3252
  date: 2020-10-11_03-57-32
  done: false
  episode_len_mean: 821.1449668474985
  episode_reward_max: 274.0505050505047
  episode_reward_mean: 229.87639505361028
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 295
  episodes_total: 3318
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6391335725784302
        entropy_coeff: 0.00010000000000000002
        kl: 0.009124331708465303
        model: {}
        policy_loss: -0.014476897860211986
        total_loss: 2.6586670875549316
        vf_explained_var: 0.9958106279373169
        vf_loss: 2.6713829892022267
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.0
    gpu_util_percent0: 0.368
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.4875
    vram_util_percent0: 0.20465726467694326
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15392901343008983
    mean_env_wait_ms: 1.232048459651644
    mean_inference_ms: 4.6238313226450005
    mean_raw_obs_processing_ms: 0.4041421625811509
  time_since_restore: 598.3301658630371
  time_this_iter_s: 35.17536497116089
  time_total_s: 598.3301658630371
  timers:
    learn_throughput: 5813.456
    learn_time_ms: 27830.606
    sample_throughput: 22571.638
    sample_time_ms: 7167.933
    update_time_ms: 41.396
  timestamp: 1602388652
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | RUNNING  | 172.17.0.4:3236 |     17 |           598.33 | 2750464 |  229.876 |              274.051 |              71.9293 |            821.145 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_79b7c_00000:
  custom_metrics:
    time_step_max: 4581
    time_step_mean: 3537.022041763341
    time_step_min: 3229
  date: 2020-10-11_03-58-07
  done: true
  episode_len_mean: 819.8731300345224
  episode_reward_max: 276.7777777777776
  episode_reward_mean: 230.4855400960119
  episode_reward_min: 71.92929292929274
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: 90090e73d5bf425c8655db56b2274fad
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 0.6262183359691075
        entropy_coeff: 0.00010000000000000002
        kl: 0.009659321047365665
        model: {}
        policy_loss: -0.015489497355052404
        total_loss: 1.956004066126687
        vf_explained_var: 0.995984673500061
        vf_loss: 1.9696243490491594
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.378048780487806
    gpu_util_percent0: 0.27926829268292686
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 6.5
    vram_util_percent0: 0.20465726467694323
    vram_util_percent1: 0.0009075233687267446
    vram_util_percent2: 0.0009075233687267446
  pid: 3236
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15374731827512336
    mean_env_wait_ms: 1.2329287274430356
    mean_inference_ms: 4.6139006100434345
    mean_raw_obs_processing_ms: 0.4035208000740649
  time_since_restore: 633.3079533576965
  time_this_iter_s: 34.977787494659424
  time_total_s: 633.3079533576965
  timers:
    learn_throughput: 5814.569
    learn_time_ms: 27825.277
    sample_throughput: 22570.653
    sample_time_ms: 7168.246
    update_time_ms: 41.499
  timestamp: 1602388687
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 79b7c_00000
  
== Status ==
Memory usage on this node: 48.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | TERMINATED |       |     18 |          633.308 | 2912256 |  230.486 |              276.778 |              71.9293 |            819.873 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


2020-10-11 03:58:07,682	WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffffef1af81501000000.
== Status ==
Memory usage on this node: 48.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/537.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_79b7c_00000 | TERMINATED |       |     18 |          633.308 | 2912256 |  230.486 |              276.778 |              71.9293 |            819.873 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[36m(pid=3255)[0m 2020-10-11 03:58:07,630	ERROR worker.py:372 -- SystemExit was raised from the worker
[2m[36m(pid=3255)[0m Traceback (most recent call last):
[2m[36m(pid=3255)[0m   File "python/ray/_raylet.pyx", line 483, in ray._raylet.execute_task
[2m[36m(pid=3255)[0m   File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
[2m[36m(pid=3255)[0m   File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py", line 553, in actor_method_executor
[2m[36m(pid=3255)[0m     return method(actor, *args, **kwargs)
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 929, in __ray_terminate__
[2m[36m(pid=3255)[0m     ray.actor.exit_actor()
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 996, in exit_actor
[2m[36m(pid=3255)[0m     raise exit
[2m[36m(pid=3255)[0m SystemExit: 0
[2m[36m(pid=3255)[0m 
[2m[36m(pid=3255)[0m During handling of the above exception, another exception occurred:
[2m[36m(pid=3255)[0m 
[2m[36m(pid=3255)[0m Traceback (most recent call last):
[2m[36m(pid=3255)[0m   File "python/ray/_raylet.pyx", line 553, in ray._raylet.task_execution_handler
[2m[36m(pid=3255)[0m   File "python/ray/_raylet.pyx", line 440, in ray._raylet.execute_task
[2m[36m(pid=3255)[0m   File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
[2m[36m(pid=3255)[0m   File "python/ray/includes/libcoreworker.pxi", line 33, in ray._raylet.ProfileEvent.__exit__
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 167, in format_exc
[2m[36m(pid=3255)[0m     return "".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 120, in format_exception
[2m[36m(pid=3255)[0m     return list(TracebackException(
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 509, in __init__
[2m[36m(pid=3255)[0m     self.stack = StackSummary.extract(
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/traceback.py", line 362, in extract
[2m[36m(pid=3255)[0m     linecache.checkcache(filename)
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/linecache.py", line 60, in checkcache
[2m[36m(pid=3255)[0m     if filename in cache:
[2m[36m(pid=3255)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 369, in sigterm_handler
[2m[36m(pid=3255)[0m     sys.exit(1)
[2m[36m(pid=3255)[0m SystemExit: 1
[2m[33m(pid=raylet)[0m E1011 03:58:07.678309  3115  3115 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
