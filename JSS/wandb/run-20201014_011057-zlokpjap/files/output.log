2020-10-14 01:11:01,049	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_20684_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=50546)[0m 2020-10-14 01:11:03,866	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=50512)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50512)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50506)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50506)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50511)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50511)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50453)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50453)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50562)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50562)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50528)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50528)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50533)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50533)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50515)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50515)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50550)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50550)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50531)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50531)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50547)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50547)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50538)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50538)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50527)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50527)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50504)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50504)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50485)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50485)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50536)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50536)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50525)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50525)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50513)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50513)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50566)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50566)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50510)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50510)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50462)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50462)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50435)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50435)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50540)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50540)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50461)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50461)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50552)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50552)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50443)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50443)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50488)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50488)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50541)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50541)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50432)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50432)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50529)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50529)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50463)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50463)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50434)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50434)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50509)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50509)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50521)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50521)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50497)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50497)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50468)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50468)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50501)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50501)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50442)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50442)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50449)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50449)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50553)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50553)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50436)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50436)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50524)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50524)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50451)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50451)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50554)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50554)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50437)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50437)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50447)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50447)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50494)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50494)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50514)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50514)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50433)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50433)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50466)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50466)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50469)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50469)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50450)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50450)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50441)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50441)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50470)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50470)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50545)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50545)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50522)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50522)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50454)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50454)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50548)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50548)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50446)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50446)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50508)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50508)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50440)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50440)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50438)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50438)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50519)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50519)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50568)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50568)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50500)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50500)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50465)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50465)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50543)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50543)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50507)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50507)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50560)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50560)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50558)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50558)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50456)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50456)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50517)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50517)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50448)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50448)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50490)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50490)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50544)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50544)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50458)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50458)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50505)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50505)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50444)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50444)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=50502)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50502)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3924
    time_step_mean: 3604.0258620689656
    time_step_min: 3359
  date: 2020-10-14_01-11-37
  done: false
  episode_len_mean: 901.8924050632911
  episode_reward_max: 263.14141414141426
  episode_reward_mean: 220.7612837233088
  episode_reward_min: 158.89898989898953
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1608085731665294
        entropy_coeff: 0.0005000000000000001
        kl: 0.004805326461791992
        model: {}
        policy_loss: -0.009253671441304808
        total_loss: 406.1570561726888
        vf_explained_var: 0.5482549071311951
        vf_loss: 406.1659342447917
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.7125
    gpu_util_percent0: 0.2984375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.553125
    vram_util_percent0: 0.08698036241390619
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16837290828336204
    mean_env_wait_ms: 1.1545925811692201
    mean_inference_ms: 5.552216977681522
    mean_raw_obs_processing_ms: 0.4475698326796285
  time_since_restore: 27.94219660758972
  time_this_iter_s: 27.94219660758972
  time_total_s: 27.94219660758972
  timers:
    learn_throughput: 8490.39
    learn_time_ms: 19055.898
    sample_throughput: 18378.325
    sample_time_ms: 8803.414
    update_time_ms: 44.869
  timestamp: 1602637897
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      1 |          27.9422 | 161792 |  220.761 |              263.141 |              158.899 |            901.892 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3600.970802919708
    time_step_min: 3333
  date: 2020-10-14_01-12-03
  done: false
  episode_len_mean: 899.1962025316456
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 221.7322273366575
  episode_reward_min: 149.65656565656565
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1335585514704387
        entropy_coeff: 0.0005000000000000001
        kl: 0.008734231581911445
        model: {}
        policy_loss: -0.010306129270854095
        total_loss: 93.4173018137614
        vf_explained_var: 0.8161735534667969
        vf_loss: 93.42730140686035
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.683870967741935
    gpu_util_percent0: 0.3483870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.751612903225806
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16540844657383857
    mean_env_wait_ms: 1.1558479401510129
    mean_inference_ms: 5.443942282987289
    mean_raw_obs_processing_ms: 0.44301289354036544
  time_since_restore: 54.73020696640015
  time_this_iter_s: 26.788010358810425
  time_total_s: 54.73020696640015
  timers:
    learn_throughput: 8573.632
    learn_time_ms: 18870.883
    sample_throughput: 19227.726
    sample_time_ms: 8414.516
    update_time_ms: 37.916
  timestamp: 1602637923
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      2 |          54.7302 | 323584 |  221.732 |              273.293 |              149.657 |            899.196 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3595.8611111111113
    time_step_min: 3295
  date: 2020-10-14_01-12-30
  done: false
  episode_len_mean: 892.9345991561181
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 222.41484464902166
  episode_reward_min: 149.65656565656565
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1224475900332134
        entropy_coeff: 0.0005000000000000001
        kl: 0.009928210948904356
        model: {}
        policy_loss: -0.013527336258751651
        total_loss: 46.36474482218424
        vf_explained_var: 0.8946500420570374
        vf_loss: 46.37784067789713
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.743333333333336
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16306194614464734
    mean_env_wait_ms: 1.157705971780028
    mean_inference_ms: 5.318844198415014
    mean_raw_obs_processing_ms: 0.43737497784752866
  time_since_restore: 80.97954273223877
  time_this_iter_s: 26.249335765838623
  time_total_s: 80.97954273223877
  timers:
    learn_throughput: 8586.168
    learn_time_ms: 18843.33
    sample_throughput: 20063.873
    sample_time_ms: 8063.847
    update_time_ms: 39.083
  timestamp: 1602637950
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      3 |          80.9795 | 485376 |  222.415 |              273.293 |              149.657 |            892.935 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3588.886440677966
    time_step_min: 3295
  date: 2020-10-14_01-12-56
  done: false
  episode_len_mean: 889.3892405063291
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 223.09178813450944
  episode_reward_min: 149.65656565656565
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1078311800956726
        entropy_coeff: 0.0005000000000000001
        kl: 0.008517272498769065
        model: {}
        policy_loss: -0.011693460956280433
        total_loss: 35.83105627695719
        vf_explained_var: 0.9196924567222595
        vf_loss: 35.84245204925537
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.926666666666666
    gpu_util_percent0: 0.32599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16118505387208754
    mean_env_wait_ms: 1.1595216619223838
    mean_inference_ms: 5.2105852443678415
    mean_raw_obs_processing_ms: 0.43192396232293856
  time_since_restore: 106.9371337890625
  time_this_iter_s: 25.95759105682373
  time_total_s: 106.9371337890625
  timers:
    learn_throughput: 8591.381
    learn_time_ms: 18831.897
    sample_throughput: 20700.27
    sample_time_ms: 7815.937
    update_time_ms: 38.327
  timestamp: 1602637976
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      4 |          106.937 | 647168 |  223.092 |              273.293 |              149.657 |            889.389 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3577.756684491979
    time_step_min: 3290
  date: 2020-10-14_01-13-22
  done: false
  episode_len_mean: 881.9746835443038
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 224.92679964198936
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0736220180988312
        entropy_coeff: 0.0005000000000000001
        kl: 0.007788454958548148
        model: {}
        policy_loss: -0.013847309475143751
        total_loss: 26.951021671295166
        vf_explained_var: 0.9353466629981995
        vf_loss: 26.964626948038738
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.190322580645166
    gpu_util_percent0: 0.3606451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15969433955501328
    mean_env_wait_ms: 1.1620490273087145
    mean_inference_ms: 5.122572789430117
    mean_raw_obs_processing_ms: 0.42719422841723964
  time_since_restore: 133.06111693382263
  time_this_iter_s: 26.123983144760132
  time_total_s: 133.06111693382263
  timers:
    learn_throughput: 8592.815
    learn_time_ms: 18828.754
    sample_throughput: 21065.738
    sample_time_ms: 7680.338
    update_time_ms: 38.487
  timestamp: 1602638002
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      5 |          133.061 | 808960 |  224.927 |              273.293 |              148.141 |            881.975 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3563.447592067989
    time_step_min: 3284
  date: 2020-10-14_01-13-48
  done: false
  episode_len_mean: 868.5930971843778
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 227.7086120056146
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 311
  episodes_total: 1101
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0425472756226857
        entropy_coeff: 0.0005000000000000001
        kl: 0.008018907392397523
        model: {}
        policy_loss: -0.0111910367947227
        total_loss: 29.674411137898762
        vf_explained_var: 0.9557506442070007
        vf_loss: 29.685321807861328
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.740000000000006
    gpu_util_percent0: 0.366
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1576449620423896
    mean_env_wait_ms: 1.167512308414395
    mean_inference_ms: 4.999495164130303
    mean_raw_obs_processing_ms: 0.4207113601421495
  time_since_restore: 159.14169931411743
  time_this_iter_s: 26.0805823802948
  time_total_s: 159.14169931411743
  timers:
    learn_throughput: 8581.465
    learn_time_ms: 18853.656
    sample_throughput: 21373.492
    sample_time_ms: 7569.75
    update_time_ms: 38.259
  timestamp: 1602638028
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      6 |          159.142 | 970752 |  227.709 |              273.293 |              148.141 |            868.593 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3552.6792144026185
    time_step_min: 3193
  date: 2020-10-14_01-14-14
  done: false
  episode_len_mean: 863.0371835443038
  episode_reward_max: 282.8383838383837
  episode_reward_mean: 229.1686964582533
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 1264
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.037640631198883
        entropy_coeff: 0.0005000000000000001
        kl: 0.007880023564212024
        model: {}
        policy_loss: -0.013542136293835938
        total_loss: 18.04273223876953
        vf_explained_var: 0.9574272632598877
        vf_loss: 18.056005160013836
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.536666666666672
    gpu_util_percent0: 0.2846666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15685069996507034
    mean_env_wait_ms: 1.1697768566815308
    mean_inference_ms: 4.951254842207691
    mean_raw_obs_processing_ms: 0.4180965066936031
  time_since_restore: 185.31145930290222
  time_this_iter_s: 26.16975998878479
  time_total_s: 185.31145930290222
  timers:
    learn_throughput: 8572.885
    learn_time_ms: 18872.526
    sample_throughput: 21564.474
    sample_time_ms: 7502.71
    update_time_ms: 37.751
  timestamp: 1602638054
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      7 |          185.311 | 1132544 |  229.169 |              282.838 |              148.141 |            863.037 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3546.1608695652176
    time_step_min: 3193
  date: 2020-10-14_01-14-40
  done: false
  episode_len_mean: 858.1448663853727
  episode_reward_max: 282.8383838383837
  episode_reward_mean: 230.0799982951881
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0278969705104828
        entropy_coeff: 0.0005000000000000001
        kl: 0.007529285775187115
        model: {}
        policy_loss: -0.011286740329524036
        total_loss: 17.775658925374348
        vf_explained_var: 0.9589188098907471
        vf_loss: 17.78670644760132
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.109999999999996
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1561825565320256
    mean_env_wait_ms: 1.1720158303491857
    mean_inference_ms: 4.9102609337103065
    mean_raw_obs_processing_ms: 0.41582034971763415
  time_since_restore: 211.28243398666382
  time_this_iter_s: 25.970974683761597
  time_total_s: 211.28243398666382
  timers:
    learn_throughput: 8567.322
    learn_time_ms: 18884.781
    sample_throughput: 21778.422
    sample_time_ms: 7429.005
    update_time_ms: 37.402
  timestamp: 1602638080
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      8 |          211.282 | 1294336 |   230.08 |              282.838 |              148.141 |            858.145 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3541.243823146944
    time_step_min: 3193
  date: 2020-10-14_01-15-06
  done: false
  episode_len_mean: 853.8582278481013
  episode_reward_max: 282.8383838383837
  episode_reward_mean: 230.84768571793876
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9909720321496328
        entropy_coeff: 0.0005000000000000001
        kl: 0.007027940048525731
        model: {}
        policy_loss: -0.012216423288919032
        total_loss: 17.90696128209432
        vf_explained_var: 0.9601621627807617
        vf_loss: 17.918970108032227
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.496666666666666
    gpu_util_percent0: 0.32566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15559623969416345
    mean_env_wait_ms: 1.1742131721403744
    mean_inference_ms: 4.873898334975106
    mean_raw_obs_processing_ms: 0.4137325530745499
  time_since_restore: 237.1759614944458
  time_this_iter_s: 25.893527507781982
  time_total_s: 237.1759614944458
  timers:
    learn_throughput: 8572.097
    learn_time_ms: 18874.261
    sample_throughput: 21909.497
    sample_time_ms: 7384.56
    update_time_ms: 35.604
  timestamp: 1602638106
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |      9 |          237.176 | 1456128 |  230.848 |              282.838 |              148.141 |            853.858 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3526.0804101457097
    time_step_min: 3193
  date: 2020-10-14_01-15-32
  done: false
  episode_len_mean: 845.9282321899736
  episode_reward_max: 284.2020202020208
  episode_reward_mean: 232.8910743317075
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 315
  episodes_total: 1895
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9543150613705317
        entropy_coeff: 0.0005000000000000001
        kl: 0.006918751479436954
        model: {}
        policy_loss: -0.0114073078148067
        total_loss: 22.750616709391277
        vf_explained_var: 0.9664721488952637
        vf_loss: 22.761809190114338
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.990000000000002
    gpu_util_percent0: 0.3516666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1546355904358744
    mean_env_wait_ms: 1.1784112426441788
    mean_inference_ms: 4.814664756229723
    mean_raw_obs_processing_ms: 0.41040128473839077
  time_since_restore: 262.9268214702606
  time_this_iter_s: 25.75085997581482
  time_total_s: 262.9268214702606
  timers:
    learn_throughput: 8577.881
    learn_time_ms: 18861.535
    sample_throughput: 22069.416
    sample_time_ms: 7331.05
    update_time_ms: 41.986
  timestamp: 1602638132
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     10 |          262.927 | 1617920 |  232.891 |              284.202 |              148.141 |            845.928 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3519.0531809145127
    time_step_min: 3193
  date: 2020-10-14_01-15-58
  done: false
  episode_len_mean: 842.6231742940604
  episode_reward_max: 284.2020202020208
  episode_reward_mean: 233.80722512368075
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 2054
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9550424267848333
        entropy_coeff: 0.0005000000000000001
        kl: 0.006833299140756329
        model: {}
        policy_loss: -0.010028554146022847
        total_loss: 14.67298936843872
        vf_explained_var: 0.9665574431419373
        vf_loss: 14.682812054951986
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.00689655172414
    gpu_util_percent0: 0.41896551724137926
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15423070291192534
    mean_env_wait_ms: 1.1801949863598702
    mean_inference_ms: 4.7897817405482535
    mean_raw_obs_processing_ms: 0.4089726283398597
  time_since_restore: 288.7371256351471
  time_this_iter_s: 25.810304164886475
  time_total_s: 288.7371256351471
  timers:
    learn_throughput: 8589.351
    learn_time_ms: 18836.347
    sample_throughput: 22657.41
    sample_time_ms: 7140.799
    update_time_ms: 41.219
  timestamp: 1602638158
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     11 |          288.737 | 1779712 |  233.807 |              284.202 |              148.141 |            842.623 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3512.883410138249
    time_step_min: 3193
  date: 2020-10-14_01-16-24
  done: false
  episode_len_mean: 839.2165461121157
  episode_reward_max: 284.2020202020208
  episode_reward_mean: 234.68375892742978
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9333042154709498
        entropy_coeff: 0.0005000000000000001
        kl: 0.006979815157440801
        model: {}
        policy_loss: -0.012605925294337794
        total_loss: 12.56671412785848
        vf_explained_var: 0.9697521328926086
        vf_loss: 12.57908852895101
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.296666666666663
    gpu_util_percent0: 0.32599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1538645324837781
    mean_env_wait_ms: 1.1819507718632671
    mean_inference_ms: 4.767330021026312
    mean_raw_obs_processing_ms: 0.40764448552184546
  time_since_restore: 314.772118806839
  time_this_iter_s: 26.034993171691895
  time_total_s: 314.772118806839
  timers:
    learn_throughput: 8581.373
    learn_time_ms: 18853.86
    sample_throughput: 22960.954
    sample_time_ms: 7046.397
    update_time_ms: 41.882
  timestamp: 1602638184
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     12 |          314.772 | 1941504 |  234.684 |              284.202 |              148.141 |            839.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3505.884792626728
    time_step_min: 3193
  date: 2020-10-14_01-16-50
  done: false
  episode_len_mean: 834.7291066282421
  episode_reward_max: 288.1414141414142
  episode_reward_mean: 235.89819146591478
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 217
  episodes_total: 2429
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8839850127696991
        entropy_coeff: 0.0005000000000000001
        kl: 0.006719793581093351
        model: {}
        policy_loss: -0.011859170898484686
        total_loss: 17.87238534291585
        vf_explained_var: 0.9688119888305664
        vf_loss: 17.884014129638672
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.51666666666667
    gpu_util_percent0: 0.349
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15340827939496268
    mean_env_wait_ms: 1.1844209321238865
    mean_inference_ms: 4.739831800834255
    mean_raw_obs_processing_ms: 0.4059691404646979
  time_since_restore: 340.6526415348053
  time_this_iter_s: 25.88052272796631
  time_total_s: 340.6526415348053
  timers:
    learn_throughput: 8583.717
    learn_time_ms: 18848.711
    sample_throughput: 23067.527
    sample_time_ms: 7013.842
    update_time_ms: 41.638
  timestamp: 1602638210
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     13 |          340.653 | 2103296 |  235.898 |              288.141 |              148.141 |            834.729 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3497.321482602118
    time_step_min: 3193
  date: 2020-10-14_01-17-16
  done: false
  episode_len_mean: 830.2896500372301
  episode_reward_max: 288.1414141414142
  episode_reward_mean: 237.30001429033445
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 257
  episodes_total: 2686
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8755106230576833
        entropy_coeff: 0.0005000000000000001
        kl: 0.006404241585793595
        model: {}
        policy_loss: -0.011880003226300081
        total_loss: 13.644367694854736
        vf_explained_var: 0.973446786403656
        vf_loss: 13.656045198440552
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.65333333333334
    gpu_util_percent0: 0.26833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15296176684026053
    mean_env_wait_ms: 1.1870082511928082
    mean_inference_ms: 4.711478710900842
    mean_raw_obs_processing_ms: 0.40432555797845593
  time_since_restore: 366.6119725704193
  time_this_iter_s: 25.959331035614014
  time_total_s: 366.6119725704193
  timers:
    learn_throughput: 8583.329
    learn_time_ms: 18849.562
    sample_throughput: 23071.795
    sample_time_ms: 7012.545
    update_time_ms: 41.753
  timestamp: 1602638236
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     14 |          366.612 | 2265088 |    237.3 |              288.141 |              148.141 |             830.29 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3490.0749464668093
    time_step_min: 3174
  date: 2020-10-14_01-17-42
  done: false
  episode_len_mean: 827.7123769338959
  episode_reward_max: 288.1414141414142
  episode_reward_mean: 238.29095810424926
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8695435126622518
        entropy_coeff: 0.0005000000000000001
        kl: 0.006383650974991421
        model: {}
        policy_loss: -0.01124639364813144
        total_loss: 11.072142362594604
        vf_explained_var: 0.9715961813926697
        vf_loss: 11.08318535486857
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.24666666666667
    gpu_util_percent0: 0.2703333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15271392295122663
    mean_env_wait_ms: 1.18845729177127
    mean_inference_ms: 4.69592437409577
    mean_raw_obs_processing_ms: 0.4033958714100395
  time_since_restore: 392.5759983062744
  time_this_iter_s: 25.964025735855103
  time_total_s: 392.5759983062744
  timers:
    learn_throughput: 8585.221
    learn_time_ms: 18845.409
    sample_throughput: 23092.389
    sample_time_ms: 7006.291
    update_time_ms: 41.044
  timestamp: 1602638262
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     15 |          392.576 | 2426880 |  238.291 |              288.141 |              148.141 |            827.712 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3484.8203572632287
    time_step_min: 3141
  date: 2020-10-14_01-18-08
  done: false
  episode_len_mean: 825.2964440013293
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 239.07910611599544
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 165
  episodes_total: 3009
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8375518967707952
        entropy_coeff: 0.0005000000000000001
        kl: 0.007002773229032755
        model: {}
        policy_loss: -0.009874908602796495
        total_loss: 12.741058508555094
        vf_explained_var: 0.9726340174674988
        vf_loss: 12.750651995340982
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.970000000000002
    gpu_util_percent0: 0.3393333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15246746815911183
    mean_env_wait_ms: 1.1899531357366615
    mean_inference_ms: 4.680752239809796
    mean_raw_obs_processing_ms: 0.4024696987556797
  time_since_restore: 418.51181626319885
  time_this_iter_s: 25.93581795692444
  time_total_s: 418.51181626319885
  timers:
    learn_throughput: 8596.697
    learn_time_ms: 18820.252
    sample_throughput: 23057.412
    sample_time_ms: 7016.919
    update_time_ms: 40.702
  timestamp: 1602638288
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     16 |          418.512 | 2588672 |  239.079 |              290.717 |              148.141 |            825.296 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3474.40231990232
    time_step_min: 3141
  date: 2020-10-14_01-18-35
  done: false
  episode_len_mean: 820.6868595539481
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 240.6259795057263
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 309
  episodes_total: 3318
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8005305876334509
        entropy_coeff: 0.0005000000000000001
        kl: 0.005828887418222924
        model: {}
        policy_loss: -0.010037912928964943
        total_loss: 13.088919321695963
        vf_explained_var: 0.9782760739326477
        vf_loss: 13.098775068918863
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.410000000000004
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520735464783244
    mean_env_wait_ms: 1.1926278641504149
    mean_inference_ms: 4.655594198897039
    mean_raw_obs_processing_ms: 0.400980016392857
  time_since_restore: 444.71045541763306
  time_this_iter_s: 26.198639154434204
  time_total_s: 444.71045541763306
  timers:
    learn_throughput: 8601.592
    learn_time_ms: 18809.541
    sample_throughput: 23016.458
    sample_time_ms: 7029.405
    update_time_ms: 40.285
  timestamp: 1602638315
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     17 |           444.71 | 2750464 |  240.626 |              290.717 |              148.141 |            820.687 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3469.9321490972625
    time_step_min: 3141
  date: 2020-10-14_01-19-01
  done: false
  episode_len_mean: 818.5382623705409
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 241.24904394927412
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8076162983973821
        entropy_coeff: 0.0005000000000000001
        kl: 0.005866569311668475
        model: {}
        policy_loss: -0.010886732585883388
        total_loss: 10.134718100229898
        vf_explained_var: 0.9754086136817932
        vf_loss: 10.145421981811523
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.916129032258063
    gpu_util_percent0: 0.37677419354838704
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15188939887817146
    mean_env_wait_ms: 1.1938614388913307
    mean_inference_ms: 4.644009734403106
    mean_raw_obs_processing_ms: 0.4002882055169036
  time_since_restore: 470.7386474609375
  time_this_iter_s: 26.028192043304443
  time_total_s: 470.7386474609375
  timers:
    learn_throughput: 8608.412
    learn_time_ms: 18794.639
    sample_throughput: 22955.141
    sample_time_ms: 7048.182
    update_time_ms: 41.601
  timestamp: 1602638341
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     18 |          470.739 | 2912256 |  241.249 |              290.717 |              148.141 |            818.538 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3465.5599443671767
    time_step_min: 3141
  date: 2020-10-14_01-19-27
  done: false
  episode_len_mean: 816.4492713775089
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 241.9722437462333
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 161
  episodes_total: 3637
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7810584157705307
        entropy_coeff: 0.0005000000000000001
        kl: 0.005782775154026846
        model: {}
        policy_loss: -0.010712247264261046
        total_loss: 10.520259141921997
        vf_explained_var: 0.9756563305854797
        vf_loss: 10.530783812204996
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.483333333333334
    gpu_util_percent0: 0.30933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1517097625905642
    mean_env_wait_ms: 1.1951220834070542
    mean_inference_ms: 4.6329085470082285
    mean_raw_obs_processing_ms: 0.39961480606796607
  time_since_restore: 496.8279902935028
  time_this_iter_s: 26.089342832565308
  time_total_s: 496.8279902935028
  timers:
    learn_throughput: 8608.626
    learn_time_ms: 18794.173
    sample_throughput: 22897.374
    sample_time_ms: 7065.963
    update_time_ms: 43.195
  timestamp: 1602638367
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     19 |          496.828 | 3074048 |  241.972 |              290.717 |              148.141 |            816.449 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3457.474923234391
    time_step_min: 3141
  date: 2020-10-14_01-19-54
  done: false
  episode_len_mean: 812.6521518987341
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 243.1544175936581
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 313
  episodes_total: 3950
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7413140634695689
        entropy_coeff: 0.0005000000000000001
        kl: 0.005068241152912378
        model: {}
        policy_loss: -0.011031584620165328
        total_loss: 13.177871386210123
        vf_explained_var: 0.9785845279693604
        vf_loss: 13.188766797383627
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.076666666666668
    gpu_util_percent0: 0.3216666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15139730754983022
    mean_env_wait_ms: 1.197452034584689
    mean_inference_ms: 4.613213036089612
    mean_raw_obs_processing_ms: 0.398453871816623
  time_since_restore: 523.09303855896
  time_this_iter_s: 26.265048265457153
  time_total_s: 523.09303855896
  timers:
    learn_throughput: 8598.077
    learn_time_ms: 18817.231
    sample_throughput: 22786.958
    sample_time_ms: 7100.202
    update_time_ms: 35.41
  timestamp: 1602638394
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     20 |          523.093 | 3235840 |  243.154 |              290.717 |              148.141 |            812.652 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3453.5418101328087
    time_step_min: 3099
  date: 2020-10-14_01-20-20
  done: false
  episode_len_mean: 811.0791139240506
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 243.73076677190602
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 4108
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7540231595436732
        entropy_coeff: 0.0005000000000000001
        kl: 0.006251458660699427
        model: {}
        policy_loss: -0.010166963950420419
        total_loss: 9.666405280431112
        vf_explained_var: 0.9757277369499207
        vf_loss: 9.676324129104614
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.116666666666667
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15125198700828937
    mean_env_wait_ms: 1.198521564803929
    mean_inference_ms: 4.604168774100605
    mean_raw_obs_processing_ms: 0.397914099556658
  time_since_restore: 548.9222254753113
  time_this_iter_s: 25.82918691635132
  time_total_s: 548.9222254753113
  timers:
    learn_throughput: 8602.997
    learn_time_ms: 18806.469
    sample_throughput: 22740.045
    sample_time_ms: 7114.85
    update_time_ms: 33.906
  timestamp: 1602638420
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     21 |          548.922 | 3397632 |  243.731 |              297.081 |              148.141 |            811.079 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3449.165326395459
    time_step_min: 3099
  date: 2020-10-14_01-20-46
  done: false
  episode_len_mean: 809.4182669789227
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 244.39348993447354
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 4270
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7344886114199957
        entropy_coeff: 0.0005000000000000001
        kl: 0.00542777714629968
        model: {}
        policy_loss: -0.010750282885661969
        total_loss: 10.668164094289144
        vf_explained_var: 0.9743292331695557
        vf_loss: 10.678739309310913
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.7
    gpu_util_percent0: 0.307
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15110832237240332
    mean_env_wait_ms: 1.1996149768223017
    mean_inference_ms: 4.595393183574686
    mean_raw_obs_processing_ms: 0.3973780556507428
  time_since_restore: 574.9067847728729
  time_this_iter_s: 25.984559297561646
  time_total_s: 574.9067847728729
  timers:
    learn_throughput: 8608.156
    learn_time_ms: 18795.199
    sample_throughput: 22719.016
    sample_time_ms: 7121.435
    update_time_ms: 32.179
  timestamp: 1602638446
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     22 |          574.907 | 3559424 |  244.393 |              297.081 |              148.141 |            809.418 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3441.851321585903
    time_step_min: 3099
  date: 2020-10-14_01-21-12
  done: false
  episode_len_mean: 806.577258838935
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 245.50656499521628
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 312
  episodes_total: 4582
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6938638339440028
        entropy_coeff: 0.0005000000000000001
        kl: 0.005188658911113937
        model: {}
        policy_loss: -0.00690779621557643
        total_loss: 12.988922595977783
        vf_explained_var: 0.9786821007728577
        vf_loss: 12.995658238728842
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.493333333333336
    gpu_util_percent0: 0.3603333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15085745401489453
    mean_env_wait_ms: 1.201605888485746
    mean_inference_ms: 4.579755504440765
    mean_raw_obs_processing_ms: 0.39644566613754734
  time_since_restore: 601.1342799663544
  time_this_iter_s: 26.227495193481445
  time_total_s: 601.1342799663544
  timers:
    learn_throughput: 8600.15
    learn_time_ms: 18812.695
    sample_throughput: 22663.241
    sample_time_ms: 7138.961
    update_time_ms: 32.157
  timestamp: 1602638472
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     23 |          601.134 | 3721216 |  245.507 |              297.081 |              148.141 |            806.577 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3438.1868880374627
    time_step_min: 3099
  date: 2020-10-14_01-21-38
  done: false
  episode_len_mean: 805.4289029535865
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 246.11207006776633
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 4740
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7138400276501974
        entropy_coeff: 0.0005000000000000001
        kl: 0.005280776919486622
        model: {}
        policy_loss: -0.011575660440333499
        total_loss: 8.737238804499308
        vf_explained_var: 0.9775153994560242
        vf_loss: 8.748643398284912
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.483333333333334
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15074077817470743
    mean_env_wait_ms: 1.2025196106024283
    mean_inference_ms: 4.572496357968513
    mean_raw_obs_processing_ms: 0.39600622525792484
  time_since_restore: 627.2138001918793
  time_this_iter_s: 26.079520225524902
  time_total_s: 627.2138001918793
  timers:
    learn_throughput: 8594.689
    learn_time_ms: 18824.649
    sample_throughput: 22666.44
    sample_time_ms: 7137.954
    update_time_ms: 32.257
  timestamp: 1602638498
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     24 |          627.214 | 3883008 |  246.112 |              297.081 |              148.141 |            805.429 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3434.383333333333
    time_step_min: 3099
  date: 2020-10-14_01-22-05
  done: false
  episode_len_mean: 804.2396980824153
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 246.67522223458576
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 4902
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6957149008909861
        entropy_coeff: 0.0005000000000000001
        kl: 0.005473119051506122
        model: {}
        policy_loss: -0.010031304535611222
        total_loss: 9.043355305989584
        vf_explained_var: 0.9785754680633545
        vf_loss: 9.053186972935995
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.12666666666667
    gpu_util_percent0: 0.3626666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15062465280028484
    mean_env_wait_ms: 1.2034471560183204
    mean_inference_ms: 4.565384164865745
    mean_raw_obs_processing_ms: 0.3955674137850878
  time_since_restore: 653.3590521812439
  time_this_iter_s: 26.145251989364624
  time_total_s: 653.3590521812439
  timers:
    learn_throughput: 8589.134
    learn_time_ms: 18836.824
    sample_throughput: 22646.207
    sample_time_ms: 7144.331
    update_time_ms: 32.777
  timestamp: 1602638525
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     25 |          653.359 | 4044800 |  246.675 |              297.081 |              148.141 |             804.24 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3427.224671307038
    time_step_min: 3099
  date: 2020-10-14_01-22-31
  done: false
  episode_len_mean: 802.1108553893364
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 247.71807836710022
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 312
  episodes_total: 5214
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6606731961170832
        entropy_coeff: 0.0005000000000000001
        kl: 0.005305661625849704
        model: {}
        policy_loss: -0.009964583994587883
        total_loss: 10.540854771931967
        vf_explained_var: 0.9823284149169922
        vf_loss: 10.55061904589335
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.69
    gpu_util_percent0: 0.2889999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15042177119049083
    mean_env_wait_ms: 1.2051065016979026
    mean_inference_ms: 4.552648148572646
    mean_raw_obs_processing_ms: 0.3947963281537733
  time_since_restore: 679.1822514533997
  time_this_iter_s: 25.82319927215576
  time_total_s: 679.1822514533997
  timers:
    learn_throughput: 8586.74
    learn_time_ms: 18842.076
    sample_throughput: 22703.614
    sample_time_ms: 7126.266
    update_time_ms: 32.752
  timestamp: 1602638551
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     26 |          679.182 | 4206592 |  247.718 |              297.081 |              148.141 |            802.111 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3423.242589118199
    time_step_min: 3085
  date: 2020-10-14_01-22-57
  done: false
  episode_len_mean: 801.129746835443
  episode_reward_max: 299.20202020202026
  episode_reward_mean: 248.3269158449725
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 5372
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6744110981623331
        entropy_coeff: 0.0005000000000000001
        kl: 0.005423576609852414
        model: {}
        policy_loss: -0.014040446258150041
        total_loss: 6.8246026039123535
        vf_explained_var: 0.9812327027320862
        vf_loss: 6.838438073794047
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.69
    gpu_util_percent0: 0.3403333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1503252830821312
    mean_env_wait_ms: 1.2058778864108048
    mean_inference_ms: 4.5466556179031885
    mean_raw_obs_processing_ms: 0.3944295339781378
  time_since_restore: 705.0304341316223
  time_this_iter_s: 25.848182678222656
  time_total_s: 705.0304341316223
  timers:
    learn_throughput: 8591.364
    learn_time_ms: 18831.934
    sample_throughput: 22786.324
    sample_time_ms: 7100.399
    update_time_ms: 33.35
  timestamp: 1602638577
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     27 |           705.03 | 4368384 |  248.327 |              299.202 |              148.141 |             801.13 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3419.3061521659993
    time_step_min: 3085
  date: 2020-10-14_01-23-23
  done: false
  episode_len_mean: 800.1177745664739
  episode_reward_max: 299.20202020202026
  episode_reward_mean: 248.8839916506102
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 5536
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6572760492563248
        entropy_coeff: 0.0005000000000000001
        kl: 0.004983084547954301
        model: {}
        policy_loss: -0.00991726026404649
        total_loss: 8.127788146336874
        vf_explained_var: 0.9796605706214905
        vf_loss: 8.13753585020701
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.510000000000005
    gpu_util_percent0: 0.3776666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1502278447767076
    mean_env_wait_ms: 1.2066768662633234
    mean_inference_ms: 4.540715247247983
    mean_raw_obs_processing_ms: 0.3940615093051956
  time_since_restore: 730.9208323955536
  time_this_iter_s: 25.890398263931274
  time_total_s: 730.9208323955536
  timers:
    learn_throughput: 8596.103
    learn_time_ms: 18821.552
    sample_throughput: 22803.036
    sample_time_ms: 7095.196
    update_time_ms: 32.456
  timestamp: 1602638603
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     28 |          730.921 | 4530176 |  248.884 |              299.202 |              148.141 |            800.118 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3412.38266712612
    time_step_min: 3077
  date: 2020-10-14_01-23-48
  done: false
  episode_len_mean: 798.2724940130004
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 249.90936736506362
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 310
  episodes_total: 5846
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6118175039688746
        entropy_coeff: 0.0005000000000000001
        kl: 0.005293370961832504
        model: {}
        policy_loss: -0.009223727480275556
        total_loss: 11.607327302296957
        vf_explained_var: 0.9801087379455566
        vf_loss: 11.616592248280844
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.466666666666665
    gpu_util_percent0: 0.3243333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15005944880731045
    mean_env_wait_ms: 1.2080792269187646
    mean_inference_ms: 4.530118189090211
    mean_raw_obs_processing_ms: 0.3934153771536075
  time_since_restore: 756.6341199874878
  time_this_iter_s: 25.713287591934204
  time_total_s: 756.6341199874878
  timers:
    learn_throughput: 8601.532
    learn_time_ms: 18809.673
    sample_throughput: 22885.579
    sample_time_ms: 7069.605
    update_time_ms: 31.084
  timestamp: 1602638628
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     29 |          756.634 | 4691968 |  249.909 |              300.414 |              148.141 |            798.272 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3408.7141898691716
    time_step_min: 3077
  date: 2020-10-14_01-24-15
  done: false
  episode_len_mean: 797.3502664890074
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 250.42060175371304
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 6004
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6205476025740305
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642907655177017
        model: {}
        policy_loss: -0.010487376295107728
        total_loss: 7.41852347056071
        vf_explained_var: 0.9801657199859619
        vf_loss: 7.429039080937703
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.54
    gpu_util_percent0: 0.31166666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14997852249163882
    mean_env_wait_ms: 1.2087418604544322
    mean_inference_ms: 4.525079254045281
    mean_raw_obs_processing_ms: 0.3931061945128594
  time_since_restore: 782.5545291900635
  time_this_iter_s: 25.920409202575684
  time_total_s: 782.5545291900635
  timers:
    learn_throughput: 8601.63
    learn_time_ms: 18809.458
    sample_throughput: 22999.296
    sample_time_ms: 7034.65
    update_time_ms: 31.246
  timestamp: 1602638655
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     30 |          782.555 | 4853760 |  250.421 |              300.414 |              148.141 |             797.35 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3404.9504240052183
    time_step_min: 3077
  date: 2020-10-14_01-24-41
  done: false
  episode_len_mean: 796.4544865565274
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 251.00721991538325
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 6174
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.609478568037351
        entropy_coeff: 0.0005000000000000001
        kl: 0.005292494820120434
        model: {}
        policy_loss: -0.010638982135181626
        total_loss: 7.736140251159668
        vf_explained_var: 0.9807620048522949
        vf_loss: 7.746819416681926
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.76
    gpu_util_percent0: 0.35700000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14989542173237902
    mean_env_wait_ms: 1.2094512826741122
    mean_inference_ms: 4.519923318998076
    mean_raw_obs_processing_ms: 0.3927891459306913
  time_since_restore: 808.7786686420441
  time_this_iter_s: 26.22413945198059
  time_total_s: 808.7786686420441
  timers:
    learn_throughput: 8587.098
    learn_time_ms: 18841.289
    sample_throughput: 22979.896
    sample_time_ms: 7040.589
    update_time_ms: 31.788
  timestamp: 1602638681
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     31 |          808.779 | 5015552 |  251.007 |              300.414 |              148.141 |            796.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3398.488657551274
    time_step_min: 3077
  date: 2020-10-14_01-25-08
  done: false
  episode_len_mean: 794.938561284347
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 251.91502552539913
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 304
  episodes_total: 6478
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5693186322848002
        entropy_coeff: 0.0005000000000000001
        kl: 0.005235559811505179
        model: {}
        policy_loss: -0.009933298317870745
        total_loss: 12.006545066833496
        vf_explained_var: 0.9790152907371521
        vf_loss: 12.016501188278198
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.280645161290323
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14975445658679043
    mean_env_wait_ms: 1.2106242625332277
    mean_inference_ms: 4.511078583862634
    mean_raw_obs_processing_ms: 0.3922478530408964
  time_since_restore: 835.3013830184937
  time_this_iter_s: 26.522714376449585
  time_total_s: 835.3013830184937
  timers:
    learn_throughput: 8569.989
    learn_time_ms: 18878.903
    sample_throughput: 22936.156
    sample_time_ms: 7054.016
    update_time_ms: 33.824
  timestamp: 1602638708
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     32 |          835.301 | 5177344 |  251.915 |              300.414 |              148.141 |            794.939 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3395.281922960267
    time_step_min: 3071
  date: 2020-10-14_01-25-34
  done: false
  episode_len_mean: 794.1461723930079
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 252.3927201490493
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 6636
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.582343578338623
        entropy_coeff: 0.0005000000000000001
        kl: 0.005434371608619888
        model: {}
        policy_loss: -0.009906434977892786
        total_loss: 6.680933952331543
        vf_explained_var: 0.9813895225524902
        vf_loss: 6.69085967540741
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.420000000000005
    gpu_util_percent0: 0.39033333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14968534303895487
    mean_env_wait_ms: 1.2111931279452939
    mean_inference_ms: 4.506777000663507
    mean_raw_obs_processing_ms: 0.3919844789782048
  time_since_restore: 861.2674939632416
  time_this_iter_s: 25.966110944747925
  time_total_s: 861.2674939632416
  timers:
    learn_throughput: 8570.809
    learn_time_ms: 18877.097
    sample_throughput: 23019.086
    sample_time_ms: 7028.602
    update_time_ms: 34.063
  timestamp: 1602638734
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     33 |          861.267 | 5339136 |  252.393 |              304.505 |              148.141 |            794.146 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3391.4129889298893
    time_step_min: 3071
  date: 2020-10-14_01-26-00
  done: false
  episode_len_mean: 793.2998386386973
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 252.95733779040222
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 181
  episodes_total: 6817
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5738237996896108
        entropy_coeff: 0.0005000000000000001
        kl: 0.005473659529040257
        model: {}
        policy_loss: -0.009085840865736827
        total_loss: 8.724187850952148
        vf_explained_var: 0.9792914390563965
        vf_loss: 8.73328693707784
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.273333333333333
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14960931836551872
    mean_env_wait_ms: 1.2118426862585603
    mean_inference_ms: 4.502101917397807
    mean_raw_obs_processing_ms: 0.39169292912734655
  time_since_restore: 887.061362028122
  time_this_iter_s: 25.79386806488037
  time_total_s: 887.061362028122
  timers:
    learn_throughput: 8578.418
    learn_time_ms: 18860.354
    sample_throughput: 23061.563
    sample_time_ms: 7015.656
    update_time_ms: 34.408
  timestamp: 1602638760
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     34 |          887.061 | 5500928 |  252.957 |              304.505 |              148.141 |              793.3 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3385.9376061120543
    time_step_min: 3071
  date: 2020-10-14_01-26-26
  done: false
  episode_len_mean: 791.9855133614627
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 253.80582193240426
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 293
  episodes_total: 7110
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5296166688203812
        entropy_coeff: 0.0005000000000000001
        kl: 0.005162264181611438
        model: {}
        policy_loss: -0.011187633450996751
        total_loss: 9.327556371688843
        vf_explained_var: 0.9825034737586975
        vf_loss: 9.33875060081482
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.483333333333334
    gpu_util_percent0: 0.37166666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14949211290162773
    mean_env_wait_ms: 1.2128160525702432
    mean_inference_ms: 4.494744573327526
    mean_raw_obs_processing_ms: 0.3912418844825424
  time_since_restore: 912.8444314002991
  time_this_iter_s: 25.783069372177124
  time_total_s: 912.8444314002991
  timers:
    learn_throughput: 8585.981
    learn_time_ms: 18843.74
    sample_throughput: 23127.879
    sample_time_ms: 6995.54
    update_time_ms: 34.335
  timestamp: 1602638786
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     35 |          912.844 | 5662720 |  253.806 |              304.505 |              148.141 |            791.986 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3383.206338223083
    time_step_min: 3071
  date: 2020-10-14_01-26-52
  done: false
  episode_len_mean: 791.3719042377545
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 254.2109009745224
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 7268
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5531562368075053
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056630732336392
        model: {}
        policy_loss: -0.009172938026798269
        total_loss: 7.078703244527181
        vf_explained_var: 0.9806365370750427
        vf_loss: 7.087869644165039
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.553333333333335
    gpu_util_percent0: 0.25966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1494321496046691
    mean_env_wait_ms: 1.213307470155845
    mean_inference_ms: 4.491018588639091
    mean_raw_obs_processing_ms: 0.39101172885413443
  time_since_restore: 938.9774725437164
  time_this_iter_s: 26.13304114341736
  time_total_s: 938.9774725437164
  timers:
    learn_throughput: 8580.908
    learn_time_ms: 18854.882
    sample_throughput: 23062.353
    sample_time_ms: 7015.416
    update_time_ms: 33.424
  timestamp: 1602638812
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     36 |          938.977 | 5824512 |  254.211 |              304.505 |              148.141 |            791.372 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3379.571044133477
    time_step_min: 3071
  date: 2020-10-14_01-27-19
  done: false
  episode_len_mean: 790.5432164838105
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 254.79522817146585
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 206
  episodes_total: 7474
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5319242378075918
        entropy_coeff: 0.0005000000000000001
        kl: 0.005267381357649962
        model: {}
        policy_loss: -0.009007454413222149
        total_loss: 8.483049949010214
        vf_explained_var: 0.981417179107666
        vf_loss: 8.49205986658732
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.169999999999998
    gpu_util_percent0: 0.37700000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1493592054667995
    mean_env_wait_ms: 1.2139605934542235
    mean_inference_ms: 4.48650353460763
    mean_raw_obs_processing_ms: 0.39073145245357466
  time_since_restore: 965.2479031085968
  time_this_iter_s: 26.27043056488037
  time_total_s: 965.2479031085968
  timers:
    learn_throughput: 8573.172
    learn_time_ms: 18871.894
    sample_throughput: 22983.673
    sample_time_ms: 7039.432
    update_time_ms: 33.45
  timestamp: 1602638839
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     37 |          965.248 | 5986304 |  254.795 |              304.505 |              148.141 |            790.543 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3374.3127272727274
    time_step_min: 3071
  date: 2020-10-14_01-27-45
  done: false
  episode_len_mean: 789.5503745802118
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 255.5712733117797
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 268
  episodes_total: 7742
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49097402890523273
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049834261105085416
        model: {}
        policy_loss: -0.0085185122055312
        total_loss: 6.79603111743927
        vf_explained_var: 0.986015796661377
        vf_loss: 6.804545998573303
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.387096774193548
    gpu_util_percent0: 0.35290322580645167
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14926378273989
    mean_env_wait_ms: 1.2147168178481413
    mean_inference_ms: 4.480551641443131
    mean_raw_obs_processing_ms: 0.3903668865769254
  time_since_restore: 991.2106754779816
  time_this_iter_s: 25.962772369384766
  time_total_s: 991.2106754779816
  timers:
    learn_throughput: 8566.202
    learn_time_ms: 18887.251
    sample_throughput: 23031.384
    sample_time_ms: 7024.849
    update_time_ms: 31.548
  timestamp: 1602638865
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     38 |          991.211 | 6148096 |  255.571 |              306.323 |              148.141 |             789.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3371.3611606006616
    time_step_min: 3071
  date: 2020-10-14_01-28-11
  done: false
  episode_len_mean: 789.0391139240506
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 255.9956143715638
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 7900
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5179962168137232
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053879035403952
        model: {}
        policy_loss: -0.012518745080645507
        total_loss: 5.8687297105789185
        vf_explained_var: 0.9830215573310852
        vf_loss: 5.881372769673665
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.486666666666665
    gpu_util_percent0: 0.258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14921147348966457
    mean_env_wait_ms: 1.2151429667718534
    mean_inference_ms: 4.477291934484746
    mean_raw_obs_processing_ms: 0.39016461069893094
  time_since_restore: 1017.4234492778778
  time_this_iter_s: 26.21277379989624
  time_total_s: 1017.4234492778778
  timers:
    learn_throughput: 8542.779
    learn_time_ms: 18939.037
    sample_throughput: 23044.771
    sample_time_ms: 7020.768
    update_time_ms: 32.85
  timestamp: 1602638891
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     39 |          1017.42 | 6309888 |  255.996 |              306.323 |              148.141 |            789.039 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3367.3386099431114
    time_step_min: 3071
  date: 2020-10-14_01-28-37
  done: false
  episode_len_mean: 788.2563976377953
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 256.63211594289356
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 8128
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.49680029104153317
        entropy_coeff: 0.0005000000000000001
        kl: 0.00554760933543245
        model: {}
        policy_loss: -0.010252086280767495
        total_loss: 7.7728356917699175
        vf_explained_var: 0.9832698702812195
        vf_loss: 7.783197442690532
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.580000000000002
    gpu_util_percent0: 0.38266666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491387307244752
    mean_env_wait_ms: 1.2157460341966702
    mean_inference_ms: 4.472774358922388
    mean_raw_obs_processing_ms: 0.3898790074863043
  time_since_restore: 1043.405723810196
  time_this_iter_s: 25.982274532318115
  time_total_s: 1043.405723810196
  timers:
    learn_throughput: 8552.595
    learn_time_ms: 18917.3
    sample_throughput: 22963.172
    sample_time_ms: 7045.716
    update_time_ms: 34.277
  timestamp: 1602638917
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     40 |          1043.41 | 6471680 |  256.632 |              306.323 |              148.141 |            788.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3362.974915986558
    time_step_min: 3071
  date: 2020-10-14_01-29-03
  done: false
  episode_len_mean: 787.4352758538333
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 257.2809767124312
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 8374
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4574356699983279
        entropy_coeff: 0.0005000000000000001
        kl: 0.004904644874234994
        model: {}
        policy_loss: -0.008429468813119456
        total_loss: 5.771370013554891
        vf_explained_var: 0.9868906140327454
        vf_loss: 5.779905597368876
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.009999999999994
    gpu_util_percent0: 0.2783333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1490631974619325
    mean_env_wait_ms: 1.2163617915092726
    mean_inference_ms: 4.46809557656374
    mean_raw_obs_processing_ms: 0.38959141639714334
  time_since_restore: 1069.3552060127258
  time_this_iter_s: 25.949482202529907
  time_total_s: 1069.3552060127258
  timers:
    learn_throughput: 8557.159
    learn_time_ms: 18907.209
    sample_throughput: 23020.481
    sample_time_ms: 7028.176
    update_time_ms: 33.822
  timestamp: 1602638943
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     41 |          1069.36 | 6633472 |  257.281 |              306.323 |              148.141 |            787.435 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3360.0226148409893
    time_step_min: 3036
  date: 2020-10-14_01-29-30
  done: false
  episode_len_mean: 786.9049460853258
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 257.7013773458921
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 8532
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.48722415914138156
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051285685040056705
        model: {}
        policy_loss: -0.009884546627290547
        total_loss: 5.357740481694539
        vf_explained_var: 0.9837141036987305
        vf_loss: 5.367804487546285
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43
    gpu_util_percent0: 0.331
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14901665701712888
    mean_env_wait_ms: 1.2167351688460712
    mean_inference_ms: 4.4652178964909925
    mean_raw_obs_processing_ms: 0.389411021952246
  time_since_restore: 1095.3842585086823
  time_this_iter_s: 26.02905249595642
  time_total_s: 1095.3842585086823
  timers:
    learn_throughput: 8567.003
    learn_time_ms: 18885.485
    sample_throughput: 23112.865
    sample_time_ms: 7000.084
    update_time_ms: 33.752
  timestamp: 1602638970
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     42 |          1095.38 | 6795264 |  257.701 |              306.626 |              148.141 |            786.905 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3355.575002856164
    time_step_min: 3036
  date: 2020-10-14_01-29-56
  done: false
  episode_len_mean: 786.059806708357
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 258.3516920196853
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 263
  episodes_total: 8795
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.46071107933918637
        entropy_coeff: 0.0005000000000000001
        kl: 0.004932920370871822
        model: {}
        policy_loss: -0.007133805193006992
        total_loss: 9.12200411160787
        vf_explained_var: 0.9816290736198425
        vf_loss: 9.129306475321451
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.610000000000003
    gpu_util_percent0: 0.3496666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489436062170461
    mean_env_wait_ms: 1.217340154711578
    mean_inference_ms: 4.460620523566059
    mean_raw_obs_processing_ms: 0.38912041258756547
  time_since_restore: 1121.271556854248
  time_this_iter_s: 25.887298345565796
  time_total_s: 1121.271556854248
  timers:
    learn_throughput: 8569.584
    learn_time_ms: 18879.797
    sample_throughput: 23123.273
    sample_time_ms: 6996.933
    update_time_ms: 33.321
  timestamp: 1602638996
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     43 |          1121.27 | 6957056 |  258.352 |              306.626 |              148.141 |             786.06 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3352.261490406069
    time_step_min: 3036
  date: 2020-10-14_01-30-22
  done: false
  episode_len_mean: 785.4203864090606
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 258.858865133682
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 211
  episodes_total: 9006
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.43047382185856503
        entropy_coeff: 0.0005000000000000001
        kl: 0.005120153616492947
        model: {}
        policy_loss: -0.01005569346170887
        total_loss: 5.529540300369263
        vf_explained_var: 0.9862681031227112
        vf_loss: 5.539779146512349
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.183870967741942
    gpu_util_percent0: 0.3196774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488850416130323
    mean_env_wait_ms: 1.2178112802081196
    mean_inference_ms: 4.457072614236185
    mean_raw_obs_processing_ms: 0.3889017885347404
  time_since_restore: 1147.3751080036163
  time_this_iter_s: 26.103551149368286
  time_total_s: 1147.3751080036163
  timers:
    learn_throughput: 8561.954
    learn_time_ms: 18896.622
    sample_throughput: 23079.796
    sample_time_ms: 7010.114
    update_time_ms: 33.244
  timestamp: 1602639022
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     44 |          1147.38 | 7118848 |  258.859 |              306.626 |              148.141 |             785.42 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3349.7587151940365
    time_step_min: 3036
  date: 2020-10-14_01-30-49
  done: false
  episode_len_mean: 784.9489305979921
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 259.2045167960708
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 9164
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.4585626423358917
        entropy_coeff: 0.0005000000000000001
        kl: 0.005443395425875981
        model: {}
        policy_loss: -0.010675911258052414
        total_loss: 5.313385645548503
        vf_explained_var: 0.9842596650123596
        vf_loss: 5.324256658554077
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.466666666666665
    gpu_util_percent0: 0.3463333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14884366525315032
    mean_env_wait_ms: 1.2181439585352973
    mean_inference_ms: 4.454508082712899
    mean_raw_obs_processing_ms: 0.3887403499449809
  time_since_restore: 1173.5743465423584
  time_this_iter_s: 26.199238538742065
  time_total_s: 1173.5743465423584
  timers:
    learn_throughput: 8550.836
    learn_time_ms: 18921.191
    sample_throughput: 23027.343
    sample_time_ms: 7026.082
    update_time_ms: 33.587
  timestamp: 1602639049
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     45 |          1173.57 | 7280640 |  259.205 |              306.626 |              148.141 |            784.949 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3345.5995323626316
    time_step_min: 3024
  date: 2020-10-14_01-31-15
  done: false
  episode_len_mean: 784.1214686276585
  episode_reward_max: 308.44444444444434
  episode_reward_mean: 259.84969897899754
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 287
  episodes_total: 9451
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.4285256241758664
        entropy_coeff: 0.0005000000000000001
        kl: 0.005139084688077371
        model: {}
        policy_loss: -0.010109816081239842
        total_loss: 8.411476532618204
        vf_explained_var: 0.9839906096458435
        vf_loss: 8.421768506368002
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07096774193549
    gpu_util_percent0: 0.3493548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1487711850350272
    mean_env_wait_ms: 1.2187289930015355
    mean_inference_ms: 4.449991970556274
    mean_raw_obs_processing_ms: 0.3884569572431
  time_since_restore: 1199.7403364181519
  time_this_iter_s: 26.165989875793457
  time_total_s: 1199.7403364181519
  timers:
    learn_throughput: 8547.401
    learn_time_ms: 18928.796
    sample_throughput: 23049.823
    sample_time_ms: 7019.229
    update_time_ms: 35.039
  timestamp: 1602639075
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     46 |          1199.74 | 7442432 |   259.85 |              308.444 |              148.141 |            784.121 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3343.2197790746145
    time_step_min: 3024
  date: 2020-10-14_01-31-41
  done: false
  episode_len_mean: 783.603755965968
  episode_reward_max: 308.44444444444434
  episode_reward_mean: 260.20273077318114
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 187
  episodes_total: 9638
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.4119056686758995
        entropy_coeff: 0.0005000000000000001
        kl: 0.004877317541589339
        model: {}
        policy_loss: -0.00912873458582908
        total_loss: 5.750106652577718
        vf_explained_var: 0.9852020144462585
        vf_loss: 5.759410897890727
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.706666666666663
    gpu_util_percent0: 0.359
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14872640991231179
    mean_env_wait_ms: 1.21910725730994
    mean_inference_ms: 4.447231490634355
    mean_raw_obs_processing_ms: 0.38828532788577247
  time_since_restore: 1225.9505741596222
  time_this_iter_s: 26.210237741470337
  time_total_s: 1225.9505741596222
  timers:
    learn_throughput: 8552.505
    learn_time_ms: 18917.498
    sample_throughput: 23032.906
    sample_time_ms: 7024.385
    update_time_ms: 34.127
  timestamp: 1602639101
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     47 |          1225.95 | 7604224 |  260.203 |              308.444 |              148.141 |            783.604 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3341.1566536805412
    time_step_min: 3024
  date: 2020-10-14_01-32-07
  done: false
  episode_len_mean: 783.1785422621479
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 260.51501746744714
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 9796
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.43464770168066025
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052989632822573185
        model: {}
        policy_loss: -0.011595062193615982
        total_loss: 6.020051558812459
        vf_explained_var: 0.982515275478363
        vf_loss: 6.03184727827708
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.64
    gpu_util_percent0: 0.2833333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14868935506164999
    mean_env_wait_ms: 1.2194081508777739
    mean_inference_ms: 4.444929465332364
    mean_raw_obs_processing_ms: 0.3881399786571089
  time_since_restore: 1251.763727426529
  time_this_iter_s: 25.81315326690674
  time_total_s: 1251.763727426529
  timers:
    learn_throughput: 8553.319
    learn_time_ms: 18915.698
    sample_throughput: 23055.973
    sample_time_ms: 7017.357
    update_time_ms: 35.835
  timestamp: 1602639127
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     48 |          1251.76 | 7766016 |  260.515 |              311.778 |              148.141 |            783.179 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3337.640946604355
    time_step_min: 3024
  date: 2020-10-14_01-32-33
  done: false
  episode_len_mean: 782.4769779186058
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 261.04423680312397
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 303
  episodes_total: 10099
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.39842069894075394
        entropy_coeff: 0.0005000000000000001
        kl: 0.004988226729134719
        model: {}
        policy_loss: -0.008534401912280979
        total_loss: 9.333666324615479
        vf_explained_var: 0.983616054058075
        vf_loss: 9.342384417851767
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.39666666666667
    gpu_util_percent0: 0.3456666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486205727109424
    mean_env_wait_ms: 1.219969030643505
    mean_inference_ms: 4.440673607845839
    mean_raw_obs_processing_ms: 0.38787310184799845
  time_since_restore: 1277.6150677204132
  time_this_iter_s: 25.851340293884277
  time_total_s: 1277.6150677204132
  timers:
    learn_throughput: 8572.181
    learn_time_ms: 18874.077
    sample_throughput: 23036.763
    sample_time_ms: 7023.209
    update_time_ms: 34.351
  timestamp: 1602639153
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     49 |          1277.62 | 7927808 |  261.044 |              311.778 |              148.141 |            782.477 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3335.3886390301136
    time_step_min: 3024
  date: 2020-10-14_01-32-59
  done: false
  episode_len_mean: 782.1044790652386
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 261.363980604487
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 10270
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3967706337571144
        entropy_coeff: 0.0005000000000000001
        kl: 0.005579448809536795
        model: {}
        policy_loss: -0.008805945467126245
        total_loss: 5.692355473836263
        vf_explained_var: 0.984670102596283
        vf_loss: 5.701350967089335
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.393333333333334
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14858378917679033
    mean_env_wait_ms: 1.2202738284172499
    mean_inference_ms: 4.438358559473198
    mean_raw_obs_processing_ms: 0.3877298079868187
  time_since_restore: 1303.3813514709473
  time_this_iter_s: 25.766283750534058
  time_total_s: 1303.3813514709473
  timers:
    learn_throughput: 8572.399
    learn_time_ms: 18873.597
    sample_throughput: 23107.611
    sample_time_ms: 7001.676
    update_time_ms: 35.117
  timestamp: 1602639179
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     50 |          1303.38 | 8089600 |  261.364 |              311.778 |              148.141 |            782.104 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3333.4857060352297
    time_step_min: 3024
  date: 2020-10-14_01-33-26
  done: false
  episode_len_mean: 781.7468123861566
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 261.63259573009367
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 161
  episodes_total: 10431
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.4086659401655197
        entropy_coeff: 0.0005000000000000001
        kl: 0.00538623674462239
        model: {}
        policy_loss: -0.009682931665641567
        total_loss: 5.9207688967386884
        vf_explained_var: 0.9842954277992249
        vf_loss: 5.930647770563762
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94333333333333
    gpu_util_percent0: 0.2903333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14855011013161126
    mean_env_wait_ms: 1.2205525664405452
    mean_inference_ms: 4.436254133679374
    mean_raw_obs_processing_ms: 0.3875981619023499
  time_since_restore: 1329.4011814594269
  time_this_iter_s: 26.019829988479614
  time_total_s: 1329.4011814594269
  timers:
    learn_throughput: 8568.52
    learn_time_ms: 18882.14
    sample_throughput: 23121.638
    sample_time_ms: 6997.428
    update_time_ms: 36.848
  timestamp: 1602639206
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     51 |           1329.4 | 8251392 |  261.633 |              311.778 |              148.141 |            781.747 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3329.986912218379
    time_step_min: 3024
  date: 2020-10-14_01-33-52
  done: false
  episode_len_mean: 781.1074587950461
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 262.160636065469
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 308
  episodes_total: 10739
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.37412210553884506
        entropy_coeff: 0.0005000000000000001
        kl: 0.005711305304430425
        model: {}
        policy_loss: -0.00832333512759457
        total_loss: 7.988898714383443
        vf_explained_var: 0.9857848286628723
        vf_loss: 7.997400124867757
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65
    gpu_util_percent0: 0.29766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14848610542594612
    mean_env_wait_ms: 1.2210620427560999
    mean_inference_ms: 4.432290952877455
    mean_raw_obs_processing_ms: 0.38734971358383974
  time_since_restore: 1355.3588020801544
  time_this_iter_s: 25.95762062072754
  time_total_s: 1355.3588020801544
  timers:
    learn_throughput: 8563.747
    learn_time_ms: 18892.666
    sample_throughput: 23183.044
    sample_time_ms: 6978.894
    update_time_ms: 36.782
  timestamp: 1602639232
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     52 |          1355.36 | 8413184 |  262.161 |              311.778 |              148.141 |            781.107 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3328.094290976059
    time_step_min: 3024
  date: 2020-10-14_01-34-18
  done: false
  episode_len_mean: 780.8067327095946
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 262.4718400293524
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 10902
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.38097796340783435
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049072811380028725
        model: {}
        policy_loss: -0.009647895698435605
        total_loss: 4.56990373134613
        vf_explained_var: 0.9864748120307922
        vf_loss: 4.579734484354655
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.166666666666664
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14845494710382257
    mean_env_wait_ms: 1.2213275305385318
    mean_inference_ms: 4.430295136426717
    mean_raw_obs_processing_ms: 0.38722705140146885
  time_since_restore: 1381.1872596740723
  time_this_iter_s: 25.828457593917847
  time_total_s: 1381.1872596740723
  timers:
    learn_throughput: 8559.605
    learn_time_ms: 18901.807
    sample_throughput: 23235.074
    sample_time_ms: 6963.266
    update_time_ms: 37.031
  timestamp: 1602639258
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     53 |          1381.19 | 8574976 |  262.472 |              311.778 |              148.141 |            780.807 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3326.0176854707056
    time_step_min: 3024
  date: 2020-10-14_01-34-44
  done: false
  episode_len_mean: 780.5256595590893
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 262.76309170490595
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 11068
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.38549116005500156
        entropy_coeff: 0.0005000000000000001
        kl: 0.005399355975290139
        model: {}
        policy_loss: -0.00958065803812739
        total_loss: 6.490892767906189
        vf_explained_var: 0.9827330708503723
        vf_loss: 6.500661730766296
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78333333333334
    gpu_util_percent0: 0.36233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484240593019163
    mean_env_wait_ms: 1.2215902442554236
    mean_inference_ms: 4.428341967312145
    mean_raw_obs_processing_ms: 0.38710591565976676
  time_since_restore: 1407.0107939243317
  time_this_iter_s: 25.8235342502594
  time_total_s: 1407.0107939243317
  timers:
    learn_throughput: 8565.293
    learn_time_ms: 18889.256
    sample_throughput: 23312.475
    sample_time_ms: 6940.147
    update_time_ms: 35.338
  timestamp: 1602639284
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     54 |          1407.01 | 8736768 |  262.763 |              311.778 |              148.141 |            780.526 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3322.6696373422747
    time_step_min: 3024
  date: 2020-10-14_01-35-10
  done: false
  episode_len_mean: 780.0428131868132
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 263.27888999889007
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 307
  episodes_total: 11375
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.352639839053154
        entropy_coeff: 0.0005000000000000001
        kl: 0.004577385921341677
        model: {}
        policy_loss: -0.0089119218828273
        total_loss: 7.640864094098409
        vf_explained_var: 0.9857537150382996
        vf_loss: 7.649948636690776
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.573333333333338
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14836624076989227
    mean_env_wait_ms: 1.2220534480040655
    mean_inference_ms: 4.424725503944226
    mean_raw_obs_processing_ms: 0.38688179957069496
  time_since_restore: 1432.8491387367249
  time_this_iter_s: 25.83834481239319
  time_total_s: 1432.8491387367249
  timers:
    learn_throughput: 8570.95
    learn_time_ms: 18876.786
    sample_throughput: 23423.07
    sample_time_ms: 6907.378
    update_time_ms: 34.697
  timestamp: 1602639310
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     55 |          1432.85 | 8898560 |  263.279 |              311.778 |              148.141 |            780.043 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3320.749216846502
    time_step_min: 3024
  date: 2020-10-14_01-35-36
  done: false
  episode_len_mean: 779.7830761227675
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 263.55831069495025
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 11534
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.36082518845796585
        entropy_coeff: 0.0005000000000000001
        kl: 0.005253409151919186
        model: {}
        policy_loss: -0.007194112098053059
        total_loss: 5.2075207233428955
        vf_explained_var: 0.9848111271858215
        vf_loss: 5.214893341064453
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.54137931034483
    gpu_util_percent0: 0.34931034482758627
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483376114137538
    mean_env_wait_ms: 1.222278083976105
    mean_inference_ms: 4.422924392542069
    mean_raw_obs_processing_ms: 0.3867697200125719
  time_since_restore: 1458.4593758583069
  time_this_iter_s: 25.61023712158203
  time_total_s: 1458.4593758583069
  timers:
    learn_throughput: 8581.238
    learn_time_ms: 18854.155
    sample_throughput: 23539.658
    sample_time_ms: 6873.167
    update_time_ms: 34.87
  timestamp: 1602639336
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     56 |          1458.46 | 9060352 |  263.558 |              315.414 |              148.141 |            779.783 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3318.733390484355
    time_step_min: 3024
  date: 2020-10-14_01-36-02
  done: false
  episode_len_mean: 779.5961390620996
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 263.86238916024524
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 11707
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.36391862233479816
        entropy_coeff: 0.0005000000000000001
        kl: 0.005209354994197686
        model: {}
        policy_loss: -0.009775459048493454
        total_loss: 5.793869376182556
        vf_explained_var: 0.9854834079742432
        vf_loss: 5.80382486184438
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.519999999999996
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14830807274131963
    mean_env_wait_ms: 1.222527629969857
    mean_inference_ms: 4.4210876093575715
    mean_raw_obs_processing_ms: 0.38665570388864484
  time_since_restore: 1484.3650097846985
  time_this_iter_s: 25.9056339263916
  time_total_s: 1484.3650097846985
  timers:
    learn_throughput: 8578.639
    learn_time_ms: 18859.869
    sample_throughput: 23670.356
    sample_time_ms: 6835.216
    update_time_ms: 35.622
  timestamp: 1602639362
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     57 |          1484.37 | 9222144 |  263.862 |              315.414 |              148.141 |            779.596 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3315.3643961554535
    time_step_min: 3024
  date: 2020-10-14_01-36-28
  done: false
  episode_len_mean: 779.2465228616641
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 264.3666085355934
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 300
  episodes_total: 12007
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.33325885236263275
        entropy_coeff: 0.0005000000000000001
        kl: 0.004892570781521499
        model: {}
        policy_loss: -0.009160817047813907
        total_loss: 6.927696585655212
        vf_explained_var: 0.9869993329048157
        vf_loss: 6.9370220104853315
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333333
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14825655128512133
    mean_env_wait_ms: 1.2229219574594863
    mean_inference_ms: 4.417811420415435
    mean_raw_obs_processing_ms: 0.38645201219238956
  time_since_restore: 1510.2485222816467
  time_this_iter_s: 25.883512496948242
  time_total_s: 1510.2485222816467
  timers:
    learn_throughput: 8576.409
    learn_time_ms: 18864.771
    sample_throughput: 23667.46
    sample_time_ms: 6836.053
    update_time_ms: 36.097
  timestamp: 1602639388
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     58 |          1510.25 | 9383936 |  264.367 |              315.414 |              148.141 |            779.247 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3313.7609699769055
    time_step_min: 3024
  date: 2020-10-14_01-36-54
  done: false
  episode_len_mean: 779.0914844649022
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 264.61368659469935
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 12166
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.35047547519207
        entropy_coeff: 0.0005000000000000001
        kl: 0.005249437099943559
        model: {}
        policy_loss: -0.010204963085319227
        total_loss: 5.445613503456116
        vf_explained_var: 0.9846398234367371
        vf_loss: 5.455992619196574
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.60689655172414
    gpu_util_percent0: 0.31344827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14823036810790513
    mean_env_wait_ms: 1.2231195655770237
    mean_inference_ms: 4.416156628240584
    mean_raw_obs_processing_ms: 0.3863491039454127
  time_since_restore: 1535.79172539711
  time_this_iter_s: 25.543203115463257
  time_total_s: 1535.79172539711
  timers:
    learn_throughput: 8580.668
    learn_time_ms: 18855.409
    sample_throughput: 23743.099
    sample_time_ms: 6814.275
    update_time_ms: 36.194
  timestamp: 1602639414
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     59 |          1535.79 | 9545728 |  264.614 |              315.414 |              148.141 |            779.091 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3312.001382226197
    time_step_min: 3024
  date: 2020-10-14_01-37-20
  done: false
  episode_len_mean: 778.980228506604
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 264.8945872303786
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 12341
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3581472461422284
        entropy_coeff: 0.0005000000000000001
        kl: 0.005312831800741454
        model: {}
        policy_loss: -0.009652167519864937
        total_loss: 5.827342351277669
        vf_explained_var: 0.9856369495391846
        vf_loss: 5.837172667185466
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.223333333333336
    gpu_util_percent0: 0.35066666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14820285515507556
    mean_env_wait_ms: 1.223335303862696
    mean_inference_ms: 4.414436535916254
    mean_raw_obs_processing_ms: 0.38624252477787513
  time_since_restore: 1561.4430899620056
  time_this_iter_s: 25.65136456489563
  time_total_s: 1561.4430899620056
  timers:
    learn_throughput: 8583.408
    learn_time_ms: 18849.39
    sample_throughput: 23759.559
    sample_time_ms: 6809.554
    update_time_ms: 34.549
  timestamp: 1602639440
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     60 |          1561.44 | 9707520 |  264.895 |              315.414 |              148.141 |             778.98 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3308.945379485551
    time_step_min: 3024
  date: 2020-10-14_01-37-46
  done: false
  episode_len_mean: 778.8282164899509
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 265.3624454706905
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 297
  episodes_total: 12638
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.32700370997190475
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520384400772552
        model: {}
        policy_loss: -0.008895810936034346
        total_loss: 6.448522210121155
        vf_explained_var: 0.9879592061042786
        vf_loss: 6.4575806856155396
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.944827586206895
    gpu_util_percent0: 0.33482758620689657
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14815580714627966
    mean_env_wait_ms: 1.223675273815625
    mean_inference_ms: 4.411457637051063
    mean_raw_obs_processing_ms: 0.38605785323152697
  time_since_restore: 1586.9879307746887
  time_this_iter_s: 25.544840812683105
  time_total_s: 1586.9879307746887
  timers:
    learn_throughput: 8601.684
    learn_time_ms: 18809.341
    sample_throughput: 23783.066
    sample_time_ms: 6802.823
    update_time_ms: 32.458
  timestamp: 1602639466
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     61 |          1586.99 | 9869312 |  265.362 |              315.414 |              148.141 |            778.828 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3307.315067419254
    time_step_min: 3024
  date: 2020-10-14_01-38-12
  done: false
  episode_len_mean: 778.7503516174402
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 265.59788224485845
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 160
  episodes_total: 12798
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3362369438012441
        entropy_coeff: 0.0005000000000000001
        kl: 0.00530867965426296
        model: {}
        policy_loss: -0.01086225847636039
        total_loss: 3.9687401254971824
        vf_explained_var: 0.9888959527015686
        vf_loss: 3.9797699650128684
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.873333333333335
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813181845363524
    mean_env_wait_ms: 1.2238491347192209
    mean_inference_ms: 4.409923405387323
    mean_raw_obs_processing_ms: 0.38596337969564803
  time_since_restore: 1612.7557859420776
  time_this_iter_s: 25.767855167388916
  time_total_s: 1612.7557859420776
  timers:
    learn_throughput: 8619.377
    learn_time_ms: 18770.731
    sample_throughput: 23719.271
    sample_time_ms: 6821.12
    update_time_ms: 32.661
  timestamp: 1602639492
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     62 |          1612.76 | 10031104 |  265.598 |              315.414 |              148.141 |             778.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3305.4487744529497
    time_step_min: 3024
  date: 2020-10-14_01-38-38
  done: false
  episode_len_mean: 778.6287475915221
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 265.87828574765
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 177
  episodes_total: 12975
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.34269552926222485
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053061171201989055
        model: {}
        policy_loss: -0.010365672062713807
        total_loss: 4.789053201675415
        vf_explained_var: 0.9874215722084045
        vf_loss: 4.79958959420522
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.693103448275863
    gpu_util_percent0: 0.3213793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275853
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14810554703210047
    mean_env_wait_ms: 1.2240411428638058
    mean_inference_ms: 4.40830789353993
    mean_raw_obs_processing_ms: 0.38586322040127136
  time_since_restore: 1638.300901889801
  time_this_iter_s: 25.54511594772339
  time_total_s: 1638.300901889801
  timers:
    learn_throughput: 8633.714
    learn_time_ms: 18739.56
    sample_throughput: 23710.587
    sample_time_ms: 6823.618
    update_time_ms: 30.755
  timestamp: 1602639518
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     63 |           1638.3 | 10192896 |  265.878 |              315.414 |              148.141 |            778.629 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3302.5839256010886
    time_step_min: 3024
  date: 2020-10-14_01-39-04
  done: false
  episode_len_mean: 778.4318661441062
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 266.31922404631183
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 293
  episodes_total: 13268
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3090958495934804
        entropy_coeff: 0.0005000000000000001
        kl: 0.004946761842196186
        model: {}
        policy_loss: -0.009172378049697727
        total_loss: 7.741289019584656
        vf_explained_var: 0.9857645630836487
        vf_loss: 7.750615358352661
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.363333333333333
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148063966531333
    mean_env_wait_ms: 1.2243297418374173
    mean_inference_ms: 4.405604600795731
    mean_raw_obs_processing_ms: 0.3856983083099701
  time_since_restore: 1663.9559922218323
  time_this_iter_s: 25.65509033203125
  time_total_s: 1663.9559922218323
  timers:
    learn_throughput: 8637.471
    learn_time_ms: 18731.408
    sample_throughput: 23721.435
    sample_time_ms: 6820.498
    update_time_ms: 31.901
  timestamp: 1602639544
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     64 |          1663.96 | 10354688 |  266.319 |              315.414 |              148.141 |            778.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3301.0759635494474
    time_step_min: 3024
  date: 2020-10-14_01-39-30
  done: false
  episode_len_mean: 778.3418466120626
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 266.5508999150102
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 13430
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.31657233089208603
        entropy_coeff: 0.0005000000000000001
        kl: 0.004489539540372789
        model: {}
        policy_loss: -0.009044326672058864
        total_loss: 5.1394115289052325
        vf_explained_var: 0.9857456684112549
        vf_loss: 5.148613969484965
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.446666666666673
    gpu_util_percent0: 0.2986666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14804092549596448
    mean_env_wait_ms: 1.2244851464153743
    mean_inference_ms: 4.404162683013375
    mean_raw_obs_processing_ms: 0.3856086313948396
  time_since_restore: 1689.6925010681152
  time_this_iter_s: 25.73650884628296
  time_total_s: 1689.6925010681152
  timers:
    learn_throughput: 8641.319
    learn_time_ms: 18723.069
    sample_throughput: 23696.506
    sample_time_ms: 6827.673
    update_time_ms: 30.884
  timestamp: 1602639570
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     65 |          1689.69 | 10516480 |  266.551 |              315.414 |              148.141 |            778.342 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3299.356784660767
    time_step_min: 2995
  date: 2020-10-14_01-39-55
  done: false
  episode_len_mean: 778.2442287898839
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 266.81794566752666
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 13602
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.32824891805648804
        entropy_coeff: 0.0005000000000000001
        kl: 0.005106824372584621
        model: {}
        policy_loss: -0.011087989172665402
        total_loss: 4.441884398460388
        vf_explained_var: 0.988169252872467
        vf_loss: 4.453136285146077
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75
    gpu_util_percent0: 0.3736666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14801634225119722
    mean_env_wait_ms: 1.224648014461421
    mean_inference_ms: 4.402693915721968
    mean_raw_obs_processing_ms: 0.385518177658335
  time_since_restore: 1715.1994817256927
  time_this_iter_s: 25.506980657577515
  time_total_s: 1715.1994817256927
  timers:
    learn_throughput: 8655.631
    learn_time_ms: 18692.109
    sample_throughput: 23622.655
    sample_time_ms: 6849.018
    update_time_ms: 29.046
  timestamp: 1602639595
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     66 |           1715.2 | 10678272 |  266.818 |              315.414 |              148.141 |            778.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3296.671333718245
    time_step_min: 2995
  date: 2020-10-14_01-40-22
  done: false
  episode_len_mean: 778.0613037847172
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 267.2281586915348
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 296
  episodes_total: 13898
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.29735277593135834
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554468595112364
        model: {}
        policy_loss: -0.008563800603345348
        total_loss: 6.99308701356252
        vf_explained_var: 0.987199604511261
        vf_loss: 7.001799424489339
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.106896551724137
    gpu_util_percent0: 0.32758620689655177
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479781440313184
    mean_env_wait_ms: 1.2249048617243699
    mean_inference_ms: 4.400187171324292
    mean_raw_obs_processing_ms: 0.38536381407462655
  time_since_restore: 1740.8804194927216
  time_this_iter_s: 25.68093776702881
  time_total_s: 1740.8804194927216
  timers:
    learn_throughput: 8661.782
    learn_time_ms: 18678.836
    sample_throughput: 23651.955
    sample_time_ms: 6840.534
    update_time_ms: 27.773
  timestamp: 1602639622
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     67 |          1740.88 | 10840064 |  267.228 |              315.414 |              148.141 |            778.061 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3295.099928673324
    time_step_min: 2995
  date: 2020-10-14_01-40-48
  done: false
  episode_len_mean: 777.9766747262125
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 267.46460623874935
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 14062
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2976345916589101
        entropy_coeff: 0.0005000000000000001
        kl: 0.004579706466756761
        model: {}
        policy_loss: -0.011283939481169606
        total_loss: 4.580750783284505
        vf_explained_var: 0.9872176647186279
        vf_loss: 4.592183470726013
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43333333333334
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14795654711185247
    mean_env_wait_ms: 1.2250389642337431
    mean_inference_ms: 4.398840926818573
    mean_raw_obs_processing_ms: 0.3852801500917307
  time_since_restore: 1766.6400496959686
  time_this_iter_s: 25.75963020324707
  time_total_s: 1766.6400496959686
  timers:
    learn_throughput: 8667.022
    learn_time_ms: 18667.542
    sample_throughput: 23659.548
    sample_time_ms: 6838.339
    update_time_ms: 27.75
  timestamp: 1602639648
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     68 |          1766.64 | 11001856 |  267.465 |              315.414 |              148.141 |            777.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3293.4242125290675
    time_step_min: 2995
  date: 2020-10-14_01-41-14
  done: false
  episode_len_mean: 777.8486615611607
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 267.69856791763635
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 14233
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.31406886130571365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004998420171129207
        model: {}
        policy_loss: -0.008851290365176586
        total_loss: 5.732178886731465
        vf_explained_var: 0.9851000905036926
        vf_loss: 5.741187214851379
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.356666666666673
    gpu_util_percent0: 0.342
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479351656104738
    mean_env_wait_ms: 1.2251766403867803
    mean_inference_ms: 4.397515915535548
    mean_raw_obs_processing_ms: 0.38519736711787395
  time_since_restore: 1792.39053440094
  time_this_iter_s: 25.750484704971313
  time_total_s: 1792.39053440094
  timers:
    learn_throughput: 8664.495
    learn_time_ms: 18672.986
    sample_throughput: 23614.042
    sample_time_ms: 6851.517
    update_time_ms: 27.813
  timestamp: 1602639674
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     69 |          1792.39 | 11163648 |  267.699 |              315.414 |              148.141 |            777.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3290.6533655505696
    time_step_min: 2995
  date: 2020-10-14_01-41-40
  done: false
  episode_len_mean: 777.6519584222482
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.11284664640493
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 294
  episodes_total: 14527
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2854648754000664
        entropy_coeff: 0.0005000000000000001
        kl: 0.004360687259274225
        model: {}
        policy_loss: -0.01067612434659774
        total_loss: 6.5105700095494585
        vf_explained_var: 0.9877765774726868
        vf_loss: 6.52138872941335
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.206666666666667
    gpu_util_percent0: 0.36099999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.796666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478980382138306
    mean_env_wait_ms: 1.2253996935972737
    mean_inference_ms: 4.3952019415638715
    mean_raw_obs_processing_ms: 0.3850506593378141
  time_since_restore: 1818.0938761234283
  time_this_iter_s: 25.703341722488403
  time_total_s: 1818.0938761234283
  timers:
    learn_throughput: 8659.533
    learn_time_ms: 18683.687
    sample_throughput: 23642.984
    sample_time_ms: 6843.129
    update_time_ms: 27.109
  timestamp: 1602639700
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     70 |          1818.09 | 11325440 |  268.113 |              315.414 |              148.141 |            777.652 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3289.0803985803987
    time_step_min: 2995
  date: 2020-10-14_01-42-05
  done: false
  episode_len_mean: 777.5599564448074
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.33892827829135
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 14694
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.28460531185070675
        entropy_coeff: 0.0005000000000000001
        kl: 0.004675061209127307
        model: {}
        policy_loss: -0.006905456558646013
        total_loss: 4.242927551269531
        vf_explained_var: 0.988189160823822
        vf_loss: 4.249975363413493
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.844827586206897
    gpu_util_percent0: 0.35620689655172416
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787786361802532
    mean_env_wait_ms: 1.2255155592838283
    mean_inference_ms: 4.393931832407853
    mean_raw_obs_processing_ms: 0.38496906759969557
  time_since_restore: 1843.6694648265839
  time_this_iter_s: 25.575588703155518
  time_total_s: 1843.6694648265839
  timers:
    learn_throughput: 8659.828
    learn_time_ms: 18683.051
    sample_throughput: 23631.094
    sample_time_ms: 6846.572
    update_time_ms: 27.233
  timestamp: 1602639725
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     71 |          1843.67 | 11487232 |  268.339 |              315.414 |              148.141 |             777.56 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3287.544258534611
    time_step_min: 2995
  date: 2020-10-14_01-42-31
  done: false
  episode_len_mean: 777.483113562971
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.57426797577506
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 14864
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.30236978580554325
        entropy_coeff: 0.0005000000000000001
        kl: 0.004458291650128861
        model: {}
        policy_loss: -0.008076763845262272
        total_loss: 4.782263120015462
        vf_explained_var: 0.9872530102729797
        vf_loss: 4.790490984916687
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.93666666666667
    gpu_util_percent0: 0.3213333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785835637708292
    mean_env_wait_ms: 1.2256364285619286
    mean_inference_ms: 4.392734048191387
    mean_raw_obs_processing_ms: 0.3848917885995378
  time_since_restore: 1869.2342953681946
  time_this_iter_s: 25.564830541610718
  time_total_s: 1869.2342953681946
  timers:
    learn_throughput: 8661.772
    learn_time_ms: 18678.857
    sample_throughput: 23687.842
    sample_time_ms: 6830.171
    update_time_ms: 25.815
  timestamp: 1602639751
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     72 |          1869.23 | 11649024 |  268.574 |              315.414 |              148.141 |            777.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3284.9156578686247
    time_step_min: 2990
  date: 2020-10-14_01-42-57
  done: false
  episode_len_mean: 777.3542450029686
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.9728933906651
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 295
  episodes_total: 15159
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2736733357111613
        entropy_coeff: 0.0005000000000000001
        kl: 0.004125134243319432
        model: {}
        policy_loss: -0.008963234004719803
        total_loss: 5.995750387509664
        vf_explained_var: 0.9887940883636475
        vf_loss: 6.004850506782532
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.89
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478231334410866
    mean_env_wait_ms: 1.2258236346012195
    mean_inference_ms: 4.390548629348055
    mean_raw_obs_processing_ms: 0.3847500428319838
  time_since_restore: 1894.8275606632233
  time_this_iter_s: 25.593265295028687
  time_total_s: 1894.8275606632233
  timers:
    learn_throughput: 8663.211
    learn_time_ms: 18675.755
    sample_throughput: 23660.221
    sample_time_ms: 6838.144
    update_time_ms: 26.095
  timestamp: 1602639777
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     73 |          1894.83 | 11810816 |  268.973 |              315.414 |              148.141 |            777.354 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3283.4514524993456
    time_step_min: 2990
  date: 2020-10-14_01-43-23
  done: false
  episode_len_mean: 777.288855539606
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 269.20196088511375
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 15326
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2707260126868884
        entropy_coeff: 0.0005000000000000001
        kl: 0.004260215442627668
        model: {}
        policy_loss: -0.009617043571779504
        total_loss: 3.375151733557383
        vf_explained_var: 0.9905970692634583
        vf_loss: 3.3849041064580283
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.456666666666667
    gpu_util_percent0: 0.296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780416385357767
    mean_env_wait_ms: 1.2259256104892682
    mean_inference_ms: 4.389377699587206
    mean_raw_obs_processing_ms: 0.3846732979007472
  time_since_restore: 1920.5670759677887
  time_this_iter_s: 25.73951530456543
  time_total_s: 1920.5670759677887
  timers:
    learn_throughput: 8663.209
    learn_time_ms: 18675.758
    sample_throughput: 23627.844
    sample_time_ms: 6847.514
    update_time_ms: 24.867
  timestamp: 1602639803
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     74 |          1920.57 | 11972608 |  269.202 |              315.414 |              148.141 |            777.289 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3281.9438834951457
    time_step_min: 2990
  date: 2020-10-14_01-43-49
  done: false
  episode_len_mean: 777.2535502194681
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 269.4389460053674
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 15492
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.29079828908046085
        entropy_coeff: 0.0005000000000000001
        kl: 0.004924953216686845
        model: {}
        policy_loss: -0.01033506359817693
        total_loss: 4.022189676761627
        vf_explained_var: 0.988994300365448
        vf_loss: 4.032670080661774
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.076666666666668
    gpu_util_percent0: 0.3259999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778591573442182
    mean_env_wait_ms: 1.2260276868922315
    mean_inference_ms: 4.388273056461375
    mean_raw_obs_processing_ms: 0.38460129832418494
  time_since_restore: 1946.3695178031921
  time_this_iter_s: 25.802441835403442
  time_total_s: 1946.3695178031921
  timers:
    learn_throughput: 8666.968
    learn_time_ms: 18667.659
    sample_throughput: 23618.751
    sample_time_ms: 6850.15
    update_time_ms: 26.242
  timestamp: 1602639829
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     75 |          1946.37 | 12134400 |  269.439 |              315.414 |              148.141 |            777.254 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3279.5305331384634
    time_step_min: 2976
  date: 2020-10-14_01-44-15
  done: false
  episode_len_mean: 777.2401926611319
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 269.81499448506236
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 287
  episodes_total: 15779
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2678173507253329
        entropy_coeff: 0.0005000000000000001
        kl: 0.004497473322165509
        model: {}
        policy_loss: -0.007366475348438446
        total_loss: 6.172531088193257
        vf_explained_var: 0.9887259006500244
        vf_loss: 6.180031458536784
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.396666666666665
    gpu_util_percent0: 0.31599999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14775369070824826
    mean_env_wait_ms: 1.2261740602443274
    mean_inference_ms: 4.3862910274742
    mean_raw_obs_processing_ms: 0.384470762403194
  time_since_restore: 1971.9432258605957
  time_this_iter_s: 25.573708057403564
  time_total_s: 1971.9432258605957
  timers:
    learn_throughput: 8660.477
    learn_time_ms: 18681.651
    sample_throughput: 23646.468
    sample_time_ms: 6842.121
    update_time_ms: 25.727
  timestamp: 1602639855
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     76 |          1971.94 | 12296192 |  269.815 |              315.717 |              148.141 |             777.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3277.913546117115
    time_step_min: 2976
  date: 2020-10-14_01-44-41
  done: false
  episode_len_mean: 777.2010903622008
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.0572956029781
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 15958
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.25022794554630917
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041656779940240085
        model: {}
        policy_loss: -0.009161299113960316
        total_loss: 3.767794907093048
        vf_explained_var: 0.9899099469184875
        vf_loss: 3.777081330617269
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.024137931034485
    gpu_util_percent0: 0.3641379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14773515348544275
    mean_env_wait_ms: 1.226269414088913
    mean_inference_ms: 4.385130135084634
    mean_raw_obs_processing_ms: 0.3843932485615236
  time_since_restore: 1997.4045777320862
  time_this_iter_s: 25.46135187149048
  time_total_s: 1997.4045777320862
  timers:
    learn_throughput: 8670.541
    learn_time_ms: 18659.966
    sample_throughput: 23653.55
    sample_time_ms: 6840.072
    update_time_ms: 27.027
  timestamp: 1602639881
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     77 |           1997.4 | 12457984 |  270.057 |              315.717 |              148.141 |            777.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3276.4429104477613
    time_step_min: 2976
  date: 2020-10-14_01-45-07
  done: false
  episode_len_mean: 777.1418558491503
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.2705863999128
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 16122
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.2755880306164424
        entropy_coeff: 0.0005000000000000001
        kl: 0.00465339922811836
        model: {}
        policy_loss: -0.009429567047239592
        total_loss: 4.504302501678467
        vf_explained_var: 0.9873778820037842
        vf_loss: 4.513869802157084
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.986666666666668
    gpu_util_percent0: 0.3313333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147717935239854
    mean_env_wait_ms: 1.2263526271557361
    mean_inference_ms: 4.384100437930915
    mean_raw_obs_processing_ms: 0.38432491878324143
  time_since_restore: 2022.8773574829102
  time_this_iter_s: 25.472779750823975
  time_total_s: 2022.8773574829102
  timers:
    learn_throughput: 8689.339
    learn_time_ms: 18619.599
    sample_throughput: 23642.552
    sample_time_ms: 6843.254
    update_time_ms: 32.467
  timestamp: 1602639907
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     78 |          2022.88 | 12619776 |  270.271 |              315.717 |              148.141 |            777.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3274.2086797066013
    time_step_min: 2976
  date: 2020-10-14_01-45-33
  done: false
  episode_len_mean: 777.0912693573954
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.6076691805263
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 280
  episodes_total: 16402
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2577415630221367
        entropy_coeff: 0.0005000000000000001
        kl: 0.004504388198256493
        model: {}
        policy_loss: -0.008300847936576853
        total_loss: 6.0371244351069135
        vf_explained_var: 0.9887755513191223
        vf_loss: 6.0455542008082075
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.641379310344828
    gpu_util_percent0: 0.3241379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476886787360341
    mean_env_wait_ms: 1.2264736247929844
    mean_inference_ms: 4.3823076955225035
    mean_raw_obs_processing_ms: 0.38420284500937785
  time_since_restore: 2048.4562628269196
  time_this_iter_s: 25.5789053440094
  time_total_s: 2048.4562628269196
  timers:
    learn_throughput: 8693.867
    learn_time_ms: 18609.901
    sample_throughput: 23669.254
    sample_time_ms: 6835.535
    update_time_ms: 32.648
  timestamp: 1602639933
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     79 |          2048.46 | 12781568 |  270.608 |              315.717 |              148.141 |            777.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3272.632946579647
    time_step_min: 2976
  date: 2020-10-14_01-45-59
  done: false
  episode_len_mean: 777.0795660036166
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.8373152866825
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 16590
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.23359899843732515
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044753586407750845
        model: {}
        policy_loss: -0.008296948430749277
        total_loss: 3.4977787931760154
        vf_explained_var: 0.9911777377128601
        vf_loss: 3.506192664305369
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16
    gpu_util_percent0: 0.35333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767011746861583
    mean_env_wait_ms: 1.2265562369942216
    mean_inference_ms: 4.381153425036385
    mean_raw_obs_processing_ms: 0.38412706517920026
  time_since_restore: 2073.973659515381
  time_this_iter_s: 25.517396688461304
  time_total_s: 2073.973659515381
  timers:
    learn_throughput: 8704.436
    learn_time_ms: 18587.304
    sample_throughput: 23656.663
    sample_time_ms: 6839.173
    update_time_ms: 34.855
  timestamp: 1602639959
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     80 |          2073.97 | 12943360 |  270.837 |              315.717 |              148.141 |             777.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3271.2847824786068
    time_step_min: 2976
  date: 2020-10-14_01-46-25
  done: false
  episode_len_mean: 777.0659583358205
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 271.064508874334
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 16753
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.26309418429931003
        entropy_coeff: 0.0005000000000000001
        kl: 0.005505056430896123
        model: {}
        policy_loss: -0.011706334364134818
        total_loss: 3.52339369058609
        vf_explained_var: 0.9898431897163391
        vf_loss: 3.535231610139211
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.686666666666667
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476542145454042
    mean_env_wait_ms: 1.2266241181572164
    mean_inference_ms: 4.380191532511809
    mean_raw_obs_processing_ms: 0.38406264775963006
  time_since_restore: 2099.661249399185
  time_this_iter_s: 25.68758988380432
  time_total_s: 2099.661249399185
  timers:
    learn_throughput: 8700.137
    learn_time_ms: 18596.489
    sample_throughput: 23686.251
    sample_time_ms: 6830.629
    update_time_ms: 35.714
  timestamp: 1602639985
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     81 |          2099.66 | 13105152 |  271.065 |              315.717 |              148.141 |            777.066 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3269.123225540437
    time_step_min: 2976
  date: 2020-10-14_01-46-51
  done: false
  episode_len_mean: 777.0140431282684
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 271.3739065251493
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 266
  episodes_total: 17019
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.25557995587587357
        entropy_coeff: 0.0005000000000000001
        kl: 0.004823980115664502
        model: {}
        policy_loss: -0.006437495481804945
        total_loss: 5.8934242725372314
        vf_explained_var: 0.9887437224388123
        vf_loss: 5.899989485740662
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.42
    gpu_util_percent0: 0.36966666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762795804997153
    mean_env_wait_ms: 1.226721572530949
    mean_inference_ms: 4.3786050960783465
    mean_raw_obs_processing_ms: 0.38395518421647024
  time_since_restore: 2125.3398311138153
  time_this_iter_s: 25.678581714630127
  time_total_s: 2125.3398311138153
  timers:
    learn_throughput: 8692.165
    learn_time_ms: 18613.545
    sample_throughput: 23708.121
    sample_time_ms: 6824.328
    update_time_ms: 36.311
  timestamp: 1602640011
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     82 |          2125.34 | 13266944 |  271.374 |              315.717 |              148.141 |            777.014 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3267.5665308498255
    time_step_min: 2976
  date: 2020-10-14_01-47-17
  done: false
  episode_len_mean: 777.0182905585879
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 271.61703376817775
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 17222
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.22632338851690292
        entropy_coeff: 0.0005000000000000001
        kl: 0.004703229797693591
        model: {}
        policy_loss: -0.00881632497961012
        total_loss: 3.778484364350637
        vf_explained_var: 0.9909164309501648
        vf_loss: 3.7874138951301575
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.472413793103453
    gpu_util_percent0: 0.35068965517241385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760909294933106
    mean_env_wait_ms: 1.2267905082714805
    mean_inference_ms: 4.377426642563562
    mean_raw_obs_processing_ms: 0.38387421142877953
  time_since_restore: 2150.9353330135345
  time_this_iter_s: 25.59550189971924
  time_total_s: 2150.9353330135345
  timers:
    learn_throughput: 8689.507
    learn_time_ms: 18619.239
    sample_throughput: 23738.386
    sample_time_ms: 6815.628
    update_time_ms: 37.841
  timestamp: 1602640037
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     83 |          2150.94 | 13428736 |  271.617 |              315.717 |              148.141 |            777.018 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3266.211970937608
    time_step_min: 2971
  date: 2020-10-14_01-47-43
  done: false
  episode_len_mean: 776.981649792913
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 271.8259673355739
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 17384
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.24729155004024506
        entropy_coeff: 0.0005000000000000001
        kl: 0.004852148044543962
        model: {}
        policy_loss: -0.008908574345696252
        total_loss: 3.4848879178365073
        vf_explained_var: 0.9898207187652588
        vf_loss: 3.493920107682546
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.750000000000004
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475944125978897
    mean_env_wait_ms: 1.2268442359835974
    mean_inference_ms: 4.376524390972517
    mean_raw_obs_processing_ms: 0.383812659148436
  time_since_restore: 2176.5721247196198
  time_this_iter_s: 25.636791706085205
  time_total_s: 2176.5721247196198
  timers:
    learn_throughput: 8692.867
    learn_time_ms: 18612.041
    sample_throughput: 23753.091
    sample_time_ms: 6811.408
    update_time_ms: 37.919
  timestamp: 1602640063
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     84 |          2176.57 | 13590528 |  271.826 |              316.475 |              148.141 |            776.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3264.239397384878
    time_step_min: 2971
  date: 2020-10-14_01-48-09
  done: false
  episode_len_mean: 776.9624546279492
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.1311899622359
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 248
  episodes_total: 17632
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2447493076324463
        entropy_coeff: 0.0005000000000000001
        kl: 0.004481222596950829
        model: {}
        policy_loss: -0.008443929565449556
        total_loss: 4.945951541264852
        vf_explained_var: 0.9899372458457947
        vf_loss: 4.9545178810755415
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.086666666666662
    gpu_util_percent0: 0.28099999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475709368418195
    mean_env_wait_ms: 1.2269191922560205
    mean_inference_ms: 4.375166633320379
    mean_raw_obs_processing_ms: 0.38371974839716494
  time_since_restore: 2202.316455602646
  time_this_iter_s: 25.744330883026123
  time_total_s: 2202.316455602646
  timers:
    learn_throughput: 8690.087
    learn_time_ms: 18617.996
    sample_throughput: 23763.552
    sample_time_ms: 6808.41
    update_time_ms: 37.856
  timestamp: 1602640089
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     85 |          2202.32 | 13752320 |  272.131 |              316.475 |              148.141 |            776.962 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3262.4823152930608
    time_step_min: 2971
  date: 2020-10-14_01-48-35
  done: false
  episode_len_mean: 776.9564803405399
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.39875737321694
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 222
  episodes_total: 17854
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21237351497014365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004442582760627071
        model: {}
        policy_loss: -0.006997300889148998
        total_loss: 4.398995478947957
        vf_explained_var: 0.9899685382843018
        vf_loss: 4.406099021434784
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.40344827586207
    gpu_util_percent0: 0.32448275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147551828989151
    mean_env_wait_ms: 1.226976803063536
    mean_inference_ms: 4.373923090057385
    mean_raw_obs_processing_ms: 0.38363334534214905
  time_since_restore: 2227.895235300064
  time_this_iter_s: 25.578779697418213
  time_total_s: 2227.895235300064
  timers:
    learn_throughput: 8686.913
    learn_time_ms: 18624.797
    sample_throughput: 23792.907
    sample_time_ms: 6800.01
    update_time_ms: 39.943
  timestamp: 1602640115
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     86 |           2227.9 | 13914112 |  272.399 |              316.475 |              148.141 |            776.956 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3261.2060541984306
    time_step_min: 2971
  date: 2020-10-14_01-49-01
  done: false
  episode_len_mean: 776.9353244878698
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.5907848820746
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 18013
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.2334639442463716
        entropy_coeff: 0.0005000000000000001
        kl: 0.004541010401832561
        model: {}
        policy_loss: -0.008917031387682073
        total_loss: 3.4354718724886575
        vf_explained_var: 0.9900593757629395
        vf_loss: 3.4445056915283203
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37333333333334
    gpu_util_percent0: 0.3090000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753821889861685
    mean_env_wait_ms: 1.2270166597218317
    mean_inference_ms: 4.373078748455754
    mean_raw_obs_processing_ms: 0.3835751508877975
  time_since_restore: 2253.4108052253723
  time_this_iter_s: 25.515569925308228
  time_total_s: 2253.4108052253723
  timers:
    learn_throughput: 8684.959
    learn_time_ms: 18628.989
    sample_throughput: 23797.833
    sample_time_ms: 6798.602
    update_time_ms: 40.468
  timestamp: 1602640141
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     87 |          2253.41 | 14075904 |  272.591 |              316.475 |              148.141 |            776.935 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3259.330548991592
    time_step_min: 2971
  date: 2020-10-14_01-49-27
  done: false
  episode_len_mean: 776.9176489939141
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.87457003280247
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 226
  episodes_total: 18239
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.24373838305473328
        entropy_coeff: 0.0005000000000000001
        kl: 0.006135160801932216
        model: {}
        policy_loss: -0.007553313926716025
        total_loss: 4.122738420963287
        vf_explained_var: 0.9909818172454834
        vf_loss: 4.1304135123888654
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475182205809572
    mean_env_wait_ms: 1.227064903124476
    mean_inference_ms: 4.371916472492551
    mean_raw_obs_processing_ms: 0.38349326026508174
  time_since_restore: 2279.235423564911
  time_this_iter_s: 25.824618339538574
  time_total_s: 2279.235423564911
  timers:
    learn_throughput: 8673.835
    learn_time_ms: 18652.88
    sample_throughput: 23728.607
    sample_time_ms: 6818.436
    update_time_ms: 33.039
  timestamp: 1602640167
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     88 |          2279.24 | 14237696 |  272.875 |              316.475 |              148.141 |            776.918 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3257.418532776663
    time_step_min: 2971
  date: 2020-10-14_01-49-53
  done: false
  episode_len_mean: 776.8952123343252
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.16792212085704
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 18485
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.2055367132027944
        entropy_coeff: 0.0005000000000000001
        kl: 0.004180201096460223
        model: {}
        policy_loss: -0.010965465280605713
        total_loss: 4.429649392763774
        vf_explained_var: 0.9903015494346619
        vf_loss: 4.4407176574071245
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.13333333333333
    gpu_util_percent0: 0.37099999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749824383591906
    mean_env_wait_ms: 1.2271175760522839
    mean_inference_ms: 4.3706258736817345
    mean_raw_obs_processing_ms: 0.3834044471565549
  time_since_restore: 2305.1586530208588
  time_this_iter_s: 25.923229455947876
  time_total_s: 2305.1586530208588
  timers:
    learn_throughput: 8659.935
    learn_time_ms: 18682.819
    sample_throughput: 23718.351
    sample_time_ms: 6821.385
    update_time_ms: 32.458
  timestamp: 1602640193
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     89 |          2305.16 | 14399488 |  273.168 |              316.475 |              148.141 |            776.895 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3256.142027738953
    time_step_min: 2971
  date: 2020-10-14_01-50-19
  done: false
  episode_len_mean: 776.8852177644283
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.35431877236215
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 18644
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21800185119112334
        entropy_coeff: 0.0005000000000000001
        kl: 0.003973691374994814
        model: {}
        policy_loss: -0.008119370116522381
        total_loss: 3.5141924818356833
        vf_explained_var: 0.9897346496582031
        vf_loss: 3.522420803705851
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.113333333333333
    gpu_util_percent0: 0.3563333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748526979614726
    mean_env_wait_ms: 1.2271480511798893
    mean_inference_ms: 4.369831252691038
    mean_raw_obs_processing_ms: 0.38334889078189505
  time_since_restore: 2330.848561525345
  time_this_iter_s: 25.689908504486084
  time_total_s: 2330.848561525345
  timers:
    learn_throughput: 8654.591
    learn_time_ms: 18694.356
    sample_throughput: 23701.67
    sample_time_ms: 6826.186
    update_time_ms: 32.433
  timestamp: 1602640219
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     90 |          2330.85 | 14561280 |  273.354 |              316.475 |              148.141 |            776.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3254.4844281462583
    time_step_min: 2971
  date: 2020-10-14_01-50-45
  done: false
  episode_len_mean: 776.8501962032029
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.6017846296244
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 214
  episodes_total: 18858
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.23492631067832312
        entropy_coeff: 0.0005000000000000001
        kl: 0.004685264585229258
        model: {}
        policy_loss: -0.010369188152253628
        total_loss: 4.419384280840556
        vf_explained_var: 0.9900035262107849
        vf_loss: 4.429870883623759
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98
    gpu_util_percent0: 0.3356666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746836062434188
    mean_env_wait_ms: 1.2271878098154352
    mean_inference_ms: 4.368823369201538
    mean_raw_obs_processing_ms: 0.3832784798185923
  time_since_restore: 2356.624759197235
  time_this_iter_s: 25.77619767189026
  time_total_s: 2356.624759197235
  timers:
    learn_throughput: 8649.15
    learn_time_ms: 18706.116
    sample_throughput: 23687.379
    sample_time_ms: 6830.304
    update_time_ms: 33.275
  timestamp: 1602640245
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     91 |          2356.62 | 14723072 |  273.602 |              316.475 |              148.141 |             776.85 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3252.4802348746985
    time_step_min: 2971
  date: 2020-10-14_01-51-12
  done: false
  episode_len_mean: 776.803881565181
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.9075183726785
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 258
  episodes_total: 19116
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.20404858887195587
        entropy_coeff: 0.0005000000000000001
        kl: 0.005306126511034866
        model: {}
        policy_loss: -0.010063391081833592
        total_loss: 3.767662982145945
        vf_explained_var: 0.9920833110809326
        vf_loss: 3.7778284152348838
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09666666666667
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744785326541388
    mean_env_wait_ms: 1.2272236218108334
    mean_inference_ms: 4.367524820987746
    mean_raw_obs_processing_ms: 0.38318710104167697
  time_since_restore: 2382.3296999931335
  time_this_iter_s: 25.704940795898438
  time_total_s: 2382.3296999931335
  timers:
    learn_throughput: 8647.7
    learn_time_ms: 18709.251
    sample_throughput: 23694.86
    sample_time_ms: 6828.148
    update_time_ms: 34.086
  timestamp: 1602640272
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     92 |          2382.33 | 14884864 |  273.908 |              316.475 |              148.141 |            776.804 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3251.193251533742
    time_step_min: 2971
  date: 2020-10-14_01-51-37
  done: false
  episode_len_mean: 776.7681054160614
  episode_reward_max: 317.98989898989885
  episode_reward_mean: 274.0990659866984
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 160
  episodes_total: 19276
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.20951137940088907
        entropy_coeff: 0.0005000000000000001
        kl: 0.004479876991050939
        model: {}
        policy_loss: -0.007850972489298632
        total_loss: 4.130880792935689
        vf_explained_var: 0.9880260825157166
        vf_loss: 4.138836522897084
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003333333333337
    gpu_util_percent0: 0.31533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474353969331054
    mean_env_wait_ms: 1.2272441391326687
    mean_inference_ms: 4.366767501888775
    mean_raw_obs_processing_ms: 0.3831335913845692
  time_since_restore: 2407.8759326934814
  time_this_iter_s: 25.5462327003479
  time_total_s: 2407.8759326934814
  timers:
    learn_throughput: 8649.73
    learn_time_ms: 18704.861
    sample_throughput: 23693.547
    sample_time_ms: 6828.526
    update_time_ms: 32.115
  timestamp: 1602640297
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     93 |          2407.88 | 15046656 |  274.099 |               317.99 |              148.141 |            776.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3249.776742989452
    time_step_min: 2971
  date: 2020-10-14_01-52-03
  done: false
  episode_len_mean: 776.7269600041074
  episode_reward_max: 317.98989898989885
  episode_reward_mean: 274.32230193291963
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 201
  episodes_total: 19477
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.22809051846464476
        entropy_coeff: 0.0005000000000000001
        kl: 0.004330610565375537
        model: {}
        policy_loss: -0.010108465377318984
        total_loss: 4.2880149483680725
        vf_explained_var: 0.990037739276886
        vf_loss: 4.298237403233846
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.59310344827586
    gpu_util_percent0: 0.3162068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742085393833565
    mean_env_wait_ms: 1.2272706455993192
    mean_inference_ms: 4.365875264220289
    mean_raw_obs_processing_ms: 0.38306932954422396
  time_since_restore: 2433.352759361267
  time_this_iter_s: 25.476826667785645
  time_total_s: 2433.352759361267
  timers:
    learn_throughput: 8653.372
    learn_time_ms: 18696.989
    sample_throughput: 23723.343
    sample_time_ms: 6819.949
    update_time_ms: 31.679
  timestamp: 1602640323
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     94 |          2433.35 | 15208448 |  274.322 |               317.99 |              148.141 |            776.727 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3247.7777101096226
    time_step_min: 2961
  date: 2020-10-14_01-52-29
  done: false
  episode_len_mean: 776.694773625038
  episode_reward_max: 317.98989898989925
  episode_reward_mean: 274.6242384341747
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 269
  episodes_total: 19746
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.19512154906988144
        entropy_coeff: 0.0005000000000000001
        kl: 0.004368953096369903
        model: {}
        policy_loss: -0.01058005481869865
        total_loss: 4.657853643099467
        vf_explained_var: 0.990476131439209
        vf_loss: 4.668531219164531
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.66
    gpu_util_percent0: 0.36833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740028650732262
    mean_env_wait_ms: 1.227296785661462
    mean_inference_ms: 4.364607919822478
    mean_raw_obs_processing_ms: 0.3829806711249754
  time_since_restore: 2458.923537015915
  time_this_iter_s: 25.570777654647827
  time_total_s: 2458.923537015915
  timers:
    learn_throughput: 8667.316
    learn_time_ms: 18666.91
    sample_throughput: 23709.645
    sample_time_ms: 6823.89
    update_time_ms: 38.642
  timestamp: 1602640349
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     95 |          2458.92 | 15370240 |  274.624 |               317.99 |              148.141 |            776.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3246.5545655894493
    time_step_min: 2961
  date: 2020-10-14_01-52-55
  done: false
  episode_len_mean: 776.6632509543902
  episode_reward_max: 317.98989898989925
  episode_reward_mean: 274.81723656090753
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 19908
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.19664488981167474
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046378555707633495
        model: {}
        policy_loss: -0.009079183825330498
        total_loss: 2.6752074162165322
        vf_explained_var: 0.9920947551727295
        vf_loss: 2.6843850016593933
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.965517241379313
    gpu_util_percent0: 0.3472413793103449
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738822549555866
    mean_env_wait_ms: 1.2273092160669545
    mean_inference_ms: 4.363870763012764
    mean_raw_obs_processing_ms: 0.38292856242913814
  time_since_restore: 2484.4436473846436
  time_this_iter_s: 25.520110368728638
  time_total_s: 2484.4436473846436
  timers:
    learn_throughput: 8666.385
    learn_time_ms: 18668.915
    sample_throughput: 23741.581
    sample_time_ms: 6814.711
    update_time_ms: 38.77
  timestamp: 1602640375
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     96 |          2484.44 | 15532032 |  274.817 |               317.99 |              148.141 |            776.663 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3244.990030407258
    time_step_min: 2961
  date: 2020-10-14_01-53-21
  done: false
  episode_len_mean: 776.6024971397304
  episode_reward_max: 317.98989898989925
  episode_reward_mean: 275.05291837943685
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 195
  episodes_total: 20103
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.2139376476407051
        entropy_coeff: 0.0005000000000000001
        kl: 0.005073503435899814
        model: {}
        policy_loss: -0.0073867844249662085
        total_loss: 3.4668161869049072
        vf_explained_var: 0.9913925528526306
        vf_loss: 3.4743099411328635
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.319999999999997
    gpu_util_percent0: 0.3316666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737523561288757
    mean_env_wait_ms: 1.227328123519512
    mean_inference_ms: 4.3630585440411425
    mean_raw_obs_processing_ms: 0.3828704173714859
  time_since_restore: 2510.0158092975616
  time_this_iter_s: 25.57216191291809
  time_total_s: 2510.0158092975616
  timers:
    learn_throughput: 8664.648
    learn_time_ms: 18672.656
    sample_throughput: 23729.924
    sample_time_ms: 6818.058
    update_time_ms: 36.862
  timestamp: 1602640401
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     97 |          2510.02 | 15693824 |  275.053 |               317.99 |              148.141 |            776.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3243.023258100998
    time_step_min: 2961
  date: 2020-10-14_01-53-47
  done: false
  episode_len_mean: 776.5060601599686
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 275.3638445399082
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 276
  episodes_total: 20379
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.191166240721941
        entropy_coeff: 0.0005000000000000001
        kl: 0.004569540731608868
        model: {}
        policy_loss: -0.008908689846672738
        total_loss: 4.590634385744731
        vf_explained_var: 0.9906993508338928
        vf_loss: 4.59963842233022
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.133333333333336
    gpu_util_percent0: 0.3363333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735492867823197
    mean_env_wait_ms: 1.2273416730027786
    mean_inference_ms: 4.361819853746971
    mean_raw_obs_processing_ms: 0.38278213889374835
  time_since_restore: 2535.5126905441284
  time_this_iter_s: 25.496881246566772
  time_total_s: 2535.5126905441284
  timers:
    learn_throughput: 8669.487
    learn_time_ms: 18662.235
    sample_throughput: 23812.95
    sample_time_ms: 6794.286
    update_time_ms: 37.772
  timestamp: 1602640427
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     98 |          2535.51 | 15855616 |  275.364 |              319.808 |              148.141 |            776.506 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3241.7598790125867
    time_step_min: 2952
  date: 2020-10-14_01-54-13
  done: false
  episode_len_mean: 776.4785296981499
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 275.5524450935844
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 161
  episodes_total: 20540
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18675213803847632
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038226922430718937
        model: {}
        policy_loss: -0.007531334820669144
        total_loss: 3.2655821442604065
        vf_explained_var: 0.990511417388916
        vf_loss: 3.2732067704200745
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89666666666667
    gpu_util_percent0: 0.3583333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734364920359727
    mean_env_wait_ms: 1.2273479722149143
    mean_inference_ms: 4.361129125068174
    mean_raw_obs_processing_ms: 0.38273299825957174
  time_since_restore: 2561.136621236801
  time_this_iter_s: 25.62393069267273
  time_total_s: 2561.136621236801
  timers:
    learn_throughput: 8686.374
    learn_time_ms: 18625.953
    sample_throughput: 23791.915
    sample_time_ms: 6800.293
    update_time_ms: 37.644
  timestamp: 1602640453
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |     99 |          2561.14 | 16017408 |  275.552 |              319.808 |              148.141 |            776.479 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3240.313593734893
    time_step_min: 2952
  date: 2020-10-14_01-54-39
  done: false
  episode_len_mean: 776.4451949054419
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 275.77515944859647
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 20728
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.20801711827516556
        entropy_coeff: 0.0005000000000000001
        kl: 0.005674606072716415
        model: {}
        policy_loss: -0.008645180948709216
        total_loss: 3.3662743965784707
        vf_explained_var: 0.9913938641548157
        vf_loss: 3.3750235637029014
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.056666666666676
    gpu_util_percent0: 0.3469999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733165855284627
    mean_env_wait_ms: 1.2273567744980756
    mean_inference_ms: 4.360386886690618
    mean_raw_obs_processing_ms: 0.38267871165953715
  time_since_restore: 2586.8344616889954
  time_this_iter_s: 25.697840452194214
  time_total_s: 2586.8344616889954
  timers:
    learn_throughput: 8688.285
    learn_time_ms: 18621.857
    sample_throughput: 23776.645
    sample_time_ms: 6804.661
    update_time_ms: 36.702
  timestamp: 1602640479
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    100 |          2586.83 | 16179200 |  275.775 |              319.808 |              148.141 |            776.445 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3238.0968188105116
    time_step_min: 2952
  date: 2020-10-14_01-55-05
  done: false
  episode_len_mean: 776.4024465705174
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 276.1068748314216
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 281
  episodes_total: 21009
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.18861758460601172
        entropy_coeff: 0.0005000000000000001
        kl: 0.004452064128903051
        model: {}
        policy_loss: -0.009123142333313202
        total_loss: 3.4913241465886435
        vf_explained_var: 0.9929396510124207
        vf_loss: 3.500541607538859
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70333333333333
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731205751479481
    mean_env_wait_ms: 1.2273591798970762
    mean_inference_ms: 4.359193064803775
    mean_raw_obs_processing_ms: 0.3825937707856664
  time_since_restore: 2612.429199695587
  time_this_iter_s: 25.594738006591797
  time_total_s: 2612.429199695587
  timers:
    learn_throughput: 8697.592
    learn_time_ms: 18601.93
    sample_throughput: 23767.184
    sample_time_ms: 6807.369
    update_time_ms: 34.626
  timestamp: 1602640505
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    101 |          2612.43 | 16340992 |  276.107 |              319.808 |              148.141 |            776.402 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3236.8575485092288
    time_step_min: 2952
  date: 2020-10-14_01-55-31
  done: false
  episode_len_mean: 776.3793689778953
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 276.29205191915383
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 21172
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.18021325394511223
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042955120249340934
        model: {}
        policy_loss: -0.006359915253900302
        total_loss: 3.6041420300801597
        vf_explained_var: 0.9898276925086975
        vf_loss: 3.6105920871098838
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.282758620689656
    gpu_util_percent0: 0.33275862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730153248748504
    mean_env_wait_ms: 1.2273593377218697
    mean_inference_ms: 4.358529598465683
    mean_raw_obs_processing_ms: 0.3825461511341776
  time_since_restore: 2637.743329524994
  time_this_iter_s: 25.31412982940674
  time_total_s: 2637.743329524994
  timers:
    learn_throughput: 8714.602
    learn_time_ms: 18565.622
    sample_throughput: 23773.343
    sample_time_ms: 6805.606
    update_time_ms: 32.455
  timestamp: 1602640531
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    102 |          2637.74 | 16502784 |  276.292 |              319.808 |              148.141 |            776.379 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3235.524117867868
    time_step_min: 2952
  date: 2020-10-14_01-55-57
  done: false
  episode_len_mean: 776.3679872623396
  episode_reward_max: 319.9595959595957
  episode_reward_mean: 276.50217828751136
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 182
  episodes_total: 21354
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.20342449098825455
        entropy_coeff: 0.0005000000000000001
        kl: 0.004762681084685028
        model: {}
        policy_loss: -0.010126531822606921
        total_loss: 3.2652917305628457
        vf_explained_var: 0.9915415644645691
        vf_loss: 3.2755199670791626
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.896774193548392
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472906232861949
    mean_env_wait_ms: 1.2273620423635816
    mean_inference_ms: 4.357854296930898
    mean_raw_obs_processing_ms: 0.3824974586760759
  time_since_restore: 2663.6141459941864
  time_this_iter_s: 25.870816469192505
  time_total_s: 2663.6141459941864
  timers:
    learn_throughput: 8705.274
    learn_time_ms: 18585.515
    sample_throughput: 23739.161
    sample_time_ms: 6815.405
    update_time_ms: 33.971
  timestamp: 1602640557
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    103 |          2663.61 | 16664576 |  276.502 |               319.96 |              148.141 |            776.368 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3233.4344805224882
    time_step_min: 2949
  date: 2020-10-14_01-56-24
  done: false
  episode_len_mean: 776.3580509454024
  episode_reward_max: 319.9595959595957
  episode_reward_mean: 276.812282129697
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 277
  episodes_total: 21631
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.18621143326163292
        entropy_coeff: 0.0005000000000000001
        kl: 0.004067339294124395
        model: {}
        policy_loss: -0.00802563641142721
        total_loss: 4.23588103055954
        vf_explained_var: 0.991602897644043
        vf_loss: 4.243999779224396
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.862068965517246
    gpu_util_percent0: 0.3724137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727215080096853
    mean_env_wait_ms: 1.2273525256027984
    mean_inference_ms: 4.356733805205864
    mean_raw_obs_processing_ms: 0.3824164492974081
  time_since_restore: 2689.1562616825104
  time_this_iter_s: 25.542115688323975
  time_total_s: 2689.1562616825104
  timers:
    learn_throughput: 8702.457
    learn_time_ms: 18591.53
    sample_throughput: 23742.049
    sample_time_ms: 6814.576
    update_time_ms: 34.55
  timestamp: 1602640584
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    104 |          2689.16 | 16826368 |  276.812 |               319.96 |              148.141 |            776.358 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3232.1961676316514
    time_step_min: 2939
  date: 2020-10-14_01-56-50
  done: false
  episode_len_mean: 776.3557145477894
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 276.9994009995387
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 21804
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.1707239386936029
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034220329641054073
        model: {}
        policy_loss: -0.008444460826770714
        total_loss: 3.379118025302887
        vf_explained_var: 0.9907882809638977
        vf_loss: 3.387647807598114
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75
    gpu_util_percent0: 0.386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726181846494302
    mean_env_wait_ms: 1.2273444292213107
    mean_inference_ms: 4.3560626410999355
    mean_raw_obs_processing_ms: 0.3823677545966596
  time_since_restore: 2714.9648258686066
  time_this_iter_s: 25.80856418609619
  time_total_s: 2714.9648258686066
  timers:
    learn_throughput: 8686.349
    learn_time_ms: 18626.008
    sample_throughput: 23756.021
    sample_time_ms: 6810.568
    update_time_ms: 27.824
  timestamp: 1602640610
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    105 |          2714.96 | 16988160 |  276.999 |              321.323 |              148.141 |            776.356 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3230.8406490724283
    time_step_min: 2939
  date: 2020-10-14_01-57-16
  done: false
  episode_len_mean: 776.3512124107184
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.20532333020395
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 177
  episodes_total: 21981
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.19544888163606325
        entropy_coeff: 0.0005000000000000001
        kl: 0.004837796402474244
        model: {}
        policy_loss: -0.007723270636536957
        total_loss: 3.552420735359192
        vf_explained_var: 0.9902071952819824
        vf_loss: 3.560241719086965
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.136666666666667
    gpu_util_percent0: 0.3126666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472515624901592
    mean_env_wait_ms: 1.2273385849913474
    mean_inference_ms: 4.355423788134629
    mean_raw_obs_processing_ms: 0.382321631423812
  time_since_restore: 2740.4681758880615
  time_this_iter_s: 25.503350019454956
  time_total_s: 2740.4681758880615
  timers:
    learn_throughput: 8689.517
    learn_time_ms: 18619.216
    sample_throughput: 23736.678
    sample_time_ms: 6816.118
    update_time_ms: 26.148
  timestamp: 1602640636
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    106 |          2740.47 | 17149952 |  277.205 |              321.323 |              148.141 |            776.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3228.768596902017
    time_step_min: 2939
  date: 2020-10-14_01-57-42
  done: false
  episode_len_mean: 776.344584269663
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.50906140052217
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 269
  episodes_total: 22250
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.18585548425714174
        entropy_coeff: 0.0005000000000000001
        kl: 0.004263442708179355
        model: {}
        policy_loss: -0.008482252967951354
        total_loss: 3.678061227003733
        vf_explained_var: 0.9925315380096436
        vf_loss: 3.6866363088289895
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.363333333333333
    gpu_util_percent0: 0.3223333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472345873450583
    mean_env_wait_ms: 1.2273188424395736
    mean_inference_ms: 4.354406542790433
    mean_raw_obs_processing_ms: 0.38224633692713705
  time_since_restore: 2766.008225440979
  time_this_iter_s: 25.54004955291748
  time_total_s: 2766.008225440979
  timers:
    learn_throughput: 8692.932
    learn_time_ms: 18611.901
    sample_throughput: 23725.481
    sample_time_ms: 6819.335
    update_time_ms: 25.991
  timestamp: 1602640662
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    107 |          2766.01 | 17311744 |  277.509 |              321.323 |              148.141 |            776.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3227.293069572207
    time_step_min: 2939
  date: 2020-10-14_01-58-08
  done: false
  episode_len_mean: 776.3161436976288
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.7186763336702
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 186
  episodes_total: 22436
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.1633200210829576
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042159569954189164
        model: {}
        policy_loss: -0.008422416833733829
        total_loss: 2.697547455628713
        vf_explained_var: 0.9925214648246765
        vf_loss: 2.706051508585612
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.45
    gpu_util_percent0: 0.318
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722399005112977
    mean_env_wait_ms: 1.2273036612962975
    mean_inference_ms: 4.353717944510139
    mean_raw_obs_processing_ms: 0.3821965051385702
  time_since_restore: 2791.5887348651886
  time_this_iter_s: 25.580509424209595
  time_total_s: 2791.5887348651886
  timers:
    learn_throughput: 8694.001
    learn_time_ms: 18609.614
    sample_throughput: 23719.568
    sample_time_ms: 6821.035
    update_time_ms: 24.962
  timestamp: 1602640688
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    108 |          2791.59 | 17473536 |  277.719 |              321.323 |              148.141 |            776.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3226.0286727232437
    time_step_min: 2939
  date: 2020-10-14_01-58-34
  done: false
  episode_len_mean: 776.3090635643828
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.90879646198806
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 22607
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.18519586200515428
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039576644776389
        model: {}
        policy_loss: -0.009751538705510635
        total_loss: 3.283907334009806
        vf_explained_var: 0.9909891486167908
        vf_loss: 3.2937514980634055
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.47931034482759
    gpu_util_percent0: 0.35620689655172416
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721428404992168
    mean_env_wait_ms: 1.2272913580339448
    mean_inference_ms: 4.3531215224257
    mean_raw_obs_processing_ms: 0.3821541370320521
  time_since_restore: 2817.0574305057526
  time_this_iter_s: 25.468695640563965
  time_total_s: 2817.0574305057526
  timers:
    learn_throughput: 8694.033
    learn_time_ms: 18609.546
    sample_throughput: 23775.005
    sample_time_ms: 6805.13
    update_time_ms: 25.316
  timestamp: 1602640714
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    109 |          2817.06 | 17635328 |  277.909 |              321.323 |              148.141 |            776.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3224.0629872974155
    time_step_min: 2939
  date: 2020-10-14_01-58-59
  done: false
  episode_len_mean: 776.2803427771948
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.1983581883897
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 265
  episodes_total: 22872
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1804860420525074
        entropy_coeff: 0.0005000000000000001
        kl: 0.004425623725789289
        model: {}
        policy_loss: -0.007380277859435107
        total_loss: 3.9703678091367087
        vf_explained_var: 0.9919913411140442
        vf_loss: 3.9778383572896323
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.10333333333334
    gpu_util_percent0: 0.3583333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471993657154486
    mean_env_wait_ms: 1.2272657936546654
    mean_inference_ms: 4.352196491819739
    mean_raw_obs_processing_ms: 0.382084131217526
  time_since_restore: 2842.4499661922455
  time_this_iter_s: 25.39253568649292
  time_total_s: 2842.4499661922455
  timers:
    learn_throughput: 8703.285
    learn_time_ms: 18589.761
    sample_throughput: 23809.299
    sample_time_ms: 6795.328
    update_time_ms: 24.153
  timestamp: 1602640739
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    110 |          2842.45 | 17797120 |  278.198 |              321.323 |              148.141 |             776.28 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3222.5806913923393
    time_step_min: 2939
  date: 2020-10-14_01-59-25
  done: false
  episode_len_mean: 776.2584532685972
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.4153867441539
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 196
  episodes_total: 23068
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.15374662106235823
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034466406796127558
        model: {}
        policy_loss: -0.009092114923987538
        total_loss: 2.9404216210047402
        vf_explained_var: 0.9921563267707825
        vf_loss: 2.949590583642324
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.319999999999997
    gpu_util_percent0: 0.32999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718807852179275
    mean_env_wait_ms: 1.2272416369538035
    mean_inference_ms: 4.351487659392776
    mean_raw_obs_processing_ms: 0.38203205339867974
  time_since_restore: 2868.157699108124
  time_this_iter_s: 25.707732915878296
  time_total_s: 2868.157699108124
  timers:
    learn_throughput: 8696.164
    learn_time_ms: 18604.985
    sample_throughput: 23829.381
    sample_time_ms: 6789.601
    update_time_ms: 24.658
  timestamp: 1602640765
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    111 |          2868.16 | 17958912 |  278.415 |              321.323 |              148.141 |            776.258 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3221.350737259636
    time_step_min: 2939
  date: 2020-10-14_01-59-52
  done: false
  episode_len_mean: 776.2295145463935
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.6003936768269
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 23236
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.1732874003549417
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037704111309722066
        model: {}
        policy_loss: -0.0089009863731917
        total_loss: 2.27765546242396
        vf_explained_var: 0.9933651089668274
        vf_loss: 2.2866430481274924
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.866666666666664
    gpu_util_percent0: 0.3083333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717917802942804
    mean_env_wait_ms: 1.2272247177031903
    mean_inference_ms: 4.350931447842748
    mean_raw_obs_processing_ms: 0.38199100361314836
  time_since_restore: 2893.750881433487
  time_this_iter_s: 25.59318232536316
  time_total_s: 2893.750881433487
  timers:
    learn_throughput: 8688.07
    learn_time_ms: 18622.318
    sample_throughput: 23800.854
    sample_time_ms: 6797.739
    update_time_ms: 26.267
  timestamp: 1602640792
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    112 |          2893.75 | 18120704 |    278.6 |              321.323 |              148.141 |             776.23 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3219.3320257536348
    time_step_min: 2939
  date: 2020-10-14_02-00-17
  done: false
  episode_len_mean: 776.1922536709939
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.89368251572984
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 23495
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.17279337719082832
        entropy_coeff: 0.0005000000000000001
        kl: 0.004146678683658441
        model: {}
        policy_loss: -0.008295049832668155
        total_loss: 2.999313751856486
        vf_explained_var: 0.9935783743858337
        vf_loss: 3.007695118586222
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.26551724137931
    gpu_util_percent0: 0.34793103448275864
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716405234602306
    mean_env_wait_ms: 1.2271894561553576
    mean_inference_ms: 4.350051731045055
    mean_raw_obs_processing_ms: 0.3819248873950704
  time_since_restore: 2919.0747644901276
  time_this_iter_s: 25.323883056640625
  time_total_s: 2919.0747644901276
  timers:
    learn_throughput: 8710.45
    learn_time_ms: 18574.471
    sample_throughput: 23826.503
    sample_time_ms: 6790.422
    update_time_ms: 26.448
  timestamp: 1602640817
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    113 |          2919.07 | 18282496 |  278.894 |              321.323 |              148.141 |            776.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3217.8046326823905
    time_step_min: 2939
  date: 2020-10-14_02-00-43
  done: false
  episode_len_mean: 776.1815611814346
  episode_reward_max: 322.08080808080814
  episode_reward_mean: 279.116097685718
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 205
  episodes_total: 23700
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.14530424028635025
        entropy_coeff: 0.0005000000000000001
        kl: 0.003431735715518395
        model: {}
        policy_loss: -0.010111487830727128
        total_loss: 2.6148765087127686
        vf_explained_var: 0.9932143092155457
        vf_loss: 2.625060558319092
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.086666666666666
    gpu_util_percent0: 0.3473333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471538585817638
    mean_env_wait_ms: 1.227161896346213
    mean_inference_ms: 4.349364070133663
    mean_raw_obs_processing_ms: 0.3818737341563798
  time_since_restore: 2944.6161391735077
  time_this_iter_s: 25.541374683380127
  time_total_s: 2944.6161391735077
  timers:
    learn_throughput: 8725.839
    learn_time_ms: 18541.713
    sample_throughput: 23752.595
    sample_time_ms: 6811.551
    update_time_ms: 28.28
  timestamp: 1602640843
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    114 |          2944.62 | 18444288 |  279.116 |              322.081 |              148.141 |            776.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3216.641130047855
    time_step_min: 2939
  date: 2020-10-14_02-01-09
  done: false
  episode_len_mean: 776.1713878645659
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.2906474229388
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 23864
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.16208608696858087
        entropy_coeff: 0.0005000000000000001
        kl: 0.003911696413221459
        model: {}
        policy_loss: -0.009447523450944573
        total_loss: 2.3124568263689675
        vf_explained_var: 0.9932478070259094
        vf_loss: 2.321985363960266
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.903448275862075
    gpu_util_percent0: 0.31413793103448273
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714535290724248
    mean_env_wait_ms: 1.227142187121108
    mean_inference_ms: 4.3488413528953895
    mean_raw_obs_processing_ms: 0.38183532677407833
  time_since_restore: 2970.0078630447388
  time_this_iter_s: 25.39172387123108
  time_total_s: 2970.0078630447388
  timers:
    learn_throughput: 8743.356
    learn_time_ms: 18504.565
    sample_throughput: 23776.988
    sample_time_ms: 6804.563
    update_time_ms: 28.656
  timestamp: 1602640869
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    115 |          2970.01 | 18606080 |  279.291 |              322.838 |              148.141 |            776.171 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3214.8228989240165
    time_step_min: 2939
  date: 2020-10-14_02-01-35
  done: false
  episode_len_mean: 776.1555592419027
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.56592550143756
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 249
  episodes_total: 24113
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.17199470102787018
        entropy_coeff: 0.0005000000000000001
        kl: 0.005043217524265249
        model: {}
        policy_loss: -0.007935532092233188
        total_loss: 3.053463876247406
        vf_explained_var: 0.9933443069458008
        vf_loss: 3.0614852905273438
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.410000000000004
    gpu_util_percent0: 0.326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713227244793653
    mean_env_wait_ms: 1.2271040813404301
    mean_inference_ms: 4.348047445378806
    mean_raw_obs_processing_ms: 0.38177528330170124
  time_since_restore: 2995.505405664444
  time_this_iter_s: 25.4975426197052
  time_total_s: 2995.505405664444
  timers:
    learn_throughput: 8746.395
    learn_time_ms: 18498.136
    sample_throughput: 23762.837
    sample_time_ms: 6808.615
    update_time_ms: 30.062
  timestamp: 1602640895
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    116 |          2995.51 | 18767872 |  279.566 |              322.838 |              148.141 |            776.156 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3213.3534521799993
    time_step_min: 2939
  date: 2020-10-14_02-02-01
  done: false
  episode_len_mean: 776.1464386996013
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.79271818924946
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 218
  episodes_total: 24331
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.14029322812954584
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033506578571784
        model: {}
        policy_loss: -0.0079016112285899
        total_loss: 2.3935567339261374
        vf_explained_var: 0.9941191077232361
        vf_loss: 2.4015283981959024
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883333333333336
    gpu_util_percent0: 0.3623333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147121392812556
    mean_env_wait_ms: 1.2270668519840136
    mean_inference_ms: 4.347350747086227
    mean_raw_obs_processing_ms: 0.38172255680796635
  time_since_restore: 3021.1955165863037
  time_this_iter_s: 25.69011092185974
  time_total_s: 3021.1955165863037
  timers:
    learn_throughput: 8741.983
    learn_time_ms: 18507.471
    sample_throughput: 23747.79
    sample_time_ms: 6812.929
    update_time_ms: 30.578
  timestamp: 1602640921
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    117 |           3021.2 | 18929664 |  279.793 |              322.838 |              148.141 |            776.146 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3212.34428039753
    time_step_min: 2939
  date: 2020-10-14_02-02-28
  done: false
  episode_len_mean: 776.1532682807333
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.95457906546795
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 24493
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.15088662753502527
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034930842424121997
        model: {}
        policy_loss: -0.010404171235402751
        total_loss: 2.4042382637659707
        vf_explained_var: 0.9930008053779602
        vf_loss: 2.4147178332010903
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.313333333333333
    gpu_util_percent0: 0.36466666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471132128894671
    mean_env_wait_ms: 1.2270412870673892
    mean_inference_ms: 4.346844977964325
    mean_raw_obs_processing_ms: 0.3816843988239033
  time_since_restore: 3046.9025876522064
  time_this_iter_s: 25.70707106590271
  time_total_s: 3046.9025876522064
  timers:
    learn_throughput: 8733.987
    learn_time_ms: 18524.416
    sample_throughput: 23735.476
    sample_time_ms: 6816.463
    update_time_ms: 30.572
  timestamp: 1602640948
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    118 |           3046.9 | 19091456 |  279.955 |              322.838 |              148.141 |            776.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3210.7152406417113
    time_step_min: 2939
  date: 2020-10-14_02-02-54
  done: false
  episode_len_mean: 776.140055002831
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 280.2083465080311
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 233
  episodes_total: 24726
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.15919426083564758
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037204567187776
        model: {}
        policy_loss: -0.008802295196801424
        total_loss: 2.924891233444214
        vf_explained_var: 0.9933714270591736
        vf_loss: 2.9337732195854187
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.443333333333335
    gpu_util_percent0: 0.30666666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471026103100235
    mean_env_wait_ms: 1.2270050943137376
    mean_inference_ms: 4.3461692088869635
    mean_raw_obs_processing_ms: 0.3816341641050922
  time_since_restore: 3072.6673641204834
  time_this_iter_s: 25.764776468276978
  time_total_s: 3072.6673641204834
  timers:
    learn_throughput: 8728.564
    learn_time_ms: 18535.924
    sample_throughput: 23709.615
    sample_time_ms: 6823.898
    update_time_ms: 32.551
  timestamp: 1602640974
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    119 |          3072.67 | 19253248 |  280.208 |              322.838 |              148.141 |             776.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3209.0445443236085
    time_step_min: 2929
  date: 2020-10-14_02-03-20
  done: false
  episode_len_mean: 776.1294819919074
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 280.45546041724094
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 235
  episodes_total: 24961
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.13122796515623728
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033048008917830884
        model: {}
        policy_loss: -0.007630673043119411
        total_loss: 2.5804986357688904
        vf_explained_var: 0.993885338306427
        vf_loss: 2.588194986184438
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.133333333333333
    gpu_util_percent0: 0.3496666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709017022068
    mean_env_wait_ms: 1.226957029974875
    mean_inference_ms: 4.345428768271396
    mean_raw_obs_processing_ms: 0.3815769176938541
  time_since_restore: 3098.404044866562
  time_this_iter_s: 25.73668074607849
  time_total_s: 3098.404044866562
  timers:
    learn_throughput: 8718.097
    learn_time_ms: 18558.178
    sample_throughput: 23705.956
    sample_time_ms: 6824.951
    update_time_ms: 41.165
  timestamp: 1602641000
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    120 |           3098.4 | 19415040 |  280.455 |              322.838 |              148.141 |            776.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3207.867549140784
    time_step_min: 2929
  date: 2020-10-14_02-03-46
  done: false
  episode_len_mean: 776.123233690244
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 280.63060369245943
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 25123
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.13777459785342216
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037779376919691763
        model: {}
        policy_loss: -0.009241481297067367
        total_loss: 1.8713032007217407
        vf_explained_var: 0.9943371415138245
        vf_loss: 1.8806135455767314
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.020000000000003
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708272274890904
    mean_env_wait_ms: 1.2269268247527292
    mean_inference_ms: 4.344945666895839
    mean_raw_obs_processing_ms: 0.3815408192336891
  time_since_restore: 3123.9043686389923
  time_this_iter_s: 25.50032377243042
  time_total_s: 3123.9043686389923
  timers:
    learn_throughput: 8725.788
    learn_time_ms: 18541.821
    sample_throughput: 23725.116
    sample_time_ms: 6819.44
    update_time_ms: 41.652
  timestamp: 1602641026
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    121 |           3123.9 | 19576832 |  280.631 |              322.838 |              148.141 |            776.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3206.2786451128413
    time_step_min: 2929
  date: 2020-10-14_02-04-13
  done: false
  episode_len_mean: 776.0978179378922
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 280.8678857389745
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 220
  episodes_total: 25343
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.1535676084458828
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037683748135653636
        model: {}
        policy_loss: -0.00816029119111287
        total_loss: 2.491507132848104
        vf_explained_var: 0.9939917922019958
        vf_loss: 2.499744196732839
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69333333333333
    gpu_util_percent0: 0.3973333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707375323026842
    mean_env_wait_ms: 1.2268908563964875
    mean_inference_ms: 4.344350498097219
    mean_raw_obs_processing_ms: 0.3814951690175875
  time_since_restore: 3149.729260444641
  time_this_iter_s: 25.824891805648804
  time_total_s: 3149.729260444641
  timers:
    learn_throughput: 8718.58
    learn_time_ms: 18557.151
    sample_throughput: 23734.76
    sample_time_ms: 6816.669
    update_time_ms: 41.979
  timestamp: 1602641053
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    122 |          3149.73 | 19738624 |  280.868 |              322.838 |              148.141 |            776.098 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3204.54029827377
    time_step_min: 2929
  date: 2020-10-14_02-04-39
  done: false
  episode_len_mean: 776.0791746453555
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 281.1281733667916
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 25589
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12881876900792122
        entropy_coeff: 0.0005000000000000001
        kl: 0.003385108410536001
        model: {}
        policy_loss: -0.0073744756446103565
        total_loss: 2.510595420996348
        vf_explained_var: 0.9941862225532532
        vf_loss: 2.5180342396100364
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.04137931034483
    gpu_util_percent0: 0.3306896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470603185055412
    mean_env_wait_ms: 1.2268337222164847
    mean_inference_ms: 4.34359017777652
    mean_raw_obs_processing_ms: 0.38143681364115334
  time_since_restore: 3175.312572002411
  time_this_iter_s: 25.583311557769775
  time_total_s: 3175.312572002411
  timers:
    learn_throughput: 8699.846
    learn_time_ms: 18597.112
    sample_throughput: 23781.502
    sample_time_ms: 6803.271
    update_time_ms: 40.507
  timestamp: 1602641079
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    123 |          3175.31 | 19900416 |  281.128 |              322.838 |              148.141 |            776.079 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3203.3868860109674
    time_step_min: 2913
  date: 2020-10-14_02-05-05
  done: false
  episode_len_mean: 776.0806057076296
  episode_reward_max: 325.2626262626264
  episode_reward_mean: 281.30228316949353
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 25755
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.13009627535939217
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038847767476302883
        model: {}
        policy_loss: -0.008719050629830841
        total_loss: 1.7567763328552246
        vf_explained_var: 0.99471515417099
        vf_loss: 1.7655604481697083
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.96666666666666
    gpu_util_percent0: 0.3443333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705343052451114
    mean_env_wait_ms: 1.2268005830191837
    mean_inference_ms: 4.343123215349118
    mean_raw_obs_processing_ms: 0.3814020586390368
  time_since_restore: 3200.754431247711
  time_this_iter_s: 25.441859245300293
  time_total_s: 3200.754431247711
  timers:
    learn_throughput: 8690.754
    learn_time_ms: 18616.566
    sample_throughput: 23849.5
    sample_time_ms: 6783.874
    update_time_ms: 38.1
  timestamp: 1602641105
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    124 |          3200.75 | 20062208 |  281.302 |              325.263 |              148.141 |            776.081 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3201.9593301435407
    time_step_min: 2913
  date: 2020-10-14_02-05-31
  done: false
  episode_len_mean: 776.0910316665382
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 281.5123198235534
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 25958
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.14802184825142226
        entropy_coeff: 0.0005000000000000001
        kl: 0.004089848545845598
        model: {}
        policy_loss: -0.008584606655252477
        total_loss: 2.518684128920237
        vf_explained_var: 0.9938375353813171
        vf_loss: 2.5273427963256836
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704572308707653
    mean_env_wait_ms: 1.2267610904779334
    mean_inference_ms: 4.342596905295805
    mean_raw_obs_processing_ms: 0.3813615318640824
  time_since_restore: 3226.548545360565
  time_this_iter_s: 25.794114112854004
  time_total_s: 3226.548545360565
  timers:
    learn_throughput: 8677.505
    learn_time_ms: 18644.991
    sample_throughput: 23807.951
    sample_time_ms: 6795.713
    update_time_ms: 37.408
  timestamp: 1602641131
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    125 |          3226.55 | 20224000 |  281.512 |              325.566 |              148.141 |            776.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3200.2166488386306
    time_step_min: 2913
  date: 2020-10-14_02-05-57
  done: false
  episode_len_mean: 776.1191166374247
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 281.7804534782567
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 260
  episodes_total: 26218
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.13204827283819517
        entropy_coeff: 0.0005000000000000001
        kl: 0.028698720193157595
        model: {}
        policy_loss: -0.007668172408254274
        total_loss: 2.3563475807507834
        vf_explained_var: 0.9947864413261414
        vf_loss: 2.3640817999839783
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16
    gpu_util_percent0: 0.3343333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703219514981936
    mean_env_wait_ms: 1.2266962279314602
    mean_inference_ms: 4.341830431236121
    mean_raw_obs_processing_ms: 0.38130240996806725
  time_since_restore: 3252.1626768112183
  time_this_iter_s: 25.614131450653076
  time_total_s: 3252.1626768112183
  timers:
    learn_throughput: 8678.087
    learn_time_ms: 18643.741
    sample_throughput: 23770.708
    sample_time_ms: 6806.36
    update_time_ms: 36.001
  timestamp: 1602641157
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    126 |          3252.16 | 20385792 |   281.78 |              325.566 |              148.141 |            776.119 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3199.1153962951716
    time_step_min: 2913
  date: 2020-10-14_02-06-23
  done: false
  episode_len_mean: 776.1385204274994
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 281.9498961417404
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 26386
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.0657581468206415e-21
        cur_lr: 5.0e-05
        entropy: 0.12528735275069872
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036473724176175892
        model: {}
        policy_loss: -0.008693015120419053
        total_loss: 1.7045717636744182
        vf_explained_var: 0.99503093957901
        vf_loss: 1.7133275071779888
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97666666666667
    gpu_util_percent0: 0.294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702543153401698
    mean_env_wait_ms: 1.2266595713002293
    mean_inference_ms: 4.341378858866497
    mean_raw_obs_processing_ms: 0.38126823948723493
  time_since_restore: 3278.123561143875
  time_this_iter_s: 25.96088433265686
  time_total_s: 3278.123561143875
  timers:
    learn_throughput: 8671.26
    learn_time_ms: 18658.418
    sample_throughput: 23735.592
    sample_time_ms: 6816.43
    update_time_ms: 37.49
  timestamp: 1602641183
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    127 |          3278.12 | 20547584 |   281.95 |              325.566 |              148.141 |            776.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3197.8714382632293
    time_step_min: 2913
  date: 2020-10-14_02-06-50
  done: false
  episode_len_mean: 776.1434108527131
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 282.1367695925159
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 26574
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0328790734103207e-21
        cur_lr: 5.0e-05
        entropy: 0.14094755674401918
        entropy_coeff: 0.0005000000000000001
        kl: 0.003903547185473144
        model: {}
        policy_loss: -0.009575619517515102
        total_loss: 2.0196957190831504
        vf_explained_var: 0.9947615265846252
        vf_loss: 2.029341826836268
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490000000000002
    gpu_util_percent0: 0.3206666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701844629066463
    mean_env_wait_ms: 1.2266177545238162
    mean_inference_ms: 4.34091448811984
    mean_raw_obs_processing_ms: 0.38123184438419416
  time_since_restore: 3303.7634992599487
  time_this_iter_s: 25.63993811607361
  time_total_s: 3303.7634992599487
  timers:
    learn_throughput: 8674.018
    learn_time_ms: 18652.486
    sample_throughput: 23747.737
    sample_time_ms: 6812.944
    update_time_ms: 38.889
  timestamp: 1602641210
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    128 |          3303.76 | 20709376 |  282.137 |              325.566 |              148.141 |            776.143 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3196.0485730274204
    time_step_min: 2913
  date: 2020-10-14_02-07-16
  done: false
  episode_len_mean: 776.1438149513912
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 282.41856378061544
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 273
  episodes_total: 26847
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0164395367051604e-21
        cur_lr: 5.0e-05
        entropy: 0.13146775464216867
        entropy_coeff: 0.0005000000000000001
        kl: 0.004833194233166675
        model: {}
        policy_loss: -0.007567190941093334
        total_loss: 2.4045091470082602
        vf_explained_var: 0.9947856068611145
        vf_loss: 2.4121420780817666
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700530288286467
    mean_env_wait_ms: 1.2265470393181865
    mean_inference_ms: 4.340154673410102
    mean_raw_obs_processing_ms: 0.38117258556337175
  time_since_restore: 3329.3254261016846
  time_this_iter_s: 25.56192684173584
  time_total_s: 3329.3254261016846
  timers:
    learn_throughput: 8682.911
    learn_time_ms: 18633.381
    sample_throughput: 23723.918
    sample_time_ms: 6819.784
    update_time_ms: 36.6
  timestamp: 1602641236
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    129 |          3329.33 | 20871168 |  282.419 |              325.566 |              148.141 |            776.144 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3194.8207666073545
    time_step_min: 2913
  date: 2020-10-14_02-07-42
  done: false
  episode_len_mean: 776.1440521134059
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 282.59960923918294
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 27018
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.082197683525802e-22
        cur_lr: 5.0e-05
        entropy: 0.11730944241086642
        entropy_coeff: 0.0005000000000000001
        kl: 0.004376026995790501
        model: {}
        policy_loss: -0.009059199013184601
        total_loss: 1.881285935640335
        vf_explained_var: 0.9945165514945984
        vf_loss: 1.8904037078221638
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.462068965517243
    gpu_util_percent0: 0.33931034482758615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699840202799333
    mean_env_wait_ms: 1.2265058233316524
    mean_inference_ms: 4.339710168064661
    mean_raw_obs_processing_ms: 0.3811388046908617
  time_since_restore: 3354.7258961200714
  time_this_iter_s: 25.40047001838684
  time_total_s: 3354.7258961200714
  timers:
    learn_throughput: 8692.077
    learn_time_ms: 18613.733
    sample_throughput: 23737.102
    sample_time_ms: 6815.996
    update_time_ms: 28.053
  timestamp: 1602641262
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    130 |          3354.73 | 21032960 |    282.6 |              325.566 |              148.141 |            776.144 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3193.5971276008104
    time_step_min: 2907
  date: 2020-10-14_02-08-08
  done: false
  episode_len_mean: 776.1513402213479
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 282.7825008922925
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 27197
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.541098841762901e-22
        cur_lr: 5.0e-05
        entropy: 0.13284215020636717
        entropy_coeff: 0.0005000000000000001
        kl: 0.004299151323114832
        model: {}
        policy_loss: -0.008989826989515374
        total_loss: 1.886713832616806
        vf_explained_var: 0.9947434067726135
        vf_loss: 1.8957700928052266
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.786666666666676
    gpu_util_percent0: 0.323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699152536248405
    mean_env_wait_ms: 1.2264608138489952
    mean_inference_ms: 4.33927260119998
    mean_raw_obs_processing_ms: 0.381104188291639
  time_since_restore: 3380.375
  time_this_iter_s: 25.64910387992859
  time_total_s: 3380.375
  timers:
    learn_throughput: 8689.185
    learn_time_ms: 18619.928
    sample_throughput: 23709.857
    sample_time_ms: 6823.829
    update_time_ms: 27.626
  timestamp: 1602641288
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    131 |          3380.38 | 21194752 |  282.783 |              326.172 |              148.141 |            776.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3191.7305673034857
    time_step_min: 2907
  date: 2020-10-14_02-08-34
  done: false
  episode_len_mean: 776.1509646887514
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 283.06908730552715
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 273
  episodes_total: 27470
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2705494208814505e-22
        cur_lr: 5.0e-05
        entropy: 0.1297167713443438
        entropy_coeff: 0.0005000000000000001
        kl: 0.003745588765013963
        model: {}
        policy_loss: -0.007870971912173749
        total_loss: 2.4279608527819314
        vf_explained_var: 0.994748055934906
        vf_loss: 2.435896654923757
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.806666666666672
    gpu_util_percent0: 0.32033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697968014884133
    mean_env_wait_ms: 1.2263866648345427
    mean_inference_ms: 4.338573090146553
    mean_raw_obs_processing_ms: 0.3810481954885094
  time_since_restore: 3405.9990923404694
  time_this_iter_s: 25.62409234046936
  time_total_s: 3405.9990923404694
  timers:
    learn_throughput: 8697.731
    learn_time_ms: 18601.634
    sample_throughput: 23686.043
    sample_time_ms: 6830.689
    update_time_ms: 27.271
  timestamp: 1602641314
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    132 |             3406 | 21356544 |  283.069 |              326.172 |              148.141 |            776.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3190.45034048102
    time_step_min: 2907
  date: 2020-10-14_02-09-00
  done: false
  episode_len_mean: 776.1473417721519
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 283.261524832411
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 180
  episodes_total: 27650
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.352747104407252e-23
        cur_lr: 5.0e-05
        entropy: 0.10594903180996577
        entropy_coeff: 0.0005000000000000001
        kl: 0.003527582564856857
        model: {}
        policy_loss: -0.006154920420764635
        total_loss: 1.3211583296457927
        vf_explained_var: 0.9961581826210022
        vf_loss: 1.3273661732673645
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.28333333333334
    gpu_util_percent0: 0.40399999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697239621197475
    mean_env_wait_ms: 1.2263393209112599
    mean_inference_ms: 4.338115460231313
    mean_raw_obs_processing_ms: 0.38101321981230596
  time_since_restore: 3431.8893938064575
  time_this_iter_s: 25.89030146598816
  time_total_s: 3431.8893938064575
  timers:
    learn_throughput: 8696.502
    learn_time_ms: 18604.263
    sample_throughput: 23592.749
    sample_time_ms: 6857.7
    update_time_ms: 27.525
  timestamp: 1602641340
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    133 |          3431.89 | 21518336 |  283.262 |              326.172 |              148.141 |            776.147 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3189.2884089272857
    time_step_min: 2907
  date: 2020-10-14_02-09-27
  done: false
  episode_len_mean: 776.1417942635325
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 283.4410625556842
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 27822
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.176373552203626e-23
        cur_lr: 5.0e-05
        entropy: 0.11555669270455837
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031900168784583607
        model: {}
        policy_loss: -0.007046100625302643
        total_loss: 1.592195341984431
        vf_explained_var: 0.995314359664917
        vf_loss: 1.5992992719014485
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.293333333333337
    gpu_util_percent0: 0.38500000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696599370791688
    mean_env_wait_ms: 1.2262942159110302
    mean_inference_ms: 4.337716578486233
    mean_raw_obs_processing_ms: 0.3809811929860615
  time_since_restore: 3457.5606241226196
  time_this_iter_s: 25.67123031616211
  time_total_s: 3457.5606241226196
  timers:
    learn_throughput: 8691.318
    learn_time_ms: 18615.359
    sample_throughput: 23559.747
    sample_time_ms: 6867.306
    update_time_ms: 28.921
  timestamp: 1602641367
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    134 |          3457.56 | 21680128 |  283.441 |              326.172 |              148.141 |            776.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3187.4292538590425
    time_step_min: 2892
  date: 2020-10-14_02-09-53
  done: false
  episode_len_mean: 776.1285729541166
  episode_reward_max: 328.4444444444443
  episode_reward_mean: 283.72969289952175
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 271
  episodes_total: 28093
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588186776101813e-23
        cur_lr: 5.0e-05
        entropy: 0.12740024427572885
        entropy_coeff: 0.0005000000000000001
        kl: 0.00971499465716382
        model: {}
        policy_loss: -0.008060082114146402
        total_loss: 2.152137597401937
        vf_explained_var: 0.9952964782714844
        vf_loss: 2.1602613031864166
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.340000000000007
    gpu_util_percent0: 0.39
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695467928486694
    mean_env_wait_ms: 1.2262161055479226
    mean_inference_ms: 4.337054017486646
    mean_raw_obs_processing_ms: 0.3809275613105411
  time_since_restore: 3483.1588604450226
  time_this_iter_s: 25.598236322402954
  time_total_s: 3483.1588604450226
  timers:
    learn_throughput: 8692.487
    learn_time_ms: 18612.856
    sample_throughput: 23613.677
    sample_time_ms: 6851.622
    update_time_ms: 27.04
  timestamp: 1602641393
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    135 |          3483.16 | 21841920 |   283.73 |              328.444 |              148.141 |            776.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3186.065049575071
    time_step_min: 2892
  date: 2020-10-14_02-10-19
  done: false
  episode_len_mean: 776.1097871437663
  episode_reward_max: 328.4444444444443
  episode_reward_mean: 283.9290122067861
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 189
  episodes_total: 28282
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588186776101813e-23
        cur_lr: 5.0e-05
        entropy: 0.11030422834058602
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043239609415953355
        model: {}
        policy_loss: -0.008025283052120358
        total_loss: 1.1861658692359924
        vf_explained_var: 0.9964837431907654
        vf_loss: 1.1942463219165802
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.959999999999997
    gpu_util_percent0: 0.31166666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694739792082312
    mean_env_wait_ms: 1.2261623546122336
    mean_inference_ms: 4.3365926752942014
    mean_raw_obs_processing_ms: 0.3808916812080857
  time_since_restore: 3508.803498983383
  time_this_iter_s: 25.644638538360596
  time_total_s: 3508.803498983383
  timers:
    learn_throughput: 8687.944
    learn_time_ms: 18622.587
    sample_throughput: 23638.273
    sample_time_ms: 6844.493
    update_time_ms: 29.021
  timestamp: 1602641419
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    136 |           3508.8 | 22003712 |  283.929 |              328.444 |              148.141 |             776.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3184.8920098556846
    time_step_min: 2892
  date: 2020-10-14_02-10-45
  done: false
  episode_len_mean: 776.0864262617742
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.10418397385934
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 28452
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.940933880509065e-24
        cur_lr: 5.0e-05
        entropy: 0.11905373570819695
        entropy_coeff: 0.0005000000000000001
        kl: 0.004403135972097516
        model: {}
        policy_loss: -0.00856983138267727
        total_loss: 1.6307201584180195
        vf_explained_var: 0.9949879050254822
        vf_loss: 1.6393495698769887
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83870967741936
    gpu_util_percent0: 0.30709677419354836
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469413492327961
    mean_env_wait_ms: 1.2261169399130667
    mean_inference_ms: 4.336218327311042
    mean_raw_obs_processing_ms: 0.3808619227844404
  time_since_restore: 3534.7950427532196
  time_this_iter_s: 25.991543769836426
  time_total_s: 3534.7950427532196
  timers:
    learn_throughput: 8681.25
    learn_time_ms: 18636.948
    sample_throughput: 23684.89
    sample_time_ms: 6831.022
    update_time_ms: 28.748
  timestamp: 1602641445
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    137 |           3534.8 | 22165504 |  284.104 |              329.354 |              148.141 |            776.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3183.162893717953
    time_step_min: 2892
  date: 2020-10-14_02-11-12
  done: false
  episode_len_mean: 776.0514436975375
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.3688552129917
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 28711
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.970466940254533e-24
        cur_lr: 5.0e-05
        entropy: 0.1257868049045404
        entropy_coeff: 0.0005000000000000001
        kl: 0.004212674160953611
        model: {}
        policy_loss: -0.006877902759394298
        total_loss: 2.365839660167694
        vf_explained_var: 0.9947009086608887
        vf_loss: 2.372780521710714
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90333333333334
    gpu_util_percent0: 0.334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693104661085252
    mean_env_wait_ms: 1.2260396201982586
    mean_inference_ms: 4.33560923361709
    mean_raw_obs_processing_ms: 0.38081157908144764
  time_since_restore: 3560.6217906475067
  time_this_iter_s: 25.82674789428711
  time_total_s: 3560.6217906475067
  timers:
    learn_throughput: 8673.883
    learn_time_ms: 18652.776
    sample_throughput: 23705.182
    sample_time_ms: 6825.174
    update_time_ms: 35.842
  timestamp: 1602641472
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    138 |          3560.62 | 22327296 |  284.369 |              329.354 |              148.141 |            776.051 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3181.7505541701303
    time_step_min: 2892
  date: 2020-10-14_02-11-38
  done: false
  episode_len_mean: 776.0364529293768
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.58066659540003
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 28914
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9852334701272663e-24
        cur_lr: 5.0e-05
        entropy: 0.10629856400191784
        entropy_coeff: 0.0005000000000000001
        kl: 0.004265572448881964
        model: {}
        policy_loss: -0.009370756107576502
        total_loss: 1.5155181686083476
        vf_explained_var: 0.9956105351448059
        vf_loss: 1.5249420702457428
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.00689655172414
    gpu_util_percent0: 0.3541379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692334971501653
    mean_env_wait_ms: 1.225977328564804
    mean_inference_ms: 4.335136052271206
    mean_raw_obs_processing_ms: 0.3807739750619267
  time_since_restore: 3586.1350049972534
  time_this_iter_s: 25.513214349746704
  time_total_s: 3586.1350049972534
  timers:
    learn_throughput: 8670.102
    learn_time_ms: 18660.911
    sample_throughput: 23752.888
    sample_time_ms: 6811.466
    update_time_ms: 35.944
  timestamp: 1602641498
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | RUNNING  | 172.17.0.4:50546 |    139 |          3586.14 | 22489088 |  284.581 |              329.354 |              148.141 |            776.036 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_20684_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3180.577926997245
    time_step_min: 2892
  date: 2020-10-14_02-12-04
  done: true
  episode_len_mean: 776.0161268138368
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.7554813661685
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 29082
  experiment_id: 2fe6efa4235546948753c324bdf67451
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.926167350636332e-25
        cur_lr: 5.0e-05
        entropy: 0.10871118493378162
        entropy_coeff: 0.0005000000000000001
        kl: 0.004006147869707395
        model: {}
        policy_loss: -0.00800386757813006
        total_loss: 1.3685668210188549
        vf_explained_var: 0.9955697655677795
        vf_loss: 1.3766250709692638
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.403225806451605
    gpu_util_percent0: 0.33774193548387094
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 50546
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469173734322605
    mean_env_wait_ms: 1.225930916478097
    mean_inference_ms: 4.334779104379572
    mean_raw_obs_processing_ms: 0.3807449800412153
  time_since_restore: 3611.9735267162323
  time_this_iter_s: 25.838521718978882
  time_total_s: 3611.9735267162323
  timers:
    learn_throughput: 8655.932
    learn_time_ms: 18691.459
    sample_throughput: 23721.189
    sample_time_ms: 6820.569
    update_time_ms: 37.366
  timestamp: 1602641524
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: '20684_00000'
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | TERMINATED |       |    140 |          3611.97 | 22650880 |  284.755 |              329.354 |              148.141 |            776.016 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.81 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_20684_00000 | TERMINATED |       |    140 |          3611.97 | 22650880 |  284.755 |              329.354 |              148.141 |            776.016 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


