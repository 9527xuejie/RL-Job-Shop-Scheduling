{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FCMaskedActionsModel' from 'JSS.models' (/JSS/JSS/models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-57e257253eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJSS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBestActionsWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mJSS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFCMaskedActionsModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_xvfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FCMaskedActionsModel' from 'JSS.models' (/JSS/JSS/models.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import plotly.io as pio\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "from JSS.CustomCallbacks import CustomCallbacks\n",
    "from JSS.env.JSS import JSS\n",
    "from ray.rllib.agents.ppo import ppo, PPOTrainer\n",
    "\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "from JSS.env_wrapper import BestActionsWrapper\n",
    "\n",
    "from JSS.models import FCMaskedActionsModel\n",
    "\n",
    "pio.orca.config.use_xvfb = True\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "'''\n",
    "            'lr': {\n",
    "                'values': [5e-5, 1e-5]\n",
    "            },\n",
    "            'lambda': {\n",
    "                'values': [0.90, 0.95, 1.0]\n",
    "            },\n",
    "            'clip_param': {\n",
    "                'values': [0.2, 0.3, 0.4]\n",
    "            },\n",
    "            'num_sgd_iter': {\n",
    "                'values': [30, 35, 40]\n",
    "            },\n",
    "            'entropy_coeff': {\n",
    "                'values': [0.0, 1e-4]\n",
    "            }\n",
    "'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"I have detected {} CPUs here, so I'm going to create {} actors\".format(mp.cpu_count(), mp.cpu_count() - 1))\n",
    "    os.environ[\"WANDB_API_KEY\"] = '3487a01956bf67cc7882bca2a38f70c8c95f8463'\n",
    "    sweep_config = {\n",
    "        'program': 'train.py',\n",
    "        'method': 'grid',\n",
    "        'metric': {\n",
    "            'name': 'time_step_min',\n",
    "            'goal': 'minimize',\n",
    "        },\n",
    "        'parameters': {\n",
    "            'lr': {\n",
    "                'values': [5e-5, 1e-5]\n",
    "            },\n",
    "            'lambda': {\n",
    "                'values': [0.90, 0.95, 1.0]\n",
    "            },\n",
    "            'clip_param': {\n",
    "                'values': [0.2, 0.3, 0.4]\n",
    "            },\n",
    "            'num_sgd_iter': {\n",
    "                'values': [30, 35, 40]\n",
    "            },\n",
    "            'entropy_coeff': {\n",
    "                'values': [0.0, 1e-4]\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 3vcawkyg\n",
      "Sweep URL: https://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"RLLIB_SWEEP_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
      "2020-11-14 14:31:47,683 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2020-11-14 14:31:48,003 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 14:31:48,003 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta51\n",
      "2020-11-14 14:31:48,005 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 14:31:53,025 - wandb.wandb_agent - INFO - Running runs: ['k1k7rtin']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mconfused-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/k1k7rtin\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_143151-k1k7rtin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 14:31:55,127\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 21.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=10128)\u001b[0m 2020-11-14 14:32:01,258\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10078)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10078)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10057)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10057)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10030)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10030)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10009)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10009)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10113)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10113)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10128)\u001b[0m 2020-11-14 14:32:13,443\tINFO trainable.py:252 -- Trainable.setup took 13.029 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=10130)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10130)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10106)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10106)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10064)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10064)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10118)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10118)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10053)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10053)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10108)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10108)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10050)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10050)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10089)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10089)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10098)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10098)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10115)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10115)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10101)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10101)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10052)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10052)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10051)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10051)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10105)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10105)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10061)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10061)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10059)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10059)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10066)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10066)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10016)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10016)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10119)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10119)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10076)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10076)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10111)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10111)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10069)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10069)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10046)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10046)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10125)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10125)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10120)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10120)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10122)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10122)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10048)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10048)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10043)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10043)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10023)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10023)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10018)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10018)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10062)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10062)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10093)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10093)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9998)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9998)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10024)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10024)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10054)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10054)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10058)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10058)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10055)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10055)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10022)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10022)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10025)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10025)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9997)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9997)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10007)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10007)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9982)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9982)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10020)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10020)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10028)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10028)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9994)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9994)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10084)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10084)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10029)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10029)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10040)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10040)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9992)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9992)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10012)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10012)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10073)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10073)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9995)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9995)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9983)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9983)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10026)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10026)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10015)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10015)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9976)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9976)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10033)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10033)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9987)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9987)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9977)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9977)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9980)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9980)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10014)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10014)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9972)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9972)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9973)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9973)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9985)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9985)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10021)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10021)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=10019)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=10019)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9978)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9978)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9971)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9971)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9990)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9990)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9975)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9975)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9968)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9968)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9969)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9969)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9974)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9974)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=9970)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=9970)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4029\n",
      "    time_step_mean: 3624.3030303030305\n",
      "    time_step_min: 3376\n",
      "  date: 2020-11-14_14-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 890.8607594936709\n",
      "  episode_reward_max: 445.01201033182053\n",
      "  episode_reward_mean: 410.3475829133991\n",
      "  episode_reward_min: 371.4786439633068\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1848209102948506\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.005303430526206891\n",
      "        model: {}\n",
      "        policy_loss: -0.010950734025148753\n",
      "        total_loss: 1151.9115193684895\n",
      "        vf_explained_var: 0.0947226881980896\n",
      "        vf_loss: 1151.9219970703125\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.35499999999999\n",
      "    gpu_util_percent0: 0.16275\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8535\n",
      "    ram_util_percent: 5.2125\n",
      "    vram_util_percent0: 0.0788537225319777\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351304\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22047438272615755\n",
      "    mean_env_wait_ms: 1.722368827459436\n",
      "    mean_inference_ms: 7.119783208340029\n",
      "    mean_raw_obs_processing_ms: 0.6006803951281092\n",
      "  time_since_restore: 37.8133909702301\n",
      "  time_this_iter_s: 37.8133909702301\n",
      "  time_total_s: 37.8133909702301\n",
      "  timers:\n",
      "    learn_throughput: 6312.734\n",
      "    learn_time_ms: 25629.467\n",
      "    sample_throughput: 13453.958\n",
      "    sample_time_ms: 12025.606\n",
      "    update_time_ms: 93.122\n",
      "  timestamp: 1605360771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      1 |          37.8134 | 161792 |  410.348 |              445.012 |              371.479 |            890.861 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4029\n",
      "    time_step_mean: 3624.9344827586206\n",
      "    time_step_min: 3362\n",
      "  date: 2020-11-14_14-33-28\n",
      "  done: false\n",
      "  episode_len_mean: 890.8417721518987\n",
      "  episode_reward_max: 445.01201033182053\n",
      "  episode_reward_mean: 410.95872733718096\n",
      "  episode_reward_min: 371.4786439633068\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1508761743704479\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.007790658700590332\n",
      "        model: {}\n",
      "        policy_loss: -0.013724471355089918\n",
      "        total_loss: 534.8116455078125\n",
      "        vf_explained_var: 0.30955448746681213\n",
      "        vf_loss: 534.8243815104166\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.21842105263158\n",
      "    gpu_util_percent0: 0.17526315789473684\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7092105263157895\n",
      "    ram_util_percent: 5.423684210526315\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2162213520612759\n",
      "    mean_env_wait_ms: 1.7002385677800633\n",
      "    mean_inference_ms: 6.985800870726215\n",
      "    mean_raw_obs_processing_ms: 0.5947635286057521\n",
      "  time_since_restore: 74.8778703212738\n",
      "  time_this_iter_s: 37.0644793510437\n",
      "  time_total_s: 74.8778703212738\n",
      "  timers:\n",
      "    learn_throughput: 6297.163\n",
      "    learn_time_ms: 25692.839\n",
      "    sample_throughput: 13983.642\n",
      "    sample_time_ms: 11570.09\n",
      "    update_time_ms: 99.043\n",
      "  timestamp: 1605360808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      2 |          74.8779 | 323584 |  410.959 |              445.012 |              371.479 |            890.842 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4029\n",
      "    time_step_mean: 3633.823660714286\n",
      "    time_step_min: 3323\n",
      "  date: 2020-11-14_14-34-05\n",
      "  done: false\n",
      "  episode_len_mean: 887.535864978903\n",
      "  episode_reward_max: 445.01201033182053\n",
      "  episode_reward_mean: 411.0353655531173\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.144042839606603\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008608704898506403\n",
      "        model: {}\n",
      "        policy_loss: -0.016264518082607538\n",
      "        total_loss: 216.14180501302084\n",
      "        vf_explained_var: 0.7074206471443176\n",
      "        vf_loss: 216.15692392985025\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.66153846153846\n",
      "    gpu_util_percent0: 0.16974358974358972\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8553846153846154\n",
      "    ram_util_percent: 5.412820512820512\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21396708125850233\n",
      "    mean_env_wait_ms: 1.6926073860096267\n",
      "    mean_inference_ms: 6.891326391983982\n",
      "    mean_raw_obs_processing_ms: 0.5921877625688047\n",
      "  time_since_restore: 111.92650032043457\n",
      "  time_this_iter_s: 37.04862999916077\n",
      "  time_total_s: 111.92650032043457\n",
      "  timers:\n",
      "    learn_throughput: 6274.611\n",
      "    learn_time_ms: 25785.182\n",
      "    sample_throughput: 14227.361\n",
      "    sample_time_ms: 11371.891\n",
      "    update_time_ms: 81.094\n",
      "  timestamp: 1605360845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      3 |          111.927 | 485376 |  411.035 |              445.012 |              366.183 |            887.536 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3627.8811881188117\n",
      "    time_step_min: 3298\n",
      "  date: 2020-11-14_14-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 884.8212025316456\n",
      "  episode_reward_max: 445.01201033182053\n",
      "  episode_reward_mean: 411.4972800026404\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.127517859141032\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009608327023064097\n",
      "        model: {}\n",
      "        policy_loss: -0.017834116394321125\n",
      "        total_loss: 51.44433752695719\n",
      "        vf_explained_var: 0.8877401351928711\n",
      "        vf_loss: 51.46081256866455\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.76756756756757\n",
      "    gpu_util_percent0: 0.19135135135135134\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8551351351351352\n",
      "    ram_util_percent: 5.435135135135135\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2125418759244267\n",
      "    mean_env_wait_ms: 1.686946150109173\n",
      "    mean_inference_ms: 6.816320300269578\n",
      "    mean_raw_obs_processing_ms: 0.5893990148584888\n",
      "  time_since_restore: 148.86378049850464\n",
      "  time_this_iter_s: 36.93728017807007\n",
      "  time_total_s: 148.86378049850464\n",
      "  timers:\n",
      "    learn_throughput: 6248.132\n",
      "    learn_time_ms: 25894.458\n",
      "    sample_throughput: 14477.498\n",
      "    sample_time_ms: 11175.412\n",
      "    update_time_ms: 78.846\n",
      "  timestamp: 1605360882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      4 |          148.864 | 647168 |  411.497 |              445.012 |              366.183 |            884.821 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3620.303664921466\n",
      "    time_step_min: 3298\n",
      "  date: 2020-11-14_14-35-20\n",
      "  done: false\n",
      "  episode_len_mean: 881.9848101265823\n",
      "  episode_reward_max: 445.01201033182053\n",
      "  episode_reward_mean: 411.35701007808876\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0994412700335185\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010827353689819574\n",
      "        model: {}\n",
      "        policy_loss: -0.02162367943674326\n",
      "        total_loss: 24.229916254679363\n",
      "        vf_explained_var: 0.9389947056770325\n",
      "        vf_loss: 24.249924818674724\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.0775\n",
      "    gpu_util_percent0: 0.178\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8547499999999999\n",
      "    ram_util_percent: 5.465000000000001\n",
      "    vram_util_percent0: 0.09060347654968842\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351304\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21172660516419778\n",
      "    mean_env_wait_ms: 1.6841593814070666\n",
      "    mean_inference_ms: 6.759845332047226\n",
      "    mean_raw_obs_processing_ms: 0.5871511089181728\n",
      "  time_since_restore: 186.2784140110016\n",
      "  time_this_iter_s: 37.41463351249695\n",
      "  time_total_s: 186.2784140110016\n",
      "  timers:\n",
      "    learn_throughput: 6242.762\n",
      "    learn_time_ms: 25916.733\n",
      "    sample_throughput: 14444.474\n",
      "    sample_time_ms: 11200.962\n",
      "    update_time_ms: 67.393\n",
      "  timestamp: 1605360920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      5 |          186.278 | 808960 |  411.357 |              445.012 |              366.183 |            881.985 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3610.8411037107517\n",
      "    time_step_min: 3295\n",
      "  date: 2020-11-14_14-35-57\n",
      "  done: false\n",
      "  episode_len_mean: 873.0649953574745\n",
      "  episode_reward_max: 446.33727710184274\n",
      "  episode_reward_mean: 411.37303931810067\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 287\n",
      "  episodes_total: 1077\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.076831301053365\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010266995135073861\n",
      "        model: {}\n",
      "        policy_loss: -0.020947431214153767\n",
      "        total_loss: 17.450903574625652\n",
      "        vf_explained_var: 0.9700104594230652\n",
      "        vf_loss: 17.470335960388184\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.2225\n",
      "    gpu_util_percent0: 0.1735\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542500000000001\n",
      "    ram_util_percent: 5.45\n",
      "    vram_util_percent0: 0.09060347654968842\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351304\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.210756257922317\n",
      "    mean_env_wait_ms: 1.6847542372786255\n",
      "    mean_inference_ms: 6.683835381939501\n",
      "    mean_raw_obs_processing_ms: 0.5836559448538908\n",
      "  time_since_restore: 223.8852527141571\n",
      "  time_this_iter_s: 37.60683870315552\n",
      "  time_total_s: 223.8852527141571\n",
      "  timers:\n",
      "    learn_throughput: 6235.782\n",
      "    learn_time_ms: 25945.743\n",
      "    sample_throughput: 14441.179\n",
      "    sample_time_ms: 11203.517\n",
      "    update_time_ms: 86.956\n",
      "  timestamp: 1605360957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      6 |          223.885 | 970752 |  411.373 |              446.337 |              366.183 |            873.065 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3607.0767366720515\n",
      "    time_step_min: 3269\n",
      "  date: 2020-11-14_14-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 868.3299050632911\n",
      "  episode_reward_max: 446.33727710184274\n",
      "  episode_reward_mean: 411.7483991442812\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 1264\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.073245108127594\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010475287524362406\n",
      "        model: {}\n",
      "        policy_loss: -0.02389539297049244\n",
      "        total_loss: 15.024248917897543\n",
      "        vf_explained_var: 0.9667276740074158\n",
      "        vf_loss: 15.046585957209269\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8945945945946\n",
      "    gpu_util_percent0: 0.17324324324324322\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8556756756756756\n",
      "    ram_util_percent: 5.429729729729729\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21009857165183027\n",
      "    mean_env_wait_ms: 1.6829763965231914\n",
      "    mean_inference_ms: 6.64589919637063\n",
      "    mean_raw_obs_processing_ms: 0.5819426239076876\n",
      "  time_since_restore: 260.825076341629\n",
      "  time_this_iter_s: 36.939823627471924\n",
      "  time_total_s: 260.825076341629\n",
      "  timers:\n",
      "    learn_throughput: 6229.169\n",
      "    learn_time_ms: 25973.288\n",
      "    sample_throughput: 14538.022\n",
      "    sample_time_ms: 11128.887\n",
      "    update_time_ms: 84.422\n",
      "  timestamp: 1605360994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      7 |          260.825 | 1132544 |  411.748 |              446.337 |              366.183 |             868.33 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3600.092406876791\n",
      "    time_step_min: 3269\n",
      "  date: 2020-11-14_14-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 864.0808720112518\n",
      "  episode_reward_max: 446.33727710184274\n",
      "  episode_reward_mean: 412.2271973664983\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0520044068495433\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010088972359274825\n",
      "        model: {}\n",
      "        policy_loss: -0.0228590602055192\n",
      "        total_loss: 14.46501890818278\n",
      "        vf_explained_var: 0.9679045081138611\n",
      "        vf_loss: 14.486385822296143\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.83157894736841\n",
      "    gpu_util_percent0: 0.18026315789473685\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8586842105263158\n",
      "    ram_util_percent: 5.444736842105263\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20966554230257398\n",
      "    mean_env_wait_ms: 1.6828495110134318\n",
      "    mean_inference_ms: 6.618436527025226\n",
      "    mean_raw_obs_processing_ms: 0.5807402296116257\n",
      "  time_since_restore: 297.87924122810364\n",
      "  time_this_iter_s: 37.05416488647461\n",
      "  time_total_s: 297.87924122810364\n",
      "  timers:\n",
      "    learn_throughput: 6232.018\n",
      "    learn_time_ms: 25961.412\n",
      "    sample_throughput: 14559.608\n",
      "    sample_time_ms: 11112.387\n",
      "    update_time_ms: 82.388\n",
      "  timestamp: 1605361032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      8 |          297.879 | 1294336 |  412.227 |              446.337 |              366.183 |            864.081 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3592.484555984556\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-37-48\n",
      "  done: false\n",
      "  episode_len_mean: 859.8696202531646\n",
      "  episode_reward_max: 451.5442263499575\n",
      "  episode_reward_mean: 412.69058213219705\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0216797093550365\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010133452635879317\n",
      "        model: {}\n",
      "        policy_loss: -0.023138205676029127\n",
      "        total_loss: 12.981405099232992\n",
      "        vf_explained_var: 0.9700682163238525\n",
      "        vf_loss: 13.00302783648173\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.40263157894736\n",
      "    gpu_util_percent0: 0.2089473684210526\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8473684210526314\n",
      "    ram_util_percent: 5.457894736842105\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20927884945456493\n",
      "    mean_env_wait_ms: 1.6829939722971547\n",
      "    mean_inference_ms: 6.593680857832407\n",
      "    mean_raw_obs_processing_ms: 0.5795841792763774\n",
      "  time_since_restore: 333.8506875038147\n",
      "  time_this_iter_s: 35.97144627571106\n",
      "  time_total_s: 333.8506875038147\n",
      "  timers:\n",
      "    learn_throughput: 6254.606\n",
      "    learn_time_ms: 25867.659\n",
      "    sample_throughput: 14624.705\n",
      "    sample_time_ms: 11062.924\n",
      "    update_time_ms: 79.791\n",
      "  timestamp: 1605361068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |      9 |          333.851 | 1456128 |  412.691 |              451.544 |              366.183 |             859.87 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3581.366391184573\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 852.9956545355785\n",
      "  episode_reward_max: 451.5442263499575\n",
      "  episode_reward_mean: 413.65111934716396\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 261\n",
      "  episodes_total: 1841\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9882914870977402\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009396027851228913\n",
      "        model: {}\n",
      "        policy_loss: -0.020341832656413317\n",
      "        total_loss: 10.947666645050049\n",
      "        vf_explained_var: 0.9828131794929504\n",
      "        vf_loss: 10.966623147328695\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.65263157894738\n",
      "    gpu_util_percent0: 0.21263157894736842\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8597368421052632\n",
      "    ram_util_percent: 5.431578947368421\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20873490504007564\n",
      "    mean_env_wait_ms: 1.6833951008084143\n",
      "    mean_inference_ms: 6.559188638376126\n",
      "    mean_raw_obs_processing_ms: 0.578051160264216\n",
      "  time_since_restore: 370.3091080188751\n",
      "  time_this_iter_s: 36.458420515060425\n",
      "  time_total_s: 370.3091080188751\n",
      "  timers:\n",
      "    learn_throughput: 6261.463\n",
      "    learn_time_ms: 25839.33\n",
      "    sample_throughput: 14668.806\n",
      "    sample_time_ms: 11029.664\n",
      "    update_time_ms: 75.545\n",
      "  timestamp: 1605361104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     10 |          370.309 | 1617920 |  413.651 |              451.544 |              366.183 |            852.996 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3576.5142998027613\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-39-01\n",
      "  done: false\n",
      "  episode_len_mean: 848.5087633885103\n",
      "  episode_reward_max: 451.5442263499575\n",
      "  episode_reward_mean: 414.1372776195071\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 213\n",
      "  episodes_total: 2054\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9757351080576578\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00984788709320128\n",
      "        model: {}\n",
      "        policy_loss: -0.02005356241716072\n",
      "        total_loss: 10.125995397567749\n",
      "        vf_explained_var: 0.981076717376709\n",
      "        vf_loss: 10.144567092259726\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.37368421052632\n",
      "    gpu_util_percent0: 0.18552631578947365\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.696578947368421\n",
      "    ram_util_percent: 5.447368421052633\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2083761350798751\n",
      "    mean_env_wait_ms: 1.6848240347263412\n",
      "    mean_inference_ms: 6.536014934902039\n",
      "    mean_raw_obs_processing_ms: 0.5769664551994728\n",
      "  time_since_restore: 407.28516936302185\n",
      "  time_this_iter_s: 36.97606134414673\n",
      "  time_total_s: 407.28516936302185\n",
      "  timers:\n",
      "    learn_throughput: 6253.931\n",
      "    learn_time_ms: 25870.45\n",
      "    sample_throughput: 14824.676\n",
      "    sample_time_ms: 10913.695\n",
      "    update_time_ms: 71.886\n",
      "  timestamp: 1605361141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     11 |          407.285 | 1779712 |  414.137 |              451.544 |              366.183 |            848.509 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3573.0562671546204\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-39-38\n",
      "  done: false\n",
      "  episode_len_mean: 845.5750452079566\n",
      "  episode_reward_max: 451.5442263499575\n",
      "  episode_reward_mean: 414.5180921289142\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2212\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9471239844957987\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009456372121348977\n",
      "        model: {}\n",
      "        policy_loss: -0.022650148409108322\n",
      "        total_loss: 9.371787071228027\n",
      "        vf_explained_var: 0.9802672266960144\n",
      "        vf_loss: 9.393019517262777\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.79210526315788\n",
      "    gpu_util_percent0: 0.16394736842105262\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8544736842105263\n",
      "    ram_util_percent: 5.439473684210526\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20814223876576274\n",
      "    mean_env_wait_ms: 1.6856536278027796\n",
      "    mean_inference_ms: 6.5213787539081585\n",
      "    mean_raw_obs_processing_ms: 0.5762981466127104\n",
      "  time_since_restore: 443.5847661495209\n",
      "  time_this_iter_s: 36.29959678649902\n",
      "  time_total_s: 443.5847661495209\n",
      "  timers:\n",
      "    learn_throughput: 6271.189\n",
      "    learn_time_ms: 25799.256\n",
      "    sample_throughput: 14846.174\n",
      "    sample_time_ms: 10897.892\n",
      "    update_time_ms: 68.202\n",
      "  timestamp: 1605361178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     12 |          443.585 | 1941504 |  414.518 |              451.544 |              366.183 |            845.575 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3569.1657435023435\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-40-14\n",
      "  done: false\n",
      "  episode_len_mean: 842.4218289085545\n",
      "  episode_reward_max: 451.5442263499575\n",
      "  episode_reward_mean: 414.8793088173279\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 161\n",
      "  episodes_total: 2373\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9162211616834005\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009542934751758972\n",
      "        model: {}\n",
      "        policy_loss: -0.02071073550420503\n",
      "        total_loss: 9.266199111938477\n",
      "        vf_explained_var: 0.9815285801887512\n",
      "        vf_loss: 9.285459518432617\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.11891891891891\n",
      "    gpu_util_percent0: 0.197027027027027\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8537837837837838\n",
      "    ram_util_percent: 5.427027027027027\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2079552145315295\n",
      "    mean_env_wait_ms: 1.6865755014249022\n",
      "    mean_inference_ms: 6.5079469070224745\n",
      "    mean_raw_obs_processing_ms: 0.5756357553204907\n",
      "  time_since_restore: 479.8975293636322\n",
      "  time_this_iter_s: 36.31276321411133\n",
      "  time_total_s: 479.8975293636322\n",
      "  timers:\n",
      "    learn_throughput: 6284.01\n",
      "    learn_time_ms: 25746.616\n",
      "    sample_throughput: 14882.176\n",
      "    sample_time_ms: 10871.528\n",
      "    update_time_ms: 68.688\n",
      "  timestamp: 1605361214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     13 |          479.898 | 2103296 |  414.879 |              451.544 |              366.183 |            842.422 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3561.3553376084496\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 837.1923795293238\n",
      "  episode_reward_max: 451.901500783417\n",
      "  episode_reward_mean: 415.67111485977483\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 304\n",
      "  episodes_total: 2677\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9005998373031616\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008788403977329532\n",
      "        model: {}\n",
      "        policy_loss: -0.020563649712130427\n",
      "        total_loss: 8.16783324877421\n",
      "        vf_explained_var: 0.9884054064750671\n",
      "        vf_loss: 8.187089363733927\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.86842105263158\n",
      "    gpu_util_percent0: 0.17815789473684213\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8578947368421053\n",
      "    ram_util_percent: 5.428947368421052\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2076218886411259\n",
      "    mean_env_wait_ms: 1.6880822208505277\n",
      "    mean_inference_ms: 6.484825066264864\n",
      "    mean_raw_obs_processing_ms: 0.5744889210447153\n",
      "  time_since_restore: 517.2054986953735\n",
      "  time_this_iter_s: 37.30796933174133\n",
      "  time_total_s: 517.2054986953735\n",
      "  timers:\n",
      "    learn_throughput: 6287.13\n",
      "    learn_time_ms: 25733.842\n",
      "    sample_throughput: 14823.228\n",
      "    sample_time_ms: 10914.762\n",
      "    update_time_ms: 67.978\n",
      "  timestamp: 1605361252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     14 |          517.205 | 2265088 |  415.671 |              451.902 |              366.183 |            837.192 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3556.431156848829\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-41-29\n",
      "  done: false\n",
      "  episode_len_mean: 834.5794655414909\n",
      "  episode_reward_max: 451.901500783417\n",
      "  episode_reward_mean: 416.05191309024394\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 167\n",
      "  episodes_total: 2844\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8784874230623245\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009080223040655255\n",
      "        model: {}\n",
      "        policy_loss: -0.020876465811549377\n",
      "        total_loss: 8.522263606389364\n",
      "        vf_explained_var: 0.9836145043373108\n",
      "        vf_loss: 8.541763226191202\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.23421052631579\n",
      "    gpu_util_percent0: 0.12394736842105263\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157896\n",
      "    ram_util_percent: 5.439473684210526\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20745110349170914\n",
      "    mean_env_wait_ms: 1.6889329774245612\n",
      "    mean_inference_ms: 6.473374419542243\n",
      "    mean_raw_obs_processing_ms: 0.5739197341845416\n",
      "  time_since_restore: 553.9505290985107\n",
      "  time_this_iter_s: 36.74503040313721\n",
      "  time_total_s: 553.9505290985107\n",
      "  timers:\n",
      "    learn_throughput: 6294.331\n",
      "    learn_time_ms: 25704.398\n",
      "    sample_throughput: 14886.419\n",
      "    sample_time_ms: 10868.43\n",
      "    update_time_ms: 71.382\n",
      "  timestamp: 1605361289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     15 |          553.951 | 2426880 |  416.052 |              451.902 |              366.183 |            834.579 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3552.515120967742\n",
      "    time_step_min: 3224\n",
      "  date: 2020-11-14_14-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 832.1229180546302\n",
      "  episode_reward_max: 451.901500783417\n",
      "  episode_reward_mean: 416.3953870564445\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3002\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8608155300219854\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008867490803822875\n",
      "        model: {}\n",
      "        policy_loss: -0.018860571435652673\n",
      "        total_loss: 7.67469600836436\n",
      "        vf_explained_var: 0.98388671875\n",
      "        vf_loss: 7.692213416099548\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.75135135135136\n",
      "    gpu_util_percent0: 0.21459459459459462\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8545945945945946\n",
      "    ram_util_percent: 5.456756756756757\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20728692860495213\n",
      "    mean_env_wait_ms: 1.6895824410216276\n",
      "    mean_inference_ms: 6.463191883007291\n",
      "    mean_raw_obs_processing_ms: 0.5734031454697385\n",
      "  time_since_restore: 590.3448579311371\n",
      "  time_this_iter_s: 36.39432883262634\n",
      "  time_total_s: 590.3448579311371\n",
      "  timers:\n",
      "    learn_throughput: 6313.362\n",
      "    learn_time_ms: 25626.917\n",
      "    sample_throughput: 14934.976\n",
      "    sample_time_ms: 10833.094\n",
      "    update_time_ms: 59.73\n",
      "  timestamp: 1605361325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | RUNNING  | 172.17.0.14:10128 |     16 |          590.345 | 2588672 |  416.395 |              451.902 |              366.183 |            832.123 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_c4f5f_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4287\n",
      "    time_step_mean: 3544.608333333333\n",
      "    time_step_min: 3216\n",
      "  date: 2020-11-14_14-42-42\n",
      "  done: true\n",
      "  episode_len_mean: 828.4761175750153\n",
      "  episode_reward_max: 453.45859389386146\n",
      "  episode_reward_mean: 416.9730354050241\n",
      "  episode_reward_min: 366.1834783590808\n",
      "  episodes_this_iter: 264\n",
      "  episodes_total: 3266\n",
      "  experiment_id: 92781af1807a431e9e18e8648e95eac9\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8314622441927592\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009435838786885142\n",
      "        model: {}\n",
      "        policy_loss: -0.01975601267380019\n",
      "        total_loss: 7.89491081237793\n",
      "        vf_explained_var: 0.9886993765830994\n",
      "        vf_loss: 7.913195490837097\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.84736842105262\n",
      "    gpu_util_percent0: 0.19842105263157894\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8476315789473684\n",
      "    ram_util_percent: 5.442105263157895\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 10128\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20702029953231976\n",
      "    mean_env_wait_ms: 1.6907364202945574\n",
      "    mean_inference_ms: 6.447800419039805\n",
      "    mean_raw_obs_processing_ms: 0.572606519482258\n",
      "  time_since_restore: 626.7417550086975\n",
      "  time_this_iter_s: 36.396897077560425\n",
      "  time_total_s: 626.7417550086975\n",
      "  timers:\n",
      "    learn_throughput: 6333.482\n",
      "    learn_time_ms: 25545.506\n",
      "    sample_throughput: 14906.388\n",
      "    sample_time_ms: 10853.87\n",
      "    update_time_ms: 58.261\n",
      "  timestamp: 1605361362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: c4f5f_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | TERMINATED |       |     17 |          626.742 | 2750464 |  416.973 |              453.459 |              366.183 |            828.476 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 42.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.15 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_c4f5f_00000 | TERMINATED |       |     17 |          626.742 | 2750464 |  416.973 |              453.459 |              366.183 |            828.476 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 14:42:42,979\tINFO tune.py:439 -- Total run time: 644.94 seconds (644.56 seconds for the tuning loop).\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 9842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_143151-k1k7rtin/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_143151-k1k7rtin/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 652\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605361363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 4287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3544.60833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 453.45859\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 366.18348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 416.97304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mconfused-sweep-1\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/k1k7rtin\u001b[0m\n",
      "2020-11-14 14:42:55,155 - wandb.wandb_agent - INFO - Cleaning up finished run: k1k7rtin\n",
      "2020-11-14 14:42:55,484 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 14:42:55,484 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta52\n",
      "2020-11-14 14:42:55,488 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 14:43:00,507 - wandb.wandb_agent - INFO - Running runs: ['37dpxos0']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclean-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/37dpxos0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_144258-37dpxos0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 14:43:02,380\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 25.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30165)\u001b[0m 2020-11-14 14:43:08,104\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30155)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30155)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30129)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30129)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30163)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30163)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30128)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30128)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30149)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30149)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30167)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30167)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30043)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30043)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30158)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30158)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30110)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30110)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30113)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30113)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30160)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30160)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30135)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30135)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30059)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30059)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30121)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30121)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30042)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30042)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30118)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30118)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30162)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30162)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30133)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30133)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30067)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30067)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30165)\u001b[0m 2020-11-14 14:43:21,201\tINFO trainable.py:252 -- Trainable.setup took 13.836 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=30076)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30076)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30055)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30055)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30116)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30116)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30122)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30122)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30115)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30115)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30142)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30142)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30057)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30057)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30112)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30112)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30104)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30104)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30092)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30092)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30106)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30106)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30087)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30087)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30138)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30138)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30025)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30025)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30018)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30018)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30063)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30063)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30103)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30103)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30062)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30062)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30068)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30068)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30071)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30071)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30072)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30072)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30066)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30066)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30064)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30064)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30017)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30017)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30027)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30027)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30065)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30065)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30037)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30037)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30099)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30099)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30020)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30020)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30026)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30026)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30108)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30108)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30040)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30040)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30075)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30075)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30019)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30019)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30006)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30006)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30010)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30010)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30016)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30016)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30009)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30009)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30047)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30047)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30046)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30046)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30035)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30035)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30079)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30079)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30015)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30015)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30049)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30049)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30044)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30044)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30005)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30005)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30003)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30003)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30023)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30023)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30069)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30069)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30052)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30052)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30007)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30007)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30008)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30008)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30060)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30060)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30011)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30011)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30070)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30070)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30012)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30012)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30051)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30051)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30014)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30014)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30004)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30004)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=30013)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=30013)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3936\n",
      "    time_step_mean: 3606.9322033898306\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 905.8101265822785\n",
      "  episode_reward_max: 413.0441840911731\n",
      "  episode_reward_mean: 385.95046932394337\n",
      "  episode_reward_min: 356.93504585023743\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1662820378939311\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.005672786423626046\n",
      "        model: {}\n",
      "        policy_loss: -0.011942101458165174\n",
      "        total_loss: 928.1277313232422\n",
      "        vf_explained_var: 0.10244659334421158\n",
      "        vf_loss: 928.1391296386719\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.54285714285713\n",
      "    gpu_util_percent0: 0.16666666666666666\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7285714285714286\n",
      "    ram_util_percent: 5.235714285714286\n",
      "    vram_util_percent0: 0.07879632666448015\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2243193802770254\n",
      "    mean_env_wait_ms: 1.7554839034991778\n",
      "    mean_inference_ms: 7.351359973611857\n",
      "    mean_raw_obs_processing_ms: 0.6221853593647866\n",
      "  time_since_restore: 39.35473847389221\n",
      "  time_this_iter_s: 39.35473847389221\n",
      "  time_total_s: 39.35473847389221\n",
      "  timers:\n",
      "    learn_throughput: 6038.269\n",
      "    learn_time_ms: 26794.432\n",
      "    sample_throughput: 13120.063\n",
      "    sample_time_ms: 12331.648\n",
      "    update_time_ms: 100.99\n",
      "  timestamp: 1605361440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      1 |          39.3547 | 161792 |   385.95 |              413.044 |              356.935 |             905.81 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3936\n",
      "    time_step_mean: 3619.9384057971015\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-44-37\n",
      "  done: false\n",
      "  episode_len_mean: 905.0696202531645\n",
      "  episode_reward_max: 414.18584105987566\n",
      "  episode_reward_mean: 386.6698545067542\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1401376624902089\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.007210661889985204\n",
      "        model: {}\n",
      "        policy_loss: -0.014223525739604762\n",
      "        total_loss: 393.2286682128906\n",
      "        vf_explained_var: 0.36154115200042725\n",
      "        vf_loss: 393.2420120239258\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.43846153846155\n",
      "    gpu_util_percent0: 0.18384615384615385\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8543589743589743\n",
      "    ram_util_percent: 5.435897435897436\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21858465220742604\n",
      "    mean_env_wait_ms: 1.7350434459979478\n",
      "    mean_inference_ms: 7.121500352288347\n",
      "    mean_raw_obs_processing_ms: 0.6057235552674116\n",
      "  time_since_restore: 76.46131324768066\n",
      "  time_this_iter_s: 37.10657477378845\n",
      "  time_total_s: 76.46131324768066\n",
      "  timers:\n",
      "    learn_throughput: 6156.522\n",
      "    learn_time_ms: 26279.774\n",
      "    sample_throughput: 13809.993\n",
      "    sample_time_ms: 11715.575\n",
      "    update_time_ms: 124.816\n",
      "  timestamp: 1605361477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      2 |          76.4613 | 323584 |   386.67 |              414.186 |              354.155 |             905.07 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3638.2741935483873\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 901.324894514768\n",
      "  episode_reward_max: 414.18584105987566\n",
      "  episode_reward_mean: 386.0131805567943\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1287761429945629\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00908343338718017\n",
      "        model: {}\n",
      "        policy_loss: -0.0182496157164375\n",
      "        total_loss: 133.91530481974283\n",
      "        vf_explained_var: 0.7524490356445312\n",
      "        vf_loss: 133.9322992960612\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.05\n",
      "    gpu_util_percent0: 0.17289473684210527\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8528947368421054\n",
      "    ram_util_percent: 5.423684210526316\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21497906831896296\n",
      "    mean_env_wait_ms: 1.7215377457353311\n",
      "    mean_inference_ms: 6.939831830410812\n",
      "    mean_raw_obs_processing_ms: 0.5959674547414966\n",
      "  time_since_restore: 113.16538119316101\n",
      "  time_this_iter_s: 36.70406794548035\n",
      "  time_total_s: 113.16538119316101\n",
      "  timers:\n",
      "    learn_throughput: 6175.513\n",
      "    learn_time_ms: 26198.955\n",
      "    sample_throughput: 14322.639\n",
      "    sample_time_ms: 11296.242\n",
      "    update_time_ms: 113.17\n",
      "  timestamp: 1605361514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      3 |          113.165 | 485376 |  386.013 |              414.186 |              354.155 |            901.325 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3643.9239864864867\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-45-51\n",
      "  done: false\n",
      "  episode_len_mean: 897.8908227848101\n",
      "  episode_reward_max: 414.18584105987566\n",
      "  episode_reward_mean: 386.1143365572859\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1092153290907543\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010611281419793764\n",
      "        model: {}\n",
      "        policy_loss: -0.019339736939097445\n",
      "        total_loss: 34.68215688069662\n",
      "        vf_explained_var: 0.8926677107810974\n",
      "        vf_loss: 34.69992891947428\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.6054054054054\n",
      "    gpu_util_percent0: 0.2064864864864865\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8516216216216218\n",
      "    ram_util_percent: 5.416216216216216\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21250552781921495\n",
      "    mean_env_wait_ms: 1.7126315745584373\n",
      "    mean_inference_ms: 6.816524589203127\n",
      "    mean_raw_obs_processing_ms: 0.5896744231950765\n",
      "  time_since_restore: 149.56441235542297\n",
      "  time_this_iter_s: 36.39903116226196\n",
      "  time_total_s: 149.56441235542297\n",
      "  timers:\n",
      "    learn_throughput: 6197.315\n",
      "    learn_time_ms: 26106.788\n",
      "    sample_throughput: 14596.385\n",
      "    sample_time_ms: 11084.388\n",
      "    update_time_ms: 102.419\n",
      "  timestamp: 1605361551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      4 |          149.564 | 647168 |  386.114 |              414.186 |              354.155 |            897.891 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3641.124\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 894.4481012658227\n",
      "  episode_reward_max: 420.3278687011911\n",
      "  episode_reward_mean: 386.4566554947891\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0817394057909648\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.011662103002890944\n",
      "        model: {}\n",
      "        policy_loss: -0.022384066212301452\n",
      "        total_loss: 21.672812302907307\n",
      "        vf_explained_var: 0.9333195686340332\n",
      "        vf_loss: 21.69340419769287\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.83947368421052\n",
      "    gpu_util_percent0: 0.15684210526315792\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157896\n",
      "    ram_util_percent: 5.457894736842105\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21076494352579206\n",
      "    mean_env_wait_ms: 1.7076473212301913\n",
      "    mean_inference_ms: 6.7276326601511816\n",
      "    mean_raw_obs_processing_ms: 0.5847660134743633\n",
      "  time_since_restore: 186.75147414207458\n",
      "  time_this_iter_s: 37.18706178665161\n",
      "  time_total_s: 186.75147414207458\n",
      "  timers:\n",
      "    learn_throughput: 6181.825\n",
      "    learn_time_ms: 26172.208\n",
      "    sample_throughput: 14712.405\n",
      "    sample_time_ms: 10996.978\n",
      "    update_time_ms: 85.99\n",
      "  timestamp: 1605361588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      5 |          186.751 | 808960 |  386.457 |              420.328 |              354.155 |            894.448 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3638.375408052231\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 889.7528675703858\n",
      "  episode_reward_max: 420.3278687011911\n",
      "  episode_reward_mean: 387.0355329728743\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 169\n",
      "  episodes_total: 959\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0393214424451191\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010392382508143783\n",
      "        model: {}\n",
      "        policy_loss: -0.021369865047745407\n",
      "        total_loss: 17.68813689549764\n",
      "        vf_explained_var: 0.9587783813476562\n",
      "        vf_loss: 17.707948366800945\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.76052631578949\n",
      "    gpu_util_percent0: 0.19842105263157894\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8528947368421054\n",
      "    ram_util_percent: 5.428947368421053\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20953040211318463\n",
      "    mean_env_wait_ms: 1.705641843428009\n",
      "    mean_inference_ms: 6.656947010449782\n",
      "    mean_raw_obs_processing_ms: 0.5810405162772817\n",
      "  time_since_restore: 223.88274478912354\n",
      "  time_this_iter_s: 37.13127064704895\n",
      "  time_total_s: 223.88274478912354\n",
      "  timers:\n",
      "    learn_throughput: 6176.67\n",
      "    learn_time_ms: 26194.049\n",
      "    sample_throughput: 14812.926\n",
      "    sample_time_ms: 10922.353\n",
      "    update_time_ms: 79.301\n",
      "  timestamp: 1605361625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      6 |          223.883 | 970752 |  387.036 |              420.328 |              354.155 |            889.753 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3633.026960784314\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-47-42\n",
      "  done: false\n",
      "  episode_len_mean: 881.9066455696203\n",
      "  episode_reward_max: 421.1033999213489\n",
      "  episode_reward_mean: 387.7342130486675\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 305\n",
      "  episodes_total: 1264\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.044483522574107\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010205054267620048\n",
      "        model: {}\n",
      "        policy_loss: -0.02159636956639588\n",
      "        total_loss: 14.226296504338583\n",
      "        vf_explained_var: 0.9707352519035339\n",
      "        vf_loss: 14.246374130249023\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.44999999999999\n",
      "    gpu_util_percent0: 0.2097368421052632\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8563157894736841\n",
      "    ram_util_percent: 5.434210526315789\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2079906100292654\n",
      "    mean_env_wait_ms: 1.7047650975698512\n",
      "    mean_inference_ms: 6.568175234426168\n",
      "    mean_raw_obs_processing_ms: 0.5764935072401807\n",
      "  time_since_restore: 260.87544107437134\n",
      "  time_this_iter_s: 36.9926962852478\n",
      "  time_total_s: 260.87544107437134\n",
      "  timers:\n",
      "    learn_throughput: 6181.402\n",
      "    learn_time_ms: 26173.997\n",
      "    sample_throughput: 14845.34\n",
      "    sample_time_ms: 10898.504\n",
      "    update_time_ms: 78.68\n",
      "  timestamp: 1605361662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      7 |          260.875 | 1132544 |  387.734 |              421.103 |              354.155 |            881.907 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3630.1903039073804\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-48-19\n",
      "  done: false\n",
      "  episode_len_mean: 877.789029535865\n",
      "  episode_reward_max: 423.1119730547744\n",
      "  episode_reward_mean: 388.22963501014664\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.023184984922409\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010703812974194685\n",
      "        model: {}\n",
      "        policy_loss: -0.023307525979665417\n",
      "        total_loss: 13.315986076990763\n",
      "        vf_explained_var: 0.9667624831199646\n",
      "        vf_loss: 13.337664286295572\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.88157894736844\n",
      "    gpu_util_percent0: 0.2005263157894737\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8544736842105263\n",
      "    ram_util_percent: 5.442105263157895\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20730099555649156\n",
      "    mean_env_wait_ms: 1.70450978594272\n",
      "    mean_inference_ms: 6.531752640780311\n",
      "    mean_raw_obs_processing_ms: 0.574681776578771\n",
      "  time_since_restore: 297.13606309890747\n",
      "  time_this_iter_s: 36.26062202453613\n",
      "  time_total_s: 297.13606309890747\n",
      "  timers:\n",
      "    learn_throughput: 6196.716\n",
      "    learn_time_ms: 26109.312\n",
      "    sample_throughput: 14927.382\n",
      "    sample_time_ms: 10838.605\n",
      "    update_time_ms: 76.125\n",
      "  timestamp: 1605361699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      8 |          297.136 | 1294336 |   388.23 |              423.112 |              354.155 |            877.789 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3628.238961038961\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-48-55\n",
      "  done: false\n",
      "  episode_len_mean: 874.2582278481012\n",
      "  episode_reward_max: 423.1119730547744\n",
      "  episode_reward_mean: 388.71442854534837\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9947953919569651\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009973581181839108\n",
      "        model: {}\n",
      "        policy_loss: -0.021764524979516864\n",
      "        total_loss: 13.02625838915507\n",
      "        vf_explained_var: 0.9678225517272949\n",
      "        vf_loss: 13.046525160471598\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58157894736841\n",
      "    gpu_util_percent0: 0.1518421052631579\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8560526315789474\n",
      "    ram_util_percent: 5.423684210526315\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20668731318129496\n",
      "    mean_env_wait_ms: 1.704176743683627\n",
      "    mean_inference_ms: 6.498232542542874\n",
      "    mean_raw_obs_processing_ms: 0.5729270078792527\n",
      "  time_since_restore: 333.6465606689453\n",
      "  time_this_iter_s: 36.51049757003784\n",
      "  time_total_s: 333.6465606689453\n",
      "  timers:\n",
      "    learn_throughput: 6203.79\n",
      "    learn_time_ms: 26079.541\n",
      "    sample_throughput: 14997.201\n",
      "    sample_time_ms: 10788.146\n",
      "    update_time_ms: 73.626\n",
      "  timestamp: 1605361735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |      9 |          333.647 | 1456128 |  388.714 |              423.112 |              354.155 |            874.258 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3626.5\n",
      "    time_step_min: 3250\n",
      "  date: 2020-11-14_14-49-32\n",
      "  done: false\n",
      "  episode_len_mean: 870.9062140391254\n",
      "  episode_reward_max: 427.0846189348235\n",
      "  episode_reward_mean: 389.1895256571656\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1738\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9606664677460989\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010052807784328857\n",
      "        model: {}\n",
      "        policy_loss: -0.021567164221778512\n",
      "        total_loss: 12.273973306020102\n",
      "        vf_explained_var: 0.9712756276130676\n",
      "        vf_loss: 12.294010003407797\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.30789473684209\n",
      "    gpu_util_percent0: 0.17157894736842105\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7121052631578948\n",
      "    ram_util_percent: 5.473684210526317\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2061925695364272\n",
      "    mean_env_wait_ms: 1.7042661012323155\n",
      "    mean_inference_ms: 6.469479747018211\n",
      "    mean_raw_obs_processing_ms: 0.5713714002704477\n",
      "  time_since_restore: 370.25083327293396\n",
      "  time_this_iter_s: 36.60427260398865\n",
      "  time_total_s: 370.25083327293396\n",
      "  timers:\n",
      "    learn_throughput: 6197.877\n",
      "    learn_time_ms: 26104.425\n",
      "    sample_throughput: 15083.312\n",
      "    sample_time_ms: 10726.556\n",
      "    update_time_ms: 70.398\n",
      "  timestamp: 1605361772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     10 |          370.251 | 1617920 |   389.19 |              427.085 |              354.155 |            870.906 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3620.156313645621\n",
      "    time_step_min: 3246\n",
      "  date: 2020-11-14_14-50-09\n",
      "  done: false\n",
      "  episode_len_mean: 865.434131736527\n",
      "  episode_reward_max: 427.0846189348235\n",
      "  episode_reward_mean: 390.07171067047244\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 266\n",
      "  episodes_total: 2004\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9240545779466629\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009648838623737296\n",
      "        model: {}\n",
      "        policy_loss: -0.020416484330780804\n",
      "        total_loss: 10.468444029490152\n",
      "        vf_explained_var: 0.9829499125480652\n",
      "        vf_loss: 10.487392822901407\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.12162162162161\n",
      "    gpu_util_percent0: 0.19837837837837838\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.854054054054054\n",
      "    ram_util_percent: 5.454054054054055\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20548589867149308\n",
      "    mean_env_wait_ms: 1.7052238309999648\n",
      "    mean_inference_ms: 6.4289457955785725\n",
      "    mean_raw_obs_processing_ms: 0.5691395556664404\n",
      "  time_since_restore: 406.56183099746704\n",
      "  time_this_iter_s: 36.31099772453308\n",
      "  time_total_s: 406.56183099746704\n",
      "  timers:\n",
      "    learn_throughput: 6218.878\n",
      "    learn_time_ms: 26016.267\n",
      "    sample_throughput: 15383.694\n",
      "    sample_time_ms: 10517.11\n",
      "    update_time_ms: 66.356\n",
      "  timestamp: 1605361809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     11 |          406.562 | 1779712 |  390.072 |              427.085 |              354.155 |            865.434 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3616.4894106813995\n",
      "    time_step_min: 3198\n",
      "  date: 2020-11-14_14-50-46\n",
      "  done: false\n",
      "  episode_len_mean: 861.9561482820976\n",
      "  episode_reward_max: 428.1432703930392\n",
      "  episode_reward_mean: 390.8149975173412\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 208\n",
      "  episodes_total: 2212\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9189932694037756\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00904487290730079\n",
      "        model: {}\n",
      "        policy_loss: -0.022064410188856225\n",
      "        total_loss: 10.00084932645162\n",
      "        vf_explained_var: 0.9810903072357178\n",
      "        vf_loss: 10.021564404169718\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.63589743589743\n",
      "    gpu_util_percent0: 0.1394871794871795\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8525641025641025\n",
      "    ram_util_percent: 5.464102564102565\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20501365136443694\n",
      "    mean_env_wait_ms: 1.7051907052725308\n",
      "    mean_inference_ms: 6.402755027341114\n",
      "    mean_raw_obs_processing_ms: 0.567838083320106\n",
      "  time_since_restore: 443.94615721702576\n",
      "  time_this_iter_s: 37.384326219558716\n",
      "  time_total_s: 443.94615721702576\n",
      "  timers:\n",
      "    learn_throughput: 6206.87\n",
      "    learn_time_ms: 26066.599\n",
      "    sample_throughput: 15426.931\n",
      "    sample_time_ms: 10487.634\n",
      "    update_time_ms: 57.278\n",
      "  timestamp: 1605361846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     12 |          443.946 | 1941504 |  390.815 |              428.143 |              354.155 |            861.956 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3613.9974248927037\n",
      "    time_step_min: 3198\n",
      "  date: 2020-11-14_14-51-23\n",
      "  done: false\n",
      "  episode_len_mean: 859.4396624472574\n",
      "  episode_reward_max: 428.1432703930392\n",
      "  episode_reward_mean: 391.33412132928635\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2370\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9046552777290344\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009154255657146374\n",
      "        model: {}\n",
      "        policy_loss: -0.02083587475741903\n",
      "        total_loss: 10.27193776766459\n",
      "        vf_explained_var: 0.9779281616210938\n",
      "        vf_loss: 10.29139494895935\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.73243243243243\n",
      "    gpu_util_percent0: 0.14837837837837836\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8554054054054054\n",
      "    ram_util_percent: 5.451351351351352\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20471196507875258\n",
      "    mean_env_wait_ms: 1.7055316454403984\n",
      "    mean_inference_ms: 6.384824537418494\n",
      "    mean_raw_obs_processing_ms: 0.5669405025961181\n",
      "  time_since_restore: 480.76152205467224\n",
      "  time_this_iter_s: 36.815364837646484\n",
      "  time_total_s: 480.76152205467224\n",
      "  timers:\n",
      "    learn_throughput: 6198.958\n",
      "    learn_time_ms: 26099.872\n",
      "    sample_throughput: 15441.585\n",
      "    sample_time_ms: 10477.681\n",
      "    update_time_ms: 51.072\n",
      "  timestamp: 1605361883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     13 |          480.762 | 2103296 |  391.334 |              428.143 |              354.155 |             859.44 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3611.0076366559488\n",
      "    time_step_min: 3186\n",
      "  date: 2020-11-14_14-52-00\n",
      "  done: false\n",
      "  episode_len_mean: 856.992088607595\n",
      "  episode_reward_max: 428.1432703930392\n",
      "  episode_reward_mean: 391.8504847754293\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2528\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8747162967920303\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009037651432057222\n",
      "        model: {}\n",
      "        policy_loss: -0.021497800946235657\n",
      "        total_loss: 9.720200935999552\n",
      "        vf_explained_var: 0.9792012572288513\n",
      "        vf_loss: 9.740328232447306\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.55897435897437\n",
      "    gpu_util_percent0: 0.20641025641025637\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8541025641025641\n",
      "    ram_util_percent: 5.482051282051282\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20443641356123404\n",
      "    mean_env_wait_ms: 1.7060940742673558\n",
      "    mean_inference_ms: 6.368638846025331\n",
      "    mean_raw_obs_processing_ms: 0.5661346931804587\n",
      "  time_since_restore: 517.7295463085175\n",
      "  time_this_iter_s: 36.968024253845215\n",
      "  time_total_s: 517.7295463085175\n",
      "  timers:\n",
      "    learn_throughput: 6187.855\n",
      "    learn_time_ms: 26146.701\n",
      "    sample_throughput: 15426.425\n",
      "    sample_time_ms: 10487.977\n",
      "    update_time_ms: 48.481\n",
      "  timestamp: 1605361920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     14 |           517.73 | 2265088 |   391.85 |              428.143 |              354.155 |            856.992 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3607.0588020452888\n",
      "    time_step_min: 3186\n",
      "  date: 2020-11-14_14-52-37\n",
      "  done: false\n",
      "  episode_len_mean: 853.5932325413967\n",
      "  episode_reward_max: 431.6059654174304\n",
      "  episode_reward_mean: 392.67287886797425\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 250\n",
      "  episodes_total: 2778\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8382627864678701\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00868772529065609\n",
      "        model: {}\n",
      "        policy_loss: -0.020066339561405282\n",
      "        total_loss: 8.629287322362265\n",
      "        vf_explained_var: 0.9865984916687012\n",
      "        vf_loss: 8.648035208384195\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.55789473684212\n",
      "    gpu_util_percent0: 0.19000000000000003\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8513157894736842\n",
      "    ram_util_percent: 5.421052631578948\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20406513321838893\n",
      "    mean_env_wait_ms: 1.7079843704223827\n",
      "    mean_inference_ms: 6.345939135804598\n",
      "    mean_raw_obs_processing_ms: 0.564900359719603\n",
      "  time_since_restore: 554.6688539981842\n",
      "  time_this_iter_s: 36.93930768966675\n",
      "  time_total_s: 554.6688539981842\n",
      "  timers:\n",
      "    learn_throughput: 6192.215\n",
      "    learn_time_ms: 26128.29\n",
      "    sample_throughput: 15435.448\n",
      "    sample_time_ms: 10481.847\n",
      "    update_time_ms: 50.747\n",
      "  timestamp: 1605361957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     15 |          554.669 | 2426880 |  392.673 |              431.606 |              354.155 |            853.593 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3601.7311719013846\n",
      "    time_step_min: 3184\n",
      "  date: 2020-11-14_14-53-15\n",
      "  done: false\n",
      "  episode_len_mean: 850.8497167610797\n",
      "  episode_reward_max: 436.29739223757707\n",
      "  episode_reward_mean: 393.5009577765717\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 223\n",
      "  episodes_total: 3001\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8258772591749827\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008349785814061761\n",
      "        model: {}\n",
      "        policy_loss: -0.01889110753351512\n",
      "        total_loss: 8.113957007726034\n",
      "        vf_explained_var: 0.9857063293457031\n",
      "        vf_loss: 8.131591041882833\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.13846153846153\n",
      "    gpu_util_percent0: 0.1394871794871795\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8543589743589743\n",
      "    ram_util_percent: 5.446153846153846\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20369242775734805\n",
      "    mean_env_wait_ms: 1.7075663614414474\n",
      "    mean_inference_ms: 6.327642584221006\n",
      "    mean_raw_obs_processing_ms: 0.5641289757897403\n",
      "  time_since_restore: 592.0898542404175\n",
      "  time_this_iter_s: 37.421000242233276\n",
      "  time_total_s: 592.0898542404175\n",
      "  timers:\n",
      "    learn_throughput: 6188.649\n",
      "    learn_time_ms: 26143.346\n",
      "    sample_throughput: 15407.451\n",
      "    sample_time_ms: 10500.893\n",
      "    update_time_ms: 62.324\n",
      "  timestamp: 1605361995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | RUNNING  | 172.17.0.14:30165 |     16 |           592.09 | 2588672 |  393.501 |              436.297 |              354.155 |             850.85 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_529f7_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4308\n",
      "    time_step_mean: 3597.136217948718\n",
      "    time_step_min: 3184\n",
      "  date: 2020-11-14_14-53-52\n",
      "  done: true\n",
      "  episode_len_mean: 848.8879746835443\n",
      "  episode_reward_max: 436.29739223757707\n",
      "  episode_reward_mean: 394.0951316199277\n",
      "  episode_reward_min: 354.15515724662833\n",
      "  episodes_this_iter: 159\n",
      "  episodes_total: 3160\n",
      "  experiment_id: d53945bda448436db909cb10a5e29d4d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8142446925242742\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00867955960954229\n",
      "        model: {}\n",
      "        policy_loss: -0.021270312912141282\n",
      "        total_loss: 8.066214044888815\n",
      "        vf_explained_var: 0.9833173751831055\n",
      "        vf_loss: 8.086155573527018\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.50263157894737\n",
      "    gpu_util_percent0: 0.2176315789473684\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157895\n",
      "    ram_util_percent: 5.444736842105263\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 30165\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2034953850431794\n",
      "    mean_env_wait_ms: 1.7081729409318782\n",
      "    mean_inference_ms: 6.316241226355105\n",
      "    mean_raw_obs_processing_ms: 0.5635795137042389\n",
      "  time_since_restore: 629.0509612560272\n",
      "  time_this_iter_s: 36.96110701560974\n",
      "  time_total_s: 629.0509612560272\n",
      "  timers:\n",
      "    learn_throughput: 6197.219\n",
      "    learn_time_ms: 26107.196\n",
      "    sample_throughput: 15350.768\n",
      "    sample_time_ms: 10539.668\n",
      "    update_time_ms: 61.345\n",
      "  timestamp: 1605362032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: 529f7_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | TERMINATED |       |     17 |          629.051 | 2750464 |  394.095 |              436.297 |              354.155 |            848.888 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 41.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_529f7_00000 | TERMINATED |       |     17 |          629.051 | 2750464 |  394.095 |              436.297 |              354.155 |            848.888 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 14:53:52,915\tINFO tune.py:439 -- Total run time: 647.70 seconds (647.38 seconds for the tuning loop).\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 29864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_144258-37dpxos0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_144258-37dpxos0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605362032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 4308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3597.13622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 436.29739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 354.15516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 394.09513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mclean-sweep-2\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/37dpxos0\u001b[0m\n",
      "2020-11-14 14:54:03,327 - wandb.wandb_agent - INFO - Cleaning up finished run: 37dpxos0\n",
      "2020-11-14 14:54:03,641 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 14:54:03,641 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta53\n",
      "2020-11-14 14:54:03,643 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 14:54:08,663 - wandb.wandb_agent - INFO - Running runs: ['t8k1bmg7']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhopeful-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/t8k1bmg7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_145406-t8k1bmg7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 14:54:10,520\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 22.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=50228)\u001b[0m 2020-11-14 14:54:16,476\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=50167)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50167)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50221)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50221)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50197)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50197)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50232)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50232)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50127)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50127)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50120)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50120)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50175)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50175)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50145)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50145)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50228)\u001b[0m 2020-11-14 14:54:28,542\tINFO trainable.py:252 -- Trainable.setup took 12.820 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=50136)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50136)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50206)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50206)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50193)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50193)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50183)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50183)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50227)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50227)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50218)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50218)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50205)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50205)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50194)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50194)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50191)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50191)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50187)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50187)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50133)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50133)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50122)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50122)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50159)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50159)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50144)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50144)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50199)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50199)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50189)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50189)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50192)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50192)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50121)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50121)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50142)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50142)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50129)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50129)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50126)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50126)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50130)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50130)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50078)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50078)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50154)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50154)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50195)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50195)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50110)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50110)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50113)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50113)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50146)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50146)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50158)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50158)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50171)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50171)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50153)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50153)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50116)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50116)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50094)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50094)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50102)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50102)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50060)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50060)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50065)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50065)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50140)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50140)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50080)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50080)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50179)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50179)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50075)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50075)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50095)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50095)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50069)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50069)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50106)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50106)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50108)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50108)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50172)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50172)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50151)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50151)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50097)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50097)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50084)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50084)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50107)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50107)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50099)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50099)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50123)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50123)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50083)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50083)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50066)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50066)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50100)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50100)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50058)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50058)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50076)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50076)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50098)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50098)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50085)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50085)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50109)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50109)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50064)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50064)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50131)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50131)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50062)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50062)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50072)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50072)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50105)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50105)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50061)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50061)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50057)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50057)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50056)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50056)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50055)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50055)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=50063)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=50063)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3669\n",
      "    time_step_mean: 3363.633333333333\n",
      "    time_step_min: 3126\n",
      "  date: 2020-11-14_14-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 881.8607594936709\n",
      "  episode_reward_max: 455.6897536177704\n",
      "  episode_reward_mean: 429.24505764767616\n",
      "  episode_reward_min: 402.7312871421093\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.169152041276296\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00583102759749939\n",
      "        model: {}\n",
      "        policy_loss: -0.011145502539875451\n",
      "        total_loss: 1241.3280131022136\n",
      "        vf_explained_var: 0.06588248163461685\n",
      "        vf_loss: 1241.3385620117188\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.29512195121951\n",
      "    gpu_util_percent0: 0.17146341463414635\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7243902439024389\n",
      "    ram_util_percent: 5.219512195121952\n",
      "    vram_util_percent0: 0.07852834595909094\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2190341077935883\n",
      "    mean_env_wait_ms: 1.69419203249162\n",
      "    mean_inference_ms: 7.360617468923137\n",
      "    mean_raw_obs_processing_ms: 0.5938034935032891\n",
      "  time_since_restore: 39.29180335998535\n",
      "  time_this_iter_s: 39.29180335998535\n",
      "  time_total_s: 39.29180335998535\n",
      "  timers:\n",
      "    learn_throughput: 6163.387\n",
      "    learn_time_ms: 26250.5\n",
      "    sample_throughput: 12701.893\n",
      "    sample_time_ms: 12737.629\n",
      "    update_time_ms: 73.602\n",
      "  timestamp: 1605362107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      1 |          39.2918 | 161792 |  429.245 |               455.69 |              402.731 |            881.861 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3369.978417266187\n",
      "    time_step_min: 3126\n",
      "  date: 2020-11-14_14-55-46\n",
      "  done: false\n",
      "  episode_len_mean: 881.0886075949367\n",
      "  episode_reward_max: 455.6897536177704\n",
      "  episode_reward_mean: 429.144893587445\n",
      "  episode_reward_min: 397.1111069881015\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1381209194660187\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.006644571898505092\n",
      "        model: {}\n",
      "        policy_loss: -0.013672153509105556\n",
      "        total_loss: 599.6269683837891\n",
      "        vf_explained_var: 0.2014096975326538\n",
      "        vf_loss: 599.6398874918619\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7974358974359\n",
      "    gpu_util_percent0: 0.17358974358974358\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8510256410256409\n",
      "    ram_util_percent: 5.4051282051282055\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21385972526468328\n",
      "    mean_env_wait_ms: 1.6673839654258318\n",
      "    mean_inference_ms: 7.10252066824386\n",
      "    mean_raw_obs_processing_ms: 0.5827117233304521\n",
      "  time_since_restore: 77.19323801994324\n",
      "  time_this_iter_s: 37.901434659957886\n",
      "  time_total_s: 77.19323801994324\n",
      "  timers:\n",
      "    learn_throughput: 6126.418\n",
      "    learn_time_ms: 26408.906\n",
      "    sample_throughput: 13649.72\n",
      "    sample_time_ms: 11853.137\n",
      "    update_time_ms: 80.286\n",
      "  timestamp: 1605362146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      2 |          77.1932 | 323584 |  429.145 |               455.69 |              397.111 |            881.089 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3379.316513761468\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_14-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 876.3987341772151\n",
      "  episode_reward_max: 457.7535399629665\n",
      "  episode_reward_mean: 429.44830696140343\n",
      "  episode_reward_min: 397.1111069881015\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1295595367749531\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008363365971793732\n",
      "        model: {}\n",
      "        policy_loss: -0.015905304967115324\n",
      "        total_loss: 265.3238220214844\n",
      "        vf_explained_var: 0.5913417935371399\n",
      "        vf_loss: 265.33861541748047\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.5108108108108\n",
      "    gpu_util_percent0: 0.17891891891891892\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8535135135135136\n",
      "    ram_util_percent: 5.443243243243244\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21106169669556127\n",
      "    mean_env_wait_ms: 1.6552779625496887\n",
      "    mean_inference_ms: 6.922724202646908\n",
      "    mean_raw_obs_processing_ms: 0.576578499775554\n",
      "  time_since_restore: 114.13555145263672\n",
      "  time_this_iter_s: 36.94231343269348\n",
      "  time_total_s: 114.13555145263672\n",
      "  timers:\n",
      "    learn_throughput: 6114.274\n",
      "    learn_time_ms: 26461.359\n",
      "    sample_throughput: 14294.475\n",
      "    sample_time_ms: 11318.499\n",
      "    update_time_ms: 70.365\n",
      "  timestamp: 1605362183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      3 |          114.136 | 485376 |  429.448 |              457.754 |              397.111 |            876.399 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3379.2003367003367\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_14-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 873.743670886076\n",
      "  episode_reward_max: 457.7535399629665\n",
      "  episode_reward_mean: 430.0520479318532\n",
      "  episode_reward_min: 397.1111069881015\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1160923540592194\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009211300561825434\n",
      "        model: {}\n",
      "        policy_loss: -0.015978755885953433\n",
      "        total_loss: 65.23927942911784\n",
      "        vf_explained_var: 0.8486657738685608\n",
      "        vf_loss: 65.25397555033366\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.24473684210525\n",
      "    gpu_util_percent0: 0.16315789473684209\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8550000000000001\n",
      "    ram_util_percent: 5.4342105263157885\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20916478511826503\n",
      "    mean_env_wait_ms: 1.6464635168437725\n",
      "    mean_inference_ms: 6.790202135504229\n",
      "    mean_raw_obs_processing_ms: 0.5709859508612205\n",
      "  time_since_restore: 150.67132687568665\n",
      "  time_this_iter_s: 36.53577542304993\n",
      "  time_total_s: 150.67132687568665\n",
      "  timers:\n",
      "    learn_throughput: 6139.919\n",
      "    learn_time_ms: 26350.834\n",
      "    sample_throughput: 14623.356\n",
      "    sample_time_ms: 11063.944\n",
      "    update_time_ms: 68.716\n",
      "  timestamp: 1605362219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      4 |          150.671 | 647168 |  430.052 |              457.754 |              397.111 |            873.744 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3379.7061170212764\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_14-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 871.9746835443038\n",
      "  episode_reward_max: 458.0112353738581\n",
      "  episode_reward_mean: 429.88711968494573\n",
      "  episode_reward_min: 397.1111069881015\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.081312119960785\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010704286862164736\n",
      "        model: {}\n",
      "        policy_loss: -0.021531161701811168\n",
      "        total_loss: 26.79539680480957\n",
      "        vf_explained_var: 0.9276894927024841\n",
      "        vf_loss: 26.815328280131023\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.76052631578946\n",
      "    gpu_util_percent0: 0.1423684210526316\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8568421052631577\n",
      "    ram_util_percent: 5.428947368421053\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20785137769239317\n",
      "    mean_env_wait_ms: 1.6418614781708163\n",
      "    mean_inference_ms: 6.697959291359764\n",
      "    mean_raw_obs_processing_ms: 0.5670194004048595\n",
      "  time_since_restore: 187.79377150535583\n",
      "  time_this_iter_s: 37.12244462966919\n",
      "  time_total_s: 187.79377150535583\n",
      "  timers:\n",
      "    learn_throughput: 6130.377\n",
      "    learn_time_ms: 26391.85\n",
      "    sample_throughput: 14811.609\n",
      "    sample_time_ms: 10923.324\n",
      "    update_time_ms: 70.031\n",
      "  timestamp: 1605362256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      5 |          187.794 | 808960 |  429.887 |              458.011 |              397.111 |            871.975 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3381.6601123595506\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_14-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 865.497287522604\n",
      "  episode_reward_max: 459.27078295817466\n",
      "  episode_reward_mean: 429.997342791999\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 316\n",
      "  episodes_total: 1106\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0742573539415996\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010395261148611704\n",
      "        model: {}\n",
      "        policy_loss: -0.019117663308861665\n",
      "        total_loss: 18.8870636622111\n",
      "        vf_explained_var: 0.9635675549507141\n",
      "        vf_loss: 18.90463940302531\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60263157894738\n",
      "    gpu_util_percent0: 0.1505263157894737\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157895\n",
      "    ram_util_percent: 5.43421052631579\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20603748619360546\n",
      "    mean_env_wait_ms: 1.6366112559959984\n",
      "    mean_inference_ms: 6.570064828667548\n",
      "    mean_raw_obs_processing_ms: 0.5616652708885156\n",
      "  time_since_restore: 225.23533582687378\n",
      "  time_this_iter_s: 37.441564321517944\n",
      "  time_total_s: 225.23533582687378\n",
      "  timers:\n",
      "    learn_throughput: 6106.999\n",
      "    learn_time_ms: 26492.881\n",
      "    sample_throughput: 14965.923\n",
      "    sample_time_ms: 10810.693\n",
      "    update_time_ms: 79.634\n",
      "  timestamp: 1605362294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      6 |          225.235 | 970752 |  429.997 |              459.271 |              389.891 |            865.497 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3379.5652528548126\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_14-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 861.5213607594936\n",
      "  episode_reward_max: 459.27078295817466\n",
      "  episode_reward_mean: 430.1840457315699\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1264\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0617437561353047\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010227986378595233\n",
      "        model: {}\n",
      "        policy_loss: -0.020388525309196364\n",
      "        total_loss: 16.509245077768963\n",
      "        vf_explained_var: 0.9557989239692688\n",
      "        vf_loss: 16.528118292490642\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.52702702702703\n",
      "    gpu_util_percent0: 0.20405405405405402\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8535135135135136\n",
      "    ram_util_percent: 5.424324324324323\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20540096354110718\n",
      "    mean_env_wait_ms: 1.6349146929446272\n",
      "    mean_inference_ms: 6.524387715417552\n",
      "    mean_raw_obs_processing_ms: 0.559942095805985\n",
      "  time_since_restore: 261.7862660884857\n",
      "  time_this_iter_s: 36.55093026161194\n",
      "  time_total_s: 261.7862660884857\n",
      "  timers:\n",
      "    learn_throughput: 6116.287\n",
      "    learn_time_ms: 26452.651\n",
      "    sample_throughput: 15085.144\n",
      "    sample_time_ms: 10725.254\n",
      "    update_time_ms: 72.187\n",
      "  timestamp: 1605362331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      7 |          261.786 | 1132544 |  430.184 |              459.271 |              389.891 |            861.521 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3376.241329479769\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_14-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 857.7151898734177\n",
      "  episode_reward_max: 459.27078295817466\n",
      "  episode_reward_mean: 430.545556710576\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0443377494812012\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0100595624341319\n",
      "        model: {}\n",
      "        policy_loss: -0.021649266748378675\n",
      "        total_loss: 14.826684554417929\n",
      "        vf_explained_var: 0.9604308009147644\n",
      "        vf_loss: 14.846844037373861\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.03684210526318\n",
      "    gpu_util_percent0: 0.17026315789473684\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8521052631578948\n",
      "    ram_util_percent: 5.4184210526315795\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20493414638757923\n",
      "    mean_env_wait_ms: 1.63460908484407\n",
      "    mean_inference_ms: 6.487122813648989\n",
      "    mean_raw_obs_processing_ms: 0.5586680026666858\n",
      "  time_since_restore: 299.0412874221802\n",
      "  time_this_iter_s: 37.25502133369446\n",
      "  time_total_s: 299.0412874221802\n",
      "  timers:\n",
      "    learn_throughput: 6123.609\n",
      "    learn_time_ms: 26421.02\n",
      "    sample_throughput: 15057.236\n",
      "    sample_time_ms: 10745.133\n",
      "    update_time_ms: 71.146\n",
      "  timestamp: 1605362368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      8 |          299.041 | 1294336 |  430.546 |              459.271 |              389.891 |            857.715 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3372.2697795071335\n",
      "    time_step_min: 3030\n",
      "  date: 2020-11-14_15-00-05\n",
      "  done: false\n",
      "  episode_len_mean: 854.3050632911393\n",
      "  episode_reward_max: 460.08252990981447\n",
      "  episode_reward_mean: 430.94387282993335\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0175931255022685\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009941150744756063\n",
      "        model: {}\n",
      "        policy_loss: -0.021458648397432018\n",
      "        total_loss: 14.07415517171224\n",
      "        vf_explained_var: 0.9639151096343994\n",
      "        vf_loss: 14.09413456916809\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.29736842105264\n",
      "    gpu_util_percent0: 0.1818421052631579\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7157894736842104\n",
      "    ram_util_percent: 5.476315789473684\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20450280838747467\n",
      "    mean_env_wait_ms: 1.634438462082854\n",
      "    mean_inference_ms: 6.45350786246966\n",
      "    mean_raw_obs_processing_ms: 0.5574037789361079\n",
      "  time_since_restore: 336.4843487739563\n",
      "  time_this_iter_s: 37.44306135177612\n",
      "  time_total_s: 336.4843487739563\n",
      "  timers:\n",
      "    learn_throughput: 6111.316\n",
      "    learn_time_ms: 26474.168\n",
      "    sample_throughput: 15116.017\n",
      "    sample_time_ms: 10703.349\n",
      "    update_time_ms: 70.451\n",
      "  timestamp: 1605362405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |      9 |          336.484 | 1456128 |  430.944 |              460.083 |              389.891 |            854.305 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3372.7463572584998\n",
      "    time_step_min: 3023\n",
      "  date: 2020-11-14_15-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 848.0682178741407\n",
      "  episode_reward_max: 460.73271180662584\n",
      "  episode_reward_mean: 431.4095791305325\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 311\n",
      "  episodes_total: 1891\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9854148079951605\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010016566142439842\n",
      "        model: {}\n",
      "        policy_loss: -0.02194759761914611\n",
      "        total_loss: 11.681188265482584\n",
      "        vf_explained_var: 0.9802281260490417\n",
      "        vf_loss: 11.701625426610311\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.71842105263157\n",
      "    gpu_util_percent0: 0.1986842105263158\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8560526315789474\n",
      "    ram_util_percent: 5.423684210526315\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20387697130753113\n",
      "    mean_env_wait_ms: 1.635357108846347\n",
      "    mean_inference_ms: 6.401114068287176\n",
      "    mean_raw_obs_processing_ms: 0.5557350420831222\n",
      "  time_since_restore: 373.0048391819\n",
      "  time_this_iter_s: 36.520490407943726\n",
      "  time_total_s: 373.0048391819\n",
      "  timers:\n",
      "    learn_throughput: 6119.137\n",
      "    learn_time_ms: 26440.329\n",
      "    sample_throughput: 15186.152\n",
      "    sample_time_ms: 10653.917\n",
      "    update_time_ms: 66.227\n",
      "  timestamp: 1605362442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     10 |          373.005 | 1617920 |   431.41 |              460.733 |              389.891 |            848.068 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3372.375992063492\n",
      "    time_step_min: 3023\n",
      "  date: 2020-11-14_15-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 845.5486854917235\n",
      "  episode_reward_max: 460.73271180662584\n",
      "  episode_reward_mean: 431.668023600655\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 163\n",
      "  episodes_total: 2054\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9729610482851664\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009989067058389386\n",
      "        model: {}\n",
      "        policy_loss: -0.023465588882875938\n",
      "        total_loss: 10.45383914311727\n",
      "        vf_explained_var: 0.9754252433776855\n",
      "        vf_loss: 10.475793361663818\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.20270270270271\n",
      "    gpu_util_percent0: 0.1608108108108108\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8527027027027028\n",
      "    ram_util_percent: 5.45945945945946\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20358049045305485\n",
      "    mean_env_wait_ms: 1.635898521348053\n",
      "    mean_inference_ms: 6.378530124839199\n",
      "    mean_raw_obs_processing_ms: 0.5549087357486089\n",
      "  time_since_restore: 409.1807773113251\n",
      "  time_this_iter_s: 36.17593812942505\n",
      "  time_total_s: 409.1807773113251\n",
      "  timers:\n",
      "    learn_throughput: 6128.366\n",
      "    learn_time_ms: 26400.511\n",
      "    sample_throughput: 15552.447\n",
      "    sample_time_ms: 10402.993\n",
      "    update_time_ms: 62.829\n",
      "  timestamp: 1605362478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     11 |          409.181 | 1779712 |  431.668 |              460.733 |              389.891 |            845.549 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3368.4691812327505\n",
      "    time_step_min: 3009\n",
      "  date: 2020-11-14_15-01-55\n",
      "  done: false\n",
      "  episode_len_mean: 842.9855334538879\n",
      "  episode_reward_max: 462.8859548659474\n",
      "  episode_reward_mean: 431.9347365704111\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2212\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9544650663932165\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009267421827341119\n",
      "        model: {}\n",
      "        policy_loss: -0.021501642419025302\n",
      "        total_loss: 9.574400424957275\n",
      "        vf_explained_var: 0.9765139222145081\n",
      "        vf_loss: 9.594525655110678\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.07567567567567\n",
      "    gpu_util_percent0: 0.1756756756756757\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8483783783783783\n",
      "    ram_util_percent: 5.454054054054054\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2033321922087173\n",
      "    mean_env_wait_ms: 1.6366480088067574\n",
      "    mean_inference_ms: 6.35926368439569\n",
      "    mean_raw_obs_processing_ms: 0.5542118123894542\n",
      "  time_since_restore: 445.9294288158417\n",
      "  time_this_iter_s: 36.7486515045166\n",
      "  time_total_s: 445.9294288158417\n",
      "  timers:\n",
      "    learn_throughput: 6138.027\n",
      "    learn_time_ms: 26358.959\n",
      "    sample_throughput: 15621.229\n",
      "    sample_time_ms: 10357.188\n",
      "    update_time_ms: 56.626\n",
      "  timestamp: 1605362515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     12 |          445.929 | 1941504 |  431.935 |              462.886 |              389.891 |            842.986 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3366.4736616702357\n",
      "    time_step_min: 3009\n",
      "  date: 2020-11-14_15-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 840.7505267593763\n",
      "  episode_reward_max: 462.8859548659474\n",
      "  episode_reward_mean: 432.2394832819653\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 161\n",
      "  episodes_total: 2373\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9224310318628947\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009582492910946408\n",
      "        model: {}\n",
      "        policy_loss: -0.020354589350366343\n",
      "        total_loss: 9.571095069249472\n",
      "        vf_explained_var: 0.9789043068885803\n",
      "        vf_loss: 9.589994589487711\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.33157894736841\n",
      "    gpu_util_percent0: 0.17\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157896\n",
      "    ram_util_percent: 5.465789473684211\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20309349647828334\n",
      "    mean_env_wait_ms: 1.6374170690773246\n",
      "    mean_inference_ms: 6.341880894164176\n",
      "    mean_raw_obs_processing_ms: 0.5536420471161698\n",
      "  time_since_restore: 482.3847348690033\n",
      "  time_this_iter_s: 36.45530605316162\n",
      "  time_total_s: 482.3847348690033\n",
      "  timers:\n",
      "    learn_throughput: 6153.179\n",
      "    learn_time_ms: 26294.053\n",
      "    sample_throughput: 15621.309\n",
      "    sample_time_ms: 10357.134\n",
      "    update_time_ms: 58.642\n",
      "  timestamp: 1605362552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     13 |          482.385 | 2103296 |  432.239 |              462.886 |              389.891 |            840.751 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3364.2320483749054\n",
      "    time_step_min: 3009\n",
      "  date: 2020-11-14_15-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 837.0305514157973\n",
      "  episode_reward_max: 462.8859548659474\n",
      "  episode_reward_mean: 432.7976510388473\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 311\n",
      "  episodes_total: 2684\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8968500991662344\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009205672036235532\n",
      "        model: {}\n",
      "        policy_loss: -0.023005598845581215\n",
      "        total_loss: 8.375084718068441\n",
      "        vf_explained_var: 0.9870464205741882\n",
      "        vf_loss: 8.396697362263998\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.38648648648646\n",
      "    gpu_util_percent0: 0.19027027027027027\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.855945945945946\n",
      "    ram_util_percent: 5.4324324324324325\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20272617749839086\n",
      "    mean_env_wait_ms: 1.6392670348607108\n",
      "    mean_inference_ms: 6.313551699970461\n",
      "    mean_raw_obs_processing_ms: 0.5527860605131206\n",
      "  time_since_restore: 518.9117517471313\n",
      "  time_this_iter_s: 36.52701687812805\n",
      "  time_total_s: 518.9117517471313\n",
      "  timers:\n",
      "    learn_throughput: 6159.405\n",
      "    learn_time_ms: 26267.474\n",
      "    sample_throughput: 15589.492\n",
      "    sample_time_ms: 10378.272\n",
      "    update_time_ms: 55.288\n",
      "  timestamp: 1605362589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     14 |          518.912 | 2265088 |  432.798 |              462.886 |              389.891 |            837.031 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3363.223806129722\n",
      "    time_step_min: 3009\n",
      "  date: 2020-11-14_15-03-46\n",
      "  done: false\n",
      "  episode_len_mean: 835.3456399437412\n",
      "  episode_reward_max: 470.9396074843843\n",
      "  episode_reward_mean: 433.0851052050602\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2844\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8824446747700373\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009113489106918374\n",
      "        model: {}\n",
      "        policy_loss: -0.02121744429071744\n",
      "        total_loss: 8.204953034718832\n",
      "        vf_explained_var: 0.9819768071174622\n",
      "        vf_loss: 8.224789142608643\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.54736842105262\n",
      "    gpu_util_percent0: 0.1702631578947368\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8547368421052632\n",
      "    ram_util_percent: 5.457894736842105\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20256925262400663\n",
      "    mean_env_wait_ms: 1.6402673551164582\n",
      "    mean_inference_ms: 6.30139302233489\n",
      "    mean_raw_obs_processing_ms: 0.5524272027922515\n",
      "  time_since_restore: 556.3665244579315\n",
      "  time_this_iter_s: 37.45477271080017\n",
      "  time_total_s: 556.3665244579315\n",
      "  timers:\n",
      "    learn_throughput: 6156.157\n",
      "    learn_time_ms: 26281.33\n",
      "    sample_throughput: 15552.056\n",
      "    sample_time_ms: 10403.255\n",
      "    update_time_ms: 52.707\n",
      "  timestamp: 1605362626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     15 |          556.367 | 2426880 |  433.085 |               470.94 |              389.891 |            835.346 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3361.7729419703105\n",
      "    time_step_min: 3009\n",
      "  date: 2020-11-14_15-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 833.8737508327781\n",
      "  episode_reward_max: 470.9396074843843\n",
      "  episode_reward_mean: 433.34578624773246\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3002\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8698093096415201\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009236207930371165\n",
      "        model: {}\n",
      "        policy_loss: -0.0197252887301147\n",
      "        total_loss: 8.071978330612183\n",
      "        vf_explained_var: 0.9812588095664978\n",
      "        vf_loss: 8.090291182200113\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.19743589743591\n",
      "    gpu_util_percent0: 0.15666666666666662\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8538461538461539\n",
      "    ram_util_percent: 5.453846153846154\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20243626002931428\n",
      "    mean_env_wait_ms: 1.6411961540271975\n",
      "    mean_inference_ms: 6.290127344736484\n",
      "    mean_raw_obs_processing_ms: 0.552094972720597\n",
      "  time_since_restore: 593.4035675525665\n",
      "  time_this_iter_s: 37.03704309463501\n",
      "  time_total_s: 593.4035675525665\n",
      "  timers:\n",
      "    learn_throughput: 6172.438\n",
      "    learn_time_ms: 26212.011\n",
      "    sample_throughput: 15518.467\n",
      "    sample_time_ms: 10425.772\n",
      "    update_time_ms: 50.343\n",
      "  timestamp: 1605362663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | RUNNING  | 172.17.0.14:50228 |     16 |          593.404 | 2588672 |  433.346 |               470.94 |              389.891 |            833.874 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_e0eee_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3993\n",
      "    time_step_mean: 3360.513986013986\n",
      "    time_step_min: 3004\n",
      "  date: 2020-11-14_15-05-01\n",
      "  done: true\n",
      "  episode_len_mean: 832.3844221105528\n",
      "  episode_reward_max: 470.9396074843843\n",
      "  episode_reward_mean: 433.6543252801864\n",
      "  episode_reward_min: 389.8908447974172\n",
      "  episodes_this_iter: 182\n",
      "  episodes_total: 3184\n",
      "  experiment_id: fff34b4788bc4902b6dbff02d0d00d1d\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8311366389195124\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008957812019313375\n",
      "        model: {}\n",
      "        policy_loss: -0.021412768750451505\n",
      "        total_loss: 7.6998817920684814\n",
      "        vf_explained_var: 0.9862262606620789\n",
      "        vf_loss: 7.719918727874756\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.49736842105263\n",
      "    gpu_util_percent0: 0.12631578947368424\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8563157894736841\n",
      "    ram_util_percent: 5.4184210526315795\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 50228\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2022860692792616\n",
      "    mean_env_wait_ms: 1.6424410210572753\n",
      "    mean_inference_ms: 6.278297533283513\n",
      "    mean_raw_obs_processing_ms: 0.5517504317756279\n",
      "  time_since_restore: 631.1345343589783\n",
      "  time_this_iter_s: 37.73096680641174\n",
      "  time_total_s: 631.1345343589783\n",
      "  timers:\n",
      "    learn_throughput: 6150.061\n",
      "    learn_time_ms: 26307.38\n",
      "    sample_throughput: 15496.263\n",
      "    sample_time_ms: 10440.711\n",
      "    update_time_ms: 52.938\n",
      "  timestamp: 1605362701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: e0eee_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | TERMINATED |       |     17 |          631.135 | 2750464 |  433.654 |               470.94 |              389.891 |            832.384 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 43.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_e0eee_00000 | TERMINATED |       |     17 |          631.135 | 2750464 |  433.654 |               470.94 |              389.891 |            832.384 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 15:05:03,063\tINFO tune.py:439 -- Total run time: 649.56 seconds (648.88 seconds for the tuning loop).\n",
      "2020-11-14 15:05:03,126\tWARNING worker.py:1091 -- A worker died or was killed while executing task fffffffffffffffff392716d01000000.\n",
      "2020-11-14 15:05:03,127\tWARNING worker.py:1091 -- A worker died or was killed while executing task ffffffffffffffff7a508a7a01000000.\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m 2020-11-14 15:05:03,093\tERROR worker.py:384 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"python/ray/_raylet.pyx\", line 551, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"python/ray/_raylet.pyx\", line 477, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"python/ray/_raylet.pyx\", line 481, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"python/ray/_raylet.pyx\", line 482, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"python/ray/_raylet.pyx\", line 436, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py\", line 553, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m     return method(actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/actor.py\", line 945, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/actor.py\", line 1007, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m     ray.state.state.disconnect()\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/state.py\", line 60, in disconnect\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m     self.global_state_accessor = None\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/worker.py\", line 381, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=50059)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m 2020-11-14 15:05:03,100\tERROR worker.py:384 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"python/ray/_raylet.pyx\", line 551, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"python/ray/_raylet.pyx\", line 477, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"python/ray/_raylet.pyx\", line 481, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"python/ray/_raylet.pyx\", line 482, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"python/ray/_raylet.pyx\", line 436, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py\", line 553, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m     return method(actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/actor.py\", line 945, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/actor.py\", line 1007, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m     ray.state.state.disconnect()\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/state.py\", line 60, in disconnect\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m     self.global_state_accessor = None\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m   File \"/root/miniconda3/lib/python3.8/site-packages/ray/worker.py\", line 381, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=50054)\u001b[0m SystemExit: 1\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 49915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_145406-t8k1bmg7/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_145406-t8k1bmg7/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605362703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 3993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3360.51399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 470.93961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 389.89084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 433.65433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mhopeful-sweep-3\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/t8k1bmg7\u001b[0m\n",
      "2020-11-14 15:05:10,923 - wandb.wandb_agent - INFO - Cleaning up finished run: t8k1bmg7\n",
      "2020-11-14 15:05:11,252 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 15:05:11,253 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta54\n",
      "2020-11-14 15:05:11,255 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta54\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 15:05:16,279 - wandb.wandb_agent - INFO - Running runs: ['9znmbsbb']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgentle-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/9znmbsbb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_150514-9znmbsbb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 15:05:18,455\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=70185)\u001b[0m 2020-11-14 15:05:24,609\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=70228)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70228)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70185)\u001b[0m 2020-11-14 15:05:34,685\tINFO trainable.py:252 -- Trainable.setup took 10.924 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=70156)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70156)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70169)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70169)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70193)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70193)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70182)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70182)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70184)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70184)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70171)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70171)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70223)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70223)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70172)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70172)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70173)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70173)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70201)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70201)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70174)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70174)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70210)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70210)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70088)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70088)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70175)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70175)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70219)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70219)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70150)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70150)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70155)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70155)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70167)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70167)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70230)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70230)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70195)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70195)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70151)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70151)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70192)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70192)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70100)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70100)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70194)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70194)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70179)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70179)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70166)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70166)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70181)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70181)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70224)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70224)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70206)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70206)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70243)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70243)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70176)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70176)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70144)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70144)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70162)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70162)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70141)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70141)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70125)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70125)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70153)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70153)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70178)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70178)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70145)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70145)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70159)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70159)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70123)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70123)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70121)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70121)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70132)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70132)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70130)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70130)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70148)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70148)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70122)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70122)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70113)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70113)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70135)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70135)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70118)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70118)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70124)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70124)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70095)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70095)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70115)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70115)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70126)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70126)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70120)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70120)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70083)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70083)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70114)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70114)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70111)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70111)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70085)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70085)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70142)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70142)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70117)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70117)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70091)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70091)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70069)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70069)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70078)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70078)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70094)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70094)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70119)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70119)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70076)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70076)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70074)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70074)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70077)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70077)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70068)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70068)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70116)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70116)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70070)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70070)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70080)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70080)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70075)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70075)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70079)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70079)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70072)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70072)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70071)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70071)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=70073)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=70073)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3569.8064516129034\n",
      "    time_step_min: 3273\n",
      "  date: 2020-11-14_15-06-13\n",
      "  done: false\n",
      "  episode_len_mean: 898.0696202531645\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 391.9676608500997\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1797406673431396\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.005714240755575399\n",
      "        model: {}\n",
      "        policy_loss: -0.011944089802758148\n",
      "        total_loss: 975.6800944010416\n",
      "        vf_explained_var: 0.07996774464845657\n",
      "        vf_loss: 975.6914927164713\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.69756097560975\n",
      "    gpu_util_percent0: 0.16780487804878047\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8541463414634145\n",
      "    ram_util_percent: 5.2024390243902445\n",
      "    vram_util_percent0: 0.07831636122199201\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2325104268548959\n",
      "    mean_env_wait_ms: 1.7509295830722205\n",
      "    mean_inference_ms: 7.645067130757469\n",
      "    mean_raw_obs_processing_ms: 0.6311340661798226\n",
      "  time_since_restore: 39.11509847640991\n",
      "  time_this_iter_s: 39.11509847640991\n",
      "  time_total_s: 39.11509847640991\n",
      "  timers:\n",
      "    learn_throughput: 6219.877\n",
      "    learn_time_ms: 26012.091\n",
      "    sample_throughput: 12513.763\n",
      "    sample_time_ms: 12929.125\n",
      "    update_time_ms: 136.923\n",
      "  timestamp: 1605362773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      1 |          39.1151 | 161792 |  391.968 |              431.979 |              344.937 |             898.07 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3576.375886524823\n",
      "    time_step_min: 3273\n",
      "  date: 2020-11-14_15-06-51\n",
      "  done: false\n",
      "  episode_len_mean: 895.1012658227849\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 393.1537796236788\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1470795174439747\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0070687745076914625\n",
      "        model: {}\n",
      "        policy_loss: -0.01303897462397193\n",
      "        total_loss: 452.48238881429035\n",
      "        vf_explained_var: 0.2499614953994751\n",
      "        vf_loss: 452.49459075927734\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.76923076923077\n",
      "    gpu_util_percent0: 0.16230769230769232\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8528205128205127\n",
      "    ram_util_percent: 5.423076923076923\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22636943092789405\n",
      "    mean_env_wait_ms: 1.7259842462986386\n",
      "    mean_inference_ms: 7.438119634063731\n",
      "    mean_raw_obs_processing_ms: 0.6212224551489557\n",
      "  time_since_restore: 77.0311632156372\n",
      "  time_this_iter_s: 37.916064739227295\n",
      "  time_total_s: 77.0311632156372\n",
      "  timers:\n",
      "    learn_throughput: 6250.445\n",
      "    learn_time_ms: 25884.878\n",
      "    sample_throughput: 12995.233\n",
      "    sample_time_ms: 12450.104\n",
      "    update_time_ms: 122.181\n",
      "  timestamp: 1605362811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      2 |          77.0312 | 323584 |  393.154 |              431.979 |              344.937 |            895.101 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3569.3681818181817\n",
      "    time_step_min: 3242\n",
      "  date: 2020-11-14_15-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 890.6497890295359\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 393.11026109716573\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1348959704240162\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008387824520468712\n",
      "        model: {}\n",
      "        policy_loss: -0.016243391398650903\n",
      "        total_loss: 168.92874145507812\n",
      "        vf_explained_var: 0.6736648678779602\n",
      "        vf_loss: 168.94388071695963\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7076923076923\n",
      "    gpu_util_percent0: 0.16153846153846152\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8546153846153848\n",
      "    ram_util_percent: 5.448717948717949\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22324358981027573\n",
      "    mean_env_wait_ms: 1.7155896553638266\n",
      "    mean_inference_ms: 7.2872056331574155\n",
      "    mean_raw_obs_processing_ms: 0.6168409491792303\n",
      "  time_since_restore: 114.5700044631958\n",
      "  time_this_iter_s: 37.538841247558594\n",
      "  time_total_s: 114.5700044631958\n",
      "  timers:\n",
      "    learn_throughput: 6249.743\n",
      "    learn_time_ms: 25887.785\n",
      "    sample_throughput: 13341.229\n",
      "    sample_time_ms: 12127.218\n",
      "    update_time_ms: 95.298\n",
      "  timestamp: 1605362849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      3 |           114.57 | 485376 |   393.11 |              431.979 |              344.937 |             890.65 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3560.740802675585\n",
      "    time_step_min: 3242\n",
      "  date: 2020-11-14_15-08-06\n",
      "  done: false\n",
      "  episode_len_mean: 886.9256329113924\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 393.59662349484336\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1185236473878224\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009615198709070683\n",
      "        model: {}\n",
      "        policy_loss: -0.016302999865729362\n",
      "        total_loss: 37.70860036214193\n",
      "        vf_explained_var: 0.878337562084198\n",
      "        vf_loss: 37.72353935241699\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.6263157894737\n",
      "    gpu_util_percent0: 0.1678947368421053\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8547368421052632\n",
      "    ram_util_percent: 5.439473684210526\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22148817311985627\n",
      "    mean_env_wait_ms: 1.7101856571278722\n",
      "    mean_inference_ms: 7.179940260306798\n",
      "    mean_raw_obs_processing_ms: 0.6135129448361195\n",
      "  time_since_restore: 151.60184836387634\n",
      "  time_this_iter_s: 37.03184390068054\n",
      "  time_total_s: 151.60184836387634\n",
      "  timers:\n",
      "    learn_throughput: 6250.1\n",
      "    learn_time_ms: 25886.304\n",
      "    sample_throughput: 13665.388\n",
      "    sample_time_ms: 11839.547\n",
      "    update_time_ms: 91.077\n",
      "  timestamp: 1605362886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      4 |          151.602 | 647168 |  393.597 |              431.979 |              344.937 |            886.926 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3555.8796296296296\n",
      "    time_step_min: 3242\n",
      "  date: 2020-11-14_15-08-43\n",
      "  done: false\n",
      "  episode_len_mean: 884.2367088607594\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 393.93772659145304\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0858115255832672\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.01113707929228743\n",
      "        model: {}\n",
      "        policy_loss: -0.0219627747622629\n",
      "        total_loss: 22.73159917195638\n",
      "        vf_explained_var: 0.9255242347717285\n",
      "        vf_loss: 22.75187651316325\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.48717948717947\n",
      "    gpu_util_percent0: 0.15307692307692308\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8541025641025641\n",
      "    ram_util_percent: 5.4282051282051285\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21999291709668178\n",
      "    mean_env_wait_ms: 1.7057020437345989\n",
      "    mean_inference_ms: 7.092349856925082\n",
      "    mean_raw_obs_processing_ms: 0.6106288990376258\n",
      "  time_since_restore: 188.50198936462402\n",
      "  time_this_iter_s: 36.90014100074768\n",
      "  time_total_s: 188.50198936462402\n",
      "  timers:\n",
      "    learn_throughput: 6275.017\n",
      "    learn_time_ms: 25783.516\n",
      "    sample_throughput: 13819.777\n",
      "    sample_time_ms: 11707.28\n",
      "    update_time_ms: 82.735\n",
      "  timestamp: 1605362923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      5 |          188.502 | 808960 |  393.938 |              431.979 |              344.937 |            884.237 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3549.3753723932473\n",
      "    time_step_min: 3242\n",
      "  date: 2020-11-14_15-09-20\n",
      "  done: false\n",
      "  episode_len_mean: 877.7463976945245\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 394.39959124956437\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 251\n",
      "  episodes_total: 1041\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0478747487068176\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010750610769415895\n",
      "        model: {}\n",
      "        policy_loss: -0.021808993925030034\n",
      "        total_loss: 15.651693423589071\n",
      "        vf_explained_var: 0.964758574962616\n",
      "        vf_loss: 15.671876033147177\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.42105263157896\n",
      "    gpu_util_percent0: 0.1955263157894737\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8552631578947368\n",
      "    ram_util_percent: 5.421052631578948\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21821494854418547\n",
      "    mean_env_wait_ms: 1.704894825606292\n",
      "    mean_inference_ms: 6.993164175288183\n",
      "    mean_raw_obs_processing_ms: 0.6068431324430501\n",
      "  time_since_restore: 225.44170451164246\n",
      "  time_this_iter_s: 36.93971514701843\n",
      "  time_total_s: 225.44170451164246\n",
      "  timers:\n",
      "    learn_throughput: 6294.496\n",
      "    learn_time_ms: 25703.724\n",
      "    sample_throughput: 13870.848\n",
      "    sample_time_ms: 11664.175\n",
      "    update_time_ms: 80.161\n",
      "  timestamp: 1605362960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      6 |          225.442 | 970752 |    394.4 |              431.979 |              344.937 |            877.746 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3543.6162601626015\n",
      "    time_step_min: 3240\n",
      "  date: 2020-11-14_15-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 872.2539556962025\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 394.6797974320791\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 223\n",
      "  episodes_total: 1264\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0562898417313893\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010894528202091655\n",
      "        model: {}\n",
      "        policy_loss: -0.020898604687924188\n",
      "        total_loss: 13.546734730402628\n",
      "        vf_explained_var: 0.9639403820037842\n",
      "        vf_loss: 13.565982739130655\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.08205128205128\n",
      "    gpu_util_percent0: 0.14512820512820515\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8510256410256412\n",
      "    ram_util_percent: 5.420512820512822\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21704483672876865\n",
      "    mean_env_wait_ms: 1.702816724564028\n",
      "    mean_inference_ms: 6.9268033325254725\n",
      "    mean_raw_obs_processing_ms: 0.6041637272727715\n",
      "  time_since_restore: 262.26638650894165\n",
      "  time_this_iter_s: 36.824681997299194\n",
      "  time_total_s: 262.26638650894165\n",
      "  timers:\n",
      "    learn_throughput: 6294.53\n",
      "    learn_time_ms: 25703.586\n",
      "    sample_throughput: 13992.383\n",
      "    sample_time_ms: 11562.863\n",
      "    update_time_ms: 78.415\n",
      "  timestamp: 1605362997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      7 |          262.266 | 1132544 |   394.68 |              431.979 |              344.937 |            872.254 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3540.5446685878965\n",
      "    time_step_min: 3240\n",
      "  date: 2020-11-14_15-10-35\n",
      "  done: false\n",
      "  episode_len_mean: 868.2545710267229\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 394.850556607729\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.03231484691302\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010077740143363675\n",
      "        model: {}\n",
      "        policy_loss: -0.022352858912199736\n",
      "        total_loss: 13.285456816355387\n",
      "        vf_explained_var: 0.9627368450164795\n",
      "        vf_loss: 13.306310335795084\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.15789473684211\n",
      "    gpu_util_percent0: 0.1773684210526316\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7247368421052632\n",
      "    ram_util_percent: 5.415789473684211\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2163524528549972\n",
      "    mean_env_wait_ms: 1.7028557776784907\n",
      "    mean_inference_ms: 6.887318723518968\n",
      "    mean_raw_obs_processing_ms: 0.6025589058644076\n",
      "  time_since_restore: 299.39634346961975\n",
      "  time_this_iter_s: 37.1299569606781\n",
      "  time_total_s: 299.39634346961975\n",
      "  timers:\n",
      "    learn_throughput: 6300.58\n",
      "    learn_time_ms: 25678.906\n",
      "    sample_throughput: 14036.803\n",
      "    sample_time_ms: 11526.271\n",
      "    update_time_ms: 76.869\n",
      "  timestamp: 1605363035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      8 |          299.396 | 1294336 |  394.851 |              431.979 |              344.937 |            868.255 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3537.7153945666237\n",
      "    time_step_min: 3182\n",
      "  date: 2020-11-14_15-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 864.5594936708861\n",
      "  episode_reward_max: 431.97891379121984\n",
      "  episode_reward_mean: 395.2540375754432\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0063489725192387\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010041987678656975\n",
      "        model: {}\n",
      "        policy_loss: -0.021779874805361032\n",
      "        total_loss: 12.2966259320577\n",
      "        vf_explained_var: 0.9650654196739197\n",
      "        vf_loss: 12.316900491714478\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.81891891891892\n",
      "    gpu_util_percent0: 0.2162162162162162\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8491891891891892\n",
      "    ram_util_percent: 5.448648648648648\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2157191042467664\n",
      "    mean_env_wait_ms: 1.7025410482006356\n",
      "    mean_inference_ms: 6.850040920792408\n",
      "    mean_raw_obs_processing_ms: 0.6008704835869031\n",
      "  time_since_restore: 335.7614469528198\n",
      "  time_this_iter_s: 36.36510348320007\n",
      "  time_total_s: 335.7614469528198\n",
      "  timers:\n",
      "    learn_throughput: 6308.975\n",
      "    learn_time_ms: 25644.738\n",
      "    sample_throughput: 14138.917\n",
      "    sample_time_ms: 11443.026\n",
      "    update_time_ms: 75.995\n",
      "  timestamp: 1605363071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |      9 |          335.761 | 1456128 |  395.254 |              431.979 |              344.937 |            864.559 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3534.4428985507247\n",
      "    time_step_min: 3182\n",
      "  date: 2020-11-14_15-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 860.3445139283684\n",
      "  episode_reward_max: 432.4980052687477\n",
      "  episode_reward_mean: 395.9076830794347\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 179\n",
      "  episodes_total: 1759\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9575732996066412\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009611299106230339\n",
      "        model: {}\n",
      "        policy_loss: -0.020956865123783548\n",
      "        total_loss: 11.504995663960775\n",
      "        vf_explained_var: 0.9752537608146667\n",
      "        vf_loss: 11.524509191513062\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.36315789473684\n",
      "    gpu_util_percent0: 0.15973684210526315\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.853421052631579\n",
      "    ram_util_percent: 5.426315789473684\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21499002783485469\n",
      "    mean_env_wait_ms: 1.7026949158037756\n",
      "    mean_inference_ms: 6.809678357422782\n",
      "    mean_raw_obs_processing_ms: 0.5987092595832414\n",
      "  time_since_restore: 372.84881496429443\n",
      "  time_this_iter_s: 37.08736801147461\n",
      "  time_total_s: 372.84881496429443\n",
      "  timers:\n",
      "    learn_throughput: 6299.805\n",
      "    learn_time_ms: 25682.066\n",
      "    sample_throughput: 14209.265\n",
      "    sample_time_ms: 11386.374\n",
      "    update_time_ms: 72.798\n",
      "  timestamp: 1605363108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     10 |          372.849 | 1617920 |  395.908 |              432.498 |              344.937 |            860.345 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3528.9623762376236\n",
      "    time_step_min: 3182\n",
      "  date: 2020-11-14_15-12-26\n",
      "  done: false\n",
      "  episode_len_mean: 854.9965920155794\n",
      "  episode_reward_max: 432.4980052687477\n",
      "  episode_reward_mean: 396.9181637882877\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 295\n",
      "  episodes_total: 2054\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.958723450700442\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009768587071448565\n",
      "        model: {}\n",
      "        policy_loss: -0.021661285155763228\n",
      "        total_loss: 9.327329079310099\n",
      "        vf_explained_var: 0.9817726612091064\n",
      "        vf_loss: 9.34751582145691\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.57435897435897\n",
      "    gpu_util_percent0: 0.14666666666666667\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8548717948717948\n",
      "    ram_util_percent: 5.464102564102564\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21401467508201316\n",
      "    mean_env_wait_ms: 1.7021546766193543\n",
      "    mean_inference_ms: 6.7585414003971565\n",
      "    mean_raw_obs_processing_ms: 0.5962497084623832\n",
      "  time_since_restore: 410.32133078575134\n",
      "  time_this_iter_s: 37.47251582145691\n",
      "  time_total_s: 410.32133078575134\n",
      "  timers:\n",
      "    learn_throughput: 6295.927\n",
      "    learn_time_ms: 25697.885\n",
      "    sample_throughput: 14438.43\n",
      "    sample_time_ms: 11205.65\n",
      "    update_time_ms: 64.133\n",
      "  timestamp: 1605363146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     11 |          410.321 | 1779712 |  396.918 |              432.498 |              344.937 |            854.997 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3525.495867768595\n",
      "    time_step_min: 3182\n",
      "  date: 2020-11-14_15-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 852.6839963833635\n",
      "  episode_reward_max: 432.4980052687477\n",
      "  episode_reward_mean: 397.408925122741\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2212\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9367341647545496\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0095334368913124\n",
      "        model: {}\n",
      "        policy_loss: -0.0224667732254602\n",
      "        total_loss: 9.797141790390015\n",
      "        vf_explained_var: 0.9761702418327332\n",
      "        vf_loss: 9.818169832229614\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.72368421052633\n",
      "    gpu_util_percent0: 0.14684210526315788\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157895\n",
      "    ram_util_percent: 5.442105263157895\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21351647749390965\n",
      "    mean_env_wait_ms: 1.702203371680018\n",
      "    mean_inference_ms: 6.73407786388826\n",
      "    mean_raw_obs_processing_ms: 0.5949843156925214\n",
      "  time_since_restore: 447.5415139198303\n",
      "  time_this_iter_s: 37.22018313407898\n",
      "  time_total_s: 447.5415139198303\n",
      "  timers:\n",
      "    learn_throughput: 6285.733\n",
      "    learn_time_ms: 25739.561\n",
      "    sample_throughput: 14575.967\n",
      "    sample_time_ms: 11099.916\n",
      "    update_time_ms: 60.745\n",
      "  timestamp: 1605363183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     12 |          447.542 | 1941504 |  397.409 |              432.498 |              344.937 |            852.684 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3524.451198630137\n",
      "    time_step_min: 3176\n",
      "  date: 2020-11-14_15-13-40\n",
      "  done: false\n",
      "  episode_len_mean: 850.3983122362869\n",
      "  episode_reward_max: 432.4980052687477\n",
      "  episode_reward_mean: 397.7754758184666\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2370\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9159021923939387\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009332886586586634\n",
      "        model: {}\n",
      "        policy_loss: -0.021269795407230657\n",
      "        total_loss: 9.709032853444418\n",
      "        vf_explained_var: 0.9756798148155212\n",
      "        vf_loss: 9.728893995285034\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.07567567567568\n",
      "    gpu_util_percent0: 0.20108108108108108\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8524324324324325\n",
      "    ram_util_percent: 5.467567567567568\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21306434282462025\n",
      "    mean_env_wait_ms: 1.7023725262742555\n",
      "    mean_inference_ms: 6.711441776904626\n",
      "    mean_raw_obs_processing_ms: 0.5937129238347874\n",
      "  time_since_restore: 483.8596999645233\n",
      "  time_this_iter_s: 36.31818604469299\n",
      "  time_total_s: 483.8596999645233\n",
      "  timers:\n",
      "    learn_throughput: 6294.071\n",
      "    learn_time_ms: 25705.46\n",
      "    sample_throughput: 14698.72\n",
      "    sample_time_ms: 11007.217\n",
      "    update_time_ms: 65.258\n",
      "  timestamp: 1605363220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     13 |           483.86 | 2103296 |  397.775 |              432.498 |              344.937 |            850.398 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3522.5988095238095\n",
      "    time_step_min: 3176\n",
      "  date: 2020-11-14_15-14-16\n",
      "  done: false\n",
      "  episode_len_mean: 847.6660140955364\n",
      "  episode_reward_max: 432.4980052687477\n",
      "  episode_reward_mean: 398.27088674148024\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 184\n",
      "  episodes_total: 2554\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8700891186793646\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009205677236119906\n",
      "        model: {}\n",
      "        policy_loss: -0.021981496751929324\n",
      "        total_loss: 9.228989521662394\n",
      "        vf_explained_var: 0.9822637438774109\n",
      "        vf_loss: 9.24956480662028\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.43589743589742\n",
      "    gpu_util_percent0: 0.16794871794871796\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8533333333333334\n",
      "    ram_util_percent: 5.461538461538461\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21255531936828775\n",
      "    mean_env_wait_ms: 1.702951488861757\n",
      "    mean_inference_ms: 6.686144450430948\n",
      "    mean_raw_obs_processing_ms: 0.5921900426026283\n",
      "  time_since_restore: 520.4932901859283\n",
      "  time_this_iter_s: 36.63359022140503\n",
      "  time_total_s: 520.4932901859283\n",
      "  timers:\n",
      "    learn_throughput: 6304.781\n",
      "    learn_time_ms: 25661.796\n",
      "    sample_throughput: 14699.482\n",
      "    sample_time_ms: 11006.646\n",
      "    update_time_ms: 61.754\n",
      "  timestamp: 1605363256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     14 |          520.493 | 2265088 |  398.271 |              432.498 |              344.937 |            847.666 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3520.9010676156586\n",
      "    time_step_min: 3176\n",
      "  date: 2020-11-14_15-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 844.0052742616034\n",
      "  episode_reward_max: 436.0817678397799\n",
      "  episode_reward_mean: 398.9388874359978\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 290\n",
      "  episodes_total: 2844\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8686322073141733\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008968470462908348\n",
      "        model: {}\n",
      "        policy_loss: -0.018810954604608316\n",
      "        total_loss: 7.765320777893066\n",
      "        vf_explained_var: 0.9860239624977112\n",
      "        vf_loss: 7.782772620519002\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.65405405405404\n",
      "    gpu_util_percent0: 0.16594594594594594\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8556756756756758\n",
      "    ram_util_percent: 5.44054054054054\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2118157720460867\n",
      "    mean_env_wait_ms: 1.7027730511202517\n",
      "    mean_inference_ms: 6.65113923503999\n",
      "    mean_raw_obs_processing_ms: 0.5902368034541694\n",
      "  time_since_restore: 556.8454191684723\n",
      "  time_this_iter_s: 36.352128982543945\n",
      "  time_total_s: 556.8454191684723\n",
      "  timers:\n",
      "    learn_throughput: 6292.562\n",
      "    learn_time_ms: 25711.626\n",
      "    sample_throughput: 14812.149\n",
      "    sample_time_ms: 10922.925\n",
      "    update_time_ms: 61.42\n",
      "  timestamp: 1605363293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     15 |          556.845 | 2426880 |  398.939 |              436.082 |              344.937 |            844.005 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3518.5343665768196\n",
      "    time_step_min: 3176\n",
      "  date: 2020-11-14_15-15-30\n",
      "  done: false\n",
      "  episode_len_mean: 842.2528314457029\n",
      "  episode_reward_max: 436.0817678397799\n",
      "  episode_reward_mean: 399.2087752345288\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3002\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8448804020881653\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008873226043457786\n",
      "        model: {}\n",
      "        policy_loss: -0.022369914144898456\n",
      "        total_loss: 7.658991535504659\n",
      "        vf_explained_var: 0.9827911257743835\n",
      "        vf_loss: 7.680009285608928\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.47368421052632\n",
      "    gpu_util_percent0: 0.18210526315789474\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8507894736842104\n",
      "    ram_util_percent: 5.465789473684211\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21144654311955768\n",
      "    mean_env_wait_ms: 1.7028953135162341\n",
      "    mean_inference_ms: 6.633578646283495\n",
      "    mean_raw_obs_processing_ms: 0.589210187820963\n",
      "  time_since_restore: 593.1988530158997\n",
      "  time_this_iter_s: 36.35343384742737\n",
      "  time_total_s: 593.1988530158997\n",
      "  timers:\n",
      "    learn_throughput: 6289.115\n",
      "    learn_time_ms: 25725.716\n",
      "    sample_throughput: 14915.108\n",
      "    sample_time_ms: 10847.524\n",
      "    update_time_ms: 59.298\n",
      "  timestamp: 1605363330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | RUNNING  | 172.17.0.14:70185 |     16 |          593.199 | 2588672 |  399.209 |              436.082 |              344.937 |            842.253 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_6f1a3_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4161\n",
      "    time_step_mean: 3517.902751119642\n",
      "    time_step_min: 3176\n",
      "  date: 2020-11-14_15-16-05\n",
      "  done: true\n",
      "  episode_len_mean: 840.7908227848101\n",
      "  episode_reward_max: 436.0817678397799\n",
      "  episode_reward_mean: 399.5087191574964\n",
      "  episode_reward_min: 344.9367198856021\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3160\n",
      "  experiment_id: afe9020e38b64acc8f5a9d3fc8ab2e06\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8327268411715826\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008561894840871295\n",
      "        model: {}\n",
      "        policy_loss: -0.02217764062030862\n",
      "        total_loss: 7.437536915143331\n",
      "        vf_explained_var: 0.982550859451294\n",
      "        vf_loss: 7.458418488502502\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.96216216216214\n",
      "    gpu_util_percent0: 0.17567567567567569\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.6964864864864866\n",
      "    ram_util_percent: 5.472972972972973\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 70185\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21108956876797438\n",
      "    mean_env_wait_ms: 1.7029785203045453\n",
      "    mean_inference_ms: 6.616862155569928\n",
      "    mean_raw_obs_processing_ms: 0.5881841491206937\n",
      "  time_since_restore: 628.9096179008484\n",
      "  time_this_iter_s: 35.71076488494873\n",
      "  time_total_s: 628.9096179008484\n",
      "  timers:\n",
      "    learn_throughput: 6300.862\n",
      "    learn_time_ms: 25677.755\n",
      "    sample_throughput: 15002.67\n",
      "    sample_time_ms: 10784.214\n",
      "    update_time_ms: 55.786\n",
      "  timestamp: 1605363365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: 6f1a3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | TERMINATED |       |     17 |           628.91 | 2750464 |  399.509 |              436.082 |              344.937 |            840.791 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 43.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_6f1a3_00000 | TERMINATED |       |     17 |           628.91 | 2750464 |  399.509 |              436.082 |              344.937 |            840.791 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 15:16:07,212\tINFO tune.py:439 -- Total run time: 645.72 seconds (644.82 seconds for the tuning loop).\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69929\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_150514-9znmbsbb/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_150514-9znmbsbb/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605363367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 4161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3517.90275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 436.08177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 344.93672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 399.50872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgentle-sweep-4\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/9znmbsbb\u001b[0m\n",
      "2020-11-14 15:16:14,269 - wandb.wandb_agent - INFO - Cleaning up finished run: 9znmbsbb\n",
      "2020-11-14 15:16:15,409 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 15:16:15,410 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta55\n",
      "2020-11-14 15:16:15,411 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 15:16:20,424 - wandb.wandb_agent - INFO - Running runs: ['eus49ukb']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweepy-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/eus49ukb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_151618-eus49ukb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 15:16:22,534\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 24.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=8590)\u001b[0m 2020-11-14 15:16:29,476\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=8617)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8617)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8604)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8604)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8588)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8588)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8615)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8615)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8630)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8630)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8640)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8640)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8631)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8631)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8620)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8620)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8609)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8609)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8590)\u001b[0m 2020-11-14 15:16:41,394\tINFO trainable.py:252 -- Trainable.setup took 12.752 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=8622)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8622)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8583)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8583)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8602)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8602)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8646)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8646)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8555)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8555)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8607)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8607)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8624)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8624)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8570)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8570)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8594)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8594)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8582)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8582)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8628)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8628)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8612)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8612)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8589)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8589)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8553)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8553)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8579)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8579)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8614)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8614)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8639)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8639)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8598)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8598)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8603)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8603)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8638)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8638)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8557)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8557)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8645)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8645)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8564)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8564)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8606)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8606)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8566)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8566)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8549)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8549)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8574)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8574)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8576)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8576)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8563)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8563)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8611)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8611)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8552)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8552)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8554)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8554)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8524)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8524)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8551)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8551)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8573)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8573)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8548)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8548)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8547)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8547)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8521)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8521)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8580)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8580)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8591)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8591)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8585)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8585)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8550)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8550)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8559)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8559)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8560)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8560)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8584)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8584)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8546)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8546)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8581)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8581)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8561)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8561)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8513)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8513)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8529)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8529)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8577)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8577)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8558)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8558)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8518)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8518)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8530)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8530)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8506)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8506)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8514)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8514)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8526)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8526)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8511)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8511)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8527)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8527)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8510)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8510)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8503)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8503)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8509)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8509)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8504)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8504)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8512)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8512)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8502)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8502)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8516)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8516)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8507)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8507)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8508)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8508)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8505)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8505)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=8522)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=8522)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3949\n",
      "    time_step_mean: 3533.5086206896553\n",
      "    time_step_min: 3241\n",
      "  date: 2020-11-14_15-17-20\n",
      "  done: false\n",
      "  episode_len_mean: 897.4873417721519\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 393.27708336141285\n",
      "  episode_reward_min: 366.5677104619969\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1723820169766743\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.005347426243436833\n",
      "        model: {}\n",
      "        policy_loss: -0.011072169848678945\n",
      "        total_loss: 1015.4135386149088\n",
      "        vf_explained_var: 0.09814902395009995\n",
      "        vf_loss: 1015.4240976969401\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.31951219512196\n",
      "    gpu_util_percent0: 0.18780487804878052\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8609756097560974\n",
      "    ram_util_percent: 5.21219512195122\n",
      "    vram_util_percent0: 0.07831636122199201\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22176768463973312\n",
      "    mean_env_wait_ms: 1.7100413340998122\n",
      "    mean_inference_ms: 7.230603814897951\n",
      "    mean_raw_obs_processing_ms: 0.6058668611814446\n",
      "  time_since_restore: 38.6918261051178\n",
      "  time_this_iter_s: 38.6918261051178\n",
      "  time_total_s: 38.6918261051178\n",
      "  timers:\n",
      "    learn_throughput: 6250.985\n",
      "    learn_time_ms: 25882.639\n",
      "    sample_throughput: 12792.945\n",
      "    sample_time_ms: 12646.97\n",
      "    update_time_ms: 80.538\n",
      "  timestamp: 1605363440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      1 |          38.6918 | 161792 |  393.277 |              427.161 |              366.568 |            897.487 |\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3949\n",
      "    time_step_mean: 3526.6459854014597\n",
      "    time_step_min: 3241\n",
      "  date: 2020-11-14_15-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 895.1487341772151\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 392.8886861845538\n",
      "  episode_reward_min: 363.57740152920735\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.148846834897995\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.007148951447258393\n",
      "        model: {}\n",
      "        policy_loss: -0.01401245578987679\n",
      "        total_loss: 457.2844924926758\n",
      "        vf_explained_var: 0.3581078052520752\n",
      "        vf_loss: 457.2976582845052\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.90263157894738\n",
      "    gpu_util_percent0: 0.23921052631578946\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8623684210526317\n",
      "    ram_util_percent: 5.442105263157894\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2179909811702571\n",
      "    mean_env_wait_ms: 1.709730695348129\n",
      "    mean_inference_ms: 7.06328409688071\n",
      "    mean_raw_obs_processing_ms: 0.5967956919624575\n",
      "  time_since_restore: 74.85023832321167\n",
      "  time_this_iter_s: 36.15841221809387\n",
      "  time_total_s: 74.85023832321167\n",
      "  timers:\n",
      "    learn_throughput: 6390.996\n",
      "    learn_time_ms: 25315.617\n",
      "    sample_throughput: 13532.482\n",
      "    sample_time_ms: 11955.826\n",
      "    update_time_ms: 71.26\n",
      "  timestamp: 1605363476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      2 |          74.8502 | 323584 |  392.889 |              427.161 |              363.577 |            895.149 |\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3949\n",
      "    time_step_mean: 3525.046296296296\n",
      "    time_step_min: 3241\n",
      "  date: 2020-11-14_15-18-32\n",
      "  done: false\n",
      "  episode_len_mean: 891.6919831223629\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 391.9699027852581\n",
      "  episode_reward_min: 360.3978818754228\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1372449696063995\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008206659539913138\n",
      "        model: {}\n",
      "        policy_loss: -0.0164019283062468\n",
      "        total_loss: 162.7292340596517\n",
      "        vf_explained_var: 0.7355320453643799\n",
      "        vf_loss: 162.74455897013345\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.89743589743591\n",
      "    gpu_util_percent0: 0.1864102564102564\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8553846153846155\n",
      "    ram_util_percent: 5.44102564102564\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21481055263219184\n",
      "    mean_env_wait_ms: 1.6998521707369472\n",
      "    mean_inference_ms: 6.892286121959365\n",
      "    mean_raw_obs_processing_ms: 0.5874473468900016\n",
      "  time_since_restore: 110.98412990570068\n",
      "  time_this_iter_s: 36.133891582489014\n",
      "  time_total_s: 110.98412990570068\n",
      "  timers:\n",
      "    learn_throughput: 6420.025\n",
      "    learn_time_ms: 25201.149\n",
      "    sample_throughput: 13966.072\n",
      "    sample_time_ms: 11584.646\n",
      "    update_time_ms: 87.277\n",
      "  timestamp: 1605363512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      3 |          110.984 | 485376 |   391.97 |              427.161 |              360.398 |            891.692 |\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3529.169491525424\n",
      "    time_step_min: 3241\n",
      "  date: 2020-11-14_15-19-09\n",
      "  done: false\n",
      "  episode_len_mean: 887.9303797468355\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 391.4227828727838\n",
      "  episode_reward_min: 358.7457030534079\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1191678941249847\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009919041767716408\n",
      "        model: {}\n",
      "        policy_loss: -0.01927357337748011\n",
      "        total_loss: 39.96915340423584\n",
      "        vf_explained_var: 0.8962006568908691\n",
      "        vf_loss: 39.98700396219889\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.30000000000001\n",
      "    gpu_util_percent0: 0.17763157894736845\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8563157894736841\n",
      "    ram_util_percent: 5.439473684210525\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21262333275530976\n",
      "    mean_env_wait_ms: 1.693646714370532\n",
      "    mean_inference_ms: 6.763458778353246\n",
      "    mean_raw_obs_processing_ms: 0.5795822160410794\n",
      "  time_since_restore: 147.28787517547607\n",
      "  time_this_iter_s: 36.30374526977539\n",
      "  time_total_s: 147.28787517547607\n",
      "  timers:\n",
      "    learn_throughput: 6385.563\n",
      "    learn_time_ms: 25337.155\n",
      "    sample_throughput: 14339.521\n",
      "    sample_time_ms: 11282.943\n",
      "    update_time_ms: 82.897\n",
      "  timestamp: 1605363549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      4 |          147.288 | 647168 |  391.423 |              427.161 |              358.746 |             887.93 |\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3528.110962566845\n",
      "    time_step_min: 3221\n",
      "  date: 2020-11-14_15-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 883.493670886076\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 391.50216546688387\n",
      "  episode_reward_min: 358.7457030534079\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.091629167397817\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.011219957377761602\n",
      "        model: {}\n",
      "        policy_loss: -0.021773166411245864\n",
      "        total_loss: 20.82156451543172\n",
      "        vf_explained_var: 0.9421625137329102\n",
      "        vf_loss: 20.841639200846355\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.13076923076923\n",
      "    gpu_util_percent0: 0.1635897435897436\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8564102564102564\n",
      "    ram_util_percent: 5.43076923076923\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21088074597518217\n",
      "    mean_env_wait_ms: 1.6902548780044873\n",
      "    mean_inference_ms: 6.665929207310323\n",
      "    mean_raw_obs_processing_ms: 0.5735498215852369\n",
      "  time_since_restore: 183.69780111312866\n",
      "  time_this_iter_s: 36.40992593765259\n",
      "  time_total_s: 183.69780111312866\n",
      "  timers:\n",
      "    learn_throughput: 6370.3\n",
      "    learn_time_ms: 25397.863\n",
      "    sample_throughput: 14554.678\n",
      "    sample_time_ms: 11116.151\n",
      "    update_time_ms: 104.611\n",
      "  timestamp: 1605363585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      5 |          183.698 | 808960 |  391.502 |              427.161 |              358.746 |            883.494 |\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3520.76527643065\n",
      "    time_step_min: 3221\n",
      "  date: 2020-11-14_15-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 874.5181733457596\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 391.3278151453339\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 283\n",
      "  episodes_total: 1073\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.064040869474411\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010875858521709839\n",
      "        model: {}\n",
      "        policy_loss: -0.02206126864378651\n",
      "        total_loss: 15.727375348409018\n",
      "        vf_explained_var: 0.9697928428649902\n",
      "        vf_loss: 15.747793515523275\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.76666666666665\n",
      "    gpu_util_percent0: 0.18794871794871795\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8615384615384616\n",
      "    ram_util_percent: 5.379487179487179\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2086561442256575\n",
      "    mean_env_wait_ms: 1.6884590541155409\n",
      "    mean_inference_ms: 6.5423910492375725\n",
      "    mean_raw_obs_processing_ms: 0.5660654053012625\n",
      "  time_since_restore: 220.03298425674438\n",
      "  time_this_iter_s: 36.33518314361572\n",
      "  time_total_s: 220.03298425674438\n",
      "  timers:\n",
      "    learn_throughput: 6372.522\n",
      "    learn_time_ms: 25389.007\n",
      "    sample_throughput: 14653.16\n",
      "    sample_time_ms: 11041.441\n",
      "    update_time_ms: 105.137\n",
      "  timestamp: 1605363622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      6 |          220.033 | 970752 |  391.328 |              427.161 |              351.775 |            874.518 |\n",
      "+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3514.536006546645\n",
      "    time_step_min: 3221\n",
      "  date: 2020-11-14_15-20-58\n",
      "  done: false\n",
      "  episode_len_mean: 869.8457278481013\n",
      "  episode_reward_max: 427.16130007330537\n",
      "  episode_reward_mean: 391.3889406181785\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 1264\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0637680888175964\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.01027807709760964\n",
      "        model: {}\n",
      "        policy_loss: -0.022146663007636864\n",
      "        total_loss: 13.945648988087973\n",
      "        vf_explained_var: 0.9670645594596863\n",
      "        vf_loss: 13.966272195180258\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.64324324324323\n",
      "    gpu_util_percent0: 0.17027027027027025\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8537837837837838\n",
      "    ram_util_percent: 5.435135135135136\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20755562699703076\n",
      "    mean_env_wait_ms: 1.6875858153052343\n",
      "    mean_inference_ms: 6.48286712160282\n",
      "    mean_raw_obs_processing_ms: 0.5628965056495733\n",
      "  time_since_restore: 256.08964586257935\n",
      "  time_this_iter_s: 36.05666160583496\n",
      "  time_total_s: 256.08964586257935\n",
      "  timers:\n",
      "    learn_throughput: 6371.736\n",
      "    learn_time_ms: 25392.139\n",
      "    sample_throughput: 14753.133\n",
      "    sample_time_ms: 10966.62\n",
      "    update_time_ms: 97.465\n",
      "  timestamp: 1605363658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      7 |           256.09 | 1132544 |  391.389 |              427.161 |              351.775 |            869.846 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3509.1652173913044\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-21-34\n",
      "  done: false\n",
      "  episode_len_mean: 866.1146272855134\n",
      "  episode_reward_max: 427.4432061427498\n",
      "  episode_reward_mean: 391.6158211861489\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0436054170131683\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010477355758969983\n",
      "        model: {}\n",
      "        policy_loss: -0.02261517142566542\n",
      "        total_loss: 12.747424920399984\n",
      "        vf_explained_var: 0.9674347043037415\n",
      "        vf_loss: 12.768466631571451\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.55897435897435\n",
      "    gpu_util_percent0: 0.17794871794871797\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7241025641025641\n",
      "    ram_util_percent: 5.4461538461538455\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20676188585498295\n",
      "    mean_env_wait_ms: 1.6876421572708464\n",
      "    mean_inference_ms: 6.441304992779538\n",
      "    mean_raw_obs_processing_ms: 0.5607512918711602\n",
      "  time_since_restore: 292.09305334091187\n",
      "  time_this_iter_s: 36.00340747833252\n",
      "  time_total_s: 292.09305334091187\n",
      "  timers:\n",
      "    learn_throughput: 6377.014\n",
      "    learn_time_ms: 25371.12\n",
      "    sample_throughput: 14848.072\n",
      "    sample_time_ms: 10896.499\n",
      "    update_time_ms: 99.976\n",
      "  timestamp: 1605363694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      8 |          292.093 | 1294336 |  391.616 |              427.443 |              351.775 |            866.115 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3507.3927178153444\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 862.4949367088608\n",
      "  episode_reward_max: 427.4432061427498\n",
      "  episode_reward_mean: 391.918837898605\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0150500535964966\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010597204944739739\n",
      "        model: {}\n",
      "        policy_loss: -0.024292595451697707\n",
      "        total_loss: 11.452072858810425\n",
      "        vf_explained_var: 0.969855010509491\n",
      "        vf_loss: 11.474753379821777\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.46578947368421\n",
      "    gpu_util_percent0: 0.14684210526315788\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8489473684210526\n",
      "    ram_util_percent: 5.439473684210526\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20610135005908406\n",
      "    mean_env_wait_ms: 1.6878932382742835\n",
      "    mean_inference_ms: 6.404977480010406\n",
      "    mean_raw_obs_processing_ms: 0.5588950271946401\n",
      "  time_since_restore: 328.2276837825775\n",
      "  time_this_iter_s: 36.13463044166565\n",
      "  time_total_s: 328.2276837825775\n",
      "  timers:\n",
      "    learn_throughput: 6372.9\n",
      "    learn_time_ms: 25387.502\n",
      "    sample_throughput: 14940.821\n",
      "    sample_time_ms: 10828.856\n",
      "    update_time_ms: 107.08\n",
      "  timestamp: 1605363730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |      9 |          328.228 | 1456128 |  391.919 |              427.443 |              351.775 |            862.495 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3503.3127524523948\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 858.2907042253521\n",
      "  episode_reward_max: 429.495002526509\n",
      "  episode_reward_mean: 392.51088448436076\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 195\n",
      "  episodes_total: 1775\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9646227161089579\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009996278444305062\n",
      "        model: {}\n",
      "        policy_loss: -0.0233594632979172\n",
      "        total_loss: 10.59533135096232\n",
      "        vf_explained_var: 0.9798431992530823\n",
      "        vf_loss: 10.617173592249552\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.9578947368421\n",
      "    gpu_util_percent0: 0.1986842105263158\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8528947368421054\n",
      "    ram_util_percent: 5.407894736842106\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20536357086480053\n",
      "    mean_env_wait_ms: 1.688003318733386\n",
      "    mean_inference_ms: 6.364290856723816\n",
      "    mean_raw_obs_processing_ms: 0.5567310003605479\n",
      "  time_since_restore: 364.3062744140625\n",
      "  time_this_iter_s: 36.078590631484985\n",
      "  time_total_s: 364.3062744140625\n",
      "  timers:\n",
      "    learn_throughput: 6361.045\n",
      "    learn_time_ms: 25434.813\n",
      "    sample_throughput: 15039.609\n",
      "    sample_time_ms: 10757.726\n",
      "    update_time_ms: 99.344\n",
      "  timestamp: 1605363766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     10 |          364.306 | 1617920 |  392.511 |              429.495 |              351.775 |            858.291 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3497.445825049702\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 852.9021421616359\n",
      "  episode_reward_max: 429.495002526509\n",
      "  episode_reward_mean: 393.2914707931872\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 2054\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9578023354212443\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009879338166986903\n",
      "        model: {}\n",
      "        policy_loss: -0.021309620855997007\n",
      "        total_loss: 9.164591153462728\n",
      "        vf_explained_var: 0.9832444787025452\n",
      "        vf_loss: 9.184403896331787\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60000000000001\n",
      "    gpu_util_percent0: 0.20076923076923078\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8553846153846155\n",
      "    ram_util_percent: 5.433333333333334\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20457118114292422\n",
      "    mean_env_wait_ms: 1.6892613380819739\n",
      "    mean_inference_ms: 6.318559282067169\n",
      "    mean_raw_obs_processing_ms: 0.5544088304050048\n",
      "  time_since_restore: 399.99167227745056\n",
      "  time_this_iter_s: 35.68539786338806\n",
      "  time_total_s: 399.99167227745056\n",
      "  timers:\n",
      "    learn_throughput: 6375.49\n",
      "    learn_time_ms: 25377.186\n",
      "    sample_throughput: 15383.622\n",
      "    sample_time_ms: 10517.159\n",
      "    update_time_ms: 98.382\n",
      "  timestamp: 1605363802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     11 |          399.992 | 1779712 |  393.291 |              429.495 |              351.775 |            852.902 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3493.064055299539\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 849.9493670886076\n",
      "  episode_reward_max: 429.495002526509\n",
      "  episode_reward_mean: 393.67123392561666\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2212\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9442804157733917\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009659482166171074\n",
      "        model: {}\n",
      "        policy_loss: -0.02190020653263976\n",
      "        total_loss: 9.218956311543783\n",
      "        vf_explained_var: 0.978411853313446\n",
      "        vf_loss: 9.239396651585897\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.87105263157896\n",
      "    gpu_util_percent0: 0.18394736842105264\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.853421052631579\n",
      "    ram_util_percent: 5.447368421052632\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20417742092907828\n",
      "    mean_env_wait_ms: 1.6898898336466806\n",
      "    mean_inference_ms: 6.296284802853958\n",
      "    mean_raw_obs_processing_ms: 0.5533541769403093\n",
      "  time_since_restore: 436.081041097641\n",
      "  time_this_iter_s: 36.08936882019043\n",
      "  time_total_s: 436.081041097641\n",
      "  timers:\n",
      "    learn_throughput: 6357.982\n",
      "    learn_time_ms: 25447.067\n",
      "    sample_throughput: 15514.445\n",
      "    sample_time_ms: 10428.475\n",
      "    update_time_ms: 98.829\n",
      "  timestamp: 1605363838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     12 |          436.081 | 1941504 |  393.671 |              429.495 |              351.775 |            849.949 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3489.0743127147766\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 847.7270042194093\n",
      "  episode_reward_max: 429.495002526509\n",
      "  episode_reward_mean: 394.20333466201134\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2370\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9204291800657908\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009908715030178428\n",
      "        model: {}\n",
      "        policy_loss: -0.022139210913640756\n",
      "        total_loss: 8.941451152165731\n",
      "        vf_explained_var: 0.9781339764595032\n",
      "        vf_loss: 8.962068875630697\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.61315789473683\n",
      "    gpu_util_percent0: 0.19184210526315787\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8518421052631578\n",
      "    ram_util_percent: 5.4526315789473685\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2038224413196352\n",
      "    mean_env_wait_ms: 1.690611140413968\n",
      "    mean_inference_ms: 6.276538565501703\n",
      "    mean_raw_obs_processing_ms: 0.5523999427314381\n",
      "  time_since_restore: 472.2737970352173\n",
      "  time_this_iter_s: 36.192755937576294\n",
      "  time_total_s: 472.2737970352173\n",
      "  timers:\n",
      "    learn_throughput: 6348.251\n",
      "    learn_time_ms: 25486.077\n",
      "    sample_throughput: 15554.57\n",
      "    sample_time_ms: 10401.573\n",
      "    update_time_ms: 92.748\n",
      "  timestamp: 1605363875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     13 |          472.274 | 2103296 |  394.203 |              429.495 |              351.775 |            847.727 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4085\n",
      "    time_step_mean: 3484.2079246763437\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-25-11\n",
      "  done: false\n",
      "  episode_len_mean: 844.8375144731764\n",
      "  episode_reward_max: 429.495002526509\n",
      "  episode_reward_mean: 395.0186622654775\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 221\n",
      "  episodes_total: 2591\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8770085275173187\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009805361274629831\n",
      "        model: {}\n",
      "        policy_loss: -0.0222737779840827\n",
      "        total_loss: 8.415510177612305\n",
      "        vf_explained_var: 0.9856352210044861\n",
      "        vf_loss: 8.436261256535849\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.92564102564104\n",
      "    gpu_util_percent0: 0.17615384615384613\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8582051282051284\n",
      "    ram_util_percent: 5.438461538461538\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2034488202969037\n",
      "    mean_env_wait_ms: 1.6924257866271983\n",
      "    mean_inference_ms: 6.252881696178331\n",
      "    mean_raw_obs_processing_ms: 0.5512576205736918\n",
      "  time_since_restore: 508.5363178253174\n",
      "  time_this_iter_s: 36.2625207901001\n",
      "  time_total_s: 508.5363178253174\n",
      "  timers:\n",
      "    learn_throughput: 6350.491\n",
      "    learn_time_ms: 25477.084\n",
      "    sample_throughput: 15577.468\n",
      "    sample_time_ms: 10386.284\n",
      "    update_time_ms: 92.544\n",
      "  timestamp: 1605363911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     14 |          508.536 | 2265088 |  395.019 |              429.495 |              351.775 |            844.838 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4126\n",
      "    time_step_mean: 3478.8868665239115\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 842.215541490858\n",
      "  episode_reward_max: 429.9376032531075\n",
      "  episode_reward_mean: 395.92578650920797\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 253\n",
      "  episodes_total: 2844\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8584191252787908\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009076375747099519\n",
      "        model: {}\n",
      "        policy_loss: -0.020305857217560213\n",
      "        total_loss: 7.125065128008525\n",
      "        vf_explained_var: 0.9869749546051025\n",
      "        vf_loss: 7.1439850727717085\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.01538461538462\n",
      "    gpu_util_percent0: 0.16333333333333336\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8505128205128204\n",
      "    ram_util_percent: 5.443589743589743\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20300906431063648\n",
      "    mean_env_wait_ms: 1.6933470090697942\n",
      "    mean_inference_ms: 6.2296087667905535\n",
      "    mean_raw_obs_processing_ms: 0.5502216635918551\n",
      "  time_since_restore: 544.9117059707642\n",
      "  time_this_iter_s: 36.37538814544678\n",
      "  time_total_s: 544.9117059707642\n",
      "  timers:\n",
      "    learn_throughput: 6352.424\n",
      "    learn_time_ms: 25469.334\n",
      "    sample_throughput: 15571.419\n",
      "    sample_time_ms: 10390.318\n",
      "    update_time_ms: 78.56\n",
      "  timestamp: 1605363948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     15 |          544.912 | 2426880 |  395.926 |              429.938 |              351.775 |            842.216 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4126\n",
      "    time_step_mean: 3476.0875\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 840.6952031978681\n",
      "  episode_reward_max: 429.9376032531075\n",
      "  episode_reward_mean: 396.41990876384915\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3002\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8591339538494746\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009283544728532434\n",
      "        model: {}\n",
      "        policy_loss: -0.02103960521829625\n",
      "        total_loss: 7.446077783902486\n",
      "        vf_explained_var: 0.9830930829048157\n",
      "        vf_loss: 7.46569009621938\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.68421052631578\n",
      "    gpu_util_percent0: 0.20789473684210524\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8536842105263159\n",
      "    ram_util_percent: 5.436842105263159\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20277832787744654\n",
      "    mean_env_wait_ms: 1.694285327158744\n",
      "    mean_inference_ms: 6.21697585623294\n",
      "    mean_raw_obs_processing_ms: 0.5496715290032242\n",
      "  time_since_restore: 581.1537067890167\n",
      "  time_this_iter_s: 36.24200081825256\n",
      "  time_total_s: 581.1537067890167\n",
      "  timers:\n",
      "    learn_throughput: 6350.521\n",
      "    learn_time_ms: 25476.963\n",
      "    sample_throughput: 15585.006\n",
      "    sample_time_ms: 10381.26\n",
      "    update_time_ms: 73.162\n",
      "  timestamp: 1605363984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | RUNNING  | 172.17.0.14:8590 |     16 |          581.154 | 2588672 |   396.42 |              429.938 |              351.775 |            840.695 |\n",
      "+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_fb736_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4126\n",
      "    time_step_mean: 3473.251122514432\n",
      "    time_step_min: 3103\n",
      "  date: 2020-11-14_15-27-00\n",
      "  done: true\n",
      "  episode_len_mean: 839.5243670886076\n",
      "  episode_reward_max: 429.9376032531075\n",
      "  episode_reward_mean: 396.9775301865219\n",
      "  episode_reward_min: 351.77468135632563\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3160\n",
      "  experiment_id: 2faa9de4804b49a29b7b4e83e8b1a99b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.827573890487353\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009433228289708495\n",
      "        model: {}\n",
      "        policy_loss: -0.024042623893668253\n",
      "        total_loss: 7.672796090443929\n",
      "        vf_explained_var: 0.9829551577568054\n",
      "        vf_loss: 7.695365746815999\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.20769230769231\n",
      "    gpu_util_percent0: 0.1641025641025641\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.053846153846153835\n",
      "    ram_util_percent: 5.561538461538461\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 8590\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20256911417515103\n",
      "    mean_env_wait_ms: 1.6951945061788174\n",
      "    mean_inference_ms: 6.205509376285342\n",
      "    mean_raw_obs_processing_ms: 0.549182524613331\n",
      "  time_since_restore: 617.2732901573181\n",
      "  time_this_iter_s: 36.11958336830139\n",
      "  time_total_s: 617.2732901573181\n",
      "  timers:\n",
      "    learn_throughput: 6349.357\n",
      "    learn_time_ms: 25481.634\n",
      "    sample_throughput: 15613.872\n",
      "    sample_time_ms: 10362.068\n",
      "    update_time_ms: 73.716\n",
      "  timestamp: 1605364020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: fb736_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | TERMINATED |       |     17 |          617.273 | 2750464 |  396.978 |              429.938 |              351.775 |            839.524 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 42.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_fb736_00000 | TERMINATED |       |     17 |          617.273 | 2750464 |  396.978 |              429.938 |              351.775 |            839.524 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 15:27:01,651\tINFO tune.py:439 -- Total run time: 635.16 seconds (634.64 seconds for the tuning loop).\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 8346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_151618-eus49ukb/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_151618-eus49ukb/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 643\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605364021\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 4126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3473.25112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 429.9376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 351.77468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 396.97753\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msweepy-sweep-5\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/eus49ukb\u001b[0m\n",
      "2020-11-14 15:27:12,707 - wandb.wandb_agent - INFO - Cleaning up finished run: eus49ukb\n",
      "2020-11-14 15:27:12,998 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 15:27:12,998 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta56\n",
      "2020-11-14 15:27:12,999 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 15:27:18,015 - wandb.wandb_agent - INFO - Running runs: ['41ih3slj']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-sweep-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/41ih3slj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_152716-41ih3slj\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 15:27:20,090\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 23.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=28661)\u001b[0m 2020-11-14 15:27:27,231\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=28610)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28610)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28569)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28569)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28599)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28599)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28635)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28635)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28606)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28606)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28661)\u001b[0m 2020-11-14 15:27:39,388\tINFO trainable.py:252 -- Trainable.setup took 12.988 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=28650)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28650)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28653)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28653)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28646)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28646)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28659)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28659)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28591)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28591)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28647)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28647)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28532)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28532)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28596)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28639)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28639)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28633)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28633)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28657)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28657)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28578)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28578)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28642)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28642)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28612)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28612)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28607)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28607)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28530)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28530)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28603)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28603)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28648)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28648)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28561)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28561)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28598)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28598)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28616)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28616)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28620)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28620)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28611)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28611)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28658)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28658)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28636)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28636)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28589)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28589)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28566)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28566)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28609)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28609)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28605)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28605)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28531)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28531)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28662)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28662)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28576)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28576)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28528)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28528)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28594)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28594)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28592)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28592)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28577)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28577)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28600)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28600)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28575)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28575)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28567)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28567)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28571)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28571)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28584)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28584)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28534)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28534)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28563)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28563)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28574)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28574)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28588)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28588)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28590)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28590)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28520)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28520)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28597)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28597)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28514)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28514)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28518)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28518)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28527)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28527)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28580)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28580)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28564)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28564)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28546)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28546)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28565)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28565)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28521)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28521)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28562)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28562)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28517)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28517)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28526)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28526)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28579)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28579)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28624)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28624)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28519)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28519)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28525)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28525)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28516)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28516)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28515)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28515)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28550)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28550)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28522)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28522)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28529)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28529)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28555)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28555)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28524)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28524)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28523)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28523)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28533)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28533)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28513)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28513)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=28552)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=28552)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3948\n",
      "    time_step_mean: 3592.910714285714\n",
      "    time_step_min: 3345\n",
      "  date: 2020-11-14_15-28-18\n",
      "  done: false\n",
      "  episode_len_mean: 901.753164556962\n",
      "  episode_reward_max: 439.61195257378034\n",
      "  episode_reward_mean: 410.54173356507283\n",
      "  episode_reward_min: 382.4432837423714\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1597254872322083\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0063105488661676645\n",
      "        model: {}\n",
      "        policy_loss: -0.011401967713027261\n",
      "        total_loss: 1031.8503926595051\n",
      "        vf_explained_var: 0.07008117437362671\n",
      "        vf_loss: 1031.8611094156902\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.78\n",
      "    gpu_util_percent0: 0.21225\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.85175\n",
      "    ram_util_percent: 5.24\n",
      "    vram_util_percent0: 0.07822646769432601\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351304\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21922652976827212\n",
      "    mean_env_wait_ms: 1.7479655550526556\n",
      "    mean_inference_ms: 7.389519874900167\n",
      "    mean_raw_obs_processing_ms: 0.6241153147015243\n",
      "  time_since_restore: 38.49440121650696\n",
      "  time_this_iter_s: 38.49440121650696\n",
      "  time_total_s: 38.49440121650696\n",
      "  timers:\n",
      "    learn_throughput: 6257.677\n",
      "    learn_time_ms: 25854.96\n",
      "    sample_throughput: 12951.032\n",
      "    sample_time_ms: 12492.595\n",
      "    update_time_ms: 87.593\n",
      "  timestamp: 1605364098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.6/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      1 |          38.4944 | 161792 |  410.542 |              439.612 |              382.443 |            901.753 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3948\n",
      "    time_step_mean: 3587.203703703704\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 900.4050632911392\n",
      "  episode_reward_max: 439.61195257378034\n",
      "  episode_reward_mean: 410.8743797507736\n",
      "  episode_reward_min: 380.11209314882217\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.132685164610545\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.007202562759630382\n",
      "        model: {}\n",
      "        policy_loss: -0.012743453145958483\n",
      "        total_loss: 461.9202067057292\n",
      "        vf_explained_var: 0.2252691388130188\n",
      "        vf_loss: 461.93208567301434\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.08947368421055\n",
      "    gpu_util_percent0: 0.15947368421052632\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8502631578947369\n",
      "    ram_util_percent: 5.4605263157894735\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21533077533656542\n",
      "    mean_env_wait_ms: 1.720300230580387\n",
      "    mean_inference_ms: 7.161857846928997\n",
      "    mean_raw_obs_processing_ms: 0.609549411541536\n",
      "  time_since_restore: 74.98929047584534\n",
      "  time_this_iter_s: 36.49488925933838\n",
      "  time_total_s: 74.98929047584534\n",
      "  timers:\n",
      "    learn_throughput: 6286.633\n",
      "    learn_time_ms: 25735.875\n",
      "    sample_throughput: 13932.217\n",
      "    sample_time_ms: 11612.797\n",
      "    update_time_ms: 68.994\n",
      "  timestamp: 1605364134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      2 |          74.9893 | 323584 |  410.874 |              439.612 |              380.112 |            900.405 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 3975\n",
      "    time_step_mean: 3592.2406542056074\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-29-30\n",
      "  done: false\n",
      "  episode_len_mean: 896.0253164556962\n",
      "  episode_reward_max: 442.5210475015971\n",
      "  episode_reward_mean: 410.4113284622285\n",
      "  episode_reward_min: 380.11209314882217\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1220273474852245\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.007875558105297387\n",
      "        model: {}\n",
      "        policy_loss: -0.016171638077745836\n",
      "        total_loss: 174.3908487955729\n",
      "        vf_explained_var: 0.6307039260864258\n",
      "        vf_loss: 174.4060084025065\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.62432432432432\n",
      "    gpu_util_percent0: 0.2037837837837838\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8537837837837838\n",
      "    ram_util_percent: 5.464864864864865\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21337122019845578\n",
      "    mean_env_wait_ms: 1.705266587197888\n",
      "    mean_inference_ms: 6.998884717021426\n",
      "    mean_raw_obs_processing_ms: 0.6007981211748264\n",
      "  time_since_restore: 110.76791572570801\n",
      "  time_this_iter_s: 35.77862524986267\n",
      "  time_total_s: 110.76791572570801\n",
      "  timers:\n",
      "    learn_throughput: 6344.969\n",
      "    learn_time_ms: 25499.257\n",
      "    sample_throughput: 14383.937\n",
      "    sample_time_ms: 11248.103\n",
      "    update_time_ms: 75.661\n",
      "  timestamp: 1605364170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      3 |          110.768 | 485376 |  410.411 |              442.521 |              380.112 |            896.025 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4008\n",
      "    time_step_mean: 3598.0955631399315\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 893.495253164557\n",
      "  episode_reward_max: 442.5210475015971\n",
      "  episode_reward_mean: 410.6153821336811\n",
      "  episode_reward_min: 380.11209314882217\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.106039543946584\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009284110739827156\n",
      "        model: {}\n",
      "        policy_loss: -0.01572197858331492\n",
      "        total_loss: 40.67746067047119\n",
      "        vf_explained_var: 0.8598275184631348\n",
      "        vf_loss: 40.69187831878662\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.66578947368421\n",
      "    gpu_util_percent0: 0.1555263157894737\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8521052631578948\n",
      "    ram_util_percent: 5.502631578947369\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2119297381606924\n",
      "    mean_env_wait_ms: 1.6955588877678862\n",
      "    mean_inference_ms: 6.880682498244329\n",
      "    mean_raw_obs_processing_ms: 0.5947938269789435\n",
      "  time_since_restore: 147.8339250087738\n",
      "  time_this_iter_s: 37.066009283065796\n",
      "  time_total_s: 147.8339250087738\n",
      "  timers:\n",
      "    learn_throughput: 6300.414\n",
      "    learn_time_ms: 25679.581\n",
      "    sample_throughput: 14557.606\n",
      "    sample_time_ms: 11113.915\n",
      "    update_time_ms: 68.979\n",
      "  timestamp: 1605364207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      4 |          147.834 | 647168 |  410.615 |              442.521 |              380.112 |            893.495 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4008\n",
      "    time_step_mean: 3601.4905913978496\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-30-44\n",
      "  done: false\n",
      "  episode_len_mean: 891.0392405063291\n",
      "  episode_reward_max: 442.5210475015971\n",
      "  episode_reward_mean: 410.9179506715324\n",
      "  episode_reward_min: 380.11209314882217\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.080323318640391\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010817618652557334\n",
      "        model: {}\n",
      "        policy_loss: -0.020636938977986574\n",
      "        total_loss: 23.73526159922282\n",
      "        vf_explained_var: 0.9154298901557922\n",
      "        vf_loss: 23.754275639851887\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.00526315789472\n",
      "    gpu_util_percent0: 0.13473684210526313\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8565789473684212\n",
      "    ram_util_percent: 5.492105263157895\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21085979781628122\n",
      "    mean_env_wait_ms: 1.689476030009965\n",
      "    mean_inference_ms: 6.793312313885347\n",
      "    mean_raw_obs_processing_ms: 0.5905153013374962\n",
      "  time_since_restore: 184.75572967529297\n",
      "  time_this_iter_s: 36.921804666519165\n",
      "  time_total_s: 184.75572967529297\n",
      "  timers:\n",
      "    learn_throughput: 6278.365\n",
      "    learn_time_ms: 25769.766\n",
      "    sample_throughput: 14679.264\n",
      "    sample_time_ms: 11021.806\n",
      "    update_time_ms: 69.457\n",
      "  timestamp: 1605364244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      5 |          184.756 | 808960 |  410.918 |              442.521 |              380.112 |            891.039 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4008\n",
      "    time_step_mean: 3604.291576673866\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-31-21\n",
      "  done: false\n",
      "  episode_len_mean: 886.3868312757202\n",
      "  episode_reward_max: 442.5210475015971\n",
      "  episode_reward_mean: 411.1184972218836\n",
      "  episode_reward_min: 380.11209314882217\n",
      "  episodes_this_iter: 182\n",
      "  episodes_total: 972\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0336136122544606\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.011088502670948705\n",
      "        model: {}\n",
      "        policy_loss: -0.022714110556989908\n",
      "        total_loss: 19.351664702097576\n",
      "        vf_explained_var: 0.9496398568153381\n",
      "        vf_loss: 19.37267780303955\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.1\n",
      "    gpu_util_percent0: 0.20351351351351352\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8545945945945944\n",
      "    ram_util_percent: 5.448648648648648\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20977918373535825\n",
      "    mean_env_wait_ms: 1.6840271665055766\n",
      "    mean_inference_ms: 6.718558587290993\n",
      "    mean_raw_obs_processing_ms: 0.586886454669951\n",
      "  time_since_restore: 221.18509364128113\n",
      "  time_this_iter_s: 36.42936396598816\n",
      "  time_total_s: 221.18509364128113\n",
      "  timers:\n",
      "    learn_throughput: 6281.431\n",
      "    learn_time_ms: 25757.188\n",
      "    sample_throughput: 14795.1\n",
      "    sample_time_ms: 10935.513\n",
      "    update_time_ms: 71.975\n",
      "  timestamp: 1605364281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      6 |          221.185 | 970752 |  411.118 |              442.521 |              380.112 |            886.387 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4148\n",
      "    time_step_mean: 3601.078882497946\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 879.0752177355503\n",
      "  episode_reward_max: 442.5210475015971\n",
      "  episode_reward_mean: 411.5530454466979\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 291\n",
      "  episodes_total: 1263\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0384625693162282\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010656860889866948\n",
      "        model: {}\n",
      "        policy_loss: -0.02256555676770707\n",
      "        total_loss: 14.699878056844076\n",
      "        vf_explained_var: 0.964512825012207\n",
      "        vf_loss: 14.720831553141275\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.56486486486487\n",
      "    gpu_util_percent0: 0.20378378378378378\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8564864864864864\n",
      "    ram_util_percent: 5.454054054054055\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20868667211599495\n",
      "    mean_env_wait_ms: 1.6828208069739612\n",
      "    mean_inference_ms: 6.638949044192419\n",
      "    mean_raw_obs_processing_ms: 0.5830328287857394\n",
      "  time_since_restore: 257.7291395664215\n",
      "  time_this_iter_s: 36.54404592514038\n",
      "  time_total_s: 257.7291395664215\n",
      "  timers:\n",
      "    learn_throughput: 6277.111\n",
      "    learn_time_ms: 25774.914\n",
      "    sample_throughput: 14884.015\n",
      "    sample_time_ms: 10870.185\n",
      "    update_time_ms: 66.198\n",
      "  timestamp: 1605364317\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      7 |          257.729 | 1132544 |  411.553 |              442.521 |              376.708 |            879.075 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4148\n",
      "    time_step_mean: 3600.0087209302324\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 875.06258790436\n",
      "  episode_reward_max: 443.073154310311\n",
      "  episode_reward_mean: 411.71797950414015\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 159\n",
      "  episodes_total: 1422\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0173982381820679\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010034582577645779\n",
      "        model: {}\n",
      "        policy_loss: -0.020484743873870077\n",
      "        total_loss: 13.296924591064453\n",
      "        vf_explained_var: 0.9614250063896179\n",
      "        vf_loss: 13.315911849339804\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.53947368421052\n",
      "    gpu_util_percent0: 0.13921052631578948\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7136842105263157\n",
      "    ram_util_percent: 5.492105263157894\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2082349868629138\n",
      "    mean_env_wait_ms: 1.6821060210634484\n",
      "    mean_inference_ms: 6.60603505138941\n",
      "    mean_raw_obs_processing_ms: 0.5815813106151599\n",
      "  time_since_restore: 294.6446626186371\n",
      "  time_this_iter_s: 36.915523052215576\n",
      "  time_total_s: 294.6446626186371\n",
      "  timers:\n",
      "    learn_throughput: 6273.882\n",
      "    learn_time_ms: 25788.18\n",
      "    sample_throughput: 14889.076\n",
      "    sample_time_ms: 10866.49\n",
      "    update_time_ms: 67.584\n",
      "  timestamp: 1605364354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      8 |          294.645 | 1294336 |  411.718 |              443.073 |              376.708 |            875.063 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4148\n",
      "    time_step_mean: 3599.0710560625816\n",
      "    time_step_min: 3268\n",
      "  date: 2020-11-14_15-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 871.5677215189874\n",
      "  episode_reward_max: 443.073154310311\n",
      "  episode_reward_mean: 412.0851828633124\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9948269675175349\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009710508941983184\n",
      "        model: {}\n",
      "        policy_loss: -0.020388507827495534\n",
      "        total_loss: 12.416308085123697\n",
      "        vf_explained_var: 0.9631545543670654\n",
      "        vf_loss: 12.43525226910909\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.9945945945946\n",
      "    gpu_util_percent0: 0.19756756756756755\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8545945945945946\n",
      "    ram_util_percent: 5.4837837837837835\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20781969697678768\n",
      "    mean_env_wait_ms: 1.681416801386078\n",
      "    mean_inference_ms: 6.576526811914124\n",
      "    mean_raw_obs_processing_ms: 0.5801193351127363\n",
      "  time_since_restore: 331.3966414928436\n",
      "  time_this_iter_s: 36.75197887420654\n",
      "  time_total_s: 331.3966414928436\n",
      "  timers:\n",
      "    learn_throughput: 6264.852\n",
      "    learn_time_ms: 25825.351\n",
      "    sample_throughput: 14958.919\n",
      "    sample_time_ms: 10815.755\n",
      "    update_time_ms: 69.384\n",
      "  timestamp: 1605364391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |      9 |          331.397 | 1456128 |  412.085 |              443.073 |              376.708 |            871.568 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4148\n",
      "    time_step_mean: 3597.9160756501183\n",
      "    time_step_min: 3246\n",
      "  date: 2020-11-14_15-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 867.7295742232451\n",
      "  episode_reward_max: 445.48061812709574\n",
      "  episode_reward_mean: 412.25057893431574\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1738\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9552670965592066\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009850591188296676\n",
      "        model: {}\n",
      "        policy_loss: -0.02283593319589272\n",
      "        total_loss: 12.456611315409342\n",
      "        vf_explained_var: 0.9667652249336243\n",
      "        vf_loss: 12.477954705556234\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.76578947368422\n",
      "    gpu_util_percent0: 0.19421052631578947\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8544736842105265\n",
      "    ram_util_percent: 5.492105263157896\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20749318855669488\n",
      "    mean_env_wait_ms: 1.6813045943483038\n",
      "    mean_inference_ms: 6.551048998947327\n",
      "    mean_raw_obs_processing_ms: 0.5788595699278498\n",
      "  time_since_restore: 367.7504608631134\n",
      "  time_this_iter_s: 36.353819370269775\n",
      "  time_total_s: 367.7504608631134\n",
      "  timers:\n",
      "    learn_throughput: 6265.572\n",
      "    learn_time_ms: 25822.384\n",
      "    sample_throughput: 15007.958\n",
      "    sample_time_ms: 10780.414\n",
      "    update_time_ms: 66.463\n",
      "  timestamp: 1605364428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     10 |           367.75 | 1617920 |  412.251 |              445.481 |              376.708 |             867.73 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4148\n",
      "    time_step_mean: 3595.1526526526527\n",
      "    time_step_min: 3246\n",
      "  date: 2020-11-14_15-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 860.6727005870841\n",
      "  episode_reward_max: 445.48061812709574\n",
      "  episode_reward_mean: 412.6742513125457\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 306\n",
      "  episodes_total: 2044\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9242753187815348\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009618723842625817\n",
      "        model: {}\n",
      "        policy_loss: -0.020633243373595178\n",
      "        total_loss: 10.194060325622559\n",
      "        vf_explained_var: 0.9815966486930847\n",
      "        vf_loss: 10.213231881459555\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.5054054054054\n",
      "    gpu_util_percent0: 0.15432432432432433\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8597297297297296\n",
      "    ram_util_percent: 5.472972972972973\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20695808606792712\n",
      "    mean_env_wait_ms: 1.6817489752162644\n",
      "    mean_inference_ms: 6.511416888180051\n",
      "    mean_raw_obs_processing_ms: 0.5769969251983262\n",
      "  time_since_restore: 404.1558003425598\n",
      "  time_this_iter_s: 36.40533947944641\n",
      "  time_total_s: 404.1558003425598\n",
      "  timers:\n",
      "    learn_throughput: 6266.453\n",
      "    learn_time_ms: 25818.754\n",
      "    sample_throughput: 15292.018\n",
      "    sample_time_ms: 10580.16\n",
      "    update_time_ms: 60.62\n",
      "  timestamp: 1605364464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     11 |          404.156 | 1779712 |  412.674 |              445.481 |              376.708 |            860.673 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4148\n",
      "    time_step_mean: 3596.088180978763\n",
      "    time_step_min: 3246\n",
      "  date: 2020-11-14_15-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 857.4543399638336\n",
      "  episode_reward_max: 445.48061812709574\n",
      "  episode_reward_mean: 413.0526912030931\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 168\n",
      "  episodes_total: 2212\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9088142365217209\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00922730720291535\n",
      "        model: {}\n",
      "        policy_loss: -0.021105474637200434\n",
      "        total_loss: 9.636857668558756\n",
      "        vf_explained_var: 0.976289689540863\n",
      "        vf_loss: 9.656572103500366\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.26756756756758\n",
      "    gpu_util_percent0: 0.1954054054054054\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8505405405405405\n",
      "    ram_util_percent: 5.486486486486486\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20674978414672396\n",
      "    mean_env_wait_ms: 1.682270478131721\n",
      "    mean_inference_ms: 6.49346048133561\n",
      "    mean_raw_obs_processing_ms: 0.5760720961591108\n",
      "  time_since_restore: 440.3769700527191\n",
      "  time_this_iter_s: 36.2211697101593\n",
      "  time_total_s: 440.3769700527191\n",
      "  timers:\n",
      "    learn_throughput: 6276.871\n",
      "    learn_time_ms: 25775.901\n",
      "    sample_throughput: 15267.438\n",
      "    sample_time_ms: 10597.194\n",
      "    update_time_ms: 58.353\n",
      "  timestamp: 1605364500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     12 |          440.377 | 1941504 |  413.053 |              445.481 |              376.708 |            857.454 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4178\n",
      "    time_step_mean: 3595.4939759036147\n",
      "    time_step_min: 3166\n",
      "  date: 2020-11-14_15-35-37\n",
      "  done: false\n",
      "  episode_len_mean: 854.7434599156118\n",
      "  episode_reward_max: 449.89713000293983\n",
      "  episode_reward_mean: 413.35819927707655\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2370\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8965768118699392\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009186817953983942\n",
      "        model: {}\n",
      "        policy_loss: -0.020214770066862304\n",
      "        total_loss: 9.287376085917154\n",
      "        vf_explained_var: 0.9759182333946228\n",
      "        vf_loss: 9.306201855341593\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.23513513513514\n",
      "    gpu_util_percent0: 0.17918918918918916\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8508108108108108\n",
      "    ram_util_percent: 5.489189189189189\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20656244580042948\n",
      "    mean_env_wait_ms: 1.6828899061687235\n",
      "    mean_inference_ms: 6.478010041883927\n",
      "    mean_raw_obs_processing_ms: 0.5752717551465745\n",
      "  time_since_restore: 476.6103413105011\n",
      "  time_this_iter_s: 36.23337125778198\n",
      "  time_total_s: 476.6103413105011\n",
      "  timers:\n",
      "    learn_throughput: 6262.407\n",
      "    learn_time_ms: 25835.434\n",
      "    sample_throughput: 15282.562\n",
      "    sample_time_ms: 10586.707\n",
      "    update_time_ms: 55.265\n",
      "  timestamp: 1605364537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     13 |           476.61 | 2103296 |  413.358 |              449.897 |              376.708 |            854.743 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4178\n",
      "    time_step_mean: 3592.831456154465\n",
      "    time_step_min: 3166\n",
      "  date: 2020-11-14_15-36-13\n",
      "  done: false\n",
      "  episode_len_mean: 852.1105845181675\n",
      "  episode_reward_max: 449.89713000293983\n",
      "  episode_reward_mean: 413.72045823398514\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 162\n",
      "  episodes_total: 2532\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8576583564281464\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008915809681639075\n",
      "        model: {}\n",
      "        policy_loss: -0.01983727600115041\n",
      "        total_loss: 8.990477959314982\n",
      "        vf_explained_var: 0.9795548915863037\n",
      "        vf_loss: 9.008960962295532\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.89999999999999\n",
      "    gpu_util_percent0: 0.17657894736842106\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8518421052631581\n",
      "    ram_util_percent: 5.473684210526316\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2063728775293372\n",
      "    mean_env_wait_ms: 1.6832522587134024\n",
      "    mean_inference_ms: 6.462703616131413\n",
      "    mean_raw_obs_processing_ms: 0.5744235743786692\n",
      "  time_since_restore: 512.9377303123474\n",
      "  time_this_iter_s: 36.32738900184631\n",
      "  time_total_s: 512.9377303123474\n",
      "  timers:\n",
      "    learn_throughput: 6276.018\n",
      "    learn_time_ms: 25779.403\n",
      "    sample_throughput: 15305.876\n",
      "    sample_time_ms: 10570.581\n",
      "    update_time_ms: 55.475\n",
      "  timestamp: 1605364573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     14 |          512.938 | 2265088 |   413.72 |              449.897 |              376.708 |            852.111 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4178\n",
      "    time_step_mean: 3589.782951289398\n",
      "    time_step_min: 3166\n",
      "  date: 2020-11-14_15-36-50\n",
      "  done: false\n",
      "  episode_len_mean: 847.7921071176885\n",
      "  episode_reward_max: 449.89713000293983\n",
      "  episode_reward_mean: 414.3778675551941\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 306\n",
      "  episodes_total: 2838\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8304955015579859\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008671223806838194\n",
      "        model: {}\n",
      "        policy_loss: -0.01995594729669392\n",
      "        total_loss: 8.057240009307861\n",
      "        vf_explained_var: 0.9867238402366638\n",
      "        vf_loss: 8.075876951217651\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.27567567567569\n",
      "    gpu_util_percent0: 0.17945945945945949\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8556756756756757\n",
      "    ram_util_percent: 5.4648648648648654\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20603261945664383\n",
      "    mean_env_wait_ms: 1.684265468420384\n",
      "    mean_inference_ms: 6.437138238154966\n",
      "    mean_raw_obs_processing_ms: 0.5729376139777228\n",
      "  time_since_restore: 549.0899350643158\n",
      "  time_this_iter_s: 36.152204751968384\n",
      "  time_total_s: 549.0899350643158\n",
      "  timers:\n",
      "    learn_throughput: 6291.489\n",
      "    learn_time_ms: 25716.01\n",
      "    sample_throughput: 15336.853\n",
      "    sample_time_ms: 10549.231\n",
      "    update_time_ms: 58.323\n",
      "  timestamp: 1605364610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     15 |           549.09 | 2426880 |  414.378 |              449.897 |              376.708 |            847.792 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4178\n",
      "    time_step_mean: 3586.7374830852505\n",
      "    time_step_min: 3166\n",
      "  date: 2020-11-14_15-37-26\n",
      "  done: false\n",
      "  episode_len_mean: 845.8121252498335\n",
      "  episode_reward_max: 449.89713000293983\n",
      "  episode_reward_mean: 414.74594996865267\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 164\n",
      "  episodes_total: 3002\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8246833980083466\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008336103288456798\n",
      "        model: {}\n",
      "        policy_loss: -0.018581290021150682\n",
      "        total_loss: 8.141860326131185\n",
      "        vf_explained_var: 0.9813858866691589\n",
      "        vf_loss: 8.159186840057373\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.80540540540541\n",
      "    gpu_util_percent0: 0.18459459459459462\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8502702702702702\n",
      "    ram_util_percent: 5.451351351351351\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20587466331724524\n",
      "    mean_env_wait_ms: 1.6848638652550063\n",
      "    mean_inference_ms: 6.42536713226669\n",
      "    mean_raw_obs_processing_ms: 0.5722892536310371\n",
      "  time_since_restore: 585.3151788711548\n",
      "  time_this_iter_s: 36.22524380683899\n",
      "  time_total_s: 585.3151788711548\n",
      "  timers:\n",
      "    learn_throughput: 6290.341\n",
      "    learn_time_ms: 25720.702\n",
      "    sample_throughput: 15363.359\n",
      "    sample_time_ms: 10531.03\n",
      "    update_time_ms: 54.202\n",
      "  timestamp: 1605364646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | RUNNING  | 172.17.0.14:28661 |     16 |          585.315 | 2588672 |  414.746 |              449.897 |              376.708 |            845.812 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_8387b_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4178\n",
      "    time_step_mean: 3584.4576107899807\n",
      "    time_step_min: 3166\n",
      "  date: 2020-11-14_15-38-04\n",
      "  done: true\n",
      "  episode_len_mean: 843.9794303797469\n",
      "  episode_reward_max: 449.89713000293983\n",
      "  episode_reward_mean: 415.12242269604303\n",
      "  episode_reward_min: 376.7077440176881\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3160\n",
      "  experiment_id: f2cd17b35d2d4b69873dbfa933c661ab\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8099462538957596\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008903185371309519\n",
      "        model: {}\n",
      "        policy_loss: -0.02189907489810139\n",
      "        total_loss: 7.863398551940918\n",
      "        vf_explained_var: 0.9807210564613342\n",
      "        vf_loss: 7.883921980857849\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.70512820512822\n",
      "    gpu_util_percent0: 0.17307692307692313\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7176923076923077\n",
      "    ram_util_percent: 5.4974358974358974\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 28661\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20572517336739332\n",
      "    mean_env_wait_ms: 1.685414617557401\n",
      "    mean_inference_ms: 6.4146463468501445\n",
      "    mean_raw_obs_processing_ms: 0.5716839703468775\n",
      "  time_since_restore: 622.682412147522\n",
      "  time_this_iter_s: 37.36723327636719\n",
      "  time_total_s: 622.682412147522\n",
      "  timers:\n",
      "    learn_throughput: 6278.287\n",
      "    learn_time_ms: 25770.088\n",
      "    sample_throughput: 15340.479\n",
      "    sample_time_ms: 10546.737\n",
      "    update_time_ms: 60.303\n",
      "  timestamp: 1605364684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: 8387b_00000\n",
      "  \n",
      "2020-11-14 15:38:04,728\tWARNING util.py:137 -- The `process_trial` operation took 0.5546679496765137 seconds to complete, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 41.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | TERMINATED |       |     17 |          622.682 | 2750464 |  415.122 |              449.897 |              376.708 |            843.979 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 41.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_8387b_00000 | TERMINATED |       |     17 |          622.682 | 2750464 |  415.122 |              449.897 |              376.708 |            843.979 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 15:38:04,870\tINFO tune.py:439 -- Total run time: 640.65 seconds (640.36 seconds for the tuning loop).\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 28356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_152716-41ih3slj/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_152716-41ih3slj/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3166\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605364685\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 4178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3584.45761\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 449.89713\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 376.70774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 415.12242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mearnest-sweep-6\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/41ih3slj\u001b[0m\n",
      "2020-11-14 15:38:14,799 - wandb.wandb_agent - INFO - Cleaning up finished run: 41ih3slj\n",
      "2020-11-14 15:38:15,650 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 15:38:15,650 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta57\n",
      "2020-11-14 15:38:15,653 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 15:38:20,678 - wandb.wandb_agent - INFO - Running runs: ['5se6iolt']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melated-sweep-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/5se6iolt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_153818-5se6iolt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 15:38:22,724\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 24.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m 2020-11-14 15:38:28,658\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=48635)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48635)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48594)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48594)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48650)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48650)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48613)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48613)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48538)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48538)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48645)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48645)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48565)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48565)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48629)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48629)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48585)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48585)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48567)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48567)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48579)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48579)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48566)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48566)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48606)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48606)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48586)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48586)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48558)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48558)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48583)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48583)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48626)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48626)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48644)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48644)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48507)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48507)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48625)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48625)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48640)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48640)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48641)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48641)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48589)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48589)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48597)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48597)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48601)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48601)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48632)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48632)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48620)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48620)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48611)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48611)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48574)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48574)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48540)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48540)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48557)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48557)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48600)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48600)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48581)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48581)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48508)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48508)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48555)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48555)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48488)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48488)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48543)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48543)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m 2020-11-14 15:38:39,652\tINFO trainable.py:252 -- Trainable.setup took 11.738 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=48580)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48580)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48535)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48535)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48528)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48528)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48487)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48487)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48525)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48525)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48523)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48523)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48504)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48504)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48541)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48541)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48532)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48532)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48578)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48578)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48572)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48572)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48542)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48542)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48587)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48587)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48591)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48591)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48568)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48568)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48549)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48549)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48595)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48595)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48527)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48527)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48536)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48536)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48607)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48607)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48560)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48560)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48496)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48496)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48495)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48495)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48498)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48498)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48505)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48505)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48502)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48502)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48503)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48503)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48494)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48494)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48500)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48500)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48539)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48539)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48506)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48506)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48501)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48501)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48489)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48489)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48490)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48490)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48554)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48554)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48561)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48561)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48545)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48545)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48491)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48491)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48497)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48497)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48493)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48493)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48492)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48492)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=48499)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=48499)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4049\n",
      "    time_step_mean: 3697.8347826086956\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-39-17\n",
      "  done: false\n",
      "  episode_len_mean: 905.6772151898734\n",
      "  episode_reward_max: 424.88762901264755\n",
      "  episode_reward_mean: 404.78945873329405\n",
      "  episode_reward_min: 378.9260360323653\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1648134489854176\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0053425175525868935\n",
      "        model: {}\n",
      "        policy_loss: -0.011442157929802002\n",
      "        total_loss: 1050.4677327473958\n",
      "        vf_explained_var: 0.0873844102025032\n",
      "        vf_loss: 1050.4786682128906\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.42249999999999\n",
      "    gpu_util_percent0: 0.17975\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8550000000000001\n",
      "    ram_util_percent: 5.26\n",
      "    vram_util_percent0: 0.07822646769432601\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351304\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21478868774809784\n",
      "    mean_env_wait_ms: 1.7043027586317163\n",
      "    mean_inference_ms: 6.974388895744378\n",
      "    mean_raw_obs_processing_ms: 0.5993707108622754\n",
      "  time_since_restore: 37.957462787628174\n",
      "  time_this_iter_s: 37.957462787628174\n",
      "  time_total_s: 37.957462787628174\n",
      "  timers:\n",
      "    learn_throughput: 6210.734\n",
      "    learn_time_ms: 26050.383\n",
      "    sample_throughput: 13728.474\n",
      "    sample_time_ms: 11785.141\n",
      "    update_time_ms: 48.116\n",
      "  timestamp: 1605364757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      1 |          37.9575 | 161792 |  404.789 |              424.888 |              378.926 |            905.677 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4096\n",
      "    time_step_mean: 3712.6373626373625\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 902.9208860759494\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 404.3492837025333\n",
      "  episode_reward_min: 366.1945532999095\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1344065368175507\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.007400532175476353\n",
      "        model: {}\n",
      "        policy_loss: -0.014400602434761822\n",
      "        total_loss: 478.4111913045247\n",
      "        vf_explained_var: 0.2908002436161041\n",
      "        vf_loss: 478.4246826171875\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.44358974358975\n",
      "    gpu_util_percent0: 0.18923076923076923\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.857179487179487\n",
      "    ram_util_percent: 5.461538461538462\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21044014224808827\n",
      "    mean_env_wait_ms: 1.6882409962697322\n",
      "    mean_inference_ms: 6.80378835183324\n",
      "    mean_raw_obs_processing_ms: 0.5862012054221337\n",
      "  time_since_restore: 75.13164353370667\n",
      "  time_this_iter_s: 37.17418074607849\n",
      "  time_total_s: 75.13164353370667\n",
      "  timers:\n",
      "    learn_throughput: 6224.157\n",
      "    learn_time_ms: 25994.201\n",
      "    sample_throughput: 14171.199\n",
      "    sample_time_ms: 11416.959\n",
      "    update_time_ms: 76.079\n",
      "  timestamp: 1605364795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      2 |          75.1316 | 323584 |  404.349 |              445.416 |              366.195 |            902.921 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4179\n",
      "    time_step_mean: 3713.051044083527\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 897.5864978902954\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 404.0730804082811\n",
      "  episode_reward_min: 366.1945532999095\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1246079901854198\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00824627474260827\n",
      "        model: {}\n",
      "        policy_loss: -0.016717119646879535\n",
      "        total_loss: 174.42303593953451\n",
      "        vf_explained_var: 0.6859872937202454\n",
      "        vf_loss: 174.43866856892905\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.6921052631579\n",
      "    gpu_util_percent0: 0.2068421052631579\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8518421052631578\n",
      "    ram_util_percent: 5.471052631578948\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2087897452928208\n",
      "    mean_env_wait_ms: 1.6852658880429354\n",
      "    mean_inference_ms: 6.68404780650817\n",
      "    mean_raw_obs_processing_ms: 0.5801508084409389\n",
      "  time_since_restore: 112.07908749580383\n",
      "  time_this_iter_s: 36.94744396209717\n",
      "  time_total_s: 112.07908749580383\n",
      "  timers:\n",
      "    learn_throughput: 6196.951\n",
      "    learn_time_ms: 26108.323\n",
      "    sample_throughput: 14566.997\n",
      "    sample_time_ms: 11106.751\n",
      "    update_time_ms: 71.879\n",
      "  timestamp: 1605364832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      3 |          112.079 | 485376 |  404.073 |              445.416 |              366.195 |            897.586 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4179\n",
      "    time_step_mean: 3709.2495755517825\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-41-09\n",
      "  done: false\n",
      "  episode_len_mean: 893.5759493670886\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 404.0833389567054\n",
      "  episode_reward_min: 366.1945532999095\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1071635286013286\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009203249666218957\n",
      "        model: {}\n",
      "        policy_loss: -0.01677427246856193\n",
      "        total_loss: 41.06610616048177\n",
      "        vf_explained_var: 0.881175696849823\n",
      "        vf_loss: 41.08159255981445\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.89999999999999\n",
      "    gpu_util_percent0: 0.16384615384615386\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8538461538461539\n",
      "    ram_util_percent: 5.48974358974359\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20783478393212287\n",
      "    mean_env_wait_ms: 1.68489156304913\n",
      "    mean_inference_ms: 6.600010891781939\n",
      "    mean_raw_obs_processing_ms: 0.5758994003067178\n",
      "  time_since_restore: 148.95803713798523\n",
      "  time_this_iter_s: 36.8789496421814\n",
      "  time_total_s: 148.95803713798523\n",
      "  timers:\n",
      "    learn_throughput: 6184.097\n",
      "    learn_time_ms: 26162.589\n",
      "    sample_throughput: 14793.152\n",
      "    sample_time_ms: 10936.952\n",
      "    update_time_ms: 72.374\n",
      "  timestamp: 1605364869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      4 |          148.958 | 647168 |  404.083 |              445.416 |              366.195 |            893.576 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4179\n",
      "    time_step_mean: 3711.5555555555557\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 890.4822784810127\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 404.16418107393923\n",
      "  episode_reward_min: 366.1945532999095\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0762562255064647\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.011377540727456411\n",
      "        model: {}\n",
      "        policy_loss: -0.02140618150588125\n",
      "        total_loss: 25.5678292910258\n",
      "        vf_explained_var: 0.9220301508903503\n",
      "        vf_loss: 25.58749787012736\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.3578947368421\n",
      "    gpu_util_percent0: 0.21657894736842107\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8526315789473685\n",
      "    ram_util_percent: 5.484210526315789\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20703653950772907\n",
      "    mean_env_wait_ms: 1.6845885645242316\n",
      "    mean_inference_ms: 6.535547661865406\n",
      "    mean_raw_obs_processing_ms: 0.572411045245866\n",
      "  time_since_restore: 185.21882033348083\n",
      "  time_this_iter_s: 36.260783195495605\n",
      "  time_total_s: 185.21882033348083\n",
      "  timers:\n",
      "    learn_throughput: 6194.869\n",
      "    learn_time_ms: 26117.096\n",
      "    sample_throughput: 14990.011\n",
      "    sample_time_ms: 10793.321\n",
      "    update_time_ms: 65.099\n",
      "  timestamp: 1605364905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      5 |          185.219 | 808960 |  404.164 |              445.416 |              366.195 |            890.482 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4182\n",
      "    time_step_mean: 3715.4564047362755\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 886.9711934156379\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 404.16613059063246\n",
      "  episode_reward_min: 366.1945532999095\n",
      "  episodes_this_iter: 182\n",
      "  episodes_total: 972\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0295766194661458\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010720426216721535\n",
      "        model: {}\n",
      "        policy_loss: -0.022667490877211094\n",
      "        total_loss: 19.21761639912923\n",
      "        vf_explained_var: 0.9568905830383301\n",
      "        vf_loss: 19.238654613494873\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.71842105263158\n",
      "    gpu_util_percent0: 0.18289473684210528\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8505263157894737\n",
      "    ram_util_percent: 5.4868421052631575\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20639684459177085\n",
      "    mean_env_wait_ms: 1.686180592025272\n",
      "    mean_inference_ms: 6.4798247322945555\n",
      "    mean_raw_obs_processing_ms: 0.5694622743982951\n",
      "  time_since_restore: 221.81199026107788\n",
      "  time_this_iter_s: 36.593169927597046\n",
      "  time_total_s: 221.81199026107788\n",
      "  timers:\n",
      "    learn_throughput: 6202.466\n",
      "    learn_time_ms: 26085.11\n",
      "    sample_throughput: 15047.807\n",
      "    sample_time_ms: 10751.866\n",
      "    update_time_ms: 65.619\n",
      "  timestamp: 1605364942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      6 |          221.812 | 970752 |  404.166 |              445.416 |              366.195 |            886.971 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4316\n",
      "    time_step_mean: 3722.1466011466014\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 878.9375\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 403.3849154153183\n",
      "  episode_reward_min: 366.1945532999095\n",
      "  episodes_this_iter: 292\n",
      "  episodes_total: 1264\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0472591320673625\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010305585960547129\n",
      "        model: {}\n",
      "        policy_loss: -0.021370492797965806\n",
      "        total_loss: 15.015830596288046\n",
      "        vf_explained_var: 0.9680449366569519\n",
      "        vf_loss: 15.035663684209188\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.54210526315791\n",
      "    gpu_util_percent0: 0.1926315789473684\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8531578947368421\n",
      "    ram_util_percent: 5.476315789473684\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20570509240428966\n",
      "    mean_env_wait_ms: 1.6903591098572213\n",
      "    mean_inference_ms: 6.420058719996405\n",
      "    mean_raw_obs_processing_ms: 0.5665605072642154\n",
      "  time_since_restore: 258.7345681190491\n",
      "  time_this_iter_s: 36.92257785797119\n",
      "  time_total_s: 258.7345681190491\n",
      "  timers:\n",
      "    learn_throughput: 6195.673\n",
      "    learn_time_ms: 26113.708\n",
      "    sample_throughput: 15100.218\n",
      "    sample_time_ms: 10714.547\n",
      "    update_time_ms: 62.248\n",
      "  timestamp: 1605364979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      7 |          258.735 | 1132544 |  403.385 |              445.416 |              366.195 |            878.938 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3724.1841914430747\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-43-36\n",
      "  done: false\n",
      "  episode_len_mean: 874.4985935302391\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 403.00714235052203\n",
      "  episode_reward_min: 363.04430947594005\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.018071840206782\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.01025858319674929\n",
      "        model: {}\n",
      "        policy_loss: -0.020872338975702103\n",
      "        total_loss: 14.666247606277466\n",
      "        vf_explained_var: 0.9625389575958252\n",
      "        vf_loss: 14.685577551523844\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.61794871794872\n",
      "    gpu_util_percent0: 0.14794871794871794\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7033333333333334\n",
      "    ram_util_percent: 5.494871794871795\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20533242254832068\n",
      "    mean_env_wait_ms: 1.692220401640687\n",
      "    mean_inference_ms: 6.3942065014263925\n",
      "    mean_raw_obs_processing_ms: 0.5652943050883844\n",
      "  time_since_restore: 295.98109579086304\n",
      "  time_this_iter_s: 37.246527671813965\n",
      "  time_total_s: 295.98109579086304\n",
      "  timers:\n",
      "    learn_throughput: 6194.127\n",
      "    learn_time_ms: 26120.228\n",
      "    sample_throughput: 15076.276\n",
      "    sample_time_ms: 10731.563\n",
      "    update_time_ms: 66.865\n",
      "  timestamp: 1605365016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      8 |          295.981 | 1294336 |  403.007 |              445.416 |              363.044 |            874.499 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3722.8139232270655\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-44-12\n",
      "  done: false\n",
      "  episode_len_mean: 870.3974683544304\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.73181532726875\n",
      "  episode_reward_min: 363.04430947594005\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9929041663805643\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009481782404085001\n",
      "        model: {}\n",
      "        policy_loss: -0.020447917942268152\n",
      "        total_loss: 12.303663651148478\n",
      "        vf_explained_var: 0.9672797322273254\n",
      "        vf_loss: 12.32271146774292\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.1111111111111\n",
      "    gpu_util_percent0: 0.18944444444444442\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.851111111111111\n",
      "    ram_util_percent: 5.480555555555556\n",
      "    vram_util_percent0: 0.09060347654968842\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20496693577586186\n",
      "    mean_env_wait_ms: 1.6941880282616413\n",
      "    mean_inference_ms: 6.371037478591046\n",
      "    mean_raw_obs_processing_ms: 0.5642320700059191\n",
      "  time_since_restore: 332.2809627056122\n",
      "  time_this_iter_s: 36.299866914749146\n",
      "  time_total_s: 332.2809627056122\n",
      "  timers:\n",
      "    learn_throughput: 6203.584\n",
      "    learn_time_ms: 26080.406\n",
      "    sample_throughput: 15127.013\n",
      "    sample_time_ms: 10695.568\n",
      "    update_time_ms: 66.969\n",
      "  timestamp: 1605365052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |      9 |          332.281 | 1456128 |  402.732 |              445.416 |              363.044 |            870.397 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3718.677286135693\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-44-50\n",
      "  done: false\n",
      "  episode_len_mean: 866.6749136939011\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.71441355685226\n",
      "  episode_reward_min: 363.04430947594005\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1738\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9521241386731466\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009783023580287894\n",
      "        model: {}\n",
      "        policy_loss: -0.022247144447950024\n",
      "        total_loss: 13.226553360621134\n",
      "        vf_explained_var: 0.9687545299530029\n",
      "        vf_loss: 13.24731969833374\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.39999999999999\n",
      "    gpu_util_percent0: 0.17230769230769227\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8507692307692307\n",
      "    ram_util_percent: 5.476923076923076\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20466817792319536\n",
      "    mean_env_wait_ms: 1.6961356332072124\n",
      "    mean_inference_ms: 6.350623843236768\n",
      "    mean_raw_obs_processing_ms: 0.5632252605434667\n",
      "  time_since_restore: 369.69721269607544\n",
      "  time_this_iter_s: 37.41624999046326\n",
      "  time_total_s: 369.69721269607544\n",
      "  timers:\n",
      "    learn_throughput: 6196.035\n",
      "    learn_time_ms: 26112.183\n",
      "    sample_throughput: 15119.24\n",
      "    sample_time_ms: 10701.067\n",
      "    update_time_ms: 67.852\n",
      "  timestamp: 1605365090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     10 |          369.697 | 1617920 |  402.714 |              445.416 |              363.044 |            866.675 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3721.5342671335666\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-45-26\n",
      "  done: false\n",
      "  episode_len_mean: 860.4720861900098\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.1497800801554\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 304\n",
      "  episodes_total: 2042\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9334697822729746\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00926545176965495\n",
      "        model: {}\n",
      "        policy_loss: -0.020670961845704976\n",
      "        total_loss: 10.511703332265219\n",
      "        vf_explained_var: 0.9823186993598938\n",
      "        vf_loss: 10.530987660090128\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.52972972972972\n",
      "    gpu_util_percent0: 0.21027027027027026\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8540540540540541\n",
      "    ram_util_percent: 5.467567567567568\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20411821355740262\n",
      "    mean_env_wait_ms: 1.6991607370955149\n",
      "    mean_inference_ms: 6.317354462245277\n",
      "    mean_raw_obs_processing_ms: 0.5617307193269727\n",
      "  time_since_restore: 406.0207796096802\n",
      "  time_this_iter_s: 36.323566913604736\n",
      "  time_total_s: 406.0207796096802\n",
      "  timers:\n",
      "    learn_throughput: 6209.677\n",
      "    learn_time_ms: 26054.817\n",
      "    sample_throughput: 15285.445\n",
      "    sample_time_ms: 10584.71\n",
      "    update_time_ms: 70.33\n",
      "  timestamp: 1605365126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     11 |          406.021 | 1779712 |   402.15 |              445.416 |              355.645 |            860.472 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3718.9944674965423\n",
      "    time_step_min: 3400\n",
      "  date: 2020-11-14_15-46-03\n",
      "  done: false\n",
      "  episode_len_mean: 857.5158227848101\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.1605322302086\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 170\n",
      "  episodes_total: 2212\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9202397813399633\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008970600242416063\n",
      "        model: {}\n",
      "        policy_loss: -0.02169952855911106\n",
      "        total_loss: 9.564369360605875\n",
      "        vf_explained_var: 0.9779212474822998\n",
      "        vf_loss: 9.584734916687012\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.21842105263156\n",
      "    gpu_util_percent0: 0.20973684210526317\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8536842105263157\n",
      "    ram_util_percent: 5.489473684210526\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2038586250475769\n",
      "    mean_env_wait_ms: 1.7008864555106225\n",
      "    mean_inference_ms: 6.301418470312744\n",
      "    mean_raw_obs_processing_ms: 0.5609673780905265\n",
      "  time_since_restore: 442.3480908870697\n",
      "  time_this_iter_s: 36.327311277389526\n",
      "  time_total_s: 442.3480908870697\n",
      "  timers:\n",
      "    learn_throughput: 6214.458\n",
      "    learn_time_ms: 26034.774\n",
      "    sample_throughput: 15366.996\n",
      "    sample_time_ms: 10528.538\n",
      "    update_time_ms: 63.691\n",
      "  timestamp: 1605365163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     12 |          442.348 | 1941504 |  402.161 |              445.416 |              355.645 |            857.516 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3717.1117318435754\n",
      "    time_step_min: 3306\n",
      "  date: 2020-11-14_15-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 855.1759493670886\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.303326525765\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2370\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9065870742003123\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009477144572883844\n",
      "        model: {}\n",
      "        policy_loss: -0.020811610117865104\n",
      "        total_loss: 9.536990801493326\n",
      "        vf_explained_var: 0.9763486385345459\n",
      "        vf_loss: 9.556360403696695\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.61842105263159\n",
      "    gpu_util_percent0: 0.1628947368421053\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8492105263157894\n",
      "    ram_util_percent: 5.484210526315789\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20364022494513048\n",
      "    mean_env_wait_ms: 1.7024849450797663\n",
      "    mean_inference_ms: 6.2883145706388985\n",
      "    mean_raw_obs_processing_ms: 0.5603951306825131\n",
      "  time_since_restore: 479.4575481414795\n",
      "  time_this_iter_s: 37.10945725440979\n",
      "  time_total_s: 479.4575481414795\n",
      "  timers:\n",
      "    learn_throughput: 6209.154\n",
      "    learn_time_ms: 26057.011\n",
      "    sample_throughput: 15374.287\n",
      "    sample_time_ms: 10523.545\n",
      "    update_time_ms: 62.082\n",
      "  timestamp: 1605365200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     13 |          479.458 | 2103296 |  402.303 |              445.416 |              355.645 |            855.176 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3717.070337620579\n",
      "    time_step_min: 3306\n",
      "  date: 2020-11-14_15-47-17\n",
      "  done: false\n",
      "  episode_len_mean: 852.9035954168313\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.3073413655865\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 161\n",
      "  episodes_total: 2531\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8686563670635223\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009412609273567796\n",
      "        model: {}\n",
      "        policy_loss: -0.02288446226157248\n",
      "        total_loss: 9.504610459009806\n",
      "        vf_explained_var: 0.9787135124206543\n",
      "        vf_loss: 9.526046752929688\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.69999999999999\n",
      "    gpu_util_percent0: 0.1845945945945946\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8505405405405404\n",
      "    ram_util_percent: 5.467567567567568\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20345318970636142\n",
      "    mean_env_wait_ms: 1.7042641529705564\n",
      "    mean_inference_ms: 6.276213431206638\n",
      "    mean_raw_obs_processing_ms: 0.5598731414504248\n",
      "  time_since_restore: 516.2141225337982\n",
      "  time_this_iter_s: 36.756574392318726\n",
      "  time_total_s: 516.2141225337982\n",
      "  timers:\n",
      "    learn_throughput: 6209.814\n",
      "    learn_time_ms: 26054.242\n",
      "    sample_throughput: 15384.847\n",
      "    sample_time_ms: 10516.322\n",
      "    update_time_ms: 59.047\n",
      "  timestamp: 1605365237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 42.5/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     14 |          516.214 | 2265088 |  402.307 |              445.416 |              355.645 |            852.904 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3717.4093525179856\n",
      "    time_step_min: 3306\n",
      "  date: 2020-11-14_15-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 849.3786751682608\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.5805444462976\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 292\n",
      "  episodes_total: 2823\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8426266262928644\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008958593864614764\n",
      "        model: {}\n",
      "        policy_loss: -0.019909938448108733\n",
      "        total_loss: 9.086193164189657\n",
      "        vf_explained_var: 0.9855614304542542\n",
      "        vf_loss: 9.104732751846313\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.67368421052632\n",
      "    gpu_util_percent0: 0.19184210526315787\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8518421052631578\n",
      "    ram_util_percent: 5.473684210526315\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2031446984473774\n",
      "    mean_env_wait_ms: 1.7067776137470494\n",
      "    mean_inference_ms: 6.256935569302545\n",
      "    mean_raw_obs_processing_ms: 0.5591071614545281\n",
      "  time_since_restore: 553.2842633724213\n",
      "  time_this_iter_s: 37.07014083862305\n",
      "  time_total_s: 553.2842633724213\n",
      "  timers:\n",
      "    learn_throughput: 6197.876\n",
      "    learn_time_ms: 26104.429\n",
      "    sample_throughput: 15343.517\n",
      "    sample_time_ms: 10544.649\n",
      "    update_time_ms: 58.14\n",
      "  timestamp: 1605365274\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     15 |          553.284 | 2426880 |  402.581 |              445.416 |              355.645 |            849.379 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3717.1828320378504\n",
      "    time_step_min: 3306\n",
      "  date: 2020-11-14_15-48-32\n",
      "  done: false\n",
      "  episode_len_mean: 847.6375749500334\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.6444406109027\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 179\n",
      "  episodes_total: 3002\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.832922359307607\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00881393626332283\n",
      "        model: {}\n",
      "        policy_loss: -0.020593561697751284\n",
      "        total_loss: 8.217459599177042\n",
      "        vf_explained_var: 0.9824800491333008\n",
      "        vf_loss: 8.236706972122192\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.98717948717949\n",
      "    gpu_util_percent0: 0.15\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7182051282051283\n",
      "    ram_util_percent: 5.487179487179487\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20296599512217703\n",
      "    mean_env_wait_ms: 1.708314927373206\n",
      "    mean_inference_ms: 6.246472203198401\n",
      "    mean_raw_obs_processing_ms: 0.5586751788623255\n",
      "  time_since_restore: 590.6174454689026\n",
      "  time_this_iter_s: 37.33318209648132\n",
      "  time_total_s: 590.6174454689026\n",
      "  timers:\n",
      "    learn_throughput: 6184.49\n",
      "    learn_time_ms: 26160.927\n",
      "    sample_throughput: 15328.323\n",
      "    sample_time_ms: 10555.101\n",
      "    update_time_ms: 55.79\n",
      "  timestamp: 1605365312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | RUNNING  | 172.17.0.14:48647 |     16 |          590.617 | 2588672 |  402.644 |              445.416 |              355.645 |            847.638 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_0dcf5_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4358\n",
      "    time_step_mean: 3716.8334937439845\n",
      "    time_step_min: 3306\n",
      "  date: 2020-11-14_15-49-09\n",
      "  done: true\n",
      "  episode_len_mean: 846.0411392405064\n",
      "  episode_reward_max: 445.41617983853456\n",
      "  episode_reward_mean: 402.76783333858907\n",
      "  episode_reward_min: 355.6448947150017\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 3160\n",
      "  experiment_id: be7cf829974947a6a918930411a6e085\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8201845834652582\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009123688330873847\n",
      "        model: {}\n",
      "        policy_loss: -0.021758562535978854\n",
      "        total_loss: 8.358800967534384\n",
      "        vf_explained_var: 0.9807761311531067\n",
      "        vf_loss: 8.37914514541626\n",
      "    num_steps_sampled: 2750464\n",
      "    num_steps_trained: 2750464\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.62105263157895\n",
      "    gpu_util_percent0: 0.15578947368421053\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.851578947368421\n",
      "    ram_util_percent: 5.489473684210527\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 48647\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20281873052132526\n",
      "    mean_env_wait_ms: 1.7095760836843423\n",
      "    mean_inference_ms: 6.2377092217042565\n",
      "    mean_raw_obs_processing_ms: 0.5583560308969385\n",
      "  time_since_restore: 627.318594455719\n",
      "  time_this_iter_s: 36.701148986816406\n",
      "  time_total_s: 627.318594455719\n",
      "  timers:\n",
      "    learn_throughput: 6189.747\n",
      "    learn_time_ms: 26138.708\n",
      "    sample_throughput: 15351.95\n",
      "    sample_time_ms: 10538.857\n",
      "    update_time_ms: 72.941\n",
      "  timestamp: 1605365349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2750464\n",
      "  training_iteration: 17\n",
      "  trial_id: 0dcf5_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | TERMINATED |       |     17 |          627.319 | 2750464 |  402.768 |              445.416 |              355.645 |            846.041 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 41.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_0dcf5_00000 | TERMINATED |       |     17 |          627.319 | 2750464 |  402.768 |              445.416 |              355.645 |            846.041 |\n",
      "+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2020-11-14 15:49:09,860\tINFO tune.py:439 -- Total run time: 644.13 seconds (643.56 seconds for the tuning loop).\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 48350\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /JSS/JSS/wandb/run-20201114_153818-5se6iolt/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /JSS/JSS/wandb/run-20201114_153818-5se6iolt/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min 3306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 651\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1605365349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max 4358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean 3716.83349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max 445.41618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min 355.64489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean 402.76783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         time_step_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        time_step_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_max ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    episode_reward_min ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   episode_reward_mean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        episodes_total ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training_iteration ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33melated-sweep-7\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/5se6iolt\u001b[0m\n",
      "2020-11-14 15:49:17,591 - wandb.wandb_agent - INFO - Cleaning up finished run: 5se6iolt\n",
      "2020-11-14 15:49:17,897 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2020-11-14 15:49:17,898 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tinstance_path: /JSS/JSS/env/instances/ta58\n",
      "2020-11-14 15:49:17,900 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2020-11-14 15:49:22,921 - wandb.wandb_agent - INFO - Running runs: ['5b075dza']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgentle-sweep-8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/sweeps/3vcawkyg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_4/runs/5b075dza\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /JSS/JSS/wandb/run-20201114_154921-5b075dza\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "2020-11-14 15:49:25,099\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 22.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------+\n",
      "| Trial name              | status   | loc   |\n",
      "|-------------------------+----------+-------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  |       |\n",
      "+-------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=68673)\u001b[0m 2020-11-14 15:49:31,454\tINFO trainer.py:617 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=68567)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68567)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68532)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68532)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68548)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68548)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68540)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68540)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68541)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68541)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68547)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68547)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68607)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68607)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68631)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68631)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68552)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68552)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68673)\u001b[0m 2020-11-14 15:49:43,559\tINFO trainable.py:252 -- Trainable.setup took 12.986 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=68657)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68657)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68662)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68662)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68618)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68618)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68656)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68656)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68596)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68596)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68669)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68669)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68621)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68621)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68643)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68643)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68614)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68614)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68543)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68543)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68650)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68650)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68642)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68642)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68583)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68583)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68591)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68591)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68581)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68581)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68619)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68619)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68672)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68672)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68660)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68660)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68623)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68623)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68580)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68580)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68582)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68582)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68546)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68546)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68602)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68602)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68617)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68617)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68517)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68517)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68533)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68533)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68616)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68616)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68605)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68605)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68561)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68561)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68613)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68613)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68524)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68524)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68545)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68545)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68626)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68626)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68557)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68557)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68500)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68500)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68508)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68508)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68496)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68496)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68480)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68480)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68590)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68590)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68521)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68521)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68484)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68484)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68499)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68499)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68531)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68531)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68493)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68493)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68575)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68575)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68477)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68477)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68520)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68520)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68519)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68519)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68528)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68528)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68471)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68471)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68509)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68509)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68510)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68510)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68478)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68478)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68475)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68475)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68470)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68470)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68472)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68472)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68491)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68491)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68486)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68486)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68495)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68495)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68573)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68573)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68498)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68498)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68494)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68494)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68476)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68476)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68489)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68489)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68523)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68523)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68513)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68513)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68529)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68529)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68474)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68474)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68473)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68473)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "\u001b[2m\u001b[36m(pid=68481)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=68481)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3766.7586206896553\n",
      "    time_step_min: 3390\n",
      "  date: 2020-11-14_15-50-22\n",
      "  done: false\n",
      "  episode_len_mean: 902.7151898734177\n",
      "  episode_reward_max: 447.713892898988\n",
      "  episode_reward_mean: 417.35598731422954\n",
      "  episode_reward_min: 378.39949176155864\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 158\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1628401676813762\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0055976578732952476\n",
      "        model: {}\n",
      "        policy_loss: -0.011848671128973365\n",
      "        total_loss: 1135.2504781087239\n",
      "        vf_explained_var: 0.07487516850233078\n",
      "        vf_loss: 1135.2618103027344\n",
      "    num_steps_sampled: 161792\n",
      "    num_steps_trained: 161792\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.21219512195123\n",
      "    gpu_util_percent0: 0.16170731707317074\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.855609756097561\n",
      "    ram_util_percent: 5.270731707317074\n",
      "    vram_util_percent0: 0.0785243462470702\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22109801329812656\n",
      "    mean_env_wait_ms: 1.7373684020129276\n",
      "    mean_inference_ms: 7.292294368290468\n",
      "    mean_raw_obs_processing_ms: 0.6099156252798309\n",
      "  time_since_restore: 38.77076315879822\n",
      "  time_this_iter_s: 38.77076315879822\n",
      "  time_total_s: 38.77076315879822\n",
      "  timers:\n",
      "    learn_throughput: 6156.055\n",
      "    learn_time_ms: 26281.766\n",
      "    sample_throughput: 13155.155\n",
      "    sample_time_ms: 12298.752\n",
      "    update_time_ms: 47.107\n",
      "  timestamp: 1605365422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161792\n",
      "  training_iteration: 1\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      1 |          38.7708 | 161792 |  417.356 |              447.714 |              378.399 |            902.715 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3757.0510948905107\n",
      "    time_step_min: 3390\n",
      "  date: 2020-11-14_15-50-59\n",
      "  done: false\n",
      "  episode_len_mean: 899.8164556962025\n",
      "  episode_reward_max: 447.713892898988\n",
      "  episode_reward_mean: 418.52295316182324\n",
      "  episode_reward_min: 378.39949176155864\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 316\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1371978521347046\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.006885322819774349\n",
      "        model: {}\n",
      "        policy_loss: -0.013616209893370979\n",
      "        total_loss: 540.2891489664713\n",
      "        vf_explained_var: 0.24477529525756836\n",
      "        vf_loss: 540.3019561767578\n",
      "    num_steps_sampled: 323584\n",
      "    num_steps_trained: 323584\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.70769230769231\n",
      "    gpu_util_percent0: 0.15666666666666668\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8505128205128205\n",
      "    ram_util_percent: 5.464102564102565\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21679065472889203\n",
      "    mean_env_wait_ms: 1.7150707179890468\n",
      "    mean_inference_ms: 7.077041492868611\n",
      "    mean_raw_obs_processing_ms: 0.5990556720016474\n",
      "  time_since_restore: 75.68781995773315\n",
      "  time_this_iter_s: 36.91705679893494\n",
      "  time_total_s: 75.68781995773315\n",
      "  timers:\n",
      "    learn_throughput: 6253.062\n",
      "    learn_time_ms: 25874.044\n",
      "    sample_throughput: 13684.051\n",
      "    sample_time_ms: 11823.4\n",
      "    update_time_ms: 36.672\n",
      "  timestamp: 1605365459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323584\n",
      "  training_iteration: 2\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      2 |          75.6878 | 323584 |  418.523 |              447.714 |              378.399 |            899.816 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3746.7268518518517\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 895.9177215189874\n",
      "  episode_reward_max: 451.6162589414546\n",
      "  episode_reward_mean: 418.5806691805085\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 474\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1265357931454976\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008072259447847804\n",
      "        model: {}\n",
      "        policy_loss: -0.01718702078020821\n",
      "        total_loss: 214.1384595235189\n",
      "        vf_explained_var: 0.6401326656341553\n",
      "        vf_loss: 214.15459314982095\n",
      "    num_steps_sampled: 485376\n",
      "    num_steps_trained: 485376\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.84210526315789\n",
      "    gpu_util_percent0: 0.17342105263157898\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8515789473684212\n",
      "    ram_util_percent: 5.476315789473684\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21352652927669516\n",
      "    mean_env_wait_ms: 1.701532335337209\n",
      "    mean_inference_ms: 6.90758698539027\n",
      "    mean_raw_obs_processing_ms: 0.5899427338011941\n",
      "  time_since_restore: 111.85836172103882\n",
      "  time_this_iter_s: 36.170541763305664\n",
      "  time_total_s: 111.85836172103882\n",
      "  timers:\n",
      "    learn_throughput: 6279.387\n",
      "    learn_time_ms: 25765.574\n",
      "    sample_throughput: 14282.395\n",
      "    sample_time_ms: 11328.072\n",
      "    update_time_ms: 50.03\n",
      "  timestamp: 1605365496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485376\n",
      "  training_iteration: 3\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.8/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      3 |          111.858 | 485376 |  418.581 |              451.616 |              376.781 |            895.918 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3742.5542372881355\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 893.379746835443\n",
      "  episode_reward_max: 451.6162589414546\n",
      "  episode_reward_mean: 418.52711800117834\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 632\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.1176806092262268\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009757785204177102\n",
      "        model: {}\n",
      "        policy_loss: -0.01738924525367717\n",
      "        total_loss: 50.65122699737549\n",
      "        vf_explained_var: 0.8609251976013184\n",
      "        vf_loss: 50.667225201924644\n",
      "    num_steps_sampled: 647168\n",
      "    num_steps_trained: 647168\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.03783783783784\n",
      "    gpu_util_percent0: 0.19891891891891894\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8564864864864865\n",
      "    ram_util_percent: 5.481081081081081\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21166698244055915\n",
      "    mean_env_wait_ms: 1.6955368602216738\n",
      "    mean_inference_ms: 6.790166112308727\n",
      "    mean_raw_obs_processing_ms: 0.5841433804849351\n",
      "  time_since_restore: 147.43632221221924\n",
      "  time_this_iter_s: 35.57796049118042\n",
      "  time_total_s: 147.43632221221924\n",
      "  timers:\n",
      "    learn_throughput: 6330.73\n",
      "    learn_time_ms: 25556.608\n",
      "    sample_throughput: 14563.968\n",
      "    sample_time_ms: 11109.061\n",
      "    update_time_ms: 56.894\n",
      "  timestamp: 1605365531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647168\n",
      "  training_iteration: 4\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.2/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      4 |          147.436 | 647168 |  418.527 |              451.616 |              376.781 |             893.38 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3737.8810160427806\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-52-47\n",
      "  done: false\n",
      "  episode_len_mean: 890.4443037974684\n",
      "  episode_reward_max: 451.6162589414546\n",
      "  episode_reward_mean: 418.3936113382468\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 790\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0879535377025604\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.011166819604113698\n",
      "        model: {}\n",
      "        policy_loss: -0.019441749357307952\n",
      "        total_loss: 26.339981079101562\n",
      "        vf_explained_var: 0.9210236668586731\n",
      "        vf_loss: 26.35773293177287\n",
      "    num_steps_sampled: 808960\n",
      "    num_steps_trained: 808960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.43243243243244\n",
      "    gpu_util_percent0: 0.16891891891891897\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8532432432432433\n",
      "    ram_util_percent: 5.47027027027027\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21022513663655132\n",
      "    mean_env_wait_ms: 1.6904184003737919\n",
      "    mean_inference_ms: 6.696204643333841\n",
      "    mean_raw_obs_processing_ms: 0.579100876895085\n",
      "  time_since_restore: 183.25969886779785\n",
      "  time_this_iter_s: 35.82337665557861\n",
      "  time_total_s: 183.25969886779785\n",
      "  timers:\n",
      "    learn_throughput: 6328.585\n",
      "    learn_time_ms: 25565.272\n",
      "    sample_throughput: 14829.66\n",
      "    sample_time_ms: 10910.028\n",
      "    update_time_ms: 50.97\n",
      "  timestamp: 1605365567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808960\n",
      "  training_iteration: 5\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      5 |           183.26 | 808960 |  418.394 |              451.616 |              376.781 |            890.444 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3743.6875664187037\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-53-23\n",
      "  done: false\n",
      "  episode_len_mean: 885.8036622583927\n",
      "  episode_reward_max: 451.8317392421169\n",
      "  episode_reward_mean: 418.58112047314273\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 983\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0432095328966777\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010476318032791218\n",
      "        model: {}\n",
      "        policy_loss: -0.021220106398686767\n",
      "        total_loss: 19.9987309773763\n",
      "        vf_explained_var: 0.9567311406135559\n",
      "        vf_loss: 20.018377939860027\n",
      "    num_steps_sampled: 970752\n",
      "    num_steps_trained: 970752\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.41052631578948\n",
      "    gpu_util_percent0: 0.17105263157894737\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8476315789473684\n",
      "    ram_util_percent: 5.463157894736841\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20885331286529082\n",
      "    mean_env_wait_ms: 1.687948146945901\n",
      "    mean_inference_ms: 6.611313018062743\n",
      "    mean_raw_obs_processing_ms: 0.5747662849047955\n",
      "  time_since_restore: 219.14252042770386\n",
      "  time_this_iter_s: 35.882821559906006\n",
      "  time_total_s: 219.14252042770386\n",
      "  timers:\n",
      "    learn_throughput: 6348.967\n",
      "    learn_time_ms: 25483.199\n",
      "    sample_throughput: 14938.891\n",
      "    sample_time_ms: 10830.255\n",
      "    update_time_ms: 81.872\n",
      "  timestamp: 1605365603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 970752\n",
      "  training_iteration: 6\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      6 |          219.143 | 970752 |  418.581 |              451.832 |              376.781 |            885.804 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3741.7266775777416\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-53-59\n",
      "  done: false\n",
      "  episode_len_mean: 878.7768987341772\n",
      "  episode_reward_max: 451.8317392421169\n",
      "  episode_reward_mean: 418.97938099849137\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 281\n",
      "  episodes_total: 1264\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0499834616978962\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010916362827022871\n",
      "        model: {}\n",
      "        policy_loss: -0.02216442107843856\n",
      "        total_loss: 16.017012039820354\n",
      "        vf_explained_var: 0.9661333560943604\n",
      "        vf_loss: 16.03751794497172\n",
      "    num_steps_sampled: 1132544\n",
      "    num_steps_trained: 1132544\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.59444444444445\n",
      "    gpu_util_percent0: 0.22027777777777777\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7016666666666667\n",
      "    ram_util_percent: 5.466666666666667\n",
      "    vram_util_percent0: 0.09060347654968842\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20747306168514262\n",
      "    mean_env_wait_ms: 1.6861354918680527\n",
      "    mean_inference_ms: 6.524666317612294\n",
      "    mean_raw_obs_processing_ms: 0.5703470026675682\n",
      "  time_since_restore: 254.53768968582153\n",
      "  time_this_iter_s: 35.395169258117676\n",
      "  time_total_s: 254.53768968582153\n",
      "  timers:\n",
      "    learn_throughput: 6364.731\n",
      "    learn_time_ms: 25420.086\n",
      "    sample_throughput: 15050.758\n",
      "    sample_time_ms: 10749.757\n",
      "    update_time_ms: 75.035\n",
      "  timestamp: 1605365639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132544\n",
      "  training_iteration: 7\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 38.4/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      7 |          254.538 | 1132544 |  418.979 |              451.832 |              376.781 |            878.777 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4325\n",
      "    time_step_mean: 3741.4217391304346\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-54-34\n",
      "  done: false\n",
      "  episode_len_mean: 875.1364275668074\n",
      "  episode_reward_max: 454.3784348059317\n",
      "  episode_reward_mean: 419.1842545570168\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1422\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.0288760960102081\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.010285804513841867\n",
      "        model: {}\n",
      "        policy_loss: -0.023307571342835825\n",
      "        total_loss: 15.195528189341227\n",
      "        vf_explained_var: 0.9629877209663391\n",
      "        vf_loss: 15.21729318300883\n",
      "    num_steps_sampled: 1294336\n",
      "    num_steps_trained: 1294336\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8162162162162\n",
      "    gpu_util_percent0: 0.22216216216216217\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8545945945945946\n",
      "    ram_util_percent: 5.47027027027027\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2068689500393363\n",
      "    mean_env_wait_ms: 1.6858325771286704\n",
      "    mean_inference_ms: 6.487245608186268\n",
      "    mean_raw_obs_processing_ms: 0.5685433472504643\n",
      "  time_since_restore: 290.08764934539795\n",
      "  time_this_iter_s: 35.549959659576416\n",
      "  time_total_s: 290.08764934539795\n",
      "  timers:\n",
      "    learn_throughput: 6376.706\n",
      "    learn_time_ms: 25372.347\n",
      "    sample_throughput: 15127.743\n",
      "    sample_time_ms: 10695.052\n",
      "    update_time_ms: 72.995\n",
      "  timestamp: 1605365674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294336\n",
      "  training_iteration: 8\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.9/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      8 |          290.088 | 1294336 |  419.184 |              454.378 |              376.781 |            875.136 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3740.9135240572173\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-55-10\n",
      "  done: false\n",
      "  episode_len_mean: 871.4727848101265\n",
      "  episode_reward_max: 456.8506823938864\n",
      "  episode_reward_mean: 419.80131573178886\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 1.008358935515086\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.01015451488395532\n",
      "        model: {}\n",
      "        policy_loss: -0.02242325486925741\n",
      "        total_loss: 14.338541825612387\n",
      "        vf_explained_var: 0.9646552205085754\n",
      "        vf_loss: 14.35943833986918\n",
      "    num_steps_sampled: 1456128\n",
      "    num_steps_trained: 1456128\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.57567567567568\n",
      "    gpu_util_percent0: 0.21540540540540537\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8521621621621622\n",
      "    ram_util_percent: 5.4972972972972975\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20634864871741201\n",
      "    mean_env_wait_ms: 1.6854783750682027\n",
      "    mean_inference_ms: 6.454329260118351\n",
      "    mean_raw_obs_processing_ms: 0.5668792646331893\n",
      "  time_since_restore: 325.7661316394806\n",
      "  time_this_iter_s: 35.67848229408264\n",
      "  time_total_s: 325.7661316394806\n",
      "  timers:\n",
      "    learn_throughput: 6380.118\n",
      "    learn_time_ms: 25358.78\n",
      "    sample_throughput: 15191.464\n",
      "    sample_time_ms: 10650.192\n",
      "    update_time_ms: 70.421\n",
      "  timestamp: 1605365710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1456128\n",
      "  training_iteration: 9\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 40.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |      9 |          325.766 | 1456128 |  419.801 |              456.851 |              376.781 |            871.473 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3738.5209192692987\n",
      "    time_step_min: 3385\n",
      "  date: 2020-11-14_15-55-46\n",
      "  done: false\n",
      "  episode_len_mean: 867.8131109833238\n",
      "  episode_reward_max: 456.8506823938864\n",
      "  episode_reward_mean: 420.28301125800704\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 159\n",
      "  episodes_total: 1739\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9689469734827677\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.0104893963628759\n",
      "        model: {}\n",
      "        policy_loss: -0.022310689363318186\n",
      "        total_loss: 13.284584522247314\n",
      "        vf_explained_var: 0.9701215624809265\n",
      "        vf_loss: 13.305281639099121\n",
      "    num_steps_sampled: 1617920\n",
      "    num_steps_trained: 1617920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.90789473684211\n",
      "    gpu_util_percent0: 0.19\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.851578947368421\n",
      "    ram_util_percent: 5.4710526315789485\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2058951478642226\n",
      "    mean_env_wait_ms: 1.6854709473118743\n",
      "    mean_inference_ms: 6.425133983382208\n",
      "    mean_raw_obs_processing_ms: 0.565362398984116\n",
      "  time_since_restore: 361.36967062950134\n",
      "  time_this_iter_s: 35.60353899002075\n",
      "  time_total_s: 361.36967062950134\n",
      "  timers:\n",
      "    learn_throughput: 6385.218\n",
      "    learn_time_ms: 25338.523\n",
      "    sample_throughput: 15247.969\n",
      "    sample_time_ms: 10610.725\n",
      "    update_time_ms: 71.637\n",
      "  timestamp: 1605365746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1617920\n",
      "  training_iteration: 10\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 45.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     10 |           361.37 | 1617920 |  420.283 |              456.851 |              376.781 |            867.813 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3732.4081325301204\n",
      "    time_step_min: 3348\n",
      "  date: 2020-11-14_15-56-22\n",
      "  done: false\n",
      "  episode_len_mean: 862.1042281219272\n",
      "  episode_reward_max: 460.26285549562834\n",
      "  episode_reward_mean: 421.2760303015754\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 295\n",
      "  episodes_total: 2034\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.93275814751784\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009755441375697652\n",
      "        model: {}\n",
      "        policy_loss: -0.019968058636247104\n",
      "        total_loss: 11.420077562332153\n",
      "        vf_explained_var: 0.9823379516601562\n",
      "        vf_loss: 11.438560962677002\n",
      "    num_steps_sampled: 1779712\n",
      "    num_steps_trained: 1779712\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.51578947368421\n",
      "    gpu_util_percent0: 0.1763157894736842\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8542105263157895\n",
      "    ram_util_percent: 5.481578947368421\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2052633965558105\n",
      "    mean_env_wait_ms: 1.6860850433394714\n",
      "    mean_inference_ms: 6.381327021672182\n",
      "    mean_raw_obs_processing_ms: 0.5629889644321168\n",
      "  time_since_restore: 397.39407753944397\n",
      "  time_this_iter_s: 36.02440690994263\n",
      "  time_total_s: 397.39407753944397\n",
      "  timers:\n",
      "    learn_throughput: 6412.011\n",
      "    learn_time_ms: 25232.646\n",
      "    sample_throughput: 15515.61\n",
      "    sample_time_ms: 10427.692\n",
      "    update_time_ms: 73.249\n",
      "  timestamp: 1605365782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1779712\n",
      "  training_iteration: 11\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.0/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     11 |          397.394 | 1779712 |  421.276 |              460.263 |              376.781 |            862.104 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3729.8027649769583\n",
      "    time_step_min: 3348\n",
      "  date: 2020-11-14_15-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 859.2002712477396\n",
      "  episode_reward_max: 461.43071370843444\n",
      "  episode_reward_mean: 422.0138644812712\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 178\n",
      "  episodes_total: 2212\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9252440283695856\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.009176532970741391\n",
      "        model: {}\n",
      "        policy_loss: -0.018881160494250555\n",
      "        total_loss: 11.254087368647257\n",
      "        vf_explained_var: 0.9776232838630676\n",
      "        vf_loss: 11.2715957959493\n",
      "    num_steps_sampled: 1941504\n",
      "    num_steps_trained: 1941504\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.56578947368419\n",
      "    gpu_util_percent0: 0.1923684210526316\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8521052631578948\n",
      "    ram_util_percent: 5.4710526315789485\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20492845069318558\n",
      "    mean_env_wait_ms: 1.6859577025439765\n",
      "    mean_inference_ms: 6.359386111074373\n",
      "    mean_raw_obs_processing_ms: 0.5618482554033134\n",
      "  time_since_restore: 433.4919764995575\n",
      "  time_this_iter_s: 36.097898960113525\n",
      "  time_total_s: 433.4919764995575\n",
      "  timers:\n",
      "    learn_throughput: 6405.077\n",
      "    learn_time_ms: 25259.961\n",
      "    sample_throughput: 15687.232\n",
      "    sample_time_ms: 10313.61\n",
      "    update_time_ms: 76.907\n",
      "  timestamp: 1605365819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1941504\n",
      "  training_iteration: 12\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     12 |          433.492 | 1941504 |  422.014 |              461.431 |              376.781 |              859.2 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3725.723797250859\n",
      "    time_step_min: 3348\n",
      "  date: 2020-11-14_15-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 856.7345991561182\n",
      "  episode_reward_max: 461.5373230486292\n",
      "  episode_reward_mean: 422.81138755279636\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 2370\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.9096968919038773\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00963998904141287\n",
      "        model: {}\n",
      "        policy_loss: -0.019396377610974014\n",
      "        total_loss: 10.914650042851767\n",
      "        vf_explained_var: 0.9762886166572571\n",
      "        vf_loss: 10.932573000590006\n",
      "    num_steps_sampled: 2103296\n",
      "    num_steps_trained: 2103296\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.99999999999999\n",
      "    gpu_util_percent0: 0.16605263157894737\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8539473684210527\n",
      "    ram_util_percent: 5.476315789473683\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.204635852682685\n",
      "    mean_env_wait_ms: 1.6860934438534954\n",
      "    mean_inference_ms: 6.341894953892532\n",
      "    mean_raw_obs_processing_ms: 0.5609268864167443\n",
      "  time_since_restore: 469.93838715553284\n",
      "  time_this_iter_s: 36.44641065597534\n",
      "  time_total_s: 469.93838715553284\n",
      "  timers:\n",
      "    learn_throughput: 6405.072\n",
      "    learn_time_ms: 25259.982\n",
      "    sample_throughput: 15647.602\n",
      "    sample_time_ms: 10339.731\n",
      "    update_time_ms: 77.266\n",
      "  timestamp: 1605365855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2103296\n",
      "  training_iteration: 13\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     13 |          469.938 | 2103296 |  422.811 |              461.537 |              376.781 |            856.735 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3721.5934861278647\n",
      "    time_step_min: 3348\n",
      "  date: 2020-11-14_15-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 854.3879003558719\n",
      "  episode_reward_max: 461.5373230486292\n",
      "  episode_reward_mean: 423.4490532239591\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 159\n",
      "  episodes_total: 2529\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8778101652860641\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00963768681200842\n",
      "        model: {}\n",
      "        policy_loss: -0.021851512991512816\n",
      "        total_loss: 9.984525283177694\n",
      "        vf_explained_var: 0.9797164797782898\n",
      "        vf_loss: 10.00488837560018\n",
      "    num_steps_sampled: 2265088\n",
      "    num_steps_trained: 2265088\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58378378378379\n",
      "    gpu_util_percent0: 0.18135135135135136\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8518918918918917\n",
      "    ram_util_percent: 5.486486486486487\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351303\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20437021321773685\n",
      "    mean_env_wait_ms: 1.6863074487135563\n",
      "    mean_inference_ms: 6.325973991431816\n",
      "    mean_raw_obs_processing_ms: 0.5601003579067513\n",
      "  time_since_restore: 506.4622166156769\n",
      "  time_this_iter_s: 36.52382946014404\n",
      "  time_total_s: 506.4622166156769\n",
      "  timers:\n",
      "    learn_throughput: 6389.195\n",
      "    learn_time_ms: 25322.754\n",
      "    sample_throughput: 15584.458\n",
      "    sample_time_ms: 10381.625\n",
      "    update_time_ms: 71.837\n",
      "  timestamp: 1605365892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265088\n",
      "  training_iteration: 14\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 41.1/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     14 |          506.462 | 2265088 |  423.449 |              461.537 |              376.781 |            854.388 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3717.372478386167\n",
      "    time_step_min: 3348\n",
      "  date: 2020-11-14_15-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 850.361958836054\n",
      "  episode_reward_max: 462.87151714792424\n",
      "  episode_reward_mean: 424.51367984471756\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 289\n",
      "  episodes_total: 2818\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8505634069442749\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.00904827689131101\n",
      "        model: {}\n",
      "        policy_loss: -0.019613412422283243\n",
      "        total_loss: 9.04050882657369\n",
      "        vf_explained_var: 0.9870941042900085\n",
      "        vf_loss: 9.058738072713217\n",
      "    num_steps_sampled: 2426880\n",
      "    num_steps_trained: 2426880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.06842105263159\n",
      "    gpu_util_percent0: 0.14947368421052634\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.8526315789473684\n",
      "    ram_util_percent: 5.471052631578947\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2039331695725179\n",
      "    mean_env_wait_ms: 1.68725573477544\n",
      "    mean_inference_ms: 6.300900681089996\n",
      "    mean_raw_obs_processing_ms: 0.5588149986611377\n",
      "  time_since_restore: 543.3481483459473\n",
      "  time_this_iter_s: 36.885931730270386\n",
      "  time_total_s: 543.3481483459473\n",
      "  timers:\n",
      "    learn_throughput: 6376.865\n",
      "    learn_time_ms: 25371.715\n",
      "    sample_throughput: 15509.408\n",
      "    sample_time_ms: 10431.862\n",
      "    update_time_ms: 73.015\n",
      "  timestamp: 1605365929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2426880\n",
      "  training_iteration: 15\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.7/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     15 |          543.348 | 2426880 |  424.514 |              462.872 |              376.781 |            850.362 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_jss_env_98ac6_00000:\n",
      "  custom_metrics:\n",
      "    time_step_max: 4411\n",
      "    time_step_mean: 3714.5070945945945\n",
      "    time_step_min: 3348\n",
      "  date: 2020-11-14_15-59-25\n",
      "  done: false\n",
      "  episode_len_mean: 848.2834776815456\n",
      "  episode_reward_max: 463.1137467582058\n",
      "  episode_reward_mean: 425.1674890402646\n",
      "  episode_reward_min: 376.7813230179491\n",
      "  episodes_this_iter: 184\n",
      "  episodes_total: 3002\n",
      "  experiment_id: 737438f2ca90430db994d783a4ef9c6b\n",
      "  experiment_tag: '0'\n",
      "  hostname: f85e62b52919\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 0.8356611231962839\n",
      "        entropy_coeff: 0.0005000000000000001\n",
      "        kl: 0.008828605680416027\n",
      "        model: {}\n",
      "        policy_loss: -0.0193888185506997\n",
      "        total_loss: 8.909531116485596\n",
      "        vf_explained_var: 0.9835204482078552\n",
      "        vf_loss: 8.92757217089335\n",
      "    num_steps_sampled: 2588672\n",
      "    num_steps_trained: 2588672\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.14\n",
      "  num_healthy_workers: 79\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.03947368421053\n",
      "    gpu_util_percent0: 0.16578947368421051\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.7155263157894737\n",
      "    ram_util_percent: 5.478947368421053\n",
      "    vram_util_percent0: 0.09060347654968841\n",
      "    vram_util_percent1: 0.0\n",
      "    vram_util_percent2: 0.9949178691351301\n",
      "  pid: 68673\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20369536601418672\n",
      "    mean_env_wait_ms: 1.687456378926087\n",
      "    mean_inference_ms: 6.286890510772659\n",
      "    mean_raw_obs_processing_ms: 0.5581650928138564\n",
      "  time_since_restore: 579.4591925144196\n",
      "  time_this_iter_s: 36.11104416847229\n",
      "  time_total_s: 579.4591925144196\n",
      "  timers:\n",
      "    learn_throughput: 6370.933\n",
      "    learn_time_ms: 25395.337\n",
      "    sample_throughput: 15485.682\n",
      "    sample_time_ms: 10447.845\n",
      "    update_time_ms: 54.441\n",
      "  timestamp: 1605365965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2588672\n",
      "  training_iteration: 16\n",
      "  trial_id: 98ac6_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 39.3/754.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/ppo-jss\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name              | status   | loc               |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_jss_env_98ac6_00000 | RUNNING  | 172.17.0.14:68673 |     16 |          579.459 | 2588672 |  425.167 |              463.114 |              376.781 |            848.283 |\n",
      "+-------------------------+----------+-------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wandb agent 3vcawkyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
