program: train_wandb.py
method: bayes
metric:
  name: episode_reward_mean
  goal: maximize
early_terminate:
  type: hyperband
  min_iter: 20
parameters:
  minibatch_buffer_size:
    min: 4096
    max: 16000
  num_envs_per_worker:
    min: 1
    max: 8
  rollout_fragment_length:
    min: 512
    max: 1256
  layer_size:
    min: 768
    max: 1256
  lr_start:
    min: 1-e6
    max: 1-e3
  lr_end:
    min: 1-e7
    max: 1-e4
  entropy_coeff_start:
    min: 0.0
    max: 1-e2
  entropy_coeff_end:
    min: 0.0
    max: 1-e4
  vtrace_clip_rho_threshold:
    min: 0.5
    max: 1.0
  vtrace_clip_pg_rho_threshold:
    min: 0.5
    max: 1.0
  train_batch_size:
    min: 16000
    max: 120000
  num_sgd_iter:
    min: 5
    max: 30
  replay_proportion:
    min: 0.0
    max: 0.5
  replay_buffer_num_slots:
    min: 1
    max: 100
  learner_queue_size:
    min: 4
    max: 32
  max_sample_requests_in_flight_per_worker:
    min: 1
    max: 8
  broadcast_interval:
    min: 1
    max: 8
  vf_loss_coeff:
    min: 0.5
    max: 1.0
  activation_fn:
    values:
      - 'tanh'
      - 'relu'
  model_net:
    values:
      - "fc_masked_model_v1"
      - "fc_masked_model_v2"